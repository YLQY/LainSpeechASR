encoder.global_cmvn.mean-torch.Size([80])-torch.float32
tensor([10.4989, 10.9486, 11.8892, 12.6349, 13.3975, 14.0109, 14.4508, 14.6497,
        14.7916, 14.7223, 14.8022, 14.8610, 15.0772, 15.2602, 15.3288, 15.3974,
        15.3959, 15.3410, 15.4662, 15.2719, 15.1083, 15.2959, 15.0736, 15.1779,
        15.0756, 15.1541, 15.0511, 15.1307, 15.0903, 15.0994, 15.1282, 15.1240,
        15.1440, 15.1980, 15.2514, 15.3300, 15.4018, 15.4509, 15.5006, 15.4357,
        15.5109, 15.4476, 15.5110, 15.4917, 15.5380, 15.6084, 15.6944, 15.7622,
        15.8215, 15.9020, 15.9072, 15.9257, 15.9523, 16.0007, 16.0303, 16.0606,
        16.0900, 16.1001, 16.0918, 16.0626, 16.0577, 15.9970, 15.9464, 15.8653,
        15.7781, 15.6763, 15.5698, 15.5160, 15.4721, 15.4234, 15.3821, 15.3459,
        15.3019, 15.2698, 15.1655, 15.0045, 14.8754, 14.5642, 14.0317, 13.1593])

encoder.global_cmvn.istd-torch.Size([80])-torch.float32
tensor([0.2522, 0.2374, 0.2319, 0.2333, 0.2320, 0.2291, 0.2252, 0.2201, 0.2196,
        0.2220, 0.2239, 0.2237, 0.2232, 0.2221, 0.2212, 0.2214, 0.2220, 0.2228,
        0.2236, 0.2243, 0.2246, 0.2258, 0.2270, 0.2285, 0.2299, 0.2311, 0.2323,
        0.2327, 0.2333, 0.2341, 0.2345, 0.2356, 0.2363, 0.2370, 0.2377, 0.2379,
        0.2380, 0.2386, 0.2394, 0.2399, 0.2409, 0.2417, 0.2424, 0.2430, 0.2434,
        0.2432, 0.2426, 0.2422, 0.2417, 0.2413, 0.2414, 0.2414, 0.2414, 0.2412,
        0.2408, 0.2403, 0.2402, 0.2403, 0.2407, 0.2412, 0.2414, 0.2415, 0.2418,
        0.2416, 0.2422, 0.2425, 0.2426, 0.2428, 0.2422, 0.2420, 0.2416, 0.2407,
        0.2397, 0.2385, 0.2360, 0.2301, 0.2290, 0.2281, 0.2289, 0.2309])

encoder.embed.pos_enc.pe-torch.Size([1, 9999, 1280])-torch.float32
tensor([[[-0.6639, -0.7478,  0.9999,  ...,  0.8705,  0.4857,  0.8741],
         [ 0.2705, -0.9627,  0.5401,  ...,  0.8706,  0.4856,  0.8742],
         [ 0.9563, -0.2925, -0.4034,  ...,  0.8706,  0.4855,  0.8742],
         ...,
         [-0.9563, -0.2925,  0.4034,  ...,  0.8706, -0.4855,  0.8742],
         [-0.2705, -0.9627, -0.5401,  ...,  0.8706, -0.4856,  0.8742],
         [ 0.6639, -0.7478, -0.9999,  ...,  0.8705, -0.4857,  0.8741]]])

encoder.embed.conv.0.weight-torch.Size([32, 1, 3, 3])-torch.float32
tensor([[[[ 2.8281e-02, -2.7699e-02,  2.6628e-02],
          [ 3.7427e-02, -6.8776e-02,  3.5076e-02],
          [-8.1822e-03, -1.4228e-01, -6.3911e-03]]],


        [[[-8.0671e-02, -1.2025e-01, -3.6727e-02],
          [ 4.3224e-03, -3.5225e-02, -6.1420e-02],
          [ 8.7680e-02,  1.4939e-01,  1.2412e-01]]],


        [[[ 4.3368e-02, -7.6936e-02, -4.6976e-03],
          [ 9.6892e-02,  9.5786e-02, -1.7138e-01],
          [-1.1007e-01,  2.1329e-01, -7.5868e-02]]],


        [[[-2.7891e-02, -1.5570e-01,  1.9808e-01],
          [ 1.4609e-04, -1.9748e-01,  1.8413e-01],
          [ 7.2242e-03, -1.5053e-01,  1.2959e-01]]],


        [[[-4.4264e-02,  2.3409e-02, -1.1173e-01],
          [-6.9889e-03, -1.1951e-02, -1.1058e-01],
          [-3.1886e-03,  6.2456e-02, -1.0968e-01]]],


        [[[-1.6611e-01,  2.1647e-01, -4.6127e-02],
          [-5.1344e-03,  1.5088e-01, -1.2489e-01],
          [ 3.2370e-02,  9.9561e-03, -6.0729e-02]]],


        [[[ 9.6451e-02, -2.1287e-01, -1.4955e-01],
          [-9.3886e-02,  2.1015e-01,  1.5549e-01],
          [-4.0488e-03,  3.1298e-03, -5.2156e-03]]],


        [[[-8.8027e-02,  7.9314e-02,  3.1330e-02],
          [-6.1430e-02,  9.9053e-02,  3.7364e-02],
          [-8.9874e-02, -2.0584e-03,  2.6392e-02]]],


        [[[ 8.0293e-02,  2.5295e-01,  1.5702e-01],
          [-1.2059e-01, -2.0301e-01, -1.8203e-01],
          [ 4.1845e-03, -2.7656e-02,  2.2286e-02]]],


        [[[ 1.1467e-02,  2.2922e-02,  2.2407e-02],
          [ 1.0667e-01,  1.1142e-01,  9.2559e-02],
          [-1.0899e-01, -1.3009e-01, -9.6507e-02]]],


        [[[ 1.7477e-01, -1.0215e-01, -7.2330e-02],
          [ 2.1240e-01, -1.9141e-01, -2.0787e-02],
          [ 1.0503e-01, -1.1909e-01,  2.4541e-02]]],


        [[[-1.7242e-01,  1.8808e-02,  3.9106e-03],
          [ 8.0011e-03,  4.5030e-03,  9.4728e-03],
          [ 2.2297e-02,  9.8699e-03, -3.0434e-03]]],


        [[[-7.5374e-03,  3.3588e-02, -1.1148e-01],
          [-2.5706e-02, -9.1995e-02,  3.8814e-01],
          [ 6.2669e-03,  2.9243e-02, -1.6165e-01]]],


        [[[-6.5611e-02, -3.3822e-02,  5.3282e-02],
          [-1.2681e-02,  1.1208e-01,  4.1144e-02],
          [-3.1472e-02, -4.0583e-02, -5.2230e-02]]],


        [[[-1.4246e-01, -1.2307e-01,  3.9355e-01],
          [-1.7405e-02,  2.3666e-02, -3.7799e-02],
          [ 2.1863e-02,  1.7114e-02, -6.5271e-02]]],


        [[[ 5.0815e-03, -3.0668e-03,  1.6940e-03],
          [-1.7799e-02, -2.6311e-01, -8.9429e-02],
          [ 4.0412e-03,  2.7845e-01,  8.4930e-02]]],


        [[[ 6.4476e-02,  9.4775e-02,  1.5040e-01],
          [-7.6874e-02, -9.9931e-02, -1.3564e-01],
          [ 1.2168e-02, -1.7209e-02,  5.6789e-03]]],


        [[[ 5.0247e-03,  1.2210e-02,  3.6344e-02],
          [-7.7063e-03, -3.6716e-02,  1.0114e-02],
          [-3.5949e-02, -1.5451e-02, -1.5578e-02]]],


        [[[ 5.2664e-02,  1.0525e-01,  8.7135e-02],
          [ 1.7904e-02, -7.4689e-02, -7.4528e-02],
          [-5.1297e-02, -1.5124e-02, -1.3729e-02]]],


        [[[ 3.0861e-03, -3.7559e-02, -3.4428e-03],
          [-6.6562e-02, -3.2269e-02, -7.4253e-02],
          [ 4.6941e-02,  5.3148e-02,  5.9143e-02]]],


        [[[-4.8629e-02, -6.5419e-03, -3.3253e-02],
          [-7.6261e-04,  5.7698e-02,  8.1814e-02],
          [-2.5814e-02, -5.7825e-03,  3.2262e-02]]],


        [[[ 7.1919e-03, -7.5646e-02, -7.1036e-03],
          [-1.4661e-01,  4.4946e-01, -1.1659e-01],
          [ 3.2673e-03, -7.8687e-02,  1.3195e-02]]],


        [[[ 3.5179e-02, -8.2815e-02,  8.7890e-02],
          [ 4.5238e-02, -1.8372e-01,  7.8621e-02],
          [ 4.3834e-02, -1.1101e-01,  1.0311e-01]]],


        [[[ 1.6163e-01, -1.7646e-01,  1.0805e-02],
          [ 2.0785e-01, -2.1905e-01, -7.5099e-03],
          [ 1.6129e-01, -1.5847e-01, -4.2323e-03]]],


        [[[-6.2403e-03, -1.2517e-02, -1.5031e-02],
          [ 1.2037e-02, -1.6211e-02, -2.5693e-02],
          [-1.0218e-02, -1.0678e-03, -1.0137e-02]]],


        [[[ 1.4176e-02, -1.7822e-01,  1.5038e-01],
          [-2.2164e-02, -1.8115e-01,  2.1203e-01],
          [-5.3569e-02, -4.8721e-02,  1.1951e-01]]],


        [[[-1.1206e-01,  3.6982e-01, -1.1426e-01],
          [-1.8434e-02, -5.9775e-02,  1.3887e-02],
          [ 4.6344e-03, -4.3933e-02,  1.8468e-02]]],


        [[[-1.4308e-01, -1.5574e-01,  1.2645e-01],
          [ 1.3544e-01,  1.7037e-01, -1.2864e-01],
          [ 2.0283e-03, -7.8836e-03,  2.2718e-03]]],


        [[[ 1.5424e-02,  5.9988e-02, -9.0445e-02],
          [ 3.4093e-02,  9.0047e-02, -9.3640e-02],
          [ 2.1575e-02,  7.9359e-02, -8.7696e-02]]],


        [[[-5.8187e-02, -8.3598e-02,  1.3504e-01],
          [ 1.0217e-01, -1.9957e-01,  7.2323e-02],
          [ 1.6383e-01, -7.7397e-02, -4.9624e-02]]],


        [[[-9.3133e-02,  3.6078e-03,  1.5040e-02],
          [-3.9862e-02,  4.4314e-02,  3.6838e-02],
          [ 2.9818e-02, -6.9065e-03,  6.7868e-02]]],


        [[[ 5.3078e-02,  8.0249e-03,  4.6328e-02],
          [ 1.2415e-02,  3.8341e-02,  4.1904e-02],
          [ 1.8986e-02,  6.1435e-03,  5.0470e-02]]]])

encoder.embed.conv.0.bias-torch.Size([32])-torch.float32
tensor([-0.0588,  0.1118, -0.0444, -0.1469, -0.1650, -0.0098,  0.0003,  0.1899,
        -0.2212,  0.1280,  0.1323,  0.1087, -0.0661,  0.1281, -0.0199, -0.0006,
         0.1127,  0.1800,  0.0666,  0.0490,  0.1494, -0.0659,  0.1437, -0.1674,
         0.2173,  0.1247, -0.0611, -0.0004,  0.1642,  0.0165,  0.1866,  0.1470])

encoder.embed.conv.2.weight-torch.Size([32, 32, 3, 3])-torch.float32
tensor([[[[ 1.8204e-02, -5.2158e-02, -4.8448e-02],
          [-7.1113e-02, -8.1134e-02, -1.6353e-02],
          [-7.2939e-03,  2.2176e-02,  1.5995e-04]],

         [[-3.5111e-02, -3.4808e-02,  2.1039e-02],
          [-5.5445e-02, -4.0192e-02, -7.6962e-02],
          [-2.7434e-02,  1.1192e-02, -8.4809e-02]],

         [[ 1.0397e-01,  1.7122e-01,  1.2563e-01],
          [-1.0217e-01, -1.6466e-01,  2.3367e-02],
          [-6.1511e-02, -1.2387e-01, -4.6171e-02]],

         ...,

         [[-3.7407e-03,  1.0864e-01,  1.2611e-01],
          [-4.2543e-02, -1.5433e-01, -1.1406e-01],
          [-6.6199e-02, -3.5355e-02,  3.2631e-03]],

         [[-8.2081e-02,  1.4603e-02,  6.7290e-02],
          [-9.0901e-02,  6.0366e-02, -4.8847e-02],
          [ 7.6458e-03, -1.6877e-02, -1.1691e-02]],

         [[ 6.1591e-02,  1.2230e-01,  3.4003e-03],
          [-2.2699e-02, -7.0114e-03,  3.4395e-02],
          [-7.0724e-02,  2.9885e-02, -2.5598e-02]]],


        [[[ 5.9693e-02, -2.1930e-03, -1.2545e-01],
          [ 2.3712e-02,  1.3126e-02, -7.8637e-02],
          [-2.7178e-02,  4.8270e-02, -4.2695e-03]],

         [[-2.8600e-02,  1.4138e-02,  3.5371e-02],
          [-8.8985e-03,  1.5067e-02,  3.2240e-02],
          [ 2.1768e-02, -4.7809e-02,  1.5361e-02]],

         [[-5.1563e-02, -2.1586e-02,  1.1000e-01],
          [-5.4162e-02, -9.6695e-03,  2.2621e-01],
          [-6.5609e-02,  4.9177e-02,  9.4328e-02]],

         ...,

         [[-7.3670e-03, -1.7100e-02,  2.5227e-01],
          [ 2.9250e-02, -5.4798e-02,  3.2825e-01],
          [ 3.4764e-02, -1.0572e-02,  1.1206e-01]],

         [[-4.4700e-02,  4.2805e-02, -6.3344e-02],
          [-5.7789e-02,  9.3671e-02,  1.0449e-02],
          [-3.8121e-02, -3.7089e-02, -1.1370e-02]],

         [[-9.0184e-02,  3.3424e-02, -1.7379e-02],
          [-3.5152e-02, -6.4466e-02,  5.7071e-02],
          [-6.9767e-02,  7.2489e-02,  8.9247e-02]]],


        [[[ 1.4062e-02, -4.7962e-02,  3.5177e-02],
          [ 4.7840e-02,  1.1924e-01,  6.2704e-02],
          [-2.6666e-02, -9.2536e-02,  9.2153e-03]],

         [[-9.3786e-04, -5.5555e-02,  1.3299e-02],
          [ 1.1930e-01,  1.5428e-01,  1.3902e-02],
          [ 3.1662e-02,  1.2229e-01,  4.8194e-02]],

         [[-4.8027e-02, -2.8040e-02, -3.5862e-02],
          [-4.7284e-02, -8.3414e-02, -6.7939e-02],
          [ 5.5148e-02,  3.5960e-02,  3.6997e-02]],

         ...,

         [[-4.3893e-02,  4.2237e-02,  2.8093e-02],
          [ 4.5396e-02, -1.2777e-02, -2.4299e-02],
          [ 2.1986e-02,  5.5258e-02,  8.9167e-02]],

         [[ 7.5344e-02,  1.2493e-02,  1.2997e-02],
          [ 4.5211e-02,  2.4291e-02,  1.4411e-02],
          [ 3.7583e-02,  2.3755e-02,  7.4493e-03]],

         [[ 2.3369e-02, -1.5144e-02, -3.9675e-02],
          [ 2.2564e-02,  3.6950e-02, -4.7449e-02],
          [ 1.0134e-01,  1.0939e-01, -3.9211e-02]]],


        ...,


        [[[ 2.7996e-02, -1.3591e-01,  1.0434e-02],
          [ 2.0795e-02, -7.0358e-03, -6.4043e-03],
          [ 5.3471e-02, -4.7071e-03,  5.7441e-02]],

         [[-1.1474e-01, -2.7065e-01, -6.1195e-02],
          [-3.3103e-02, -6.8744e-02, -1.0273e-02],
          [-2.7423e-02, -6.0461e-02, -3.5074e-02]],

         [[-1.1465e-01,  5.0413e-02,  3.3917e-02],
          [ 1.1261e-01, -3.2045e-03, -1.5865e-02],
          [ 7.6574e-02,  1.5291e-02,  1.2045e-02]],

         ...,

         [[-9.2340e-02,  8.2865e-02,  9.0175e-02],
          [ 8.8232e-03,  4.3238e-02,  1.4137e-02],
          [ 4.9650e-02,  7.4517e-02, -1.3672e-02]],

         [[-6.2105e-02, -5.4604e-02, -5.6310e-02],
          [-3.8908e-02, -6.8513e-02,  5.3209e-02],
          [-4.4794e-02, -2.0973e-02,  8.0873e-02]],

         [[ 3.6721e-02,  1.9826e-01,  4.8729e-02],
          [ 4.9870e-02,  8.2276e-03,  1.2841e-02],
          [ 1.9100e-02, -6.5530e-02, -1.7927e-02]]],


        [[[-2.0878e-02, -7.4712e-02, -5.3617e-02],
          [-1.0098e-03, -1.7988e-02, -3.2778e-02],
          [ 2.0794e-02, -8.3886e-03,  1.2397e-02]],

         [[ 1.8082e-02,  6.1983e-02,  1.2239e-01],
          [-1.0759e-02, -2.2400e-02, -3.3534e-02],
          [-3.6620e-03,  3.4017e-02, -2.5963e-02]],

         [[ 9.7783e-02,  2.6147e-01,  1.9341e-01],
          [-1.9832e-02,  6.1880e-02,  8.8750e-02],
          [ 2.0246e-02, -4.9274e-02, -2.2390e-02]],

         ...,

         [[ 3.5212e-02,  1.6486e-01,  1.9462e-01],
          [-6.5590e-02, -4.6892e-02,  1.0546e-01],
          [-2.1897e-02, -4.1346e-02, -5.7660e-03]],

         [[ 5.9795e-02,  3.5519e-02, -1.9161e-02],
          [ 1.3771e-02, -5.7121e-02,  7.0808e-02],
          [ 6.1016e-02, -1.0612e-01, -2.2074e-02]],

         [[-2.4993e-02,  5.4141e-02,  1.4615e-02],
          [-3.7635e-02,  5.9712e-02,  4.0527e-02],
          [ 4.3187e-02,  5.9505e-02,  3.4122e-02]]],


        [[[ 7.2387e-02,  4.2768e-02, -4.3008e-02],
          [ 4.3203e-02, -7.7966e-03, -3.7521e-02],
          [-2.8231e-02, -1.9483e-02, -9.8038e-02]],

         [[ 4.5719e-02,  5.0059e-02, -2.5645e-02],
          [ 2.9091e-02,  8.2779e-02,  1.1062e-01],
          [ 3.7896e-02,  8.5312e-02,  8.6228e-02]],

         [[-2.4237e-02,  2.7226e-02,  3.8241e-02],
          [ 1.0466e-01,  8.6055e-02,  9.3714e-02],
          [ 1.8000e-02,  3.9410e-02, -5.3105e-03]],

         ...,

         [[-9.3177e-03, -6.5313e-02, -2.4098e-02],
          [ 8.9902e-02,  5.2880e-02, -4.9554e-03],
          [-3.2257e-02, -1.8571e-02, -4.8637e-02]],

         [[-2.7976e-02, -2.1017e-02, -8.7401e-02],
          [-4.1889e-02, -3.4969e-02, -7.0309e-02],
          [-1.3035e-03, -1.0156e-02,  3.5389e-02]],

         [[-5.4118e-02, -2.2645e-02, -4.6540e-02],
          [ 2.1215e-02, -5.2648e-02,  2.4486e-02],
          [ 1.1740e-01,  1.2886e-02,  1.4253e-01]]]])

encoder.embed.conv.2.bias-torch.Size([32])-torch.float32
tensor([ 0.0181, -0.0162,  0.0613,  0.0213,  0.0261, -0.0328, -0.0034, -0.0497,
         0.0496,  0.0083,  0.0346,  0.0412,  0.0039, -0.0222,  0.0314, -0.0734,
         0.0220, -0.0246,  0.0559, -0.0034, -0.0028, -0.0334,  0.0394, -0.0166,
        -0.0574, -0.0316,  0.0568,  0.0078, -0.0119,  0.0097, -0.0253,  0.0375])

encoder.embed.out.0.weight-torch.Size([1280, 608])-torch.float32
tensor([[-1.9076e-02, -1.1854e-01, -1.1678e-01,  ..., -6.0192e-02,
          8.5767e-02,  1.1013e-01],
        [ 1.7268e-02, -3.0861e-02, -2.6437e-02,  ...,  1.1654e-02,
          4.9727e-02,  2.7499e-02],
        [-4.3208e-02,  7.9016e-02, -7.3907e-02,  ..., -7.4144e-03,
         -5.2365e-02,  4.5270e-02],
        ...,
        [-7.5653e-02, -4.6413e-02, -4.2725e-02,  ...,  4.4384e-02,
          4.8182e-02, -5.9737e-03],
        [ 8.7144e-02,  4.2777e-02, -1.0328e-01,  ...,  1.5133e-02,
          9.7764e-03,  7.9517e-02],
        [ 4.0778e-03,  8.2760e-02, -1.4450e-02,  ..., -9.2484e-03,
         -4.9798e-05,  3.6978e-02]])

encoder.embed.out.0.bias-torch.Size([1280])-torch.float32
tensor([-0.0185, -0.0731, -0.0571,  ..., -0.0372,  0.0148,  0.0129])

encoder.after_norm.weight-torch.Size([1280])-torch.float32
tensor([1., 1., 1.,  ..., 1., 1., 1.])

encoder.after_norm.bias-torch.Size([1280])-torch.float32
tensor([0., 0., 0.,  ..., 0., 0., 0.])

encoder.encoders.0.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[-0.0280,  0.0855, -0.1191,  ...,  0.0856,  0.2689,  0.1109],
        [-0.1491,  0.0644,  0.0655,  ...,  0.3202, -0.1302, -0.0756],
        [ 0.2431,  0.0845,  0.1974,  ..., -0.2450,  0.0689, -0.2949],
        ...,
        [-0.0733,  0.0431, -0.1403,  ...,  0.2047,  0.1575, -0.2656],
        [ 0.0693,  0.3105,  0.3337,  ...,  0.0086, -0.0441,  0.2172],
        [ 0.2790, -0.1262,  0.1271,  ..., -0.0306, -0.2197,  0.1210]])

encoder.encoders.0.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[ 0.0907, -0.0650, -0.0315,  ...,  0.0897,  0.1281,  0.1405],
        [ 0.3353,  0.0148,  0.0188,  ...,  0.0649, -0.2202,  0.3754],
        [-0.0508, -0.0227,  0.1885,  ...,  0.1253, -0.2569,  0.2946],
        ...,
        [-0.1028, -0.0576,  0.2420,  ..., -0.0831,  0.0263,  0.3224],
        [ 0.3756, -0.0055,  0.2608,  ...,  0.0434, -0.1858,  0.1051],
        [-0.1069,  0.1762,  0.0143,  ...,  0.0297, -0.3720,  0.0052]])

encoder.encoders.0.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0322,  0.0256,  0.0006,  ...,  0.0638,  0.0083, -0.0413],
        [-0.0178,  0.0273, -0.0132,  ..., -0.0074,  0.0186, -0.0418],
        [-0.0213, -0.0137,  0.0197,  ...,  0.0778, -0.0209, -0.0196],
        ...,
        [-0.0686,  0.0293, -0.0069,  ..., -0.0400, -0.0013,  0.0499],
        [ 0.0232, -0.0347,  0.0614,  ...,  0.0263,  0.0568,  0.0324],
        [ 0.0481, -0.0135, -0.0216,  ..., -0.0129, -0.0359,  0.0684]])

encoder.encoders.0.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0077, -0.0805, -0.0556,  ..., -0.0420, -0.0038,  0.0080],
        [-0.0599, -0.0449, -0.0391,  ..., -0.0235, -0.0061, -0.0277],
        [-0.0089, -0.0445,  0.0374,  ...,  0.0520,  0.0262,  0.0238],
        ...,
        [-0.0325,  0.0038, -0.0558,  ..., -0.0206,  0.0174, -0.0368],
        [ 0.0570, -0.1027, -0.1240,  ..., -0.0188,  0.0853, -0.0265],
        [-0.0221,  0.0405,  0.0175,  ...,  0.0637,  0.0014,  0.0663]])

encoder.encoders.0.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0277,  0.0700, -0.0609,  ..., -0.0683,  0.1286, -0.1129],
        [-0.0010, -0.0648,  0.0147,  ...,  0.0437, -0.0318,  0.0312],
        [ 0.0107, -0.0786, -0.0669,  ...,  0.0432,  0.0243,  0.0347],
        ...,
        [-0.0562, -0.0146,  0.0379,  ..., -0.0046,  0.0058, -0.0013],
        [ 0.1587,  0.0384, -0.0813,  ...,  0.0119,  0.0512, -0.0268],
        [ 0.0384,  0.0278,  0.0602,  ..., -0.0306,  0.0126,  0.0486]])

encoder.encoders.0.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0719,  0.0491,  0.0330,  ...,  0.0232,  0.0027, -0.0138],
        [-0.0655,  0.0129,  0.0647,  ..., -0.0371, -0.0299, -0.0671],
        [ 0.0569, -0.0158,  0.0008,  ...,  0.0709, -0.0404, -0.0366],
        ...,
        [-0.0332, -0.0317, -0.0732,  ...,  0.0670,  0.0123,  0.0365],
        [-0.0406, -0.1014, -0.0028,  ..., -0.0662,  0.0070, -0.0472],
        [ 0.0153, -0.0643,  0.0221,  ..., -0.0369, -0.0587,  0.0250]])

encoder.encoders.0.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0418, -0.0624, -0.0029,  ...,  0.0055,  0.0540, -0.0277],
        [-0.0263,  0.0110,  0.0268,  ..., -0.0268, -0.0317,  0.0764],
        [-0.0188, -0.0611, -0.0215,  ...,  0.0278,  0.0140,  0.0428],
        ...,
        [-0.0257,  0.0352, -0.0304,  ...,  0.0049, -0.0224, -0.0653],
        [ 0.0147,  0.0226, -0.0060,  ..., -0.0377,  0.0169,  0.0144],
        [-0.0274, -0.0358, -0.0012,  ...,  0.0501,  0.0272, -0.0306]])

encoder.encoders.0.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.3860, 0.2878, 0.3624,  ..., 0.4368, 0.4475, 0.3944])

encoder.encoders.0.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([-0.0411, -0.0327,  0.0195,  ..., -0.0019,  0.0327,  0.0067])

encoder.encoders.0.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.6319, 0.5122, 0.7641,  ..., 0.7765, 0.7220, 0.7098])

encoder.encoders.0.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([ 0.0013, -0.0544, -0.0321,  ...,  0.0071, -0.0268,  0.0031])

encoder.encoders.0.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.6084, 0.5610, 0.7003,  ..., 0.6364, 0.7914, 0.7070])

encoder.encoders.0.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([ 0.0448,  0.0306, -0.0119,  ..., -0.0094,  0.0133,  0.0149])

encoder.encoders.0.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0818,  0.0180,  0.0029,  ...,  0.0238,  0.0583,  0.0382],
        [ 0.0386,  0.0175,  0.0723,  ..., -0.0715,  0.0169,  0.0815],
        [ 0.0198, -0.1005,  0.0753,  ..., -0.0101, -0.0741, -0.1054],
        ...,
        [ 0.0277,  0.0029,  0.0471,  ..., -0.0171,  0.0651, -0.0191],
        [ 0.0606, -0.0586, -0.0492,  ..., -0.0040,  0.0222, -0.0148],
        [ 0.0040,  0.0668,  0.0626,  ...,  0.0482, -0.0166, -0.0647]])

encoder.encoders.0.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([ 0.0024, -0.0368, -0.0302,  ..., -0.0129, -0.0223, -0.0291])

encoder.encoders.0.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0061, -0.0746, -0.0449,  ..., -0.0065, -0.0634,  0.0055],
        [-0.0503,  0.0458,  0.0443,  ..., -0.0599,  0.0657,  0.0280],
        [ 0.0398, -0.0511, -0.1163,  ...,  0.0238,  0.1445, -0.1074],
        ...,
        [ 0.0190, -0.0353, -0.0192,  ..., -0.0236,  0.0110, -0.0741],
        [-0.0683, -0.0372,  0.0136,  ..., -0.0305, -0.0372,  0.1001],
        [ 0.0176, -0.0687,  0.1464,  ..., -0.0328,  0.0470,  0.0634]])

encoder.encoders.0.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0686, -0.0594, -0.0220,  ..., -0.0289,  0.0111, -0.0712])

encoder.encoders.0.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 0.0039,  0.0218, -0.0704,  ...,  0.0742, -0.0140, -0.0394],
        [ 0.0625, -0.0180, -0.0400,  ...,  0.0098,  0.0076,  0.0073],
        [ 0.0036,  0.0321,  0.0459,  ...,  0.0715,  0.0018, -0.0236],
        ...,
        [ 0.0112, -0.0296, -0.0627,  ...,  0.0621, -0.0274, -0.0867],
        [ 0.0118,  0.0089, -0.0226,  ...,  0.0483, -0.0673,  0.0587],
        [-0.0101,  0.0549, -0.0760,  ..., -0.0126,  0.0391, -0.0277]])

encoder.encoders.0.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0322, -0.0061, -0.0394,  ..., -0.0376,  0.0003, -0.0513])

encoder.encoders.0.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[-0.0106, -0.0265,  0.0727,  ..., -0.0095, -0.0312,  0.0215],
        [ 0.0247,  0.0745,  0.0500,  ..., -0.0866, -0.0667,  0.0403],
        [ 0.0229, -0.0145, -0.0121,  ...,  0.0232, -0.0042, -0.0142],
        ...,
        [ 0.0343, -0.0170, -0.0411,  ..., -0.0172,  0.0216, -0.0215],
        [-0.0437,  0.0328, -0.0588,  ...,  0.0198,  0.0250, -0.0076],
        [-0.0118, -0.0440,  0.0408,  ..., -0.0335,  0.0524, -0.0306]])

encoder.encoders.0.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.1003,  0.0371,  0.0157,  ...,  0.0178,  0.0776, -0.0269])

encoder.encoders.0.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[-0.0406],
         [ 0.0078],
         [ 0.0714],
         ...,
         [-0.0577],
         [-0.0324],
         [-0.0084]],

        [[-0.0456],
         [ 0.0039],
         [ 0.0278],
         ...,
         [-0.0148],
         [-0.0492],
         [-0.0067]],

        [[ 0.0213],
         [-0.0279],
         [ 0.0474],
         ...,
         [-0.0621],
         [-0.0322],
         [-0.0383]],

        ...,

        [[-0.0056],
         [-0.0065],
         [-0.0106],
         ...,
         [ 0.0599],
         [ 0.0363],
         [-0.0506]],

        [[ 0.0317],
         [ 0.0303],
         [ 0.0003],
         ...,
         [-0.0377],
         [-0.0165],
         [-0.0135]],

        [[-0.0511],
         [ 0.0626],
         [-0.0009],
         ...,
         [ 0.0621],
         [ 0.1036],
         [-0.0262]]])

encoder.encoders.0.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[ 0.0031,  0.0024,  0.0030,  ...,  0.0010, -0.0011,  0.0037]],

        [[-0.0084, -0.0053,  0.0012,  ...,  0.0039, -0.0017,  0.0051]],

        [[ 0.0040,  0.0045,  0.0070,  ..., -0.0007, -0.0069,  0.0124]],

        ...,

        [[ 0.0088,  0.0072, -0.0033,  ...,  0.0067,  0.0002,  0.0090]],

        [[ 0.0038, -0.0031, -0.0037,  ..., -0.0031, -0.0084,  0.0093]],

        [[ 0.0222,  0.0067,  0.0020,  ..., -0.0028, -0.0133, -0.0183]]])

encoder.encoders.0.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([1.0941, 1.0583, 1.0744,  ..., 1.0519, 1.0031, 1.0969])

encoder.encoders.0.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.0473, -0.0629, -0.0790,  ..., -0.1410,  0.0371, -0.0915])

encoder.encoders.0.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[-0.0083],
         [-0.0874],
         [-0.1027],
         ...,
         [-0.0019],
         [-0.0264],
         [ 0.0708]],

        [[-0.0403],
         [ 0.0617],
         [-0.0899],
         ...,
         [ 0.0473],
         [ 0.0087],
         [-0.0256]],

        [[-0.0328],
         [ 0.0312],
         [ 0.0341],
         ...,
         [ 0.0105],
         [-0.0325],
         [-0.1468]],

        ...,

        [[-0.0372],
         [ 0.0447],
         [ 0.0012],
         ...,
         [-0.1252],
         [ 0.0658],
         [ 0.0269]],

        [[ 0.0115],
         [ 0.0472],
         [ 0.0302],
         ...,
         [-0.0023],
         [-0.0661],
         [ 0.0056]],

        [[-0.0674],
         [-0.2129],
         [-0.0196],
         ...,
         [ 0.0758],
         [ 0.0013],
         [ 0.1173]]])

encoder.encoders.0.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([0.9505, 0.8514, 1.0366,  ..., 1.0300, 1.1326, 1.0101])

encoder.encoders.0.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([-0.0416,  0.1763,  0.1129,  ..., -0.1151,  0.1197,  0.0958])

encoder.encoders.0.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([0.7978, 0.8321, 0.8183,  ..., 0.8867, 0.9822, 0.8301])

encoder.encoders.0.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([-0.0524,  0.0238,  0.0813,  ...,  0.0048,  0.1505,  0.0437])

encoder.encoders.0.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([0.9499, 0.9007, 1.0512,  ..., 1.0377, 1.1112, 1.0602])

encoder.encoders.0.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([-0.0345,  0.1877,  0.0285,  ..., -0.0498,  0.0117,  0.0303])

encoder.encoders.0.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.2683, 1.3269, 1.2448,  ..., 1.2045, 1.2751, 1.1687])

encoder.encoders.0.norm_final.bias-torch.Size([1280])-torch.float32
tensor([-0.0395, -0.0304,  0.0731,  ..., -0.0541,  0.1104, -0.0582])

encoder.encoders.1.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[ 0.2233, -0.0425, -0.1206,  ..., -0.1057,  0.0754, -0.0481],
        [-0.0111,  0.0357, -0.1205,  ..., -0.0259,  0.0429, -0.1595],
        [ 0.0343, -0.1466, -0.1429,  ..., -0.0177,  0.1849, -0.1139],
        ...,
        [ 0.1939, -0.2174,  0.3079,  ..., -0.0972,  0.0815,  0.0184],
        [ 0.2392,  0.0505,  0.0283,  ..., -0.0699, -0.2192, -0.1892],
        [ 0.0984,  0.0739, -0.0010,  ...,  0.1913, -0.2002,  0.1455]])

encoder.encoders.1.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[-0.1636, -0.0947, -0.3589,  ..., -0.0648,  0.2456,  0.2124],
        [ 0.1982,  0.0690, -0.0171,  ..., -0.0767,  0.2541, -0.0888],
        [-0.0143,  0.2593, -0.1014,  ..., -0.0290,  0.1454, -0.0874],
        ...,
        [ 0.0475, -0.2165, -0.3309,  ...,  0.1180,  0.2003,  0.0488],
        [ 0.0327, -0.1123, -0.1018,  ..., -0.2713,  0.1492,  0.1192],
        [ 0.0400,  0.2603,  0.1863,  ...,  0.0678, -0.0334,  0.1508]])

encoder.encoders.1.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0520, -0.0325,  0.0454,  ...,  0.0387, -0.0181, -0.0128],
        [-0.0263,  0.0594, -0.0514,  ...,  0.0007, -0.0168, -0.0036],
        [ 0.0099, -0.0267,  0.0404,  ..., -0.0130,  0.0807,  0.0064],
        ...,
        [ 0.0265,  0.0350,  0.0115,  ..., -0.0118,  0.0039,  0.0100],
        [-0.0102, -0.0009,  0.0162,  ..., -0.0468,  0.0725, -0.0276],
        [ 0.0193, -0.0469,  0.0387,  ..., -0.0151,  0.0152, -0.0011]])

encoder.encoders.1.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0118,  0.0045,  0.0231,  ...,  0.0665, -0.0386,  0.1032],
        [ 0.0282, -0.0111,  0.0592,  ...,  0.0264, -0.0149, -0.0179],
        [ 0.0589, -0.0091, -0.0262,  ...,  0.0737, -0.0529,  0.0245],
        ...,
        [ 0.0204,  0.1113, -0.0130,  ...,  0.0411,  0.0259, -0.1106],
        [ 0.0351, -0.0117,  0.0011,  ...,  0.0654,  0.0953, -0.0127],
        [-0.0190, -0.0309,  0.0371,  ...,  0.0248,  0.0681,  0.0449]])

encoder.encoders.1.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0351,  0.0163, -0.0718,  ..., -0.0205, -0.0382,  0.0335],
        [ 0.0175,  0.0166, -0.0691,  ..., -0.0334,  0.0050,  0.0041],
        [-0.0227,  0.0087, -0.0906,  ...,  0.0238, -0.0247,  0.0128],
        ...,
        [ 0.0166, -0.0541, -0.0356,  ..., -0.0381, -0.0411, -0.0153],
        [-0.0308, -0.0179, -0.0694,  ..., -0.0111,  0.0480,  0.0205],
        [ 0.0128, -0.0249, -0.0030,  ...,  0.0037, -0.0015, -0.0405]])

encoder.encoders.1.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0481,  0.0224,  0.0270,  ..., -0.0430, -0.0249, -0.0627],
        [ 0.0078, -0.0418, -0.0059,  ...,  0.0207, -0.0097,  0.0023],
        [-0.0343,  0.0272,  0.0269,  ..., -0.0020,  0.0054,  0.0395],
        ...,
        [ 0.0470,  0.0038,  0.0120,  ...,  0.0304, -0.0064, -0.0880],
        [ 0.0102, -0.0904,  0.0534,  ..., -0.0286, -0.0240, -0.0646],
        [ 0.0163, -0.0143, -0.0468,  ...,  0.0063,  0.1104, -0.1074]])

encoder.encoders.1.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0853, -0.0614, -0.0593,  ..., -0.0322, -0.0026, -0.0424],
        [ 0.0947,  0.0488,  0.0674,  ..., -0.0035,  0.0203,  0.0196],
        [-0.0366,  0.0362, -0.0125,  ...,  0.0124,  0.0409,  0.0353],
        ...,
        [ 0.1120,  0.0688,  0.0126,  ...,  0.0053,  0.0236,  0.0424],
        [ 0.0359, -0.0706, -0.0114,  ...,  0.0214,  0.0443,  0.0618],
        [-0.0214, -0.0207, -0.0107,  ..., -0.0074, -0.0259,  0.0535]])

encoder.encoders.1.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.4109, 0.3025, 0.3826,  ..., 0.4892, 0.4276, 0.5312])

encoder.encoders.1.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([ 0.0655,  0.0334,  0.0298,  ...,  0.0363, -0.0302,  0.0555])

encoder.encoders.1.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.7598, 0.6175, 0.7435,  ..., 0.8413, 0.7261, 0.8327])

encoder.encoders.1.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([-0.0614,  0.0003, -0.0059,  ..., -0.0600, -0.0078, -0.0315])

encoder.encoders.1.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.6353, 0.5612, 0.7059,  ..., 0.6687, 0.6789, 0.6715])

encoder.encoders.1.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([ 0.1108,  0.0496, -0.0157,  ...,  0.0474, -0.0691,  0.0726])

encoder.encoders.1.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 0.0318, -0.0169,  0.0248,  ..., -0.0236, -0.0299, -0.0080],
        [ 0.0367,  0.0946, -0.0523,  ..., -0.0034,  0.0071, -0.0134],
        [ 0.1054,  0.0209, -0.0182,  ...,  0.0405, -0.0175, -0.0104],
        ...,
        [ 0.0087,  0.0565, -0.0211,  ..., -0.0077, -0.0140,  0.0702],
        [-0.0290,  0.1137, -0.0223,  ...,  0.0942, -0.0319, -0.0041],
        [ 0.0265,  0.0401, -0.0366,  ...,  0.0617, -0.0102,  0.0511]])

encoder.encoders.1.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0233,  0.0078, -0.0025,  ..., -0.0364, -0.0174, -0.0326])

encoder.encoders.1.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0878,  0.0409, -0.0919,  ...,  0.0742,  0.0704, -0.0044],
        [-0.0440, -0.0009,  0.0201,  ..., -0.0031,  0.0347, -0.0166],
        [-0.0038, -0.0018, -0.0246,  ...,  0.0194, -0.0509,  0.0070],
        ...,
        [-0.0171,  0.0532,  0.0058,  ..., -0.0102, -0.0312, -0.0667],
        [-0.0121,  0.0262,  0.0369,  ...,  0.0019,  0.0634, -0.0256],
        [-0.0436,  0.0242,  0.0646,  ...,  0.0410,  0.0164, -0.0457]])

encoder.encoders.1.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0340, -0.0759,  0.0197,  ..., -0.0239,  0.0389, -0.0649])

encoder.encoders.1.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 0.0876, -0.0359, -0.0485,  ..., -0.0244, -0.0110, -0.0116],
        [ 0.0265,  0.0497,  0.0492,  ..., -0.0739,  0.0432, -0.0125],
        [-0.0403,  0.0774,  0.0034,  ..., -0.0512, -0.0470, -0.0076],
        ...,
        [-0.0497, -0.1132, -0.0722,  ...,  0.0593, -0.0245,  0.0929],
        [-0.0595, -0.0320, -0.0192,  ...,  0.0394, -0.0315, -0.0314],
        [-0.0251, -0.0085, -0.0234,  ..., -0.0649,  0.0126,  0.0033]])

encoder.encoders.1.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0454, -0.0481, -0.0210,  ..., -0.0486,  0.0021, -0.0548])

encoder.encoders.1.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[-0.0216, -0.0016, -0.0387,  ...,  0.0349,  0.0498,  0.0130],
        [-0.0047,  0.0259, -0.0088,  ..., -0.0069,  0.0225, -0.0579],
        [ 0.0033, -0.0197, -0.0218,  ..., -0.0072, -0.0131,  0.0917],
        ...,
        [-0.0329,  0.0430, -0.0047,  ...,  0.0209,  0.0152,  0.0331],
        [-0.0278,  0.0715, -0.0225,  ..., -0.0559, -0.0833,  0.0193],
        [-0.0933, -0.0416, -0.0009,  ...,  0.0614, -0.0378, -0.0147]])

encoder.encoders.1.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0132, -0.0561,  0.0638,  ..., -0.0197,  0.0215, -0.0030])

encoder.encoders.1.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[ 6.6271e-02],
         [-8.1321e-02],
         [ 3.5724e-02],
         ...,
         [-6.6951e-02],
         [ 7.3143e-02],
         [-2.5076e-02]],

        [[-3.1505e-02],
         [-1.3858e-02],
         [ 2.6690e-02],
         ...,
         [ 1.6216e-02],
         [-2.9602e-02],
         [ 2.5855e-02]],

        [[-4.8504e-03],
         [-2.4903e-03],
         [-6.6688e-02],
         ...,
         [ 1.9260e-03],
         [ 1.3080e-02],
         [ 4.5899e-03]],

        ...,

        [[-2.7657e-02],
         [-5.0695e-03],
         [ 4.5058e-02],
         ...,
         [ 2.4498e-02],
         [ 2.1302e-03],
         [-3.5519e-03]],

        [[ 4.8205e-02],
         [-4.2128e-04],
         [-6.5873e-02],
         ...,
         [-2.9215e-02],
         [-2.9702e-02],
         [ 4.3726e-02]],

        [[-2.0917e-03],
         [ 3.6375e-02],
         [-7.7727e-03],
         ...,
         [ 7.0498e-02],
         [ 2.4353e-05],
         [ 6.7154e-02]]])

encoder.encoders.1.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[-0.0007,  0.0040,  0.0052,  ..., -0.0027,  0.0040,  0.0028]],

        [[-0.0260, -0.0101, -0.0071,  ..., -0.0098, -0.0079, -0.0198]],

        [[-0.0070, -0.0014,  0.0053,  ...,  0.0093,  0.0034, -0.0010]],

        ...,

        [[-0.0027, -0.0072,  0.0036,  ..., -0.0047,  0.0040, -0.0042]],

        [[ 0.0027,  0.0050, -0.0061,  ...,  0.0032,  0.0023, -0.0093]],

        [[ 0.0235,  0.0187,  0.0160,  ...,  0.0180,  0.0167, -0.0005]]])

encoder.encoders.1.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([1.1422, 1.1478, 1.0030,  ..., 1.0091, 1.0142, 1.0607])

encoder.encoders.1.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.0987, -0.0774, -0.1061,  ..., -0.0767, -0.0955, -0.0657])

encoder.encoders.1.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[-0.1018],
         [-0.0269],
         [-0.1128],
         ...,
         [-0.0244],
         [ 0.0102],
         [ 0.0127]],

        [[ 0.0298],
         [ 0.0086],
         [ 0.0044],
         ...,
         [ 0.0284],
         [-0.0403],
         [ 0.0917]],

        [[-0.0946],
         [-0.0272],
         [ 0.0508],
         ...,
         [ 0.0134],
         [ 0.0131],
         [ 0.0284]],

        ...,

        [[ 0.0463],
         [-0.0369],
         [-0.0419],
         ...,
         [ 0.0880],
         [-0.0073],
         [ 0.0393]],

        [[ 0.0321],
         [ 0.0588],
         [-0.0189],
         ...,
         [-0.0221],
         [-0.0363],
         [ 0.0459]],

        [[-0.0428],
         [-0.0110],
         [ 0.0010],
         ...,
         [ 0.0287],
         [-0.0267],
         [ 0.0176]]])

encoder.encoders.1.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([0.9755, 0.9004, 0.9273,  ..., 0.9687, 0.9967, 1.0307])

encoder.encoders.1.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([-0.1743, -0.1871,  0.1853,  ...,  0.0125,  0.2426, -0.2346])

encoder.encoders.1.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([0.8795, 0.8159, 1.0190,  ..., 0.9768, 1.0666, 1.0227])

encoder.encoders.1.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([-0.0291,  0.1003,  0.0687,  ..., -0.0379,  0.1167,  0.0205])

encoder.encoders.1.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([1.0227, 1.0142, 1.0491,  ..., 1.1001, 1.0781, 1.0531])

encoder.encoders.1.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([ 0.0129,  0.0383,  0.0818,  ..., -0.0786, -0.0061, -0.0625])

encoder.encoders.1.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.2570, 1.2996, 1.2216,  ..., 1.2034, 1.2130, 1.1727])

encoder.encoders.1.norm_final.bias-torch.Size([1280])-torch.float32
tensor([-0.0020, -0.1435,  0.0576,  ..., -0.0583,  0.0468, -0.1026])

encoder.encoders.2.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[-0.0632, -0.0261, -0.2142,  ...,  0.0033, -0.1584,  0.0187],
        [ 0.1407, -0.2260, -0.0621,  ..., -0.0663, -0.2071, -0.3868],
        [ 0.0417, -0.1475,  0.0931,  ..., -0.0602,  0.1570,  0.2920],
        ...,
        [-0.0252,  0.2368, -0.1390,  ...,  0.1155, -0.0512,  0.0183],
        [ 0.2199,  0.1756, -0.0789,  ...,  0.0313, -0.0605,  0.0137],
        [-0.1224, -0.1310, -0.1074,  ..., -0.0116,  0.2705, -0.0607]])

encoder.encoders.2.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[-0.2889,  0.1180, -0.1702,  ...,  0.1844, -0.1908,  0.0231],
        [-0.3000, -0.0459,  0.0315,  ..., -0.1926,  0.0331,  0.0838],
        [-0.0014, -0.0527,  0.0684,  ..., -0.1346,  0.2246, -0.2980],
        ...,
        [ 0.2095,  0.0481, -0.1760,  ...,  0.0362, -0.0179, -0.0182],
        [-0.1456, -0.0340,  0.0835,  ...,  0.0681,  0.1112,  0.0178],
        [ 0.2157,  0.0198,  0.0213,  ..., -0.1014, -0.0185, -0.1182]])

encoder.encoders.2.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0135, -0.0321,  0.0533,  ..., -0.0125, -0.0383, -0.0236],
        [-0.0133, -0.0006, -0.0371,  ...,  0.0054,  0.0114,  0.0143],
        [ 0.0003, -0.0469, -0.0356,  ...,  0.0010,  0.0254, -0.0300],
        ...,
        [ 0.0284, -0.0632, -0.0634,  ...,  0.0655, -0.0718, -0.0489],
        [ 0.0597, -0.0534, -0.0465,  ...,  0.0364,  0.0322, -0.0193],
        [-0.0017, -0.0744,  0.0008,  ..., -0.0430,  0.0060, -0.0184]])

encoder.encoders.2.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0514,  0.0093,  0.0633,  ...,  0.0306,  0.0159,  0.0189],
        [-0.0291, -0.0745, -0.0808,  ...,  0.0771, -0.0066,  0.1029],
        [ 0.0435, -0.0802, -0.0414,  ..., -0.0039,  0.0355, -0.0151],
        ...,
        [ 0.0220,  0.0820,  0.0999,  ...,  0.0158, -0.0248, -0.0298],
        [ 0.0746,  0.0323,  0.0322,  ...,  0.0182,  0.0267,  0.0332],
        [-0.0510, -0.0669, -0.0375,  ..., -0.0157, -0.0112, -0.0430]])

encoder.encoders.2.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0011,  0.0127, -0.0304,  ...,  0.0006, -0.0480,  0.0569],
        [ 0.0082, -0.0419, -0.0486,  ..., -0.0423, -0.0154,  0.0309],
        [-0.0095,  0.0015, -0.0031,  ...,  0.0354,  0.1095, -0.0265],
        ...,
        [-0.0094, -0.0101, -0.0186,  ...,  0.0461,  0.0368,  0.0261],
        [ 0.0859,  0.0269, -0.0282,  ..., -0.0095,  0.0227,  0.0269],
        [-0.0502, -0.0140,  0.0127,  ...,  0.0229,  0.0460, -0.0268]])

encoder.encoders.2.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 7.3610e-02, -1.8906e-02,  8.9180e-02,  ..., -5.1647e-02,
         -1.1826e-02, -8.8065e-03],
        [ 9.5857e-05,  1.7025e-02, -6.2213e-02,  ...,  3.1020e-02,
         -4.0570e-03,  5.6013e-02],
        [-4.5452e-02, -2.3200e-02,  4.9800e-02,  ..., -5.5276e-02,
          5.7344e-02, -4.4274e-02],
        ...,
        [ 9.5890e-03,  3.6971e-02, -5.7168e-02,  ...,  2.1337e-02,
          1.5907e-02, -6.0010e-02],
        [-4.9115e-02, -7.7338e-02,  1.4668e-02,  ...,  8.1497e-03,
         -1.8906e-02, -1.7252e-02],
        [ 3.4370e-02,  1.6670e-02, -2.8260e-02,  ...,  9.3547e-03,
         -4.5759e-02,  1.6315e-02]])

encoder.encoders.2.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0592,  0.0200,  0.0376,  ..., -0.0078,  0.0521,  0.0033],
        [ 0.1260,  0.0057,  0.0368,  ..., -0.0017,  0.0318, -0.0250],
        [ 0.1259,  0.0923,  0.0569,  ...,  0.0145, -0.0156,  0.0441],
        ...,
        [ 0.0406,  0.1573,  0.0029,  ..., -0.0095,  0.0476,  0.0326],
        [ 0.0184,  0.0847,  0.0039,  ..., -0.0295, -0.0377,  0.0123],
        [ 0.0111, -0.0644,  0.0063,  ...,  0.0156, -0.0173,  0.0456]])

encoder.encoders.2.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.4190, 0.3878, 0.3918,  ..., 0.4635, 0.4025, 0.4600])

encoder.encoders.2.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([ 0.0789,  0.0700, -0.0048,  ..., -0.0282,  0.0020,  0.0580])

encoder.encoders.2.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.7654, 0.7537, 0.6976,  ..., 0.7812, 0.6843, 0.7987])

encoder.encoders.2.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([ 0.0066, -0.0392,  0.0078,  ...,  0.0168,  0.0379, -0.0300])

encoder.encoders.2.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.6112, 0.6310, 0.5866,  ..., 0.6277, 0.6741, 0.6482])

encoder.encoders.2.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([ 0.0780,  0.1359, -0.0170,  ...,  0.0320, -0.0278,  0.1161])

encoder.encoders.2.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 0.0349,  0.0301, -0.0272,  ..., -0.0545,  0.0069,  0.0009],
        [-0.0204,  0.0274, -0.0616,  ...,  0.0543,  0.1284,  0.0131],
        [-0.0681,  0.0261, -0.0491,  ..., -0.0085,  0.0442, -0.0311],
        ...,
        [-0.0079,  0.0256, -0.0453,  ...,  0.0564,  0.0310,  0.0697],
        [ 0.0058, -0.0163,  0.1101,  ..., -0.0081,  0.0168,  0.0520],
        [ 0.0556,  0.0540,  0.0456,  ...,  0.0622,  0.0151,  0.0850]])

encoder.encoders.2.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([ 0.0011, -0.0392, -0.0197,  ...,  0.0009,  0.0180, -0.0043])

encoder.encoders.2.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[-1.3345e-03, -5.1226e-02,  8.2752e-02,  ..., -5.5625e-02,
         -8.6229e-03, -5.5645e-02],
        [ 6.6731e-03, -8.3395e-03, -5.9115e-03,  ..., -7.7307e-02,
          5.5390e-02, -1.0309e-01],
        [-2.6547e-02,  5.0127e-02,  1.9082e-02,  ...,  1.1515e-03,
          3.3767e-03, -5.9610e-02],
        ...,
        [-1.0783e-02, -4.8774e-02,  9.0031e-02,  ...,  7.4978e-05,
         -3.2420e-02, -3.6717e-02],
        [ 2.7085e-02,  1.5440e-02,  2.9958e-03,  ...,  2.7549e-02,
          1.8056e-02, -5.6238e-02],
        [ 4.1936e-03, -3.4451e-02,  5.6637e-02,  ..., -4.9646e-02,
         -1.7774e-02,  1.1088e-02]])

encoder.encoders.2.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([ 0.0431, -0.0458,  0.0203,  ..., -0.0193, -0.0067, -0.0493])

encoder.encoders.2.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 0.0480,  0.0211, -0.0263,  ...,  0.0313, -0.0650, -0.0175],
        [ 0.1057,  0.1297,  0.0502,  ...,  0.0406, -0.0475, -0.0507],
        [-0.0218,  0.0240, -0.0246,  ..., -0.0160,  0.0107,  0.0677],
        ...,
        [-0.0138,  0.0726, -0.0449,  ...,  0.0244, -0.0624,  0.0211],
        [ 0.0236,  0.0579, -0.0209,  ..., -0.0178,  0.0236,  0.0289],
        [ 0.0078,  0.0032,  0.0799,  ...,  0.0605,  0.0273, -0.0537]])

encoder.encoders.2.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0137, -0.0412, -0.0296,  ..., -0.0369, -0.0449, -0.0170])

encoder.encoders.2.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[-0.0257, -0.0208, -0.0044,  ..., -0.0141, -0.0138,  0.0545],
        [ 0.0213, -0.0289,  0.0248,  ..., -0.0224,  0.0816,  0.0333],
        [-0.0285, -0.0753,  0.0477,  ..., -0.0194, -0.0655, -0.0076],
        ...,
        [ 0.0262, -0.0076,  0.0220,  ...,  0.0349, -0.0061,  0.0896],
        [ 0.0224,  0.0144,  0.0450,  ...,  0.0124, -0.0307,  0.0860],
        [ 0.0088, -0.0676, -0.0333,  ..., -0.0027,  0.0398, -0.0243]])

encoder.encoders.2.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([ 0.0577, -0.0288,  0.0430,  ..., -0.0606,  0.0196, -0.0491])

encoder.encoders.2.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[-0.0629],
         [ 0.0156],
         [ 0.0344],
         ...,
         [ 0.1167],
         [-0.0652],
         [-0.0313]],

        [[-0.0639],
         [ 0.0003],
         [ 0.0684],
         ...,
         [-0.0083],
         [ 0.0565],
         [ 0.0164]],

        [[ 0.1028],
         [-0.0174],
         [-0.0149],
         ...,
         [-0.0977],
         [ 0.1012],
         [-0.0389]],

        ...,

        [[-0.0303],
         [-0.0291],
         [-0.0460],
         ...,
         [-0.0242],
         [ 0.0009],
         [-0.0565]],

        [[ 0.0392],
         [-0.0640],
         [ 0.0056],
         ...,
         [ 0.0021],
         [-0.0167],
         [-0.0907]],

        [[-0.0222],
         [-0.0632],
         [ 0.0379],
         ...,
         [-0.0518],
         [ 0.0213],
         [-0.0101]]])

encoder.encoders.2.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[ 0.0028,  0.0009, -0.0107,  ...,  0.0008, -0.0148, -0.0055]],

        [[-0.0088, -0.0119, -0.0068,  ...,  0.0005,  0.0005,  0.0042]],

        [[-0.0412, -0.0149, -0.0194,  ..., -0.0235, -0.0237, -0.0528]],

        ...,

        [[ 0.0072, -0.0056, -0.0048,  ...,  0.0056,  0.0051, -0.0038]],

        [[ 0.0013, -0.0049,  0.0050,  ...,  0.0055, -0.0021, -0.0048]],

        [[-0.0119, -0.0155, -0.0069,  ...,  0.0055, -0.0055,  0.0067]]])

encoder.encoders.2.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([0.9865, 0.9660, 1.0682,  ..., 0.9744, 1.0171, 0.9943])

encoder.encoders.2.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.0890, -0.0888, -0.0521,  ..., -0.1108, -0.0735, -0.0883])

encoder.encoders.2.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[-0.0185],
         [-0.0762],
         [ 0.0925],
         ...,
         [ 0.1007],
         [-0.0132],
         [ 0.0889]],

        [[-0.0427],
         [-0.0710],
         [ 0.0105],
         ...,
         [-0.0513],
         [-0.0006],
         [-0.0306]],

        [[ 0.0626],
         [ 0.0935],
         [ 0.0916],
         ...,
         [-0.0106],
         [ 0.0743],
         [ 0.0339]],

        ...,

        [[ 0.0363],
         [-0.0262],
         [ 0.0316],
         ...,
         [-0.1003],
         [-0.0463],
         [-0.0903]],

        [[ 0.0125],
         [-0.0408],
         [-0.0327],
         ...,
         [ 0.0251],
         [-0.0525],
         [ 0.0439]],

        [[ 0.0367],
         [-0.0313],
         [ 0.0408],
         ...,
         [-0.0070],
         [ 0.0069],
         [-0.0685]]])

encoder.encoders.2.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([0.9699, 0.9220, 0.9997,  ..., 0.9671, 1.0037, 1.0212])

encoder.encoders.2.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([ 0.2027, -0.0505,  0.0685,  ..., -0.0407, -0.0390, -0.2155])

encoder.encoders.2.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([1.0131, 0.9933, 1.0113,  ..., 1.0285, 1.0221, 1.0353])

encoder.encoders.2.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([-0.1001, -0.2032,  0.0719,  ..., -0.0092,  0.1395, -0.0741])

encoder.encoders.2.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([1.0179, 1.0602, 1.0600,  ..., 1.0556, 1.0837, 1.0340])

encoder.encoders.2.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([-0.0116, -0.0753,  0.0738,  ..., -0.0587,  0.0270, -0.1749])

encoder.encoders.2.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.2831, 1.2299, 1.1950,  ..., 1.2381, 1.1881, 1.2530])

encoder.encoders.2.norm_final.bias-torch.Size([1280])-torch.float32
tensor([ 0.1131, -0.0449,  0.0266,  ..., -0.0920, -0.0476, -0.1248])

encoder.encoders.3.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[ 0.1010, -0.1498, -0.1622,  ..., -0.0256,  0.1864,  0.1765],
        [ 0.1800, -0.0765, -0.0071,  ...,  0.3054, -0.1986,  0.2681],
        [-0.0977,  0.0209, -0.0667,  ..., -0.1627,  0.1002, -0.0373],
        ...,
        [-0.1810,  0.0728,  0.0526,  ..., -0.1359, -0.1867, -0.0913],
        [ 0.2668,  0.0227, -0.1639,  ...,  0.1358,  0.0010, -0.2593],
        [ 0.1218, -0.2796, -0.3011,  ...,  0.0556,  0.2139,  0.0286]])

encoder.encoders.3.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[ 0.1981, -0.0827,  0.0287,  ...,  0.1402, -0.0291, -0.1125],
        [ 0.0932,  0.0620,  0.2450,  ...,  0.0825,  0.0667, -0.3551],
        [-0.1211,  0.0101,  0.0744,  ..., -0.1136,  0.1250, -0.1366],
        ...,
        [ 0.1954,  0.0564, -0.0822,  ..., -0.4012,  0.0866, -0.1481],
        [-0.1863, -0.1325, -0.0845,  ..., -0.0372, -0.0313,  0.2201],
        [ 0.2215,  0.4111,  0.3674,  ...,  0.3668,  0.2390,  0.0387]])

encoder.encoders.3.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0061, -0.0223,  0.0073,  ..., -0.0653,  0.0194,  0.0172],
        [ 0.0069,  0.0106, -0.0313,  ...,  0.0398,  0.0043,  0.0678],
        [ 0.0466, -0.0063,  0.0424,  ..., -0.0532, -0.0188, -0.0142],
        ...,
        [-0.0472, -0.0151,  0.0283,  ..., -0.0102, -0.0773,  0.0732],
        [-0.0428,  0.0123, -0.0103,  ..., -0.0325, -0.0486,  0.0124],
        [-0.0181, -0.0051, -0.0755,  ...,  0.0062,  0.0037, -0.0139]])

encoder.encoders.3.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0333,  0.0031, -0.0067,  ...,  0.0199, -0.0322,  0.0214],
        [-0.0116,  0.0715, -0.0099,  ..., -0.0308, -0.0854, -0.0741],
        [ 0.0679, -0.0661, -0.0682,  ...,  0.0168,  0.0262, -0.0189],
        ...,
        [ 0.0077, -0.0625, -0.0533,  ...,  0.0101,  0.0142, -0.0859],
        [ 0.0086, -0.0309,  0.0780,  ...,  0.0289, -0.0550, -0.1019],
        [-0.0148, -0.0073,  0.0371,  ..., -0.0089, -0.0543, -0.0324]])

encoder.encoders.3.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0654,  0.0024, -0.0031,  ...,  0.0212,  0.1001,  0.0712],
        [ 0.0062, -0.0541,  0.0111,  ...,  0.0368,  0.0438,  0.0521],
        [-0.0493, -0.0022,  0.0419,  ..., -0.0009,  0.0095, -0.0393],
        ...,
        [-0.0058, -0.0127,  0.0531,  ..., -0.0459,  0.0413, -0.0273],
        [-0.0151, -0.0463, -0.0578,  ...,  0.0602, -0.0733, -0.0112],
        [ 0.0848, -0.0075,  0.0600,  ..., -0.0507, -0.0016,  0.0636]])

encoder.encoders.3.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0321, -0.0432,  0.0413,  ..., -0.0195,  0.0551,  0.0069],
        [-0.0098, -0.0353, -0.0227,  ...,  0.0241,  0.0206, -0.0618],
        [-0.0786,  0.0184,  0.0057,  ...,  0.0548,  0.0354, -0.0137],
        ...,
        [ 0.0126, -0.0275, -0.0140,  ...,  0.0251, -0.0276,  0.0432],
        [-0.0506, -0.0419, -0.0428,  ..., -0.0255, -0.0272,  0.0349],
        [-0.0326,  0.0302, -0.0359,  ..., -0.0227,  0.0272, -0.0219]])

encoder.encoders.3.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0279, -0.0697,  0.0186,  ..., -0.0316,  0.0327, -0.0470],
        [ 0.0280,  0.0347,  0.0421,  ...,  0.0286, -0.0389,  0.0317],
        [ 0.0067,  0.0022,  0.0258,  ...,  0.0113, -0.0439,  0.0099],
        ...,
        [-0.0030,  0.0589,  0.0686,  ...,  0.0129,  0.0311,  0.0062],
        [ 0.0461,  0.1009,  0.0642,  ..., -0.0365,  0.0078, -0.0214],
        [ 0.0208,  0.0570, -0.0056,  ...,  0.0371, -0.0218,  0.0637]])

encoder.encoders.3.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.4045, 0.4224, 0.4122,  ..., 0.3870, 0.4481, 0.4074])

encoder.encoders.3.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([0.0258, 0.0727, 0.0004,  ..., 0.0303, 0.0395, 0.1106])

encoder.encoders.3.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.6584, 0.7467, 0.7078,  ..., 0.7198, 0.6687, 0.7318])

encoder.encoders.3.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([-0.0037,  0.0053, -0.0096,  ..., -0.0318,  0.0271,  0.0240])

encoder.encoders.3.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.5410, 0.6280, 0.6135,  ..., 0.5948, 0.5991, 0.5766])

encoder.encoders.3.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([-0.0220,  0.1142, -0.0370,  ...,  0.0908,  0.0551,  0.1806])

encoder.encoders.3.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 0.0051, -0.0213, -0.0265,  ..., -0.0536, -0.0243,  0.0058],
        [-0.0541,  0.0649,  0.0094,  ...,  0.0640, -0.0258,  0.0984],
        [ 0.0671, -0.0300, -0.0377,  ..., -0.0401, -0.0099, -0.0383],
        ...,
        [-0.0060,  0.0044, -0.0356,  ...,  0.0801, -0.0148,  0.0704],
        [-0.0682, -0.0462, -0.0367,  ..., -0.0246,  0.0502, -0.0556],
        [ 0.1009, -0.0182, -0.1040,  ...,  0.0336, -0.0460, -0.0242]])

encoder.encoders.3.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([ 0.0143, -0.0094,  0.0044,  ..., -0.0319, -0.0299,  0.0178])

encoder.encoders.3.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[-3.3612e-02,  2.1440e-02,  2.3706e-02,  ..., -3.6867e-02,
         -4.6127e-03, -6.6302e-02],
        [ 1.7449e-02, -6.6478e-02,  6.0681e-03,  ..., -1.0064e-03,
          2.9532e-02,  2.0655e-02],
        [-2.0994e-02,  1.2230e-02, -3.1016e-02,  ...,  6.9320e-02,
         -9.0820e-02, -2.3535e-02],
        ...,
        [ 9.1786e-06,  1.5390e-03,  5.7624e-02,  ...,  5.5655e-02,
         -3.6314e-02, -3.0708e-02],
        [-2.2336e-03,  4.1572e-03,  5.5476e-02,  ...,  2.7787e-02,
          2.4348e-02,  4.5742e-02],
        [ 7.0037e-02, -5.8279e-02,  7.0575e-03,  ..., -6.6567e-02,
         -5.3157e-02,  8.7667e-02]])

encoder.encoders.3.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([ 0.0157,  0.0009,  0.0035,  ..., -0.0367, -0.0486, -0.0147])

encoder.encoders.3.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0552,  0.0790, -0.0207,  ...,  0.0859, -0.0695,  0.0115],
        [-0.0330,  0.0600, -0.0173,  ..., -0.0185, -0.0184, -0.0282],
        [-0.0686,  0.0078,  0.0241,  ...,  0.0308, -0.0698, -0.0639],
        ...,
        [ 0.0549, -0.0614,  0.0553,  ..., -0.0625,  0.0244,  0.0015],
        [-0.1313, -0.1035, -0.0774,  ..., -0.0837,  0.0553,  0.0164],
        [ 0.0381, -0.0293, -0.0405,  ..., -0.0480,  0.0208,  0.0172]])

encoder.encoders.3.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([ 0.0194, -0.0285, -0.0020,  ..., -0.0045,  0.0025, -0.0490])

encoder.encoders.3.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[-0.0513,  0.0057, -0.0441,  ..., -0.0438, -0.0377,  0.0443],
        [ 0.0013,  0.0049,  0.0501,  ..., -0.0411,  0.0237,  0.0577],
        [ 0.0426,  0.0116,  0.0189,  ..., -0.0460,  0.0044,  0.0008],
        ...,
        [-0.0584, -0.1276,  0.0250,  ..., -0.0059, -0.0494, -0.0341],
        [-0.0170,  0.0458,  0.0223,  ...,  0.0021, -0.0135,  0.0479],
        [ 0.0076,  0.0789, -0.0241,  ..., -0.0052, -0.0180,  0.0180]])

encoder.encoders.3.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([ 0.0397, -0.0164,  0.0065,  ..., -0.0548, -0.0187, -0.0335])

encoder.encoders.3.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[ 0.0386],
         [-0.1047],
         [-0.0003],
         ...,
         [-0.0097],
         [-0.0579],
         [-0.0111]],

        [[ 0.0311],
         [-0.0651],
         [ 0.0033],
         ...,
         [ 0.0354],
         [-0.0512],
         [ 0.0236]],

        [[-0.1059],
         [-0.0486],
         [-0.0624],
         ...,
         [-0.0261],
         [-0.0157],
         [-0.0200]],

        ...,

        [[ 0.0153],
         [ 0.0455],
         [-0.0396],
         ...,
         [ 0.1140],
         [-0.0332],
         [-0.0207]],

        [[-0.0253],
         [ 0.0385],
         [ 0.0079],
         ...,
         [ 0.0266],
         [-0.0118],
         [ 0.0360]],

        [[ 0.0292],
         [ 0.0798],
         [ 0.0055],
         ...,
         [ 0.1437],
         [ 0.0370],
         [ 0.0373]]])

encoder.encoders.3.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[-4.2038e-02, -1.5815e-02, -3.6948e-02,  ...,  3.2023e-03,
           1.0967e-02, -2.9246e-03]],

        [[ 8.1775e-02,  3.5579e-02,  6.0533e-02,  ...,  4.0014e-02,
           5.0869e-02,  4.8693e-02]],

        [[-7.4076e-03, -3.2345e-02, -1.1618e-02,  ..., -2.4148e-04,
          -1.6054e-03, -9.5252e-03]],

        ...,

        [[-2.8248e-05, -1.0297e-02, -1.5272e-03,  ...,  4.9337e-03,
          -1.8899e-03,  1.4705e-03]],

        [[ 3.8124e-02, -3.4600e-03,  2.2240e-02,  ...,  3.1774e-02,
           2.5257e-02,  3.5376e-02]],

        [[-3.0502e-03, -1.5521e-03, -7.3439e-03,  ...,  1.0609e-03,
           9.8815e-04, -5.1526e-03]]])

encoder.encoders.3.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([1.0540, 1.0657, 0.9569,  ..., 0.9935, 0.9879, 1.0167])

encoder.encoders.3.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.1336, -0.0381, -0.0857,  ..., -0.0714, -0.0320, -0.0644])

encoder.encoders.3.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[-0.0285],
         [ 0.0362],
         [ 0.0284],
         ...,
         [-0.0334],
         [ 0.0024],
         [ 0.0034]],

        [[ 0.0024],
         [ 0.0229],
         [-0.0252],
         ...,
         [-0.0685],
         [-0.0735],
         [-0.0231]],

        [[ 0.0427],
         [ 0.0303],
         [-0.0186],
         ...,
         [ 0.0399],
         [ 0.0451],
         [-0.0532]],

        ...,

        [[ 0.0581],
         [ 0.0377],
         [ 0.0440],
         ...,
         [-0.0345],
         [-0.0067],
         [ 0.0542]],

        [[ 0.0530],
         [ 0.0489],
         [ 0.0103],
         ...,
         [-0.0893],
         [-0.0341],
         [-0.0789]],

        [[-0.0437],
         [-0.0710],
         [ 0.0382],
         ...,
         [-0.0258],
         [ 0.0182],
         [-0.0485]]])

encoder.encoders.3.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([0.9173, 0.9687, 0.9269,  ..., 1.0471, 1.1044, 0.9652])

encoder.encoders.3.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([ 0.0695, -0.0284,  0.0393,  ..., -0.2637, -0.2849, -0.1814])

encoder.encoders.3.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([1.0175, 0.9146, 1.0195,  ..., 1.0002, 1.0310, 0.9864])

encoder.encoders.3.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([ 0.1556, -0.0614,  0.0728,  ..., -0.0401,  0.0235, -0.1075])

encoder.encoders.3.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([1.0189, 1.0326, 1.0623,  ..., 1.0735, 1.1217, 1.0149])

encoder.encoders.3.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([ 0.1153, -0.1529,  0.0277,  ..., -0.0744, -0.1537, -0.1745])

encoder.encoders.3.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.3967, 1.2477, 1.2342,  ..., 1.3011, 1.2307, 1.3351])

encoder.encoders.3.norm_final.bias-torch.Size([1280])-torch.float32
tensor([ 0.0521, -0.0196,  0.0422,  ..., -0.1820, -0.0795, -0.1355])

encoder.encoders.4.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[ 0.1246,  0.1110, -0.0251,  ..., -0.2574,  0.2213, -0.1251],
        [-0.0594, -0.0235,  0.0192,  ...,  0.1818,  0.2199, -0.0487],
        [-0.1530,  0.1871,  0.2539,  ...,  0.1766, -0.1106,  0.0557],
        ...,
        [ 0.0597,  0.1722, -0.1239,  ...,  0.2450, -0.0565, -0.0402],
        [ 0.0942,  0.0419,  0.0790,  ..., -0.1949,  0.0993,  0.0066],
        [-0.2370,  0.0235,  0.0410,  ...,  0.2872, -0.1436, -0.2455]])

encoder.encoders.4.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[-0.0942, -0.2785,  0.1971,  ..., -0.0317,  0.0932, -0.2032],
        [-0.0621,  0.0154,  0.0085,  ...,  0.0995,  0.1714, -0.0792],
        [ 0.2443, -0.2369,  0.2462,  ...,  0.2548,  0.1130,  0.2206],
        ...,
        [ 0.0356,  0.1395,  0.1207,  ..., -0.2266, -0.0674,  0.0603],
        [-0.2110, -0.0749,  0.1516,  ...,  0.2192,  0.2750,  0.1941],
        [ 0.0969, -0.3149, -0.2249,  ...,  0.0159,  0.0391, -0.0731]])

encoder.encoders.4.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-4.8144e-02,  7.1130e-02,  1.1305e-02,  ...,  6.9272e-05,
         -2.7959e-02,  3.9879e-02],
        [ 2.4948e-03,  2.0409e-02, -2.7741e-02,  ...,  1.1439e-02,
         -2.3331e-02, -5.9986e-03],
        [-6.6972e-04, -6.8414e-02,  1.4489e-02,  ...,  1.1544e-02,
         -2.9906e-03,  1.2450e-03],
        ...,
        [ 4.9123e-02,  4.4579e-02,  4.1092e-02,  ...,  4.1027e-02,
         -1.7969e-02,  1.0715e-02],
        [ 2.7682e-02, -1.1098e-01,  7.1311e-03,  ..., -9.3237e-04,
         -1.0077e-01,  4.1289e-03],
        [-3.6523e-02, -2.1837e-02,  3.3072e-02,  ...,  1.4093e-02,
          4.4023e-02, -6.2216e-02]])

encoder.encoders.4.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0135,  0.0253,  0.0983,  ...,  0.0495,  0.0075,  0.0337],
        [-0.0089,  0.0255,  0.0077,  ..., -0.0461,  0.0036, -0.0524],
        [-0.1020, -0.0241,  0.0012,  ...,  0.0735, -0.0076, -0.0106],
        ...,
        [-0.0010, -0.0357, -0.0389,  ..., -0.0156,  0.0937, -0.0378],
        [-0.0211,  0.0355, -0.0199,  ..., -0.0997,  0.0333,  0.0105],
        [ 0.0539, -0.0070, -0.0241,  ..., -0.1030, -0.0942, -0.0818]])

encoder.encoders.4.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0221, -0.0121,  0.0427,  ..., -0.0033,  0.0987,  0.0244],
        [-0.0012,  0.0344,  0.0288,  ...,  0.0332,  0.0095, -0.0248],
        [-0.0374,  0.0854,  0.0014,  ..., -0.0732,  0.0136,  0.0216],
        ...,
        [ 0.0049,  0.0332, -0.0514,  ...,  0.0142,  0.0192, -0.0207],
        [ 0.0004, -0.0154, -0.0085,  ...,  0.0068,  0.0483,  0.0374],
        [-0.0528, -0.0458, -0.0181,  ...,  0.0294, -0.0299, -0.0015]])

encoder.encoders.4.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0513,  0.0446, -0.0151,  ..., -0.0063, -0.0281,  0.0124],
        [-0.0528,  0.0051,  0.0057,  ...,  0.0164,  0.0449, -0.0077],
        [-0.0170, -0.0026, -0.0068,  ...,  0.0117,  0.0400, -0.0166],
        ...,
        [ 0.0271,  0.0239, -0.0074,  ...,  0.0383,  0.0195,  0.0142],
        [-0.0440, -0.0309, -0.0354,  ...,  0.0130, -0.0015,  0.0415],
        [-0.0463, -0.0111,  0.0453,  ...,  0.0155, -0.0939, -0.0378]])

encoder.encoders.4.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0462, -0.0038,  0.0240,  ...,  0.0104, -0.0359, -0.0049],
        [ 0.0610,  0.0755,  0.0817,  ...,  0.0103, -0.0215, -0.0297],
        [ 0.0536,  0.0740,  0.0050,  ..., -0.0404,  0.0248, -0.0052],
        ...,
        [-0.0655, -0.0359, -0.0485,  ..., -0.0317,  0.0447, -0.0075],
        [ 0.0915,  0.0326,  0.0690,  ..., -0.0187, -0.0342,  0.0289],
        [ 0.0142,  0.0208,  0.0096,  ..., -0.0046, -0.0300, -0.0214]])

encoder.encoders.4.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.3526, 0.3561, 0.3949,  ..., 0.4588, 0.3917, 0.3815])

encoder.encoders.4.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([-0.0118,  0.0293, -0.0249,  ...,  0.0720, -0.0088,  0.1689])

encoder.encoders.4.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.5475, 0.6338, 0.5610,  ..., 0.6258, 0.6340, 0.6224])

encoder.encoders.4.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([ 0.0219, -0.0429, -0.0092,  ..., -0.0067,  0.0169, -0.0148])

encoder.encoders.4.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.4991, 0.6241, 0.5790,  ..., 0.5945, 0.5694, 0.5390])

encoder.encoders.4.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([-0.0516,  0.0296, -0.0681,  ...,  0.1565,  0.0585,  0.2286])

encoder.encoders.4.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0635,  0.0128,  0.0158,  ..., -0.0361, -0.0946,  0.0415],
        [-0.0545,  0.0875,  0.0378,  ..., -0.0608,  0.0103,  0.0216],
        [ 0.0250, -0.0611, -0.0281,  ..., -0.0064,  0.0494, -0.0003],
        ...,
        [-0.0513, -0.0707,  0.0108,  ..., -0.0188, -0.0742,  0.0676],
        [ 0.0143, -0.0127, -0.0111,  ...,  0.0403, -0.0125,  0.0358],
        [ 0.0231, -0.0126, -0.0520,  ..., -0.0312,  0.1022,  0.0218]])

encoder.encoders.4.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([ 0.0050, -0.0097, -0.0222,  ...,  0.0137, -0.0113, -0.0406])

encoder.encoders.4.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0438, -0.0113,  0.0839,  ...,  0.1086, -0.0124,  0.0211],
        [ 0.0050,  0.0269, -0.0158,  ...,  0.0110, -0.0122,  0.0553],
        [ 0.0398, -0.0342, -0.0208,  ...,  0.0334, -0.0180, -0.0081],
        ...,
        [-0.0476,  0.0701,  0.0304,  ..., -0.0187,  0.0016,  0.0099],
        [ 0.0547,  0.0215, -0.0045,  ...,  0.0499,  0.0340, -0.0189],
        [ 0.1181, -0.0544, -0.0945,  ..., -0.0594, -0.0239,  0.0073]])

encoder.encoders.4.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([ 0.0081, -0.0050,  0.0134,  ..., -0.0702, -0.0131, -0.0164])

encoder.encoders.4.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0234, -0.1032,  0.0204,  ...,  0.0424,  0.0307,  0.0354],
        [-0.0557, -0.0394, -0.0675,  ...,  0.0436, -0.0121, -0.0543],
        [-0.0924, -0.0517,  0.0079,  ...,  0.0517, -0.0198,  0.0758],
        ...,
        [ 0.0158, -0.0289,  0.0332,  ...,  0.0207, -0.0108, -0.0344],
        [ 0.0119,  0.0676, -0.0091,  ..., -0.0186,  0.0415,  0.0133],
        [ 0.0143,  0.0091,  0.0186,  ...,  0.0145, -0.0281,  0.0434]])

encoder.encoders.4.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0111, -0.0022, -0.0058,  ..., -0.0378, -0.0204, -0.0149])

encoder.encoders.4.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0175,  0.0150, -0.0334,  ...,  0.0195, -0.0226,  0.0743],
        [-0.0399, -0.0708,  0.0769,  ...,  0.0304,  0.0207,  0.0046],
        [-0.0313,  0.0583,  0.0054,  ..., -0.0032, -0.0017, -0.0147],
        ...,
        [-0.0040,  0.0023,  0.0287,  ...,  0.0236, -0.0780, -0.0115],
        [ 0.0642,  0.0061,  0.0536,  ..., -0.0604,  0.0017, -0.0035],
        [ 0.0310, -0.0174,  0.0121,  ..., -0.0066, -0.0152,  0.0338]])

encoder.encoders.4.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([ 0.0287, -0.0144, -0.0024,  ..., -0.0643, -0.0265, -0.0341])

encoder.encoders.4.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[-0.0597],
         [ 0.0137],
         [-0.0688],
         ...,
         [-0.0339],
         [-0.0443],
         [-0.0032]],

        [[-0.0988],
         [-0.0233],
         [ 0.0139],
         ...,
         [ 0.0595],
         [-0.0408],
         [ 0.0349]],

        [[-0.0745],
         [-0.0078],
         [-0.0514],
         ...,
         [-0.0471],
         [ 0.0802],
         [ 0.0834]],

        ...,

        [[ 0.1359],
         [ 0.0338],
         [-0.0285],
         ...,
         [ 0.0028],
         [ 0.0329],
         [ 0.0450]],

        [[ 0.0374],
         [ 0.0253],
         [ 0.0239],
         ...,
         [-0.0226],
         [-0.0546],
         [-0.0010]],

        [[-0.0236],
         [ 0.0693],
         [ 0.0509],
         ...,
         [-0.0016],
         [ 0.0362],
         [ 0.0250]]])

encoder.encoders.4.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[ 0.0395,  0.0260,  0.0223,  ..., -0.0025, -0.0068, -0.0196]],

        [[ 0.1274,  0.0969,  0.0852,  ...,  0.0611,  0.0700,  0.1095]],

        [[ 0.0829,  0.0346,  0.0545,  ...,  0.0549,  0.0486,  0.0847]],

        ...,

        [[ 0.0052, -0.0121, -0.0015,  ..., -0.0034, -0.0049, -0.0073]],

        [[-0.0059,  0.0062, -0.0087,  ...,  0.0101, -0.0035,  0.0067]],

        [[-0.0231, -0.0384, -0.0010,  ...,  0.0255,  0.0216,  0.0577]]])

encoder.encoders.4.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([1.0198, 1.0882, 1.0286,  ..., 1.0525, 1.0188, 0.9578])

encoder.encoders.4.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.0576, -0.0324, -0.0346,  ..., -0.0630, -0.0869, -0.0531])

encoder.encoders.4.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[-0.0107],
         [-0.1046],
         [ 0.0024],
         ...,
         [-0.0457],
         [-0.0096],
         [-0.0667]],

        [[ 0.0590],
         [-0.0107],
         [ 0.0456],
         ...,
         [ 0.0366],
         [-0.0007],
         [-0.0286]],

        [[-0.0002],
         [-0.0485],
         [-0.0289],
         ...,
         [-0.0195],
         [ 0.0451],
         [ 0.1047]],

        ...,

        [[-0.0203],
         [ 0.0130],
         [-0.0192],
         ...,
         [-0.0286],
         [ 0.0979],
         [ 0.0247]],

        [[ 0.0610],
         [ 0.0689],
         [-0.0924],
         ...,
         [-0.0529],
         [-0.0466],
         [ 0.0024]],

        [[ 0.0212],
         [ 0.0413],
         [ 0.1119],
         ...,
         [-0.0166],
         [-0.0456],
         [ 0.0164]]])

encoder.encoders.4.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([0.8105, 0.9432, 0.9347,  ..., 1.1014, 0.9712, 1.0245])

encoder.encoders.4.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([ 0.1682,  0.0188,  0.1335,  ..., -0.2498, -0.0964, -0.2129])

encoder.encoders.4.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([0.8447, 0.9564, 0.9513,  ..., 1.1055, 1.0195, 0.9153])

encoder.encoders.4.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([ 0.0636, -0.0480,  0.1009,  ..., -0.2449, -0.1429, -0.0904])

encoder.encoders.4.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([0.8659, 1.0695, 1.0186,  ..., 1.0806, 1.0655, 0.9950])

encoder.encoders.4.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([ 0.0990, -0.0753,  0.0211,  ..., -0.0834, -0.1545, -0.1118])

encoder.encoders.4.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.3805, 1.2672, 1.2233,  ..., 1.3044, 1.2448, 1.2455])

encoder.encoders.4.norm_final.bias-torch.Size([1280])-torch.float32
tensor([ 0.0560, -0.0141,  0.0564,  ..., -0.2319, -0.0607, -0.0542])

encoder.encoders.5.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[ 1.4916e-01,  7.9902e-02,  5.4409e-05,  ..., -2.4824e-01,
          2.0559e-01,  1.0363e-01],
        [-3.3817e-01, -1.7686e-01,  2.0932e-02,  ..., -6.2848e-03,
         -1.4766e-01, -8.3028e-02],
        [ 1.7736e-01, -2.1702e-01, -1.7083e-01,  ...,  1.4651e-01,
          5.9234e-03,  4.4594e-02],
        ...,
        [ 1.5784e-01,  9.4733e-02, -2.7006e-01,  ...,  1.2773e-01,
         -5.3201e-02,  1.1929e-01],
        [ 2.2981e-01, -1.0486e-01, -1.6498e-01,  ...,  1.3418e-01,
         -1.6286e-01, -2.0702e-01],
        [ 2.6109e-01, -9.7622e-02, -5.9776e-04,  ..., -2.5111e-01,
         -3.8664e-02, -2.1269e-01]])

encoder.encoders.5.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[ 0.0943, -0.4294,  0.2911,  ...,  0.1117, -0.3233, -0.5141],
        [ 0.0488,  0.1591, -0.0930,  ...,  0.1234,  0.0526,  0.2369],
        [ 0.0531,  0.0470, -0.2825,  ..., -0.0060, -0.0216, -0.3752],
        ...,
        [-0.4397, -0.2175, -0.0245,  ...,  0.2723, -0.1483, -0.0089],
        [-0.2810,  0.2240,  0.2306,  ...,  0.1352,  0.2343, -0.3003],
        [ 0.0978,  0.2696, -0.2259,  ...,  0.3104, -0.1461,  0.2452]])

encoder.encoders.5.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0685, -0.0405,  0.0428,  ...,  0.0221,  0.0256,  0.0972],
        [ 0.0011, -0.0373, -0.0610,  ...,  0.0400, -0.0710,  0.0157],
        [-0.0556, -0.0083, -0.0582,  ...,  0.0005, -0.0423,  0.0529],
        ...,
        [-0.0763,  0.0101, -0.0427,  ...,  0.0427, -0.0847,  0.0211],
        [-0.0044, -0.0821, -0.0083,  ...,  0.0351, -0.0258,  0.0422],
        [-0.0095,  0.0281, -0.1075,  ..., -0.0262, -0.0666, -0.0531]])

encoder.encoders.5.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0191, -0.0295,  0.0627,  ..., -0.0139, -0.0187,  0.0184],
        [-0.1071,  0.0700,  0.0692,  ..., -0.0312,  0.0409,  0.0017],
        [ 0.0053, -0.0028,  0.0510,  ..., -0.0429, -0.0056,  0.0160],
        ...,
        [-0.0216, -0.0185,  0.0035,  ...,  0.0535,  0.0256,  0.0323],
        [ 0.0006, -0.0458, -0.0212,  ...,  0.0281, -0.0192,  0.0004],
        [ 0.0134,  0.0516,  0.0782,  ..., -0.0475, -0.0478,  0.0274]])

encoder.encoders.5.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0663, -0.0625,  0.0211,  ..., -0.0235,  0.0505, -0.0443],
        [ 0.0088,  0.0324,  0.0036,  ..., -0.0246, -0.0285, -0.0069],
        [ 0.0334,  0.0476, -0.0482,  ..., -0.0230,  0.0279, -0.0630],
        ...,
        [ 0.0140,  0.0896, -0.0093,  ...,  0.0187,  0.0236, -0.0045],
        [ 0.0162,  0.0628, -0.0375,  ..., -0.0120, -0.0327,  0.0541],
        [-0.0110, -0.0244,  0.0098,  ...,  0.0087,  0.0467, -0.0159]])

encoder.encoders.5.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0223, -0.0138, -0.0159,  ..., -0.0005,  0.0130, -0.0238],
        [-0.0050,  0.0272, -0.0007,  ...,  0.0385,  0.0195,  0.0173],
        [ 0.0043, -0.0055, -0.0138,  ...,  0.0174, -0.0093, -0.0421],
        ...,
        [ 0.0517, -0.0275, -0.0389,  ..., -0.0196, -0.0453, -0.0102],
        [ 0.0146, -0.0490,  0.0410,  ...,  0.0745,  0.0464,  0.0301],
        [ 0.0404, -0.0468,  0.0342,  ..., -0.0132, -0.0150,  0.0284]])

encoder.encoders.5.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0167,  0.0041,  0.0314,  ...,  0.0079,  0.0131,  0.0537],
        [ 0.0894, -0.0340,  0.1202,  ..., -0.0026, -0.0144, -0.0137],
        [ 0.0262,  0.1350, -0.0691,  ...,  0.0452, -0.0219,  0.0161],
        ...,
        [-0.0444,  0.0239, -0.0944,  ..., -0.0105,  0.0371, -0.0185],
        [-0.0595,  0.0178, -0.0070,  ...,  0.0164, -0.0100,  0.0397],
        [ 0.0052,  0.0347, -0.0123,  ...,  0.0089, -0.0179, -0.0007]])

encoder.encoders.5.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.6378, 0.7791, 0.6775,  ..., 0.6529, 0.6745, 0.7084])

encoder.encoders.5.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([-0.0665,  0.0077, -0.0573,  ...,  0.0754,  0.0250,  0.0339])

encoder.encoders.5.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.6789, 0.7560, 0.7479,  ..., 0.7662, 0.6869, 0.8319])

encoder.encoders.5.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([ 0.0150,  0.0542,  0.0014,  ...,  0.0531, -0.0139,  0.0580])

encoder.encoders.5.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.6034, 0.6163, 0.6125,  ..., 0.5819, 0.5932, 0.6461])

encoder.encoders.5.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([-0.0747,  0.0145, -0.0611,  ...,  0.1415,  0.0313,  0.0478])

encoder.encoders.5.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 0.0097,  0.0871,  0.0504,  ...,  0.0173,  0.0676,  0.0959],
        [-0.0259, -0.0252,  0.1060,  ..., -0.0231, -0.0798, -0.0374],
        [-0.0415, -0.0282, -0.0212,  ..., -0.0024, -0.0692,  0.0080],
        ...,
        [-0.0351, -0.0147,  0.0472,  ...,  0.0090, -0.0041, -0.0177],
        [-0.0420, -0.0580,  0.0146,  ...,  0.0508,  0.0828, -0.0188],
        [ 0.0040, -0.0566, -0.1094,  ..., -0.0517,  0.1026, -0.0445]])

encoder.encoders.5.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0289, -0.0099, -0.0026,  ..., -0.0208, -0.0292, -0.0328])

encoder.encoders.5.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0319,  0.0147, -0.0685,  ..., -0.0505,  0.0129,  0.0339],
        [-0.0554,  0.0025,  0.0263,  ..., -0.0135,  0.0084,  0.0172],
        [ 0.0208,  0.1263, -0.0105,  ..., -0.0071,  0.0125,  0.0020],
        ...,
        [ 0.0207, -0.0054,  0.0204,  ...,  0.0371, -0.0220, -0.0098],
        [-0.0455,  0.0101, -0.0240,  ...,  0.0380,  0.0474, -0.0136],
        [ 0.0351,  0.0342,  0.0287,  ..., -0.0661, -0.0048, -0.0496]])

encoder.encoders.5.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0059, -0.0121, -0.0057,  ..., -0.0434,  0.0018,  0.0229])

encoder.encoders.5.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-1.2205e-02, -7.0109e-02,  3.4512e-02,  ...,  1.2375e-03,
         -3.4559e-02,  2.6807e-02],
        [ 2.6943e-02,  1.3581e-02,  2.4263e-03,  ...,  3.4035e-02,
          6.4367e-02,  6.4497e-02],
        [-1.1088e-02, -8.2016e-02,  6.7622e-02,  ..., -2.7228e-02,
          2.0171e-02,  1.2771e-02],
        ...,
        [-2.2296e-03,  7.8287e-02, -6.6517e-02,  ...,  9.2770e-02,
         -2.6002e-02,  8.6361e-02],
        [ 5.0213e-02,  1.2361e-02, -2.3314e-02,  ..., -8.5673e-04,
         -3.6500e-02, -5.1121e-03],
        [ 3.2760e-03,  7.8602e-02, -1.6397e-01,  ..., -7.0500e-02,
          1.1878e-02, -6.3480e-05]])

encoder.encoders.5.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0587, -0.0280, -0.0022,  ..., -0.0181, -0.0511,  0.0015])

encoder.encoders.5.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[-0.0590,  0.0118,  0.0131,  ..., -0.0182, -0.0096, -0.0256],
        [ 0.0550, -0.0459, -0.0190,  ...,  0.0005, -0.0148,  0.0682],
        [-0.0234, -0.0389,  0.0115,  ..., -0.0401,  0.0104, -0.0143],
        ...,
        [ 0.0311,  0.0574, -0.0221,  ...,  0.0044, -0.0011, -0.0331],
        [-0.0184,  0.0712,  0.0676,  ...,  0.0643, -0.0196,  0.0139],
        [-0.0614,  0.0139, -0.0130,  ..., -0.0754,  0.0491,  0.0218]])

encoder.encoders.5.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([ 0.0013, -0.0151,  0.0280,  ..., -0.0801, -0.0319, -0.0139])

encoder.encoders.5.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[ 0.1418],
         [ 0.0739],
         [-0.0012],
         ...,
         [-0.0021],
         [-0.0901],
         [ 0.0566]],

        [[ 0.0475],
         [ 0.0366],
         [-0.0257],
         ...,
         [ 0.0237],
         [ 0.0630],
         [ 0.0181]],

        [[-0.0180],
         [-0.0568],
         [-0.0363],
         ...,
         [-0.0665],
         [-0.0038],
         [ 0.0595]],

        ...,

        [[ 0.0100],
         [ 0.0236],
         [ 0.0566],
         ...,
         [ 0.1158],
         [-0.0353],
         [ 0.0015]],

        [[-0.0363],
         [-0.0765],
         [-0.0352],
         ...,
         [-0.0006],
         [ 0.0404],
         [ 0.0802]],

        [[-0.0040],
         [ 0.0008],
         [-0.0545],
         ...,
         [ 0.0584],
         [-0.0255],
         [ 0.0463]]])

encoder.encoders.5.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[-0.0361,  0.0036, -0.0121,  ..., -0.0123, -0.0098, -0.0029]],

        [[-0.0289, -0.0011,  0.0130,  ...,  0.0177,  0.0085,  0.0172]],

        [[-0.0015,  0.0083, -0.0065,  ..., -0.0094,  0.0153, -0.0007]],

        ...,

        [[-0.0028, -0.0032,  0.0021,  ...,  0.0012, -0.0174,  0.0151]],

        [[-0.0105, -0.0018,  0.0178,  ..., -0.0166, -0.0122, -0.0050]],

        [[-0.0086,  0.0018,  0.0024,  ...,  0.0044, -0.0008,  0.0107]]])

encoder.encoders.5.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([1.1776, 1.0164, 1.0939,  ..., 1.0462, 1.0063, 1.0055])

encoder.encoders.5.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.0607, -0.0762, -0.0621,  ..., -0.0619, -0.0756, -0.0893])

encoder.encoders.5.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[ 0.0184],
         [ 0.0070],
         [ 0.0372],
         ...,
         [ 0.0189],
         [ 0.0841],
         [ 0.0067]],

        [[-0.1435],
         [-0.0225],
         [-0.0406],
         ...,
         [ 0.0708],
         [ 0.0056],
         [-0.0217]],

        [[-0.0319],
         [ 0.0004],
         [ 0.0286],
         ...,
         [ 0.0462],
         [ 0.0315],
         [ 0.0467]],

        ...,

        [[ 0.0996],
         [ 0.0387],
         [ 0.0129],
         ...,
         [-0.0249],
         [-0.0395],
         [-0.0134]],

        [[ 0.0191],
         [-0.1037],
         [-0.0809],
         ...,
         [ 0.0015],
         [-0.0269],
         [ 0.0141]],

        [[ 0.0971],
         [-0.0106],
         [ 0.0217],
         ...,
         [ 0.0290],
         [ 0.0215],
         [-0.0192]]])

encoder.encoders.5.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([0.8495, 0.9976, 1.0196,  ..., 1.2646, 0.9822, 1.1243])

encoder.encoders.5.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([ 0.1780, -0.0165,  0.0455,  ..., -0.3698,  0.0022,  0.2537])

encoder.encoders.5.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([0.7034, 0.8899, 0.8951,  ..., 1.0659, 0.9349, 0.9433])

encoder.encoders.5.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([ 0.1092,  0.0020,  0.0804,  ..., -0.2465, -0.1083, -0.1654])

encoder.encoders.5.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([0.8015, 1.1225, 1.1691,  ..., 1.1727, 1.0473, 1.1460])

encoder.encoders.5.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([ 0.0029, -0.0653,  0.0965,  ..., -0.1377, -0.1050,  0.0236])

encoder.encoders.5.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.3615, 1.2988, 1.2716,  ..., 1.3306, 1.2980, 1.2839])

encoder.encoders.5.norm_final.bias-torch.Size([1280])-torch.float32
tensor([ 0.0172, -0.0192,  0.0327,  ..., -0.1555, -0.0220,  0.0770])

encoder.encoders.6.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[ 0.1960, -0.1364,  0.0906,  ...,  0.0464,  0.1023,  0.2171],
        [ 0.2215, -0.1564, -0.1166,  ...,  0.0908,  0.2041,  0.2038],
        [-0.2173, -0.0973, -0.0331,  ...,  0.0349,  0.2273, -0.2146],
        ...,
        [-0.1451,  0.0402,  0.1561,  ...,  0.2124,  0.3020, -0.2398],
        [-0.1882,  0.2250, -0.3197,  ...,  0.1280, -0.0264, -0.1437],
        [ 0.0523,  0.0534,  0.2477,  ...,  0.0252, -0.1920,  0.1603]])

encoder.encoders.6.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[-0.2961, -0.2722, -0.0614,  ...,  0.1612, -0.0256,  0.0977],
        [ 0.0501, -0.1756,  0.2936,  ...,  0.0178,  0.1963, -0.1418],
        [-0.1169, -0.2146,  0.2068,  ..., -0.0503,  0.0377, -0.3558],
        ...,
        [ 0.1323,  0.1819, -0.0098,  ..., -0.2043, -0.1472,  0.2114],
        [ 0.1386,  0.1372, -0.1065,  ...,  0.0218, -0.0372, -0.1201],
        [ 0.0342,  0.1006, -0.2139,  ...,  0.2318,  0.3462, -0.0818]])

encoder.encoders.6.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0970, -0.0128, -0.0188,  ..., -0.0309,  0.0918,  0.0039],
        [ 0.0629, -0.0016,  0.0034,  ..., -0.0475,  0.0066, -0.0055],
        [ 0.0640,  0.0083, -0.0685,  ..., -0.0296, -0.0388,  0.0330],
        ...,
        [ 0.0283, -0.0332, -0.0343,  ..., -0.0025, -0.0214,  0.0339],
        [-0.0844,  0.0116,  0.0620,  ...,  0.0099,  0.0285,  0.0015],
        [-0.0268, -0.0597, -0.0228,  ..., -0.0138,  0.0111,  0.0324]])

encoder.encoders.6.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0271, -0.0054, -0.0274,  ...,  0.0318, -0.0046, -0.0591],
        [-0.0071, -0.0063, -0.0024,  ..., -0.0423,  0.0133, -0.0269],
        [-0.0208,  0.0377, -0.0149,  ..., -0.0213, -0.0220,  0.0749],
        ...,
        [ 0.0691,  0.0009,  0.0102,  ...,  0.0422, -0.0265,  0.0296],
        [ 0.0505,  0.0431, -0.0338,  ...,  0.0167, -0.0290,  0.0482],
        [ 0.0276,  0.0333,  0.0576,  ...,  0.0016, -0.0354,  0.0880]])

encoder.encoders.6.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0618, -0.0796,  0.0450,  ...,  0.0068,  0.0294, -0.0106],
        [-0.0624, -0.0335,  0.0423,  ..., -0.0482,  0.0383, -0.0084],
        [-0.0148,  0.1457, -0.0376,  ...,  0.0319,  0.0731, -0.0722],
        ...,
        [ 0.0411,  0.0141, -0.0099,  ..., -0.0227,  0.0396, -0.0506],
        [ 0.0203, -0.0391, -0.0309,  ...,  0.0370, -0.0583, -0.0220],
        [-0.0286, -0.0231, -0.0304,  ..., -0.0630,  0.0643, -0.0361]])

encoder.encoders.6.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0128,  0.0311,  0.0698,  ..., -0.0556,  0.0013,  0.0384],
        [ 0.0616, -0.0770, -0.0466,  ..., -0.0010, -0.0350,  0.0385],
        [-0.0538,  0.0314, -0.0269,  ...,  0.0093, -0.0257,  0.0404],
        ...,
        [-0.0699, -0.0274, -0.0447,  ...,  0.0250,  0.0487,  0.0201],
        [ 0.0032,  0.0654,  0.0432,  ...,  0.0010,  0.0544,  0.0314],
        [-0.0269,  0.0309,  0.1122,  ..., -0.0327,  0.0036, -0.0188]])

encoder.encoders.6.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0320, -0.0338, -0.0765,  ..., -0.0328, -0.0347,  0.0386],
        [-0.0362,  0.0492, -0.0260,  ...,  0.0440, -0.0434, -0.0213],
        [ 0.0288,  0.0338,  0.0114,  ..., -0.0399, -0.0044, -0.0240],
        ...,
        [-0.0894, -0.0651, -0.0421,  ...,  0.0058,  0.0622,  0.0262],
        [-0.1186, -0.0067, -0.1207,  ...,  0.0055, -0.0208,  0.0190],
        [ 0.0453,  0.0421,  0.0338,  ...,  0.0203, -0.0281, -0.0023]])

encoder.encoders.6.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.6783, 0.6661, 0.6689,  ..., 0.6006, 0.6146, 0.6214])

encoder.encoders.6.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([-0.0833, -0.0381, -0.0216,  ...,  0.0397,  0.0269, -0.0347])

encoder.encoders.6.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.7592, 0.7869, 0.7561,  ..., 0.8088, 0.7807, 0.7776])

encoder.encoders.6.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([ 0.0729, -0.0020,  0.0278,  ...,  0.0342, -0.0354,  0.0489])

encoder.encoders.6.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.6678, 0.6721, 0.6323,  ..., 0.5982, 0.5903, 0.6650])

encoder.encoders.6.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([-0.0609, -0.0210, -0.0398,  ...,  0.1471,  0.0247, -0.0598])

encoder.encoders.6.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 0.0513,  0.0190, -0.0661,  ...,  0.0281, -0.0737,  0.0216],
        [-0.0718,  0.0216,  0.0002,  ..., -0.0258,  0.1094,  0.0171],
        [-0.0678, -0.0253,  0.0163,  ..., -0.0735, -0.0961, -0.0595],
        ...,
        [-0.0183, -0.0467,  0.0206,  ...,  0.0078,  0.0126,  0.0808],
        [-0.0706, -0.0011, -0.0231,  ...,  0.0826,  0.0026, -0.0235],
        [-0.0428, -0.0266,  0.0200,  ...,  0.0488, -0.0569, -0.0162]])

encoder.encoders.6.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0286,  0.0037, -0.0138,  ..., -0.0338, -0.0233, -0.0415])

encoder.encoders.6.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[-0.0329, -0.0243,  0.0116,  ..., -0.0832,  0.0375,  0.0411],
        [ 0.0750,  0.0048,  0.0480,  ...,  0.0316,  0.0109, -0.0428],
        [-0.0466,  0.0407, -0.1163,  ...,  0.0629,  0.0149,  0.0207],
        ...,
        [ 0.0341, -0.0445, -0.0099,  ..., -0.0065, -0.0412,  0.0229],
        [ 0.0341,  0.0336,  0.0207,  ..., -0.0095,  0.0059, -0.0306],
        [ 0.0157,  0.0908,  0.0246,  ...,  0.0010, -0.0379,  0.0696]])

encoder.encoders.6.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([ 0.0118, -0.0200, -0.0052,  ..., -0.0133, -0.0150,  0.0041])

encoder.encoders.6.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 0.0034, -0.1316, -0.0437,  ...,  0.0412,  0.0194,  0.0256],
        [-0.0043,  0.0492,  0.0013,  ...,  0.0110, -0.0858, -0.0118],
        [-0.0329,  0.0247, -0.0278,  ...,  0.0210,  0.0714,  0.0026],
        ...,
        [ 0.0154,  0.0521,  0.0112,  ...,  0.0466,  0.0602,  0.0306],
        [ 0.0571, -0.0799, -0.0171,  ...,  0.0988, -0.0744, -0.0953],
        [-0.0586,  0.0495, -0.1399,  ...,  0.0462,  0.0139, -0.1243]])

encoder.encoders.6.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0162, -0.0278, -0.0082,  ..., -0.0168, -0.0018, -0.0355])

encoder.encoders.6.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0211,  0.0094,  0.0238,  ...,  0.0472,  0.0715,  0.0032],
        [-0.0024, -0.0063, -0.0115,  ..., -0.0233, -0.0227,  0.0573],
        [-0.0194, -0.0523,  0.0347,  ...,  0.0290,  0.0186,  0.0417],
        ...,
        [-0.0692, -0.0145, -0.0220,  ...,  0.0574,  0.0238,  0.0058],
        [-0.0423, -0.0499,  0.0171,  ..., -0.0384,  0.0412,  0.0780],
        [ 0.0576,  0.0006,  0.0907,  ..., -0.0484,  0.0542, -0.0908]])

encoder.encoders.6.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0275, -0.0194, -0.0027,  ..., -0.0366, -0.0160,  0.0194])

encoder.encoders.6.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[-0.0684],
         [-0.1375],
         [-0.0734],
         ...,
         [-0.0635],
         [ 0.0256],
         [ 0.0343]],

        [[ 0.0494],
         [ 0.1350],
         [ 0.0656],
         ...,
         [-0.0607],
         [-0.0425],
         [ 0.0849]],

        [[ 0.0657],
         [ 0.0576],
         [-0.0003],
         ...,
         [ 0.0134],
         [-0.0113],
         [ 0.0612]],

        ...,

        [[-0.0333],
         [ 0.0815],
         [-0.0430],
         ...,
         [ 0.0621],
         [-0.1145],
         [-0.0013]],

        [[-0.0609],
         [ 0.0187],
         [ 0.0069],
         ...,
         [-0.0705],
         [ 0.0235],
         [ 0.0653]],

        [[ 0.0260],
         [ 0.0193],
         [ 0.0651],
         ...,
         [-0.0399],
         [ 0.1397],
         [-0.0573]]])

encoder.encoders.6.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[ 1.2257e-02,  2.3786e-02,  1.6498e-03,  ..., -8.0673e-03,
           2.0628e-02,  1.2943e-02]],

        [[ 2.7953e-03, -1.7737e-02, -1.1161e-04,  ...,  2.8125e-03,
           4.6054e-05,  1.6219e-02]],

        [[ 7.6365e-03, -1.0226e-03, -3.6504e-02,  ..., -3.5330e-03,
           1.5611e-02,  3.6166e-02]],

        ...,

        [[-1.2341e-02,  1.5649e-03, -5.8392e-03,  ..., -2.0267e-02,
          -6.6154e-03, -3.5864e-02]],

        [[ 8.7836e-02,  4.8983e-02,  4.4197e-02,  ...,  4.9808e-04,
          -5.6957e-04,  2.2717e-02]],

        [[ 1.0906e-02,  7.3283e-03,  2.0472e-03,  ...,  3.0727e-02,
           8.3535e-03,  3.8833e-02]]])

encoder.encoders.6.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([1.0476, 1.0158, 1.0115,  ..., 0.9803, 1.0013, 0.9732])

encoder.encoders.6.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.0603, -0.0682, -0.0312,  ..., -0.0641, -0.0573, -0.0650])

encoder.encoders.6.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[-0.0114],
         [-0.0194],
         [ 0.0762],
         ...,
         [-0.0888],
         [-0.0786],
         [ 0.0354]],

        [[ 0.0209],
         [-0.1065],
         [ 0.0295],
         ...,
         [-0.0832],
         [ 0.0041],
         [ 0.0019]],

        [[-0.0510],
         [ 0.0784],
         [-0.0145],
         ...,
         [-0.0202],
         [-0.0148],
         [ 0.0280]],

        ...,

        [[ 0.0241],
         [-0.0217],
         [ 0.0251],
         ...,
         [ 0.0503],
         [ 0.0056],
         [-0.0340]],

        [[ 0.0463],
         [ 0.0660],
         [ 0.0312],
         ...,
         [ 0.0505],
         [ 0.0124],
         [ 0.0543]],

        [[-0.0119],
         [-0.0275],
         [ 0.1075],
         ...,
         [-0.0312],
         [-0.1319],
         [-0.0149]]])

encoder.encoders.6.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([0.9712, 0.9112, 1.0164,  ..., 0.9585, 0.9671, 1.1792])

encoder.encoders.6.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([ 0.2485, -0.0329,  0.1071,  ..., -0.1257, -0.0872,  0.2117])

encoder.encoders.6.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([0.7567, 0.9076, 0.9745,  ..., 1.1360, 0.9636, 0.9992])

encoder.encoders.6.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([ 0.0861, -0.0243,  0.1217,  ..., -0.2828, -0.0640,  0.1387])

encoder.encoders.6.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([0.9325, 1.1421, 1.0880,  ..., 1.1522, 1.0302, 1.1231])

encoder.encoders.6.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([-0.0551,  0.0624, -0.0686,  ..., -0.0734,  0.0098,  0.1091])

encoder.encoders.6.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.2866, 1.2857, 1.2543,  ..., 1.2989, 1.2914, 1.2695])

encoder.encoders.6.norm_final.bias-torch.Size([1280])-torch.float32
tensor([ 0.0548, -0.0062,  0.0428,  ..., -0.0709, -0.0299,  0.0930])

encoder.encoders.7.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[-0.0572,  0.0948, -0.1351,  ...,  0.1504,  0.1220, -0.0893],
        [ 0.0949,  0.1468, -0.1946,  ..., -0.2441,  0.2101, -0.3710],
        [-0.2783, -0.2349,  0.1307,  ..., -0.0352,  0.1736,  0.0614],
        ...,
        [ 0.0310,  0.2195, -0.0050,  ...,  0.1510,  0.2315,  0.1002],
        [ 0.1788,  0.1799,  0.1755,  ...,  0.2520, -0.1568, -0.2130],
        [ 0.0719, -0.1602,  0.1959,  ..., -0.0039,  0.1723,  0.0503]])

encoder.encoders.7.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[-0.4060,  0.0120, -0.0594,  ..., -0.0544,  0.0867,  0.1029],
        [-0.0743, -0.3454,  0.1164,  ..., -0.0623, -0.0153,  0.2575],
        [ 0.3241, -0.0879,  0.3952,  ...,  0.1756, -0.2108, -0.0822],
        ...,
        [-0.0593,  0.0241, -0.1780,  ..., -0.3349, -0.1497,  0.2076],
        [-0.3256, -0.0536,  0.0064,  ...,  0.0315, -0.1753,  0.1604],
        [ 0.0144,  0.1332,  0.0733,  ..., -0.1948, -0.0759, -0.1252]])

encoder.encoders.7.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0173, -0.0457, -0.0350,  ..., -0.0781,  0.0613, -0.0063],
        [-0.0191,  0.0073, -0.0156,  ...,  0.0018,  0.0061,  0.0190],
        [-0.0644, -0.0313, -0.0640,  ..., -0.0158,  0.0371,  0.0577],
        ...,
        [-0.0200, -0.0056, -0.0156,  ..., -0.0142, -0.0517,  0.0124],
        [ 0.0161, -0.0111, -0.0102,  ...,  0.0173, -0.0295, -0.0495],
        [-0.0643, -0.0331, -0.0250,  ..., -0.0284, -0.0344,  0.0597]])

encoder.encoders.7.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0565, -0.0370, -0.0236,  ...,  0.0660,  0.0107,  0.0620],
        [-0.0718, -0.0493, -0.1112,  ..., -0.0351,  0.0397,  0.0327],
        [ 0.0165, -0.1150, -0.0301,  ..., -0.0746,  0.0142,  0.0980],
        ...,
        [-0.0452, -0.1453, -0.0091,  ..., -0.0031, -0.0267, -0.0347],
        [ 0.0651, -0.0351,  0.0134,  ...,  0.0181, -0.0496, -0.0367],
        [-0.0640, -0.0402,  0.0181,  ..., -0.0122, -0.0104,  0.0747]])

encoder.encoders.7.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0103,  0.0340,  0.0040,  ..., -0.0331,  0.0003, -0.0665],
        [ 0.0072, -0.0625,  0.0216,  ..., -0.0368, -0.0128, -0.0633],
        [-0.0873,  0.0577, -0.0336,  ..., -0.0293,  0.0213,  0.0556],
        ...,
        [-0.0547,  0.0010,  0.0140,  ..., -0.0310, -0.0325,  0.1024],
        [ 0.0290,  0.0431, -0.0101,  ...,  0.0035, -0.0696,  0.0192],
        [-0.0301, -0.0284,  0.0138,  ..., -0.0454,  0.0498,  0.0049]])

encoder.encoders.7.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0443, -0.0456,  0.0500,  ...,  0.0133, -0.0355, -0.0801],
        [ 0.0050, -0.0238, -0.0040,  ..., -0.0983, -0.0127,  0.0526],
        [-0.0056,  0.0521,  0.0294,  ..., -0.1025,  0.0263,  0.0593],
        ...,
        [ 0.0124, -0.0396,  0.0155,  ..., -0.0573,  0.0084,  0.0277],
        [ 0.0196, -0.0068, -0.0291,  ..., -0.0791,  0.0470, -0.0389],
        [-0.0231, -0.0386, -0.0248,  ..., -0.0997,  0.0138,  0.0094]])

encoder.encoders.7.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0831,  0.0510,  0.0332,  ..., -0.0339, -0.0160,  0.0231],
        [-0.0606, -0.0139, -0.0552,  ..., -0.0495,  0.0237, -0.0068],
        [ 0.0236,  0.0105, -0.0118,  ...,  0.0777,  0.0067,  0.0052],
        ...,
        [-0.0045,  0.0131, -0.0299,  ..., -0.0088,  0.0190, -0.0301],
        [-0.0167,  0.0063, -0.0008,  ..., -0.0199,  0.0037, -0.0326],
        [-0.0102, -0.0336,  0.0340,  ..., -0.0330,  0.0079,  0.0854]])

encoder.encoders.7.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.6953, 0.6778, 0.6753,  ..., 0.7308, 0.6473, 0.6958])

encoder.encoders.7.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([-0.0311, -0.0014, -0.0251,  ...,  0.0589,  0.0419, -0.0450])

encoder.encoders.7.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.8068, 0.8481, 0.7908,  ..., 0.7827, 0.7529, 0.7714])

encoder.encoders.7.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([ 0.0252,  0.0029, -0.0313,  ...,  0.0258, -0.0376, -0.0078])

encoder.encoders.7.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.6691, 0.6596, 0.6345,  ..., 0.6276, 0.6362, 0.6682])

encoder.encoders.7.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([-0.0217, -0.0107, -0.0341,  ...,  0.0814,  0.0449, -0.0907])

encoder.encoders.7.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0318,  0.0101,  0.0140,  ...,  0.0013,  0.0100,  0.0308],
        [ 0.0409, -0.0563,  0.0017,  ...,  0.0543,  0.0471, -0.0714],
        [-0.0896,  0.0561,  0.0540,  ...,  0.0217,  0.0238, -0.0991],
        ...,
        [ 0.0069, -0.0089,  0.0070,  ...,  0.0003, -0.0044,  0.0116],
        [-0.0130,  0.0504,  0.0991,  ...,  0.0178, -0.0404, -0.0185],
        [-0.0295,  0.0640,  0.1156,  ...,  0.0438,  0.0183,  0.0399]])

encoder.encoders.7.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0327, -0.0005,  0.0115,  ..., -0.0023,  0.0021,  0.0124])

encoder.encoders.7.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[-0.0216, -0.0186, -0.0253,  ...,  0.0911, -0.0984, -0.0087],
        [ 0.0033, -0.0484, -0.0078,  ...,  0.0694, -0.0606, -0.0061],
        [ 0.0401, -0.0318, -0.0638,  ...,  0.0079,  0.0368,  0.0269],
        ...,
        [-0.0319, -0.0535, -0.0786,  ...,  0.0608, -0.0239, -0.0106],
        [ 0.0248,  0.0054,  0.0455,  ...,  0.0018, -0.0166, -0.0741],
        [-0.0170, -0.0017, -0.0163,  ..., -0.0225, -0.0539, -0.0452]])

encoder.encoders.7.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0145,  0.0037,  0.0034,  ...,  0.0024, -0.0227,  0.0011])

encoder.encoders.7.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0009, -0.0443,  0.0386,  ...,  0.0355,  0.0739, -0.0375],
        [ 0.0037,  0.0797,  0.0743,  ...,  0.0051,  0.0108,  0.0481],
        [-0.0859, -0.0597, -0.0012,  ...,  0.0096,  0.0063, -0.0559],
        ...,
        [ 0.0132, -0.0681, -0.0658,  ...,  0.0499,  0.0292, -0.0225],
        [ 0.0399, -0.0765, -0.0037,  ..., -0.0187, -0.0175,  0.0183],
        [-0.0008, -0.0560, -0.0725,  ...,  0.1135, -0.0196, -0.0412]])

encoder.encoders.7.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0208, -0.0528, -0.0112,  ..., -0.0034, -0.0241, -0.0042])

encoder.encoders.7.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0211, -0.0086,  0.0136,  ...,  0.0151,  0.0215,  0.0826],
        [-0.0069, -0.0354, -0.0305,  ..., -0.0192,  0.0541, -0.0491],
        [-0.0268,  0.0199, -0.0502,  ..., -0.0431, -0.0015, -0.0587],
        ...,
        [ 0.0190, -0.0190, -0.0496,  ..., -0.0035,  0.0287,  0.0622],
        [-0.0171,  0.0292, -0.0594,  ..., -0.0112,  0.0120, -0.0607],
        [ 0.0434, -0.0766, -0.0155,  ..., -0.0474,  0.0281, -0.0244]])

encoder.encoders.7.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([ 0.0148,  0.0173,  0.0306,  ..., -0.0339, -0.0277,  0.0050])

encoder.encoders.7.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[-0.0423],
         [ 0.0139],
         [ 0.0015],
         ...,
         [ 0.0119],
         [-0.0336],
         [ 0.0243]],

        [[ 0.0666],
         [ 0.0146],
         [-0.0056],
         ...,
         [ 0.0137],
         [-0.0488],
         [ 0.0451]],

        [[ 0.0201],
         [-0.0353],
         [-0.0428],
         ...,
         [-0.0542],
         [-0.0484],
         [ 0.0050]],

        ...,

        [[ 0.0678],
         [ 0.0561],
         [-0.0075],
         ...,
         [-0.0111],
         [-0.0446],
         [ 0.0135]],

        [[-0.0136],
         [-0.1124],
         [-0.1040],
         ...,
         [ 0.0184],
         [-0.0094],
         [-0.0559]],

        [[ 0.0533],
         [ 0.0255],
         [-0.0218],
         ...,
         [ 0.0421],
         [ 0.0028],
         [-0.0223]]])

encoder.encoders.7.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[-0.0439, -0.0112, -0.0105,  ..., -0.0341, -0.0135, -0.0204]],

        [[-0.0377, -0.0180, -0.0256,  ...,  0.0076, -0.0101, -0.0293]],

        [[-0.0446, -0.0783, -0.0112,  ..., -0.0323,  0.0077,  0.0364]],

        ...,

        [[ 0.0454, -0.0057, -0.0069,  ...,  0.0449,  0.0193,  0.0644]],

        [[ 0.0131,  0.0036, -0.0133,  ...,  0.0102,  0.0085,  0.0307]],

        [[ 0.0439,  0.0246,  0.0253,  ..., -0.0024,  0.0021, -0.0049]]])

encoder.encoders.7.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([0.9939, 1.0022, 1.0051,  ..., 1.0390, 1.0194, 1.0165])

encoder.encoders.7.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.0242, -0.0182,  0.0030,  ..., -0.0195, -0.0607, -0.0321])

encoder.encoders.7.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[ 0.1179],
         [-0.0213],
         [ 0.0001],
         ...,
         [-0.0476],
         [-0.0615],
         [ 0.0458]],

        [[ 0.0504],
         [-0.0003],
         [-0.0584],
         ...,
         [ 0.0235],
         [-0.0339],
         [ 0.0514]],

        [[-0.0585],
         [ 0.0838],
         [-0.0375],
         ...,
         [-0.0363],
         [-0.0452],
         [ 0.0609]],

        ...,

        [[-0.0207],
         [-0.0534],
         [-0.0435],
         ...,
         [-0.0340],
         [ 0.0628],
         [-0.0169]],

        [[-0.0007],
         [-0.0289],
         [-0.0030],
         ...,
         [-0.0922],
         [ 0.0007],
         [-0.0090]],

        [[ 0.0166],
         [ 0.0501],
         [ 0.0144],
         ...,
         [-0.0217],
         [-0.0246],
         [ 0.0634]]])

encoder.encoders.7.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([1.0305, 0.9904, 1.0030,  ..., 1.1509, 0.9807, 1.1076])

encoder.encoders.7.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([ 0.0614,  0.0087,  0.0569,  ..., -0.2591, -0.1370,  0.1785])

encoder.encoders.7.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([0.9312, 0.9352, 0.9327,  ..., 0.9222, 0.8809, 1.0734])

encoder.encoders.7.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([ 0.1771, -0.0191,  0.0819,  ..., -0.0673, -0.0217,  0.1822])

encoder.encoders.7.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([1.0830, 1.1425, 1.1091,  ..., 1.1854, 1.1175, 1.1771])

encoder.encoders.7.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([-0.0376,  0.0849,  0.0785,  ..., -0.0810, -0.0470,  0.1146])

encoder.encoders.7.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.2834, 1.2890, 1.3371,  ..., 1.3175, 1.3532, 1.2808])

encoder.encoders.7.norm_final.bias-torch.Size([1280])-torch.float32
tensor([ 0.0188,  0.0558,  0.1235,  ..., -0.1143, -0.1100,  0.0756])

encoder.encoders.8.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[-0.0594, -0.0683,  0.1292,  ...,  0.2048,  0.2765,  0.0597],
        [ 0.1731, -0.0321, -0.2246,  ..., -0.0785, -0.1020, -0.0892],
        [-0.2126, -0.1641, -0.1715,  ..., -0.0474, -0.0035, -0.1657],
        ...,
        [ 0.0596,  0.1387, -0.2820,  ..., -0.3250,  0.1054, -0.1111],
        [ 0.2890, -0.1855, -0.0323,  ...,  0.0674,  0.1544,  0.1910],
        [ 0.0003, -0.0613, -0.0383,  ...,  0.0494, -0.1498, -0.0705]])

encoder.encoders.8.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[-0.1488, -0.0365, -0.1307,  ..., -0.1192,  0.1185, -0.2711],
        [-0.2230, -0.1299, -0.2002,  ...,  0.3338,  0.2138,  0.0455],
        [ 0.0915, -0.1310, -0.3113,  ..., -0.2657, -0.1179,  0.1195],
        ...,
        [ 0.0934,  0.2005, -0.1145,  ...,  0.3491,  0.3591,  0.3133],
        [ 0.1895,  0.0251, -0.1225,  ...,  0.2849,  0.1377, -0.1316],
        [ 0.2071, -0.1813, -0.2290,  ..., -0.0820, -0.3114, -0.2797]])

encoder.encoders.8.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0315, -0.0029,  0.0369,  ..., -0.0204,  0.1183,  0.0679],
        [ 0.0028,  0.0202,  0.1066,  ..., -0.0602, -0.0181, -0.0189],
        [ 0.0550, -0.0841,  0.1127,  ..., -0.0256,  0.0123, -0.0502],
        ...,
        [-0.0180,  0.0307,  0.0231,  ...,  0.1048,  0.0368,  0.0103],
        [ 0.0728,  0.0646,  0.0517,  ...,  0.1132, -0.0321,  0.0238],
        [ 0.0760,  0.1058,  0.0834,  ..., -0.0043, -0.0456, -0.0543]])

encoder.encoders.8.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 1.0770e-01, -1.1761e-02, -1.4959e-01,  ..., -3.2893e-02,
          4.3655e-02, -6.2789e-03],
        [ 2.9300e-02, -9.3104e-02, -3.5269e-03,  ..., -6.5115e-03,
         -2.0981e-02, -7.4469e-02],
        [ 3.4379e-02,  7.0729e-02, -5.0849e-02,  ..., -1.2830e-01,
         -2.0024e-02,  8.6626e-02],
        ...,
        [-2.1361e-02,  6.0377e-02, -1.0563e-02,  ..., -9.9854e-02,
          2.7179e-02, -3.0815e-02],
        [-2.4831e-02,  4.8150e-02, -1.1823e-02,  ..., -2.5109e-02,
         -3.9858e-02,  8.8870e-04],
        [-2.6579e-02,  5.7970e-02, -3.8944e-05,  ...,  3.3292e-02,
          4.2788e-02, -5.8401e-02]])

encoder.encoders.8.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0049, -0.0054,  0.0371,  ..., -0.0684,  0.0060,  0.0050],
        [-0.0418,  0.0589,  0.0406,  ...,  0.0244, -0.0012,  0.0306],
        [-0.0253,  0.0771,  0.0637,  ..., -0.0464, -0.0212,  0.0394],
        ...,
        [ 0.0404,  0.0311, -0.0088,  ..., -0.0653, -0.0198,  0.0528],
        [-0.0498,  0.0597,  0.0298,  ...,  0.0296, -0.0449, -0.0050],
        [ 0.0482, -0.0006, -0.0104,  ..., -0.0079, -0.0285,  0.0039]])

encoder.encoders.8.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0356,  0.0553,  0.0379,  ...,  0.0107, -0.0288, -0.0833],
        [-0.0322,  0.0030, -0.0190,  ...,  0.0338, -0.0465, -0.0090],
        [-0.0787, -0.0095,  0.0566,  ..., -0.0479,  0.0032, -0.0901],
        ...,
        [ 0.0331,  0.0379,  0.0198,  ..., -0.0457,  0.0257,  0.0139],
        [-0.0856,  0.0383,  0.0043,  ...,  0.0152, -0.0359, -0.0030],
        [-0.0151, -0.0504,  0.0270,  ..., -0.0005, -0.0319,  0.0225]])

encoder.encoders.8.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-8.1301e-02, -3.5025e-02, -6.6265e-02,  ..., -7.5924e-03,
          2.5228e-03,  3.4724e-02],
        [-2.9173e-02,  3.9984e-02, -4.5188e-02,  ...,  1.3227e-02,
         -3.2008e-02,  1.9464e-03],
        [-1.9014e-02, -2.7207e-07,  8.8417e-03,  ..., -6.0468e-03,
          4.0801e-02, -2.2590e-02],
        ...,
        [-1.4065e-02, -8.8223e-02, -5.8405e-02,  ..., -1.7699e-02,
         -2.6443e-02,  1.2079e-02],
        [-1.2280e-01, -1.3305e-02, -1.3720e-01,  ...,  2.4816e-02,
          2.3673e-02, -1.7090e-02],
        [-7.8804e-02, -1.7723e-01,  1.1372e-02,  ...,  1.4644e-02,
         -4.6433e-03,  4.9293e-03]])

encoder.encoders.8.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.8074, 0.7687, 0.7222,  ..., 0.8304, 0.7949, 0.8231])

encoder.encoders.8.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([-0.0254, -0.0462, -0.1768,  ...,  0.0225,  0.0641,  0.0006])

encoder.encoders.8.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.8980, 0.8697, 0.8274,  ..., 0.8783, 0.8575, 0.8606])

encoder.encoders.8.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([ 0.0049, -0.0169,  0.0462,  ...,  0.0287,  0.0397,  0.0226])

encoder.encoders.8.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.5349, 0.5162, 0.4801,  ..., 0.4775, 0.4932, 0.5257])

encoder.encoders.8.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([-0.0109, -0.0159, -0.1561,  ...,  0.0816,  0.0903, -0.0767])

encoder.encoders.8.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0077,  0.0052,  0.0204,  ...,  0.0858,  0.0996,  0.0531],
        [-0.0012,  0.0278, -0.0145,  ..., -0.0445,  0.0462, -0.0776],
        [-0.0250,  0.0160,  0.0037,  ...,  0.0503, -0.0173, -0.0281],
        ...,
        [-0.0524, -0.0636,  0.0155,  ...,  0.0133,  0.0180,  0.0004],
        [ 0.0597, -0.0210, -0.0537,  ...,  0.0789,  0.0283, -0.0818],
        [-0.0672,  0.0326, -0.0465,  ...,  0.0685,  0.0372,  0.0028]])

encoder.encoders.8.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([ 0.0111, -0.0361, -0.0052,  ..., -0.0253,  0.0176,  0.0091])

encoder.encoders.8.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0432, -0.0398, -0.0240,  ...,  0.0320,  0.0079,  0.0036],
        [ 0.0123, -0.0042, -0.0555,  ...,  0.0307,  0.0037, -0.0067],
        [ 0.0145,  0.0478,  0.0077,  ..., -0.0816,  0.0289,  0.0338],
        ...,
        [-0.0343, -0.0252,  0.0062,  ...,  0.0369,  0.1259,  0.0266],
        [ 0.0063, -0.0297, -0.0021,  ..., -0.0220,  0.0131,  0.0056],
        [ 0.0201, -0.0471,  0.0017,  ...,  0.0358, -0.0538,  0.0021]])

encoder.encoders.8.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0258,  0.0117,  0.0359,  ...,  0.0009, -0.0049,  0.0014])

encoder.encoders.8.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 2.6792e-02, -5.1396e-02, -1.6896e-02,  ...,  3.4971e-02,
          1.6453e-03, -6.9827e-02],
        [-3.5691e-02,  2.3466e-02,  4.2275e-02,  ...,  4.8958e-02,
          9.3461e-02, -2.5318e-02],
        [-4.6792e-02, -6.3594e-02, -6.1981e-02,  ..., -5.0822e-02,
          4.0003e-05,  5.4069e-02],
        ...,
        [-1.0037e-01, -6.2878e-02,  9.2963e-03,  ..., -6.4345e-02,
          5.8843e-02, -4.7236e-03],
        [-8.0708e-02,  1.4355e-02,  3.5653e-02,  ...,  5.5767e-02,
         -2.6713e-02, -3.2637e-02],
        [-1.8611e-02,  1.6133e-02, -4.6198e-03,  ..., -4.4177e-02,
         -3.8728e-02,  1.7692e-02]])

encoder.encoders.8.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0420, -0.0084,  0.0110,  ..., -0.0052, -0.0086, -0.0458])

encoder.encoders.8.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[-0.1057, -0.0060, -0.0159,  ...,  0.0402, -0.0272, -0.0607],
        [-0.0442,  0.0240, -0.0711,  ..., -0.0050, -0.0800,  0.0012],
        [ 0.0141,  0.0146,  0.0378,  ..., -0.0050,  0.0244, -0.0139],
        ...,
        [-0.0149,  0.0505,  0.0567,  ...,  0.0171,  0.0252, -0.0527],
        [ 0.0203,  0.0358,  0.0401,  ...,  0.0582,  0.0352,  0.0380],
        [-0.0166,  0.0359, -0.0054,  ...,  0.0338,  0.0167,  0.0329]])

encoder.encoders.8.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([ 0.0086,  0.0233,  0.0490,  ..., -0.0222, -0.0525,  0.0421])

encoder.encoders.8.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[ 0.0842],
         [ 0.0081],
         [ 0.0489],
         ...,
         [-0.0360],
         [-0.0230],
         [ 0.0328]],

        [[ 0.0077],
         [-0.0393],
         [-0.0080],
         ...,
         [-0.0069],
         [ 0.0095],
         [-0.0249]],

        [[-0.0359],
         [-0.1066],
         [-0.0071],
         ...,
         [ 0.0390],
         [ 0.0180],
         [ 0.0349]],

        ...,

        [[ 0.0579],
         [ 0.0057],
         [-0.0028],
         ...,
         [ 0.0356],
         [ 0.1074],
         [ 0.0420]],

        [[ 0.0527],
         [-0.0216],
         [-0.0078],
         ...,
         [ 0.0162],
         [ 0.0562],
         [ 0.0142]],

        [[-0.0464],
         [-0.0238],
         [ 0.0024],
         ...,
         [-0.0589],
         [-0.0152],
         [-0.0081]]])

encoder.encoders.8.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[ 0.0154, -0.0092,  0.0094,  ..., -0.0206,  0.0127,  0.0135]],

        [[ 0.0195, -0.0087,  0.0018,  ..., -0.0044, -0.0411,  0.0082]],

        [[-0.0171,  0.0288,  0.0177,  ...,  0.0173, -0.0024,  0.0081]],

        ...,

        [[ 0.0345,  0.0173,  0.0078,  ...,  0.0024, -0.0059,  0.0232]],

        [[-0.0038, -0.0279, -0.0172,  ...,  0.0106, -0.0009, -0.0603]],

        [[ 0.0245, -0.0077,  0.0035,  ...,  0.0160, -0.0013, -0.0218]]])

encoder.encoders.8.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([1.0001, 1.0500, 1.0467,  ..., 1.0451, 0.9802, 1.0253])

encoder.encoders.8.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.0226, -0.0165, -0.0272,  ..., -0.0462, -0.0286, -0.0174])

encoder.encoders.8.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[ 0.0423],
         [-0.0321],
         [ 0.0621],
         ...,
         [-0.0416],
         [ 0.0530],
         [-0.0447]],

        [[ 0.0603],
         [ 0.0046],
         [ 0.0294],
         ...,
         [ 0.0694],
         [ 0.0126],
         [-0.0256]],

        [[-0.0165],
         [ 0.0918],
         [-0.0492],
         ...,
         [ 0.0186],
         [-0.0260],
         [-0.0334]],

        ...,

        [[-0.0740],
         [ 0.0577],
         [ 0.0163],
         ...,
         [ 0.0171],
         [ 0.0356],
         [-0.1161]],

        [[ 0.0178],
         [ 0.0495],
         [ 0.0039],
         ...,
         [ 0.0671],
         [ 0.0023],
         [ 0.0381]],

        [[ 0.0587],
         [ 0.0010],
         [ 0.0283],
         ...,
         [-0.0474],
         [-0.0068],
         [-0.0805]]])

encoder.encoders.8.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([1.0802, 1.0044, 1.5298,  ..., 1.0335, 1.2772, 1.1716])

encoder.encoders.8.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([ 0.2427, -0.1091,  0.6379,  ..., -0.1173, -0.4729,  0.3516])

encoder.encoders.8.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([0.9335, 0.9085, 0.9445,  ..., 1.0021, 0.8417, 0.9644])

encoder.encoders.8.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([ 0.0370,  0.1319, -0.0031,  ..., -0.2096, -0.0856,  0.1253])

encoder.encoders.8.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([1.2956, 1.0823, 1.0444,  ..., 1.1859, 1.1218, 1.1561])

encoder.encoders.8.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([-0.0695,  0.0652, -0.0299,  ...,  0.0096, -0.1134, -0.0154])

encoder.encoders.8.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.1784, 1.2880, 1.4707,  ..., 1.3006, 1.3392, 1.2619])

encoder.encoders.8.norm_final.bias-torch.Size([1280])-torch.float32
tensor([-0.0056,  0.0583,  0.4165,  ..., -0.1086, -0.1686,  0.0627])

encoder.encoders.9.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[-0.2477, -0.0402,  0.1097,  ..., -0.0631,  0.0140,  0.0793],
        [ 0.1503, -0.1035,  0.1520,  ...,  0.0304, -0.0933, -0.2738],
        [-0.2461, -0.3283,  0.2258,  ..., -0.1420, -0.0220, -0.2613],
        ...,
        [-0.1589, -0.2526,  0.1721,  ..., -0.2795, -0.1540, -0.1244],
        [ 0.0721, -0.2023, -0.1094,  ...,  0.1544,  0.0729, -0.0139],
        [ 0.0551,  0.0908, -0.1142,  ...,  0.0179, -0.1501,  0.1360]])

encoder.encoders.9.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[ 0.1816, -0.0675,  0.0208,  ..., -0.0726, -0.0239,  0.1369],
        [ 0.1090,  0.1295, -0.0292,  ...,  0.2376,  0.2983,  0.2399],
        [ 0.0712,  0.2862, -0.2081,  ..., -0.2103,  0.0723,  0.1730],
        ...,
        [-0.0852,  0.2708, -0.2129,  ...,  0.4031, -0.0040,  0.0906],
        [-0.0259, -0.0352, -0.1799,  ...,  0.0952, -0.0913,  0.2364],
        [ 0.1245,  0.1566,  0.0512,  ..., -0.2199,  0.1042,  0.0136]])

encoder.encoders.9.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0068, -0.0893, -0.0003,  ...,  0.0493,  0.0587,  0.0409],
        [-0.0449,  0.0293,  0.0134,  ..., -0.0106, -0.0062, -0.0392],
        [-0.1245,  0.0571,  0.0318,  ...,  0.0219,  0.0376, -0.0371],
        ...,
        [ 0.0244, -0.0167,  0.0221,  ..., -0.0357, -0.0297, -0.0283],
        [-0.0215,  0.0122,  0.0350,  ...,  0.0008, -0.0179,  0.0539],
        [ 0.0459,  0.0095, -0.0488,  ..., -0.0180,  0.0498, -0.0027]])

encoder.encoders.9.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0329, -0.0249, -0.0576,  ...,  0.0180,  0.0172, -0.0115],
        [ 0.0361,  0.0146,  0.0109,  ..., -0.0724, -0.0221, -0.0375],
        [ 0.0451,  0.0495,  0.0162,  ..., -0.0409,  0.0082,  0.0196],
        ...,
        [-0.0019, -0.0280, -0.0229,  ..., -0.0026,  0.0680, -0.0314],
        [ 0.0501,  0.0517, -0.0604,  ...,  0.0198, -0.0763, -0.0413],
        [ 0.0755, -0.0059,  0.0224,  ...,  0.0960, -0.0269,  0.0191]])

encoder.encoders.9.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0045,  0.0611, -0.0161,  ..., -0.0410,  0.0609,  0.0406],
        [ 0.0025,  0.0158,  0.0130,  ...,  0.0437, -0.0090, -0.0314],
        [-0.0133,  0.0503,  0.0067,  ...,  0.0524, -0.0555, -0.0044],
        ...,
        [-0.0417, -0.0135,  0.0146,  ..., -0.0005,  0.0123,  0.0334],
        [ 0.0406,  0.0403,  0.0115,  ...,  0.0016, -0.0348, -0.0362],
        [-0.0028, -0.0425, -0.0135,  ...,  0.0540, -0.0384, -0.0165]])

encoder.encoders.9.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0174, -0.0436, -0.0492,  ..., -0.0251, -0.0519,  0.0619],
        [-0.0312,  0.0215,  0.0161,  ..., -0.0319,  0.0225, -0.0259],
        [-0.0227,  0.0110,  0.0414,  ...,  0.0022, -0.0474,  0.0074],
        ...,
        [ 0.0099, -0.0240,  0.0072,  ...,  0.0453, -0.0076, -0.0198],
        [-0.0283, -0.0433, -0.0138,  ..., -0.0081,  0.0271,  0.0309],
        [-0.0170, -0.0417,  0.0019,  ..., -0.0239, -0.0103,  0.0272]])

encoder.encoders.9.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0752,  0.0017,  0.0270,  ...,  0.0141,  0.0395,  0.0235],
        [-0.1023,  0.1244, -0.1032,  ...,  0.0479,  0.0013,  0.0278],
        [ 0.0019,  0.0816,  0.0106,  ..., -0.0243, -0.0215, -0.0126],
        ...,
        [ 0.0387,  0.0035,  0.0094,  ..., -0.0188, -0.0254, -0.0353],
        [ 0.0247, -0.0251, -0.0448,  ..., -0.0022, -0.0074,  0.0020],
        [-0.1262, -0.1280, -0.0378,  ..., -0.0609, -0.0255, -0.0117]])

encoder.encoders.9.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.7588, 0.7090, 0.6524,  ..., 0.8548, 0.7498, 0.7836])

encoder.encoders.9.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([-0.0252, -0.0253, -0.2910,  ...,  0.0718,  0.0913, -0.0054])

encoder.encoders.9.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.8695, 0.8681, 0.7151,  ..., 0.8778, 0.8612, 0.8415])

encoder.encoders.9.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([-0.0147, -0.0239, -0.0196,  ..., -0.0016,  0.0346, -0.0392])

encoder.encoders.9.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.5313, 0.4508, 0.4355,  ..., 0.4752, 0.4505, 0.4116])

encoder.encoders.9.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([ 0.0128, -0.0301, -0.3042,  ...,  0.0405,  0.1126, -0.0422])

encoder.encoders.9.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 0.0555, -0.0355,  0.0604,  ...,  0.0549, -0.0478, -0.0199],
        [ 0.0618,  0.0277, -0.1037,  ...,  0.0403,  0.0765, -0.0398],
        [ 0.0172,  0.1343, -0.0655,  ...,  0.0944,  0.0437,  0.0254],
        ...,
        [-0.0614, -0.0711, -0.0014,  ...,  0.0071,  0.0590,  0.0107],
        [-0.0156, -0.0413, -0.0317,  ..., -0.0105,  0.0138,  0.0547],
        [ 0.0780, -0.0121,  0.0341,  ...,  0.0365,  0.0239, -0.0272]])

encoder.encoders.9.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0051, -0.0176, -0.0258,  ..., -0.0396, -0.0126, -0.0129])

encoder.encoders.9.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0903,  0.0403, -0.0450,  ...,  0.0284, -0.0810,  0.0157],
        [-0.0369, -0.0420, -0.0517,  ...,  0.0280, -0.0170,  0.0088],
        [-0.0016, -0.0392,  0.0109,  ...,  0.0322, -0.0034,  0.0696],
        ...,
        [-0.0014,  0.0932,  0.0303,  ..., -0.0115, -0.0092, -0.0199],
        [ 0.0192, -0.0052,  0.0271,  ...,  0.0197, -0.0302, -0.0305],
        [-0.0261, -0.0026,  0.0097,  ..., -0.0804,  0.0152,  0.0650]])

encoder.encoders.9.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0552,  0.0182,  0.0592,  ..., -0.0122,  0.0025, -0.0024])

encoder.encoders.9.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0462,  0.0173, -0.0888,  ...,  0.0449, -0.0420, -0.0718],
        [ 0.0892,  0.0215, -0.0170,  ...,  0.0394,  0.0275, -0.0005],
        [-0.0062,  0.0240, -0.0359,  ...,  0.0558,  0.0368,  0.0034],
        ...,
        [-0.0376, -0.0117,  0.0852,  ...,  0.0401, -0.0468,  0.0171],
        [-0.1036, -0.0219, -0.0633,  ..., -0.0322,  0.0649,  0.0119],
        [ 0.0448, -0.0447, -0.0021,  ...,  0.0567, -0.0657,  0.0586]])

encoder.encoders.9.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0334, -0.0125, -0.0187,  ..., -0.0059, -0.0001,  0.0090])

encoder.encoders.9.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[-0.0379, -0.0774,  0.0149,  ...,  0.0441, -0.0293, -0.0146],
        [-0.0083,  0.0073,  0.0413,  ...,  0.0242, -0.0090,  0.0162],
        [ 0.0004, -0.0146, -0.0494,  ...,  0.0055,  0.0048,  0.0450],
        ...,
        [-0.0145, -0.1013, -0.0506,  ...,  0.0480, -0.0355, -0.0719],
        [ 0.0002,  0.0173, -0.0253,  ..., -0.0411, -0.0384, -0.0226],
        [ 0.0225,  0.0185,  0.0060,  ...,  0.0574,  0.0058,  0.0442]])

encoder.encoders.9.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0442,  0.0478,  0.0854,  ..., -0.0431, -0.0391,  0.0196])

encoder.encoders.9.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[-0.0060],
         [-0.0132],
         [-0.0068],
         ...,
         [-0.0075],
         [ 0.0394],
         [-0.0038]],

        [[-0.0277],
         [ 0.0138],
         [-0.0079],
         ...,
         [-0.0236],
         [-0.0195],
         [ 0.0442]],

        [[ 0.0005],
         [ 0.0348],
         [-0.0059],
         ...,
         [-0.0082],
         [-0.0719],
         [ 0.0547]],

        ...,

        [[ 0.0378],
         [ 0.0396],
         [ 0.0456],
         ...,
         [ 0.1513],
         [-0.0412],
         [-0.0164]],

        [[ 0.0090],
         [-0.0655],
         [-0.0632],
         ...,
         [ 0.0088],
         [-0.0202],
         [-0.0582]],

        [[ 0.0454],
         [-0.0261],
         [-0.0745],
         ...,
         [ 0.1104],
         [-0.0747],
         [ 0.0008]]])

encoder.encoders.9.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[-0.0169, -0.0022, -0.0040,  ...,  0.0147,  0.0024,  0.0318]],

        [[-0.0225, -0.0166, -0.0144,  ..., -0.0272,  0.0064, -0.0377]],

        [[-0.0310, -0.0051, -0.0278,  ..., -0.0190, -0.0088, -0.0417]],

        ...,

        [[-0.0242, -0.0122, -0.0289,  ..., -0.0085,  0.0040, -0.0091]],

        [[-0.0390, -0.0035, -0.0278,  ..., -0.0188,  0.0011, -0.0060]],

        [[-0.0398, -0.0101, -0.0085,  ...,  0.0245, -0.0397, -0.0179]]])

encoder.encoders.9.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([1.0253, 1.0002, 1.0116,  ..., 0.9816, 1.0279, 1.0115])

encoder.encoders.9.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.0728, -0.0174, -0.0440,  ..., -0.0335, -0.0343, -0.0755])

encoder.encoders.9.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[ 0.0802],
         [ 0.0168],
         [-0.0607],
         ...,
         [ 0.0334],
         [ 0.0383],
         [ 0.0422]],

        [[-0.0338],
         [ 0.0168],
         [-0.0384],
         ...,
         [ 0.0527],
         [ 0.0053],
         [ 0.0191]],

        [[ 0.0170],
         [ 0.0090],
         [-0.0852],
         ...,
         [-0.0157],
         [-0.0251],
         [-0.0257]],

        ...,

        [[-0.0859],
         [-0.0061],
         [ 0.0383],
         ...,
         [-0.0241],
         [ 0.0190],
         [ 0.1255]],

        [[-0.0508],
         [-0.0359],
         [ 0.0341],
         ...,
         [-0.0253],
         [-0.0867],
         [-0.0618]],

        [[ 0.0069],
         [ 0.0746],
         [-0.0236],
         ...,
         [-0.1057],
         [ 0.0356],
         [ 0.0480]]])

encoder.encoders.9.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([1.2731, 1.1004, 1.2174,  ..., 1.0444, 1.2335, 1.0153])

encoder.encoders.9.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([-0.2665,  0.1487,  0.2769,  ..., -0.0319, -0.4689,  0.0500])

encoder.encoders.9.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([1.0368, 0.8666, 1.1983,  ..., 1.0503, 1.0320, 0.9420])

encoder.encoders.9.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([ 0.1692,  0.0277,  0.4107,  ..., -0.2702, -0.3117,  0.1677])

encoder.encoders.9.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([1.2819, 1.1154, 1.0225,  ..., 1.2839, 1.0827, 1.0473])

encoder.encoders.9.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([-0.0679,  0.0324, -0.0565,  ..., -0.0223, -0.0539,  0.0070])

encoder.encoders.9.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.1863, 1.2252, 1.2391,  ..., 1.2174, 1.2759, 1.2431])

encoder.encoders.9.norm_final.bias-torch.Size([1280])-torch.float32
tensor([-0.1577,  0.1011,  0.1743,  ..., -0.0040, -0.0966, -0.0452])

encoder.encoders.10.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[ 0.0451, -0.0841,  0.1533,  ...,  0.1209,  0.1396, -0.2546],
        [-0.1646,  0.2855, -0.2212,  ...,  0.0343, -0.0396, -0.0971],
        [-0.1073, -0.1808, -0.2573,  ..., -0.0877, -0.0630,  0.0411],
        ...,
        [-0.2286,  0.1131,  0.1212,  ..., -0.1943, -0.0921,  0.0730],
        [ 0.0709,  0.1718,  0.2786,  ..., -0.1484, -0.2108,  0.0683],
        [ 0.0655,  0.0299,  0.2925,  ..., -0.0902, -0.0491,  0.0778]])

encoder.encoders.10.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[ 2.3196e-04, -4.7444e-02,  7.9353e-03,  ..., -2.8626e-01,
          3.3940e-01,  1.7077e-01],
        [ 1.3339e-01,  1.2146e-01,  1.9339e-01,  ..., -1.8535e-01,
         -1.8620e-01, -3.3459e-02],
        [-1.2643e-01, -2.9321e-01,  2.4969e-01,  ...,  3.5625e-01,
         -1.9418e-01, -3.6147e-04],
        ...,
        [-2.2655e-01,  1.1806e-01, -1.5169e-01,  ..., -2.7812e-01,
          1.7286e-01,  9.9863e-02],
        [-1.1922e-01, -6.3926e-02,  9.4816e-03,  ..., -3.3968e-01,
          1.0202e-01, -2.7710e-02],
        [ 2.4564e-01,  2.2202e-01, -2.7181e-01,  ...,  2.0464e-01,
         -1.4221e-02, -2.6208e-02]])

encoder.encoders.10.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0067, -0.0154,  0.0242,  ..., -0.0191, -0.0583,  0.0370],
        [ 0.0125, -0.0500, -0.0025,  ..., -0.1299,  0.0155, -0.0335],
        [ 0.0561, -0.0727,  0.0170,  ...,  0.0667, -0.0387,  0.0190],
        ...,
        [-0.0156,  0.0134, -0.0575,  ..., -0.0431,  0.0016, -0.0548],
        [ 0.0552, -0.0498,  0.0341,  ...,  0.0233,  0.0551, -0.0118],
        [ 0.0675, -0.0598,  0.0316,  ...,  0.0366,  0.0629, -0.0463]])

encoder.encoders.10.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0264, -0.0063,  0.0107,  ..., -0.0280, -0.0186,  0.0628],
        [-0.0919, -0.0228, -0.0257,  ...,  0.0405,  0.0143,  0.0004],
        [-0.0897, -0.0462, -0.0720,  ..., -0.0524, -0.0485, -0.0106],
        ...,
        [-0.0203,  0.0116,  0.0606,  ...,  0.0872,  0.0218, -0.0322],
        [ 0.1091, -0.0483,  0.0747,  ...,  0.0417,  0.1045,  0.0692],
        [-0.0006,  0.0193, -0.0204,  ...,  0.0157,  0.1051, -0.0207]])

encoder.encoders.10.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0551, -0.0838, -0.0102,  ..., -0.0185, -0.0434,  0.0369],
        [-0.0080,  0.0052,  0.0360,  ..., -0.0463, -0.0394, -0.0319],
        [-0.0357,  0.0443,  0.0481,  ...,  0.0056, -0.0775,  0.0345],
        ...,
        [ 0.0175, -0.0110, -0.0263,  ...,  0.0264, -0.0216, -0.0355],
        [ 0.0167,  0.0317, -0.0056,  ...,  0.0627,  0.0250, -0.0183],
        [ 0.0132,  0.0448, -0.0450,  ...,  0.0375, -0.0504, -0.0356]])

encoder.encoders.10.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0022, -0.0085, -0.0364,  ...,  0.0480, -0.0577,  0.0302],
        [-0.0057,  0.0492, -0.0159,  ..., -0.0054, -0.0011, -0.0116],
        [ 0.0456, -0.0405, -0.0027,  ...,  0.0628,  0.0173, -0.0343],
        ...,
        [ 0.0986, -0.0079, -0.0316,  ...,  0.0476, -0.0497,  0.0237],
        [ 0.0970,  0.0407, -0.0618,  ..., -0.0543,  0.0193,  0.0325],
        [ 0.0403,  0.0594, -0.0755,  ..., -0.0280, -0.0040,  0.0095]])

encoder.encoders.10.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0105, -0.0720, -0.0228,  ..., -0.0054, -0.0025,  0.0603],
        [-0.0003, -0.0003,  0.0153,  ...,  0.0189, -0.0290,  0.0159],
        [ 0.0366, -0.0092,  0.0628,  ..., -0.0417,  0.0243,  0.0243],
        ...,
        [-0.0200,  0.0779, -0.0683,  ..., -0.0316, -0.0369,  0.0123],
        [-0.0737, -0.0844, -0.0886,  ...,  0.0059, -0.0616, -0.0410],
        [-0.0909, -0.1226, -0.0833,  ..., -0.0142, -0.0395,  0.0032]])

encoder.encoders.10.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.7613, 0.7487, 0.6955,  ..., 0.7960, 0.7861, 0.7629])

encoder.encoders.10.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([ 0.0442, -0.1228, -0.1722,  ..., -0.0359,  0.0455, -0.0138])

encoder.encoders.10.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.8911, 0.8589, 0.8707,  ..., 0.8777, 0.8760, 0.8646])

encoder.encoders.10.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([-0.0217, -0.0170,  0.0112,  ...,  0.0038, -0.0312, -0.0225])

encoder.encoders.10.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.4894, 0.5050, 0.4500,  ..., 0.4930, 0.5219, 0.4873])

encoder.encoders.10.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([ 0.0636, -0.0993, -0.1652,  ..., -0.0522,  0.0570, -0.0041])

encoder.encoders.10.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0403,  0.0606, -0.0222,  ..., -0.0563,  0.0161,  0.0177],
        [ 0.0051,  0.0224,  0.0152,  ..., -0.0198,  0.0863,  0.0213],
        [ 0.0461, -0.0318,  0.0073,  ...,  0.0125, -0.0245, -0.0470],
        ...,
        [ 0.0327, -0.0023, -0.0586,  ..., -0.0491, -0.0128, -0.0357],
        [-0.0064, -0.0110, -0.0921,  ..., -0.0744,  0.0172, -0.0288],
        [ 0.0056,  0.0249,  0.0565,  ..., -0.0801, -0.0302,  0.0593]])

encoder.encoders.10.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0001,  0.0240, -0.0314,  ...,  0.0049, -0.0166, -0.0395])

encoder.encoders.10.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0312, -0.0785,  0.0284,  ..., -0.0264, -0.0078, -0.0377],
        [-0.0037, -0.0325,  0.0144,  ...,  0.0544, -0.0131, -0.0058],
        [-0.0378,  0.0499, -0.0242,  ..., -0.0113,  0.0038,  0.0176],
        ...,
        [ 0.1034,  0.0490, -0.0238,  ...,  0.0463, -0.0210,  0.0283],
        [-0.0071, -0.0868,  0.0018,  ..., -0.0443,  0.1539,  0.0077],
        [-0.0504, -0.0185, -0.0457,  ..., -0.0099, -0.1475,  0.0126]])

encoder.encoders.10.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0332,  0.0089, -0.0051,  ..., -0.0181, -0.0175,  0.0067])

encoder.encoders.10.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0390, -0.0534,  0.0309,  ...,  0.0298,  0.0784,  0.0418],
        [ 0.0753,  0.0513, -0.0180,  ...,  0.0112, -0.0072, -0.0425],
        [-0.0601,  0.0233,  0.0262,  ..., -0.0090,  0.0799, -0.0439],
        ...,
        [ 0.0358, -0.0287, -0.0543,  ...,  0.0839,  0.0385, -0.0421],
        [ 0.0514,  0.0195,  0.0297,  ..., -0.0596, -0.0046, -0.0455],
        [-0.0132, -0.0904,  0.0184,  ...,  0.0934,  0.0619,  0.1101]])

encoder.encoders.10.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0512, -0.0209,  0.0006,  ..., -0.0269, -0.0371, -0.0016])

encoder.encoders.10.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0214,  0.0784,  0.0269,  ...,  0.0404,  0.0470,  0.0934],
        [-0.0270,  0.0214,  0.0211,  ...,  0.0160, -0.0100, -0.0075],
        [ 0.0603, -0.0035,  0.0410,  ...,  0.0075, -0.0180, -0.0169],
        ...,
        [ 0.0570,  0.0396,  0.0459,  ..., -0.0027, -0.0180, -0.0095],
        [ 0.0262,  0.0567,  0.0077,  ...,  0.0171, -0.0247,  0.0264],
        [-0.0051, -0.0033, -0.0084,  ..., -0.0139,  0.0193,  0.0055]])

encoder.encoders.10.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0304,  0.0344,  0.0589,  ...,  0.0227, -0.0254, -0.0213])

encoder.encoders.10.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[-0.0095],
         [ 0.0170],
         [-0.1020],
         ...,
         [ 0.0151],
         [ 0.0246],
         [-0.0190]],

        [[ 0.0724],
         [ 0.0642],
         [-0.0236],
         ...,
         [-0.0430],
         [-0.0506],
         [ 0.0091]],

        [[-0.0241],
         [-0.0046],
         [-0.0423],
         ...,
         [ 0.0603],
         [ 0.1083],
         [-0.0123]],

        ...,

        [[-0.0283],
         [-0.0532],
         [ 0.0434],
         ...,
         [ 0.0007],
         [-0.0298],
         [-0.0679]],

        [[-0.0241],
         [-0.0052],
         [-0.0315],
         ...,
         [ 0.0417],
         [-0.1020],
         [ 0.0015]],

        [[ 0.0442],
         [-0.0064],
         [ 0.0333],
         ...,
         [ 0.0997],
         [-0.0069],
         [ 0.0701]]])

encoder.encoders.10.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[ 0.0002, -0.0222,  0.0147,  ...,  0.0075, -0.0253, -0.0047]],

        [[ 0.0242, -0.0114,  0.0141,  ..., -0.0139, -0.0063,  0.0027]],

        [[ 0.0097,  0.0137, -0.0004,  ..., -0.0041,  0.0183,  0.0123]],

        ...,

        [[ 0.0103,  0.0018, -0.0149,  ..., -0.0029, -0.0042,  0.0085]],

        [[-0.0036,  0.0074, -0.0031,  ..., -0.0034,  0.0012, -0.0258]],

        [[ 0.0020,  0.0015, -0.0131,  ...,  0.0070, -0.0132, -0.0283]]])

encoder.encoders.10.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([1.0283, 1.0257, 1.0639,  ..., 1.0438, 1.0546, 1.0731])

encoder.encoders.10.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([ 0.0330,  0.0158, -0.0173,  ..., -0.0215, -0.0645, -0.0517])

encoder.encoders.10.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[-0.0269],
         [-0.1120],
         [-0.0007],
         ...,
         [ 0.0401],
         [ 0.0280],
         [-0.0165]],

        [[-0.0654],
         [ 0.0687],
         [-0.0320],
         ...,
         [-0.0347],
         [-0.0054],
         [ 0.0048]],

        [[ 0.0977],
         [ 0.0032],
         [ 0.0491],
         ...,
         [-0.0233],
         [ 0.0188],
         [ 0.0334]],

        ...,

        [[-0.0022],
         [ 0.0379],
         [-0.0639],
         ...,
         [-0.0642],
         [-0.0618],
         [ 0.0752]],

        [[ 0.0026],
         [-0.0014],
         [ 0.0052],
         ...,
         [ 0.0058],
         [-0.0873],
         [-0.1144]],

        [[-0.0085],
         [ 0.0489],
         [ 0.0179],
         ...,
         [-0.0173],
         [-0.0253],
         [-0.0436]]])

encoder.encoders.10.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([1.3247, 1.2436, 1.2805,  ..., 1.2321, 1.1391, 1.1031])

encoder.encoders.10.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([-0.2698,  0.2331,  0.4035,  ...,  0.3023, -0.2206, -0.1166])

encoder.encoders.10.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([1.2288, 0.9722, 1.0906,  ..., 1.0215, 1.0140, 0.8663])

encoder.encoders.10.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([-0.3225,  0.1140,  0.3233,  ..., -0.1698, -0.2433, -0.0813])

encoder.encoders.10.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([1.1906, 1.1425, 1.1164,  ..., 1.1797, 1.0836, 1.1597])

encoder.encoders.10.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([ 0.0280,  0.0908,  0.0395,  ...,  0.1493,  0.0284, -0.0905])

encoder.encoders.10.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.2410, 1.2960, 1.3266,  ..., 1.2903, 1.2694, 1.2971])

encoder.encoders.10.norm_final.bias-torch.Size([1280])-torch.float32
tensor([-0.1118,  0.1615,  0.0777,  ...,  0.0703, -0.0427, -0.0166])

encoder.encoders.11.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[ 5.3027e-03, -1.7507e-01, -1.4948e-01,  ...,  4.2053e-02,
          1.7534e-01, -5.5207e-02],
        [ 1.7834e-01, -1.5694e-01, -1.0236e-01,  ..., -9.1147e-02,
          2.0463e-01,  3.3022e-01],
        [-5.5642e-02, -6.9605e-02,  2.6039e-01,  ..., -1.0457e-04,
          9.3818e-02,  2.8685e-01],
        ...,
        [-8.9862e-02,  2.1495e-01,  1.4706e-01,  ...,  1.3606e-01,
          1.8333e-01, -2.1516e-01],
        [ 8.8410e-02, -1.7785e-02, -4.3972e-02,  ..., -2.9867e-01,
         -1.9636e-01,  4.8122e-02],
        [-5.2172e-02, -1.0778e-01,  2.9408e-01,  ...,  2.1760e-01,
          4.2247e-01, -2.0095e-01]])

encoder.encoders.11.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[-0.0946,  0.1601,  0.0759,  ..., -0.1782,  0.0755, -0.1792],
        [-0.2153, -0.0107,  0.0752,  ..., -0.2984, -0.2062, -0.0039],
        [ 0.0224,  0.2413, -0.2397,  ..., -0.0415, -0.1730, -0.0275],
        ...,
        [ 0.1684,  0.1158,  0.1098,  ...,  0.0294, -0.3486, -0.2614],
        [-0.0021,  0.2480, -0.2438,  ...,  0.1604, -0.2291, -0.1291],
        [ 0.0783, -0.4011, -0.3781,  ...,  0.1261, -0.0988,  0.2317]])

encoder.encoders.11.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0547, -0.0596,  0.0490,  ..., -0.0267,  0.0117,  0.0760],
        [-0.0297, -0.0234,  0.0756,  ..., -0.0328,  0.0219,  0.0946],
        [-0.0208,  0.0098,  0.0455,  ..., -0.0093, -0.0307,  0.0329],
        ...,
        [-0.0042,  0.0060,  0.0099,  ...,  0.0415,  0.0569, -0.0917],
        [ 0.0110, -0.1158, -0.0868,  ...,  0.0208,  0.1101, -0.0409],
        [ 0.0041, -0.0799,  0.0586,  ..., -0.0204, -0.0539,  0.0038]])

encoder.encoders.11.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0105, -0.0669, -0.0154,  ..., -0.0193,  0.0250,  0.0105],
        [-0.0059,  0.0259, -0.0441,  ...,  0.0344,  0.0040,  0.0268],
        [-0.0751,  0.0144, -0.0553,  ...,  0.0100,  0.0149,  0.0074],
        ...,
        [-0.0893,  0.0127, -0.0592,  ..., -0.0386, -0.0122,  0.0126],
        [-0.0210, -0.0485, -0.0558,  ...,  0.0113,  0.0396, -0.0469],
        [ 0.0577, -0.0291, -0.0025,  ..., -0.0283,  0.0227, -0.0689]])

encoder.encoders.11.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0372, -0.0061, -0.0442,  ..., -0.1004,  0.0406,  0.0402],
        [-0.0955, -0.0246, -0.0658,  ...,  0.0003,  0.0050, -0.1108],
        [-0.0599, -0.0236, -0.0361,  ...,  0.0241, -0.0118,  0.0197],
        ...,
        [-0.0244,  0.0067,  0.0002,  ...,  0.0002, -0.0049,  0.0109],
        [-0.0163,  0.0343,  0.0055,  ...,  0.0374, -0.0454, -0.0076],
        [ 0.0198,  0.0558,  0.0040,  ...,  0.0527, -0.0372,  0.0265]])

encoder.encoders.11.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0428,  0.0204,  0.0625,  ...,  0.0005,  0.0500,  0.0163],
        [ 0.0559,  0.0297, -0.0960,  ..., -0.0235,  0.0171,  0.0323],
        [-0.0030,  0.0125, -0.0592,  ..., -0.0355, -0.0151,  0.0481],
        ...,
        [ 0.0538, -0.0289, -0.0086,  ...,  0.0042,  0.0262, -0.0319],
        [-0.0519, -0.0026,  0.0736,  ..., -0.0182,  0.0084, -0.0453],
        [-0.0582,  0.0677, -0.0331,  ...,  0.0136,  0.0285, -0.0166]])

encoder.encoders.11.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0307,  0.0411, -0.0554,  ...,  0.0299, -0.0162,  0.0177],
        [ 0.0206,  0.0337, -0.0314,  ..., -0.0009, -0.0256, -0.0397],
        [-0.0191, -0.0098, -0.0450,  ...,  0.0200, -0.0270, -0.0197],
        ...,
        [ 0.0163, -0.0203, -0.0285,  ...,  0.0329, -0.0039, -0.0059],
        [-0.0448, -0.2872, -0.0270,  ..., -0.0075,  0.0330,  0.0070],
        [ 0.0463,  0.1307,  0.0310,  ..., -0.0660,  0.0054,  0.0958]])

encoder.encoders.11.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.9243, 0.8222, 0.8364,  ..., 0.8823, 0.8656, 0.8615])

encoder.encoders.11.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([-0.0017, -0.1146, -0.0802,  ..., -0.0173, -0.0339, -0.0354])

encoder.encoders.11.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([1.0063, 0.9710, 0.8242,  ..., 0.9063, 0.9117, 0.9559])

encoder.encoders.11.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([-0.0045, -0.0224, -0.0215,  ..., -0.0009,  0.0173,  0.0038])

encoder.encoders.11.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.4155, 0.4965, 0.4504,  ..., 0.4528, 0.4850, 0.4776])

encoder.encoders.11.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([ 0.0409, -0.1396, -0.1262,  ..., -0.0537, -0.0010, -0.0318])

encoder.encoders.11.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0632,  0.0920,  0.0264,  ..., -0.0334, -0.0868,  0.0494],
        [-0.0857,  0.0852,  0.0639,  ...,  0.0360, -0.0783,  0.0477],
        [-0.0606, -0.0114, -0.0085,  ..., -0.0126,  0.0227,  0.0316],
        ...,
        [ 0.0047, -0.0530, -0.0052,  ..., -0.0553, -0.0269,  0.0748],
        [-0.0919,  0.0152,  0.0414,  ...,  0.0210, -0.0479,  0.0300],
        [-0.0083, -0.0162,  0.0667,  ...,  0.0405,  0.0120, -0.0287]])

encoder.encoders.11.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([ 0.0070, -0.0354, -0.0009,  ...,  0.0224, -0.0013, -0.0003])

encoder.encoders.11.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[-0.0576, -0.0446, -0.0927,  ..., -0.0643, -0.0125,  0.0035],
        [-0.0157,  0.0181,  0.0218,  ...,  0.0935, -0.0279,  0.1146],
        [-0.0101, -0.0006, -0.0092,  ...,  0.0352,  0.0700, -0.0635],
        ...,
        [-0.0894,  0.0333, -0.0275,  ..., -0.0018,  0.0405, -0.0613],
        [ 0.0659, -0.0002,  0.0986,  ...,  0.0090, -0.0130, -0.0206],
        [-0.0435, -0.1017,  0.0139,  ...,  0.0060,  0.0308, -0.0069]])

encoder.encoders.11.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0239, -0.0056, -0.0263,  ..., -0.0284, -0.0014,  0.0026])

encoder.encoders.11.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0530, -0.0239, -0.0296,  ..., -0.0067,  0.0217, -0.0145],
        [-0.0440, -0.0101,  0.0294,  ..., -0.0557,  0.0460, -0.0808],
        [ 0.0716, -0.0591, -0.0112,  ..., -0.0615,  0.0335,  0.0650],
        ...,
        [-0.0029, -0.0989,  0.0271,  ...,  0.0655, -0.0405,  0.0409],
        [ 0.0339, -0.0020,  0.0435,  ..., -0.0342,  0.0526,  0.0176],
        [ 0.0295,  0.0224,  0.0337,  ..., -0.0091, -0.0365,  0.0652]])

encoder.encoders.11.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0530,  0.0102, -0.0070,  ..., -0.0305, -0.0212, -0.0378])

encoder.encoders.11.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0659, -0.0648, -0.1122,  ..., -0.0158,  0.0313, -0.0309],
        [ 0.0188,  0.0148,  0.0354,  ..., -0.0234,  0.0535,  0.0149],
        [ 0.0073,  0.0612,  0.0098,  ..., -0.0245, -0.0013, -0.0416],
        ...,
        [-0.0305,  0.0151,  0.0258,  ..., -0.0278, -0.0285, -0.0016],
        [ 0.0472, -0.0385, -0.0858,  ...,  0.0105,  0.0037,  0.0026],
        [-0.0039, -0.0320,  0.0568,  ...,  0.0560, -0.0317,  0.0753]])

encoder.encoders.11.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0476, -0.0097,  0.0163,  ..., -0.0181, -0.0206, -0.0022])

encoder.encoders.11.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[ 0.1269],
         [-0.0224],
         [-0.0020],
         ...,
         [-0.0201],
         [ 0.1750],
         [ 0.0385]],

        [[ 0.0354],
         [ 0.0070],
         [-0.0014],
         ...,
         [ 0.0073],
         [ 0.0062],
         [ 0.0058]],

        [[ 0.0165],
         [ 0.0432],
         [-0.0379],
         ...,
         [-0.0665],
         [-0.0219],
         [ 0.0671]],

        ...,

        [[-0.0334],
         [ 0.0412],
         [-0.0427],
         ...,
         [ 0.0454],
         [ 0.0760],
         [-0.0252]],

        [[-0.0854],
         [-0.0636],
         [ 0.0290],
         ...,
         [-0.0300],
         [-0.0585],
         [ 0.0261]],

        [[-0.1069],
         [ 0.0467],
         [ 0.0155],
         ...,
         [-0.0100],
         [ 0.0220],
         [-0.0117]]])

encoder.encoders.11.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[-0.0063, -0.0015, -0.0206,  ..., -0.0025,  0.0008, -0.0454]],

        [[ 0.0502,  0.0342,  0.0341,  ..., -0.0705, -0.0616, -0.1067]],

        [[ 0.0035,  0.0213, -0.0086,  ..., -0.0391, -0.0314, -0.0621]],

        ...,

        [[-0.0168, -0.0167, -0.0070,  ...,  0.0061,  0.0184,  0.0371]],

        [[-0.0405, -0.0511, -0.0395,  ..., -0.0106, -0.0010,  0.0196]],

        [[ 0.0393,  0.0087,  0.0275,  ...,  0.0348,  0.0172,  0.0466]]])

encoder.encoders.11.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([1.0240, 1.0226, 1.0253,  ..., 1.0354, 1.0391, 1.0070])

encoder.encoders.11.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.0567, -0.0250, -0.0409,  ..., -0.0394, -0.0662, -0.0614])

encoder.encoders.11.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[-0.0485],
         [-0.0275],
         [ 0.0500],
         ...,
         [-0.0239],
         [-0.0162],
         [-0.0613]],

        [[-0.0062],
         [ 0.0199],
         [ 0.0076],
         ...,
         [ 0.0079],
         [ 0.0091],
         [-0.0669]],

        [[-0.0017],
         [ 0.0197],
         [-0.0547],
         ...,
         [-0.0146],
         [-0.0330],
         [-0.0643]],

        ...,

        [[-0.0050],
         [-0.0094],
         [ 0.0313],
         ...,
         [ 0.0806],
         [-0.0972],
         [-0.0344]],

        [[-0.0006],
         [ 0.0184],
         [-0.0546],
         ...,
         [-0.0044],
         [-0.0200],
         [ 0.0393]],

        [[-0.0360],
         [-0.0270],
         [-0.0499],
         ...,
         [ 0.0560],
         [-0.0734],
         [ 0.0114]]])

encoder.encoders.11.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([1.2195, 1.1631, 1.2035,  ..., 1.0475, 1.0909, 1.0317])

encoder.encoders.11.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([-0.2012,  0.2619,  0.3522,  ..., -0.1486,  0.0673,  0.0282])

encoder.encoders.11.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([1.1768, 1.2162, 0.9682,  ..., 1.1199, 1.0232, 0.9553])

encoder.encoders.11.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([-0.2837,  0.4813,  0.1566,  ...,  0.2918, -0.1989, -0.0901])

encoder.encoders.11.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([1.1797, 1.1582, 1.1549,  ..., 1.2483, 1.1553, 1.1005])

encoder.encoders.11.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([ 0.0419,  0.0364, -0.0105,  ..., -0.0960,  0.0921, -0.0744])

encoder.encoders.11.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.2256, 1.3242, 1.3748,  ..., 1.2739, 1.2667, 1.3294])

encoder.encoders.11.norm_final.bias-torch.Size([1280])-torch.float32
tensor([-0.1079,  0.0501,  0.0607,  ..., -0.0183,  0.0152,  0.0356])

encoder.encoders.12.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[-0.0802,  0.0692,  0.2054,  ...,  0.2116, -0.3946, -0.3337],
        [ 0.1129, -0.1937, -0.0988,  ...,  0.0534, -0.0175,  0.3229],
        [-0.1072, -0.2158, -0.2111,  ...,  0.2249,  0.0510, -0.0157],
        ...,
        [ 0.0489,  0.0427,  0.2333,  ..., -0.0645,  0.0025, -0.2285],
        [-0.1250, -0.2239, -0.2227,  ..., -0.2326,  0.0127,  0.1158],
        [ 0.1913, -0.2420, -0.3757,  ..., -0.1067,  0.1042,  0.0652]])

encoder.encoders.12.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[-0.1468, -0.2969, -0.1696,  ..., -0.1844,  0.4958, -0.0459],
        [ 0.0699, -0.2560,  0.0601,  ...,  0.0997, -0.2079,  0.1098],
        [ 0.1945,  0.3556,  0.3665,  ...,  0.0127, -0.2122,  0.1934],
        ...,
        [ 0.0839,  0.1697, -0.4067,  ...,  0.3571,  0.2058,  0.1759],
        [-0.1453, -0.0509, -0.0578,  ...,  0.0067,  0.0082, -0.0957],
        [-0.1056,  0.4474,  0.4407,  ...,  0.1696, -0.1699,  0.0307]])

encoder.encoders.12.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 7.6243e-02, -4.8817e-02,  2.6692e-02,  ...,  7.5084e-02,
          5.8172e-02, -3.3152e-02],
        [ 4.3128e-02, -2.4666e-02,  3.3488e-04,  ...,  2.9929e-02,
          2.8263e-02,  3.7203e-03],
        [ 6.8008e-02, -5.1544e-02,  8.2867e-02,  ...,  3.3396e-02,
          7.4457e-02, -4.8800e-02],
        ...,
        [-7.9163e-02,  2.1211e-02, -1.9062e-02,  ..., -1.3504e-01,
          2.3023e-02, -1.6076e-02],
        [ 1.8215e-05, -7.1969e-03,  1.1984e-02,  ..., -3.0973e-02,
          6.1067e-02,  8.4545e-02],
        [ 4.8408e-05,  3.2424e-02, -5.3722e-02,  ...,  4.3791e-02,
         -5.5985e-02,  3.8780e-02]])

encoder.encoders.12.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0382,  0.0101,  0.0020,  ...,  0.0352, -0.0104,  0.0403],
        [-0.0506,  0.0128,  0.0628,  ..., -0.0033,  0.0430, -0.0708],
        [ 0.0481,  0.0072,  0.0237,  ...,  0.0550, -0.0094,  0.0010],
        ...,
        [ 0.0303, -0.0866,  0.0257,  ..., -0.0286,  0.0482,  0.0275],
        [ 0.0283,  0.0758,  0.0059,  ..., -0.0183,  0.0407, -0.0201],
        [-0.0165, -0.0032,  0.0096,  ..., -0.0156, -0.0212,  0.0390]])

encoder.encoders.12.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0118, -0.0222, -0.0324,  ..., -0.0004, -0.0199,  0.0320],
        [ 0.0522,  0.0618, -0.0263,  ..., -0.0216, -0.1002, -0.0564],
        [-0.0047,  0.0027,  0.0386,  ..., -0.0372,  0.0620, -0.0208],
        ...,
        [ 0.0263,  0.0011, -0.0303,  ...,  0.0517,  0.0676,  0.0264],
        [-0.0424,  0.0274, -0.0062,  ...,  0.0071, -0.0736, -0.0479],
        [-0.0372, -0.0024,  0.0312,  ..., -0.0378,  0.0377,  0.0112]])

encoder.encoders.12.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0662, -0.0488, -0.0152,  ...,  0.0172,  0.0127, -0.0438],
        [ 0.0096,  0.0182, -0.0132,  ...,  0.0189,  0.0128, -0.0086],
        [ 0.0629, -0.0628,  0.0134,  ...,  0.0177, -0.0337,  0.0235],
        ...,
        [-0.0132,  0.0020, -0.0297,  ...,  0.0030, -0.0296, -0.0693],
        [-0.0131,  0.0346, -0.1088,  ..., -0.0241, -0.0435,  0.0017],
        [ 0.0111, -0.0314,  0.0635,  ...,  0.0192,  0.0032, -0.0281]])

encoder.encoders.12.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0583, -0.1464,  0.0409,  ..., -0.0411, -0.0083, -0.0758],
        [ 0.0894, -0.0944,  0.0792,  ..., -0.0492,  0.0031, -0.0918],
        [ 0.0544, -0.0721, -0.0480,  ..., -0.0079, -0.0341,  0.0175],
        ...,
        [ 0.0116,  0.2302, -0.0618,  ...,  0.0023,  0.0401, -0.0284],
        [ 0.0377, -0.3094,  0.0435,  ..., -0.0163,  0.0498, -0.0139],
        [ 0.0450,  0.3294, -0.0287,  ..., -0.0426,  0.0275, -0.0215]])

encoder.encoders.12.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.8695, 0.7625, 0.7987,  ..., 0.7675, 0.8065, 0.7598])

encoder.encoders.12.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([-0.0134, -0.0611, -0.0633,  ..., -0.0404, -0.0358, -0.0551])

encoder.encoders.12.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.8718, 0.8499, 0.8109,  ..., 0.9015, 0.8644, 0.8302])

encoder.encoders.12.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([ 0.0110, -0.0003, -0.0046,  ..., -0.0058,  0.0122, -0.0069])

encoder.encoders.12.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.5162, 0.5458, 0.5319,  ..., 0.5574, 0.5714, 0.5502])

encoder.encoders.12.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([ 0.0452, -0.0802, -0.1147,  ..., -0.0529, -0.0353, -0.0526])

encoder.encoders.12.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 3.5901e-02,  4.5855e-02,  1.5316e-02,  ...,  6.2524e-02,
         -1.2672e-02, -4.2123e-02],
        [-6.7794e-02, -3.3353e-02,  2.7967e-05,  ...,  3.9372e-02,
         -5.8239e-02,  5.4850e-02],
        [ 1.1218e-03, -3.6728e-02,  4.8152e-02,  ...,  6.4937e-02,
          7.4288e-02,  7.0959e-03],
        ...,
        [ 7.3811e-02,  5.6182e-03,  3.8436e-02,  ...,  5.6175e-02,
         -9.1392e-03, -7.7480e-02],
        [ 4.1943e-02,  3.4807e-02, -4.9710e-02,  ..., -3.8068e-02,
          3.3514e-02, -6.9210e-02],
        [ 1.2841e-02, -2.5337e-02, -1.3042e-02,  ...,  6.5232e-02,
         -1.6589e-02,  2.9233e-02]])

encoder.encoders.12.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0118,  0.0069,  0.0055,  ..., -0.0027, -0.0187, -0.0239])

encoder.encoders.12.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0134,  0.0225,  0.0735,  ...,  0.0409,  0.0093,  0.0471],
        [ 0.1254, -0.0273,  0.0082,  ...,  0.0020,  0.0179,  0.0051],
        [ 0.0088,  0.1115, -0.0762,  ..., -0.0516,  0.0015,  0.0748],
        ...,
        [-0.0464, -0.0087,  0.0288,  ...,  0.0757, -0.0165,  0.0206],
        [-0.0418,  0.0358, -0.0446,  ...,  0.0172,  0.0072,  0.0544],
        [-0.0143, -0.0478, -0.0384,  ..., -0.0623, -0.0565, -0.0244]])

encoder.encoders.12.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([ 0.0011, -0.0075, -0.0053,  ...,  0.0313,  0.0133,  0.0102])

encoder.encoders.12.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0327,  0.0430, -0.0112,  ...,  0.0224,  0.0306, -0.0055],
        [ 0.0991, -0.0025, -0.0616,  ...,  0.0724,  0.0799, -0.0951],
        [-0.0061,  0.0007,  0.0419,  ..., -0.0535,  0.0238, -0.0095],
        ...,
        [ 0.0686,  0.0015,  0.0050,  ...,  0.0094,  0.0384, -0.0152],
        [-0.0290, -0.0326, -0.0803,  ..., -0.0193,  0.0607, -0.0401],
        [ 0.0329,  0.0213, -0.0119,  ..., -0.0178,  0.0218, -0.0782]])

encoder.encoders.12.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([ 0.0121, -0.0362,  0.0073,  ..., -0.0482, -0.0487,  0.0009])

encoder.encoders.12.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0275,  0.0267, -0.0438,  ..., -0.0118, -0.0394, -0.0531],
        [-0.0077,  0.0022,  0.0359,  ...,  0.0385, -0.0326,  0.0103],
        [ 0.0062,  0.0135, -0.0095,  ..., -0.0405, -0.0511,  0.0483],
        ...,
        [ 0.0029,  0.0420, -0.0073,  ..., -0.0171,  0.0796, -0.0320],
        [ 0.0010,  0.0180, -0.0570,  ...,  0.0490,  0.0662, -0.0640],
        [ 0.0008, -0.0439, -0.0189,  ..., -0.0871, -0.0048, -0.0386]])

encoder.encoders.12.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0426,  0.0092,  0.0374,  ...,  0.0129,  0.0203, -0.0071])

encoder.encoders.12.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[ 0.0710],
         [-0.0294],
         [-0.0170],
         ...,
         [ 0.0019],
         [ 0.0840],
         [-0.0258]],

        [[ 0.0163],
         [-0.0662],
         [ 0.0306],
         ...,
         [ 0.0245],
         [-0.0687],
         [ 0.0081]],

        [[ 0.0027],
         [ 0.0101],
         [-0.0172],
         ...,
         [ 0.0211],
         [ 0.0603],
         [-0.0005]],

        ...,

        [[ 0.0627],
         [ 0.0808],
         [ 0.0550],
         ...,
         [-0.0065],
         [ 0.0507],
         [-0.0365]],

        [[-0.0351],
         [ 0.1149],
         [ 0.0839],
         ...,
         [ 0.0808],
         [-0.0005],
         [-0.0255]],

        [[ 0.0202],
         [-0.0023],
         [-0.0422],
         ...,
         [ 0.0726],
         [-0.1007],
         [-0.0412]]])

encoder.encoders.12.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[-0.0477, -0.0071, -0.0147,  ..., -0.0193, -0.0262, -0.0632]],

        [[-0.0124, -0.0005, -0.0021,  ...,  0.0054, -0.0010, -0.0136]],

        [[-0.0187, -0.0159, -0.0026,  ..., -0.0294, -0.0219, -0.0305]],

        ...,

        [[ 0.0312, -0.0075, -0.0167,  ..., -0.0042, -0.0110, -0.0378]],

        [[ 0.0102,  0.0064,  0.0066,  ..., -0.0308,  0.0058, -0.0817]],

        [[ 0.0066,  0.0117,  0.0213,  ..., -0.0095, -0.0100, -0.0385]]])

encoder.encoders.12.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([1.0009, 1.0332, 1.0786,  ..., 1.0221, 1.0318, 1.0514])

encoder.encoders.12.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.0685, -0.0481, -0.0344,  ..., -0.0660, -0.1002, -0.0505])

encoder.encoders.12.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[-0.0435],
         [ 0.0006],
         [ 0.0750],
         ...,
         [-0.1161],
         [-0.0759],
         [-0.0797]],

        [[-0.0094],
         [ 0.0072],
         [ 0.0397],
         ...,
         [ 0.0164],
         [ 0.0169],
         [-0.0762]],

        [[-0.0749],
         [ 0.0128],
         [ 0.0059],
         ...,
         [-0.0504],
         [ 0.0317],
         [-0.0359]],

        ...,

        [[ 0.0403],
         [-0.0257],
         [ 0.0499],
         ...,
         [-0.0465],
         [ 0.0176],
         [-0.0357]],

        [[ 0.0064],
         [ 0.0016],
         [-0.0192],
         ...,
         [-0.0469],
         [ 0.0028],
         [ 0.0163]],

        [[ 0.0777],
         [-0.0386],
         [ 0.0673],
         ...,
         [ 0.0142],
         [ 0.0611],
         [-0.0250]]])

encoder.encoders.12.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([1.0582, 1.0065, 0.9776,  ..., 1.1329, 1.0781, 1.0318])

encoder.encoders.12.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([-0.1780,  0.0386,  0.1750,  ...,  0.2301,  0.0054,  0.2302])

encoder.encoders.12.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([1.0914, 1.0331, 0.9024,  ..., 0.9543, 0.9731, 0.9241])

encoder.encoders.12.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([-0.2501,  0.1491,  0.0980,  ..., -0.2258,  0.0920,  0.1474])

encoder.encoders.12.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([1.1510, 1.1311, 1.0025,  ..., 1.1675, 1.2116, 1.0882])

encoder.encoders.12.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([ 0.0064, -0.0298,  0.0520,  ...,  0.0148,  0.0331, -0.1210])

encoder.encoders.12.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.3146, 1.4014, 1.4733,  ..., 1.2979, 1.2895, 1.3557])

encoder.encoders.12.norm_final.bias-torch.Size([1280])-torch.float32
tensor([-0.0886,  0.0096,  0.0816,  ...,  0.1468, -0.0323,  0.0633])

encoder.encoders.13.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[ 3.5103e-01, -1.9401e-01, -2.3225e-01,  ...,  5.4396e-02,
          5.7708e-02,  2.4428e-01],
        [ 7.4495e-03,  8.8894e-02, -1.0643e-01,  ..., -7.3466e-02,
         -5.3155e-02, -6.1503e-02],
        [-3.4695e-01, -6.1503e-02,  3.4965e-01,  ...,  1.9633e-01,
         -8.8438e-02,  1.9019e-01],
        ...,
        [-7.4795e-02, -2.0808e-01,  2.7484e-01,  ..., -1.6671e-01,
          1.7007e-01,  1.6637e-01],
        [-2.5505e-02,  3.1732e-02,  1.5111e-01,  ..., -2.4717e-01,
          1.0237e-01,  6.7842e-03],
        [-3.2596e-01, -2.7741e-01,  2.6494e-05,  ...,  2.8740e-01,
          1.6925e-01, -2.7460e-01]])

encoder.encoders.13.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[ 0.1111,  0.1870,  0.0156,  ..., -0.1340, -0.1902, -0.2029],
        [-0.0119,  0.0881,  0.0249,  ...,  0.4007, -0.0174, -0.0167],
        [ 0.3667, -0.1805, -0.0336,  ..., -0.3332,  0.3529, -0.1219],
        ...,
        [-0.1821,  0.1491,  0.0209,  ..., -0.2720,  0.2655,  0.3101],
        [ 0.2130,  0.0504,  0.0982,  ..., -0.0284, -0.1180,  0.2160],
        [-0.0810,  0.3582, -0.3268,  ..., -0.0938, -0.2853,  0.4437]])

encoder.encoders.13.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0281, -0.0413,  0.0550,  ...,  0.0549,  0.0171,  0.0427],
        [ 0.0842,  0.0383,  0.0017,  ...,  0.0530,  0.0160,  0.0235],
        [-0.0851, -0.0269, -0.0180,  ...,  0.0227, -0.0239, -0.0767],
        ...,
        [ 0.0638, -0.0479,  0.0097,  ...,  0.0093,  0.0200, -0.0138],
        [ 0.0251, -0.0332, -0.0297,  ..., -0.0959,  0.0333, -0.1116],
        [-0.0043,  0.0388, -0.0522,  ...,  0.0160, -0.0931,  0.0749]])

encoder.encoders.13.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0684, -0.0005, -0.0050,  ..., -0.0152,  0.0215,  0.0249],
        [-0.0053,  0.0119, -0.0855,  ..., -0.0358,  0.0052, -0.0246],
        [-0.0445,  0.0601,  0.0213,  ..., -0.0040,  0.0411, -0.0205],
        ...,
        [-0.0275, -0.0641,  0.0142,  ...,  0.0304,  0.0411, -0.0308],
        [ 0.0098,  0.0645, -0.0269,  ...,  0.0551, -0.0002, -0.0472],
        [-0.0107, -0.0179,  0.0186,  ..., -0.0395,  0.0175, -0.0348]])

encoder.encoders.13.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0251,  0.0470,  0.0176,  ..., -0.0190, -0.0334,  0.0432],
        [-0.0530,  0.0471,  0.0272,  ...,  0.0132, -0.0521, -0.0309],
        [-0.0256, -0.0202,  0.0045,  ...,  0.0455,  0.0179, -0.0511],
        ...,
        [-0.1056, -0.0018, -0.0205,  ..., -0.0050,  0.0278,  0.0054],
        [ 0.0220, -0.0755, -0.0269,  ..., -0.0528,  0.0754,  0.0157],
        [ 0.0325,  0.0014, -0.0489,  ..., -0.0252,  0.0296, -0.0205]])

encoder.encoders.13.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 3.9941e-02, -1.2248e-02, -6.0961e-02,  ..., -2.9257e-02,
          3.3591e-02, -2.6007e-02],
        [-2.6581e-02,  4.1826e-02, -3.4616e-02,  ..., -3.1713e-03,
         -8.7520e-03,  1.6950e-02],
        [ 8.1424e-03,  1.5761e-02,  3.2182e-02,  ..., -5.6744e-02,
         -2.1092e-02, -5.8544e-02],
        ...,
        [ 6.2438e-02,  3.2126e-02, -1.2616e-02,  ..., -1.5531e-02,
         -6.8339e-03,  6.9802e-03],
        [ 1.9857e-03,  1.1001e-02,  1.7496e-02,  ...,  1.2504e-02,
          1.8827e-02,  3.4902e-02],
        [-4.6817e-02, -5.2144e-03,  2.0250e-02,  ...,  7.3805e-02,
         -4.5313e-06, -1.1734e-02]])

encoder.encoders.13.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0113, -0.1241,  0.0277,  ..., -0.0157, -0.0086,  0.0176],
        [-0.0230,  0.0140,  0.0346,  ..., -0.0089,  0.0390, -0.0900],
        [ 0.0457,  0.1179,  0.0137,  ..., -0.0425,  0.0325, -0.0188],
        ...,
        [-0.0006, -0.2832,  0.0170,  ..., -0.0228, -0.0549, -0.0693],
        [ 0.0044, -0.1269, -0.0159,  ..., -0.0492,  0.0171, -0.0545],
        [-0.0113,  0.3636,  0.0170,  ..., -0.0403,  0.0095,  0.0553]])

encoder.encoders.13.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.8602, 0.6566, 0.6022,  ..., 0.7731, 0.7686, 0.6798])

encoder.encoders.13.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([ 0.0024, -0.0662, -0.1507,  ..., -0.0592, -0.0362, -0.0216])

encoder.encoders.13.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.8534, 0.7837, 0.7547,  ..., 0.8152, 0.8140, 0.7578])

encoder.encoders.13.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([ 0.0260, -0.0374,  0.0052,  ..., -0.0041, -0.0215,  0.0462])

encoder.encoders.13.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.5183, 0.4801, 0.4924,  ..., 0.5369, 0.5009, 0.5217])

encoder.encoders.13.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([ 0.0852, -0.0714, -0.1110,  ..., -0.0978, -0.0298, -0.0304])

encoder.encoders.13.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 0.0371, -0.0132,  0.0359,  ...,  0.1419,  0.0141, -0.0599],
        [-0.0625,  0.0566, -0.0706,  ...,  0.0032, -0.0225, -0.0237],
        [-0.0208, -0.0191,  0.0248,  ..., -0.0802,  0.0375, -0.0023],
        ...,
        [ 0.0431, -0.0345,  0.0327,  ..., -0.0209,  0.0612, -0.0783],
        [-0.0490,  0.0315,  0.0165,  ..., -0.1059,  0.1979,  0.0040],
        [ 0.0235,  0.0257, -0.0485,  ...,  0.0466,  0.1006,  0.0917]])

encoder.encoders.13.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0193, -0.0200, -0.0257,  ..., -0.0153, -0.0026, -0.0319])

encoder.encoders.13.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[-0.0629, -0.0175,  0.0204,  ...,  0.0097,  0.0575,  0.0433],
        [-0.0757,  0.0404,  0.0471,  ..., -0.0393, -0.0011,  0.0397],
        [-0.0038, -0.0501, -0.0713,  ...,  0.0641,  0.1492, -0.0422],
        ...,
        [-0.0066,  0.0473, -0.0692,  ..., -0.0064, -0.0434,  0.0380],
        [ 0.0003,  0.0614, -0.0971,  ...,  0.0772, -0.0314, -0.0182],
        [ 0.0823,  0.1338, -0.0145,  ..., -0.0085, -0.0366, -0.0446]])

encoder.encoders.13.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0841,  0.0020, -0.0279,  ...,  0.0045, -0.0165, -0.0420])

encoder.encoders.13.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0226,  0.0449,  0.0289,  ..., -0.0128, -0.0090,  0.0832],
        [ 0.0453,  0.0080,  0.0658,  ..., -0.0161,  0.0105, -0.0026],
        [-0.0632, -0.0958, -0.0050,  ..., -0.0038, -0.0005, -0.0116],
        ...,
        [-0.0373,  0.0941,  0.0331,  ..., -0.1481, -0.0302, -0.0635],
        [ 0.0801, -0.0693,  0.0494,  ..., -0.0818, -0.0251,  0.0373],
        [ 0.0299, -0.0180, -0.0077,  ...,  0.0233, -0.0102, -0.0157]])

encoder.encoders.13.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0249, -0.0149, -0.0221,  ..., -0.0373, -0.0321, -0.0326])

encoder.encoders.13.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0003,  0.0514, -0.0030,  ..., -0.0519,  0.0457, -0.1011],
        [ 0.0637, -0.0506, -0.1318,  ...,  0.0557,  0.0250, -0.0730],
        [-0.1033,  0.0111,  0.0102,  ..., -0.0176, -0.0674, -0.0355],
        ...,
        [ 0.0329,  0.0746,  0.0563,  ..., -0.0345, -0.0179, -0.0231],
        [ 0.0111,  0.0263,  0.0224,  ...,  0.0099,  0.0202, -0.0113],
        [ 0.1053, -0.0627,  0.0745,  ..., -0.0198,  0.0510,  0.0900]])

encoder.encoders.13.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0530,  0.0061,  0.0015,  ...,  0.0492, -0.0390, -0.0170])

encoder.encoders.13.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[ 0.0640],
         [-0.0849],
         [-0.0410],
         ...,
         [-0.0061],
         [-0.0072],
         [-0.0058]],

        [[-0.0779],
         [ 0.0198],
         [-0.0027],
         ...,
         [-0.0426],
         [-0.0031],
         [ 0.1141]],

        [[ 0.0090],
         [ 0.0203],
         [ 0.0139],
         ...,
         [-0.0093],
         [ 0.0881],
         [-0.0128]],

        ...,

        [[ 0.0819],
         [-0.1099],
         [-0.0520],
         ...,
         [-0.0343],
         [ 0.0365],
         [ 0.0688]],

        [[-0.0245],
         [-0.0748],
         [-0.0263],
         ...,
         [-0.0034],
         [ 0.0987],
         [ 0.0051]],

        [[ 0.0092],
         [-0.0398],
         [ 0.0374],
         ...,
         [-0.0225],
         [ 0.0709],
         [ 0.0534]]])

encoder.encoders.13.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[-7.1014e-03, -1.5985e-04,  1.8030e-03,  ..., -2.0133e-02,
          -1.8002e-02, -8.1141e-02]],

        [[ 5.2390e-03, -3.7325e-03,  3.3690e-03,  ..., -1.1293e-05,
          -9.0329e-03,  9.1034e-03]],

        [[-4.0831e-02, -4.6862e-03, -2.8367e-02,  ...,  2.3165e-03,
           1.6998e-03,  1.4776e-02]],

        ...,

        [[-2.7158e-02,  5.1601e-03, -1.1840e-02,  ..., -2.0740e-02,
          -1.9483e-02, -4.5722e-02]],

        [[-4.1316e-02, -1.8265e-02, -3.2543e-02,  ..., -1.7113e-03,
           3.7289e-05, -5.4635e-03]],

        [[ 3.9390e-02,  1.0301e-02,  7.7494e-03,  ...,  1.8807e-02,
           1.0245e-02,  6.3403e-02]]])

encoder.encoders.13.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([1.0383, 0.9688, 0.9830,  ..., 1.0026, 0.9958, 0.9822])

encoder.encoders.13.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.0795, -0.1113, -0.1185,  ..., -0.0534, -0.0097, -0.0808])

encoder.encoders.13.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[-0.0298],
         [ 0.0216],
         [-0.0249],
         ...,
         [ 0.0197],
         [ 0.0431],
         [ 0.0031]],

        [[-0.0342],
         [ 0.0513],
         [-0.0302],
         ...,
         [-0.0325],
         [-0.0808],
         [ 0.0794]],

        [[-0.0197],
         [ 0.0165],
         [ 0.0164],
         ...,
         [-0.0601],
         [ 0.0613],
         [-0.0264]],

        ...,

        [[ 0.0455],
         [ 0.0724],
         [ 0.0255],
         ...,
         [-0.0305],
         [-0.0278],
         [ 0.0316]],

        [[ 0.0940],
         [-0.0310],
         [-0.0289],
         ...,
         [ 0.0336],
         [ 0.0999],
         [-0.0334]],

        [[ 0.0886],
         [-0.0348],
         [ 0.0395],
         ...,
         [-0.0190],
         [ 0.0772],
         [-0.0713]]])

encoder.encoders.13.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([1.1770, 1.0626, 0.9918,  ..., 1.0994, 1.0431, 0.9872])

encoder.encoders.13.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([-0.2629,  0.2844,  0.2182,  ...,  0.1239, -0.1781,  0.0587])

encoder.encoders.13.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([0.9267, 0.8806, 0.7852,  ..., 1.0289, 0.9407, 0.9483])

encoder.encoders.13.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([ 0.0068, -0.0265,  0.1536,  ...,  0.3101,  0.0364,  0.2533])

encoder.encoders.13.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([1.2861, 1.0334, 0.9422,  ..., 1.2466, 1.2353, 1.0718])

encoder.encoders.13.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([-0.1077,  0.1413, -0.0246,  ...,  0.1706,  0.0627, -0.0920])

encoder.encoders.13.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.3039, 1.4086, 1.4913,  ..., 1.2921, 1.2788, 1.3891])

encoder.encoders.13.norm_final.bias-torch.Size([1280])-torch.float32
tensor([-0.1887,  0.0088,  0.0478,  ...,  0.0082, -0.0725, -0.0694])

encoder.encoders.14.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[ 0.0061,  0.0813, -0.3046,  ...,  0.1523,  0.2793,  0.1701],
        [-0.3276, -0.0065, -0.0198,  ..., -0.1420,  0.3321, -0.2630],
        [-0.3311,  0.1995, -0.2412,  ..., -0.1172, -0.1570,  0.1033],
        ...,
        [-0.3053,  0.1364,  0.1500,  ...,  0.0538,  0.0859, -0.1626],
        [-0.1080, -0.1489, -0.1896,  ..., -0.0577,  0.0267, -0.2405],
        [-0.2985, -0.1411,  0.0681,  ...,  0.1505, -0.2284,  0.0105]])

encoder.encoders.14.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[-0.2539, -0.0846,  0.3427,  ..., -0.2672, -0.3249, -0.3035],
        [ 0.3435, -0.0079, -0.1729,  ..., -0.0915, -0.1825, -0.0767],
        [ 0.2857, -0.0751, -0.0468,  ..., -0.0497,  0.0552,  0.0596],
        ...,
        [ 0.3171,  0.3194, -0.3077,  ..., -0.0733, -0.1387,  0.3851],
        [-0.1662,  0.1563,  0.1759,  ..., -0.0896,  0.0019, -0.2468],
        [-0.1876, -0.0406,  0.2050,  ..., -0.0524,  0.0974, -0.0356]])

encoder.encoders.14.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0543,  0.0125, -0.0536,  ...,  0.0363, -0.0358, -0.0076],
        [ 0.0213, -0.0322, -0.0699,  ..., -0.0045,  0.0127,  0.0800],
        [-0.0619, -0.0352, -0.0434,  ..., -0.0621,  0.0622,  0.0504],
        ...,
        [ 0.0049, -0.0409, -0.0142,  ...,  0.0560,  0.0186,  0.0761],
        [-0.0147, -0.0229, -0.0101,  ..., -0.0186, -0.0075, -0.0051],
        [ 0.0203,  0.0396, -0.0432,  ..., -0.0118,  0.0647,  0.0238]])

encoder.encoders.14.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0220,  0.0427,  0.0052,  ...,  0.0549, -0.1030, -0.0056],
        [-0.0475, -0.0613, -0.0561,  ...,  0.0373, -0.0340,  0.0151],
        [ 0.0742,  0.0311,  0.0155,  ..., -0.0116, -0.0032,  0.0035],
        ...,
        [ 0.0064,  0.0770, -0.0639,  ..., -0.0190,  0.0073,  0.0256],
        [ 0.0678,  0.0367,  0.0020,  ...,  0.0492, -0.0419,  0.0061],
        [-0.0384,  0.0480, -0.0537,  ..., -0.0025,  0.0208, -0.0697]])

encoder.encoders.14.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 6.1338e-05,  2.2624e-02,  2.1585e-03,  ..., -5.0278e-02,
          1.4934e-02,  2.8384e-02],
        [ 1.8085e-02,  8.7058e-03, -3.2831e-02,  ...,  6.8049e-04,
          1.7746e-02,  2.5753e-02],
        [ 5.8042e-02,  7.7754e-03,  6.6277e-02,  ..., -1.1352e-02,
          1.3108e-02, -5.7559e-02],
        ...,
        [-2.3490e-02,  2.8328e-02,  5.9321e-02,  ..., -5.2571e-02,
          6.5492e-03, -9.7834e-02],
        [-1.5645e-02,  1.0805e-02,  2.1869e-02,  ..., -3.1286e-02,
         -1.9372e-02, -1.7356e-02],
        [ 2.3242e-02, -1.3387e-02,  1.4730e-02,  ...,  4.6511e-02,
         -4.0402e-02,  3.3472e-02]])

encoder.encoders.14.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0305,  0.0311,  0.0044,  ...,  0.0306, -0.0136,  0.0143],
        [-0.0245,  0.0348, -0.0284,  ..., -0.0067,  0.0047,  0.0531],
        [ 0.0149, -0.1006,  0.0305,  ...,  0.0151,  0.0273,  0.0325],
        ...,
        [-0.0034, -0.0025,  0.0306,  ...,  0.0844, -0.0222, -0.0414],
        [-0.0083, -0.0061,  0.0091,  ..., -0.0537, -0.0532, -0.0718],
        [-0.0207,  0.0165, -0.0121,  ...,  0.0381,  0.0228, -0.0480]])

encoder.encoders.14.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0679,  0.0189, -0.0186,  ..., -0.0687,  0.0201,  0.0134],
        [-0.0123, -0.0110, -0.0556,  ..., -0.0250, -0.0365,  0.0511],
        [ 0.0190,  0.0840, -0.0565,  ..., -0.0073,  0.0109, -0.0481],
        ...,
        [-0.0080,  0.0373,  0.0095,  ..., -0.0855, -0.0070, -0.0216],
        [-0.0306,  0.0440, -0.0194,  ..., -0.0058,  0.0405, -0.0262],
        [ 0.0066,  0.0317,  0.0067,  ..., -0.0259,  0.0293,  0.0638]])

encoder.encoders.14.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.6921, 0.6181, 0.5527,  ..., 0.6971, 0.6855, 0.6448])

encoder.encoders.14.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([-0.0262, -0.0204, -0.0119,  ..., -0.0752,  0.0042, -0.0187])

encoder.encoders.14.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.8028, 0.8283, 0.8260,  ..., 0.8348, 0.8083, 0.7859])

encoder.encoders.14.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([-0.0145,  0.0001, -0.0348,  ...,  0.0004,  0.0205, -0.0016])

encoder.encoders.14.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.5791, 0.5549, 0.5411,  ..., 0.5664, 0.5638, 0.5801])

encoder.encoders.14.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([ 0.1099, -0.0322, -0.0626,  ..., -0.0349,  0.0535,  0.0276])

encoder.encoders.14.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 0.1097,  0.0134, -0.0166,  ..., -0.0520, -0.0102,  0.0503],
        [ 0.0270, -0.0312,  0.0980,  ...,  0.0944, -0.0604, -0.0253],
        [ 0.1276, -0.0350, -0.0611,  ...,  0.0653,  0.0381, -0.0482],
        ...,
        [ 0.0328, -0.0605,  0.0134,  ...,  0.0003,  0.0765,  0.0066],
        [-0.0215,  0.0693, -0.0061,  ...,  0.0583, -0.0682,  0.0775],
        [ 0.0257,  0.0522,  0.0402,  ..., -0.0471,  0.0261, -0.0396]])

encoder.encoders.14.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0317,  0.0061, -0.0276,  ..., -0.0301, -0.0294, -0.0448])

encoder.encoders.14.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0445,  0.0016,  0.0097,  ...,  0.0338,  0.0888,  0.0757],
        [ 0.0836, -0.0119, -0.0035,  ..., -0.0154,  0.0108,  0.0439],
        [-0.0529,  0.0424, -0.0022,  ..., -0.0300,  0.0030,  0.0095],
        ...,
        [ 0.0176, -0.0513,  0.1055,  ..., -0.0599, -0.0749, -0.0261],
        [-0.0247,  0.0481,  0.0533,  ..., -0.0417,  0.0209, -0.0662],
        [ 0.1004, -0.0135,  0.0059,  ...,  0.0619,  0.0002, -0.0014]])

encoder.encoders.14.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0880, -0.0290,  0.0123,  ...,  0.0116, -0.0592, -0.0307])

encoder.encoders.14.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0277, -0.0338, -0.0711,  ..., -0.0125,  0.0368, -0.0459],
        [ 0.0163, -0.0213,  0.0164,  ...,  0.0248,  0.1345, -0.0772],
        [ 0.0257,  0.0133, -0.0405,  ...,  0.0008,  0.0021,  0.0363],
        ...,
        [-0.0074, -0.0183,  0.0956,  ..., -0.0324,  0.0015,  0.0094],
        [ 0.0073,  0.0081, -0.0772,  ..., -0.0448,  0.0082, -0.0577],
        [-0.0034,  0.0287, -0.0109,  ..., -0.0239,  0.0489, -0.1168]])

encoder.encoders.14.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0285, -0.0142, -0.0252,  ..., -0.0335, -0.0196, -0.0144])

encoder.encoders.14.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0209,  0.0018, -0.0143,  ..., -0.0583,  0.0291,  0.0384],
        [ 0.0486, -0.0530,  0.0309,  ...,  0.0128, -0.0207,  0.0028],
        [ 0.0471, -0.0148, -0.0121,  ...,  0.0119, -0.0072,  0.0028],
        ...,
        [ 0.0012, -0.0191, -0.0196,  ...,  0.0062, -0.0267,  0.0286],
        [-0.0214,  0.0889, -0.0213,  ..., -0.0916,  0.1174,  0.0659],
        [ 0.0503, -0.0451,  0.0283,  ..., -0.0451,  0.0282, -0.0535]])

encoder.encoders.14.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0852, -0.0211,  0.0253,  ...,  0.0025, -0.0407, -0.0372])

encoder.encoders.14.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[-0.1185],
         [ 0.0439],
         [-0.0097],
         ...,
         [-0.0425],
         [-0.0053],
         [-0.0296]],

        [[ 0.0144],
         [-0.0074],
         [ 0.0191],
         ...,
         [ 0.0264],
         [-0.0329],
         [-0.0645]],

        [[ 0.0536],
         [-0.0734],
         [-0.0912],
         ...,
         [ 0.0308],
         [ 0.0094],
         [-0.0152]],

        ...,

        [[-0.0413],
         [-0.1480],
         [-0.0411],
         ...,
         [-0.0507],
         [ 0.1471],
         [-0.0868]],

        [[ 0.0774],
         [ 0.0162],
         [-0.0069],
         ...,
         [-0.0039],
         [ 0.0426],
         [ 0.0592]],

        [[-0.0155],
         [ 0.0347],
         [ 0.0543],
         ...,
         [ 0.0516],
         [-0.0014],
         [ 0.0727]]])

encoder.encoders.14.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[-0.0032, -0.0074, -0.0151,  ...,  0.0075,  0.0187,  0.0017]],

        [[-0.0068, -0.0137, -0.0327,  ..., -0.0214, -0.0165, -0.0514]],

        [[-0.0015, -0.0091, -0.0026,  ..., -0.0318, -0.0483, -0.0594]],

        ...,

        [[-0.0035, -0.0068, -0.0024,  ..., -0.0123, -0.0067, -0.0019]],

        [[ 0.0231, -0.0088, -0.0122,  ..., -0.0016,  0.0039, -0.0195]],

        [[ 0.0117, -0.0185,  0.0183,  ..., -0.0293, -0.0350, -0.0500]]])

encoder.encoders.14.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([0.9896, 1.0612, 1.0053,  ..., 1.0727, 0.9666, 0.9692])

encoder.encoders.14.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.0914, -0.0812, -0.1025,  ..., -0.0376, -0.0656, -0.0607])

encoder.encoders.14.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[-0.0452],
         [ 0.0051],
         [ 0.0578],
         ...,
         [ 0.0360],
         [-0.0695],
         [-0.0308]],

        [[ 0.1022],
         [ 0.1118],
         [-0.0501],
         ...,
         [-0.0764],
         [ 0.0687],
         [ 0.0451]],

        [[ 0.0187],
         [-0.0403],
         [-0.0013],
         ...,
         [-0.1023],
         [-0.0455],
         [ 0.0947]],

        ...,

        [[ 0.0711],
         [-0.0073],
         [ 0.0072],
         ...,
         [ 0.0044],
         [-0.0414],
         [ 0.0377]],

        [[-0.0720],
         [-0.0341],
         [-0.0899],
         ...,
         [ 0.0002],
         [-0.0469],
         [ 0.0121]],

        [[-0.0018],
         [ 0.0472],
         [-0.0432],
         ...,
         [-0.0638],
         [ 0.0225],
         [-0.0727]]])

encoder.encoders.14.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([1.1478, 0.9703, 1.0202,  ..., 1.0648, 1.2245, 1.0931])

encoder.encoders.14.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([-0.2401, -0.1693,  0.1009,  ...,  0.0252, -0.3201, -0.0838])

encoder.encoders.14.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([1.1255, 0.8932, 0.7986,  ..., 1.0005, 0.9885, 0.9211])

encoder.encoders.14.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([-0.2415,  0.1281,  0.0296,  ...,  0.0853,  0.0518, -0.0175])

encoder.encoders.14.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([1.2011, 1.1030, 0.9857,  ..., 1.1431, 1.1633, 1.1389])

encoder.encoders.14.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([-0.1679, -0.1434, -0.0160,  ..., -0.0562, -0.0684, -0.1146])

encoder.encoders.14.norm_final.weight-torch.Size([1280])-torch.float32
tensor([1.3235, 1.3747, 1.4153,  ..., 1.3189, 1.2951, 1.3354])

encoder.encoders.14.norm_final.bias-torch.Size([1280])-torch.float32
tensor([-0.1399, -0.0076,  0.0232,  ..., -0.0300, -0.0930, -0.1141])

encoder.encoders.15.self_attn.pos_bias_u-torch.Size([20, 64])-torch.float32
tensor([[-0.1553,  0.3293, -0.3554,  ..., -0.2377, -0.0101, -0.0135],
        [-0.0516,  0.3237,  0.0522,  ...,  0.0894, -0.1985, -0.1012],
        [ 0.3389, -0.1938,  0.1606,  ..., -0.2090, -0.1052,  0.2724],
        ...,
        [ 0.0325,  0.1826, -0.1643,  ...,  0.1826, -0.0655,  0.1485],
        [ 0.0064,  0.0775, -0.0935,  ...,  0.1797, -0.1065, -0.0264],
        [-0.0132,  0.2183,  0.2171,  ..., -0.1016, -0.2352, -0.2216]])

encoder.encoders.15.self_attn.pos_bias_v-torch.Size([20, 64])-torch.float32
tensor([[-0.0092, -0.2497,  0.2289,  ...,  0.2044, -0.1002, -0.0619],
        [ 0.3271,  0.0525,  0.2372,  ..., -0.1301,  0.1912, -0.0094],
        [ 0.0504,  0.1773,  0.0570,  ..., -0.1757,  0.0429,  0.1098],
        ...,
        [-0.0655,  0.0270, -0.1417,  ..., -0.1047, -0.1959,  0.1833],
        [-0.1053,  0.0876, -0.0401,  ...,  0.2908,  0.3270, -0.2756],
        [ 0.2107,  0.1604, -0.0986,  ...,  0.1448,  0.1995,  0.1383]])

encoder.encoders.15.self_attn.linear_q.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0322,  0.0024, -0.0022,  ..., -0.0477, -0.0401, -0.0464],
        [ 0.0199, -0.0757,  0.0415,  ...,  0.0315, -0.0147, -0.0587],
        [-0.0611,  0.0776, -0.0048,  ...,  0.0056, -0.0040,  0.0005],
        ...,
        [ 0.0400,  0.0330, -0.0288,  ..., -0.0090, -0.0406, -0.0142],
        [ 0.0391, -0.0418, -0.0388,  ...,  0.0674,  0.0059, -0.0076],
        [ 0.0272, -0.0404,  0.0563,  ..., -0.0103, -0.0336,  0.0378]])

encoder.encoders.15.self_attn.linear_k.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0521,  0.0146,  0.0216,  ..., -0.0086, -0.0045,  0.0394],
        [-0.0336,  0.0334, -0.0356,  ..., -0.0863,  0.0005,  0.0135],
        [-0.0295,  0.0201,  0.0608,  ..., -0.0579,  0.0047, -0.0247],
        ...,
        [-0.0495,  0.0507,  0.0347,  ...,  0.0015, -0.0170, -0.0203],
        [-0.0516, -0.0241, -0.0206,  ..., -0.0505, -0.0359,  0.0378],
        [ 0.0318, -0.0273, -0.0235,  ...,  0.0141, -0.0108, -0.0060]])

encoder.encoders.15.self_attn.linear_v.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0267, -0.0174, -0.0370,  ..., -0.0007, -0.0169, -0.0904],
        [ 0.0133, -0.0675, -0.0555,  ...,  0.0061, -0.0465, -0.0251],
        [-0.0349, -0.0482,  0.0810,  ...,  0.0109,  0.0267, -0.0252],
        ...,
        [-0.0241,  0.0003, -0.0378,  ..., -0.0273,  0.0333, -0.0003],
        [ 0.0337, -0.0396,  0.0276,  ...,  0.0201, -0.0006, -0.0001],
        [ 0.0128,  0.0542,  0.0287,  ..., -0.0486,  0.0600, -0.0116]])

encoder.encoders.15.self_attn.linear_out.weight-torch.Size([1280, 1280])-torch.float32
tensor([[ 0.0042, -0.0211, -0.0173,  ...,  0.0442,  0.0328,  0.0218],
        [ 0.0255, -0.0344, -0.0766,  ...,  0.0424,  0.0534,  0.0134],
        [ 0.0337,  0.0556,  0.0396,  ...,  0.0187, -0.0709,  0.0379],
        ...,
        [-0.0622, -0.0366,  0.0250,  ...,  0.0266,  0.0011, -0.0265],
        [ 0.0306,  0.0213, -0.0317,  ..., -0.0699, -0.0103, -0.0114],
        [ 0.0416, -0.0323, -0.0195,  ...,  0.0128, -0.1071, -0.0542]])

encoder.encoders.15.self_attn.linear_pos.weight-torch.Size([1280, 1280])-torch.float32
tensor([[-0.0667, -0.0003,  0.0043,  ..., -0.0543, -0.0069,  0.0295],
        [ 0.0767, -0.0256,  0.1010,  ...,  0.0078,  0.0060,  0.0677],
        [-0.0596, -0.0016, -0.0593,  ...,  0.0093,  0.0337,  0.0603],
        ...,
        [-0.0226, -0.0298, -0.0362,  ...,  0.0750,  0.0504,  0.0154],
        [ 0.0106,  0.0587,  0.0075,  ..., -0.0404, -0.0131,  0.0099],
        [-0.0272, -0.0490, -0.0280,  ...,  0.0403, -0.0099, -0.0240]])

encoder.encoders.15.self_attn.layer_norm_q.weight-torch.Size([1280])-torch.float32
tensor([0.6994, 0.6781, 0.5876,  ..., 0.7122, 0.6647, 0.7006])

encoder.encoders.15.self_attn.layer_norm_q.bias-torch.Size([1280])-torch.float32
tensor([ 0.0119, -0.0244, -0.0450,  ...,  0.0271, -0.0180,  0.0178])

encoder.encoders.15.self_attn.layer_norm_k.weight-torch.Size([1280])-torch.float32
tensor([0.9151, 0.8666, 0.7634,  ..., 0.8922, 0.8997, 0.9040])

encoder.encoders.15.self_attn.layer_norm_k.bias-torch.Size([1280])-torch.float32
tensor([-0.0330, -0.0522, -0.0211,  ..., -0.0732, -0.0023, -0.0018])

encoder.encoders.15.self_attn.layer_norm_v.weight-torch.Size([1280])-torch.float32
tensor([0.6175, 0.6296, 0.6113,  ..., 0.6501, 0.6466, 0.6512])

encoder.encoders.15.self_attn.layer_norm_v.bias-torch.Size([1280])-torch.float32
tensor([ 0.0544, -0.0080, -0.0250,  ..., -0.0049, -0.0063,  0.0042])

encoder.encoders.15.feed_forward.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[-0.0288,  0.0252,  0.0208,  ...,  0.0816, -0.0312,  0.0326],
        [ 0.0383, -0.0774,  0.0467,  ..., -0.0387,  0.0709,  0.0226],
        [-0.0599, -0.0901,  0.0359,  ...,  0.0504, -0.0261, -0.0178],
        ...,
        [-0.0014, -0.0536, -0.1070,  ..., -0.0383,  0.0204, -0.0381],
        [ 0.0523, -0.0864,  0.0855,  ...,  0.0298,  0.0212,  0.0136],
        [ 0.0010,  0.0954, -0.0065,  ..., -0.0272, -0.0521, -0.0763]])

encoder.encoders.15.feed_forward.w_1.bias-torch.Size([5120])-torch.float32
tensor([-0.0056, -0.0485, -0.0564,  ...,  0.0037, -0.0129,  0.0043])

encoder.encoders.15.feed_forward.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0082,  0.0035, -0.0670,  ..., -0.0386,  0.0226, -0.0403],
        [ 0.0011, -0.0007,  0.0486,  ...,  0.0077,  0.0433, -0.0462],
        [-0.0400,  0.0082,  0.0117,  ..., -0.0027, -0.0228, -0.0045],
        ...,
        [-0.0661,  0.0756, -0.0555,  ...,  0.0072,  0.0992, -0.0665],
        [ 0.0088, -0.0190,  0.0097,  ..., -0.0044,  0.0010, -0.0230],
        [ 0.0172,  0.0701, -0.0429,  ...,  0.0070, -0.0122, -0.0852]])

encoder.encoders.15.feed_forward.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.1033, -0.0339, -0.0162,  ..., -0.0404,  0.0138, -0.0640])

encoder.encoders.15.feed_forward_macaron.w_1.weight-torch.Size([5120, 1280])-torch.float32
tensor([[ 0.0404,  0.0392, -0.0154,  ...,  0.0756,  0.0263,  0.0310],
        [ 0.0087,  0.0127, -0.0021,  ..., -0.0479,  0.0098, -0.0279],
        [-0.0367, -0.0657,  0.0499,  ..., -0.0015,  0.0105,  0.0467],
        ...,
        [-0.0531, -0.0404,  0.0149,  ...,  0.0291,  0.0305, -0.0291],
        [ 0.0112,  0.0228, -0.0764,  ...,  0.0070,  0.0822,  0.0647],
        [-0.0021,  0.0052,  0.0358,  ..., -0.0949,  0.0360,  0.0284]])

encoder.encoders.15.feed_forward_macaron.w_1.bias-torch.Size([5120])-torch.float32
tensor([ 0.0079, -0.0435, -0.0517,  ..., -0.0401, -0.0369,  0.0039])

encoder.encoders.15.feed_forward_macaron.w_2.weight-torch.Size([1280, 5120])-torch.float32
tensor([[ 0.0058, -0.0129, -0.0316,  ..., -0.0818, -0.0439,  0.0186],
        [-0.0544,  0.0262,  0.0174,  ..., -0.0056, -0.1133,  0.0149],
        [ 0.0250,  0.0069, -0.0076,  ..., -0.0600,  0.0115, -0.0255],
        ...,
        [ 0.0564, -0.0658,  0.0382,  ...,  0.0663, -0.0013, -0.0384],
        [-0.0715, -0.0413,  0.0310,  ..., -0.0068,  0.0282, -0.0330],
        [ 0.0076, -0.0309,  0.0007,  ..., -0.0531,  0.0870,  0.0480]])

encoder.encoders.15.feed_forward_macaron.w_2.bias-torch.Size([1280])-torch.float32
tensor([-0.0791,  0.0136,  0.0007,  ...,  0.0024, -0.0880, -0.0821])

encoder.encoders.15.conv_module.pointwise_conv1.weight-torch.Size([5120, 1280, 1])-torch.float32
tensor([[[ 0.0029],
         [ 0.0481],
         [ 0.0486],
         ...,
         [ 0.0008],
         [-0.0345],
         [ 0.0483]],

        [[-0.0449],
         [ 0.0149],
         [ 0.0622],
         ...,
         [ 0.0724],
         [-0.0693],
         [ 0.0397]],

        [[-0.0506],
         [ 0.0044],
         [ 0.0485],
         ...,
         [-0.0457],
         [-0.0376],
         [-0.0331]],

        ...,

        [[ 0.0462],
         [-0.0356],
         [ 0.0648],
         ...,
         [ 0.1145],
         [-0.0026],
         [-0.0492]],

        [[ 0.0867],
         [ 0.0926],
         [-0.0517],
         ...,
         [ 0.0021],
         [ 0.0627],
         [ 0.0622]],

        [[ 0.0265],
         [-0.0246],
         [-0.0124],
         ...,
         [ 0.0124],
         [-0.0002],
         [-0.0686]]])

encoder.encoders.15.conv_module.depthwise_conv.weight-torch.Size([2560, 1, 33])-torch.float32
tensor([[[-0.0124, -0.0139, -0.0089,  ..., -0.0111,  0.0109,  0.0156]],

        [[-0.0388, -0.0285, -0.0477,  ..., -0.0215,  0.0067,  0.0153]],

        [[-0.0158, -0.0005, -0.0117,  ..., -0.0266, -0.0085,  0.0010]],

        ...,

        [[ 0.0117,  0.0019,  0.0182,  ..., -0.0284, -0.0185, -0.0496]],

        [[-0.0068, -0.0322, -0.0144,  ..., -0.0162, -0.0237, -0.0029]],

        [[-0.0237, -0.0254, -0.0125,  ..., -0.0108, -0.0085, -0.0184]]])

encoder.encoders.15.conv_module.norm.weight-torch.Size([2560])-torch.float32
tensor([0.9710, 0.9905, 1.0345,  ..., 1.0157, 1.0097, 0.9831])

encoder.encoders.15.conv_module.norm.bias-torch.Size([2560])-torch.float32
tensor([-0.0876, -0.0600, -0.0595,  ..., -0.0598, -0.0557, -0.0630])

encoder.encoders.15.conv_module.pointwise_conv2.weight-torch.Size([1280, 2560, 1])-torch.float32
tensor([[[ 0.0160],
         [-0.0048],
         [-0.0102],
         ...,
         [ 0.0154],
         [-0.0719],
         [-0.0626]],

        [[-0.0096],
         [ 0.0360],
         [-0.0108],
         ...,
         [ 0.0859],
         [ 0.0163],
         [-0.0386]],

        [[-0.0355],
         [-0.0366],
         [-0.0352],
         ...,
         [-0.0251],
         [-0.0006],
         [-0.0732]],

        ...,

        [[ 0.0279],
         [ 0.0273],
         [-0.0500],
         ...,
         [ 0.0112],
         [ 0.0437],
         [-0.1378]],

        [[-0.0562],
         [-0.0769],
         [-0.0617],
         ...,
         [ 0.0717],
         [-0.0659],
         [-0.0637]],

        [[ 0.0778],
         [-0.0634],
         [ 0.0168],
         ...,
         [ 0.0756],
         [ 0.0030],
         [ 0.0169]]])

encoder.encoders.15.norm_ff.weight-torch.Size([1280])-torch.float32
tensor([1.1133, 1.1245, 1.0759,  ..., 1.2147, 1.2728, 1.2305])

encoder.encoders.15.norm_ff.bias-torch.Size([1280])-torch.float32
tensor([-0.0370,  0.0720,  0.0641,  ..., -0.1279, -0.1719, -0.1422])

encoder.encoders.15.norm_ff_macaron.weight-torch.Size([1280])-torch.float32
tensor([1.1024, 0.9293, 0.9175,  ..., 0.9875, 1.0390, 1.0064])

encoder.encoders.15.norm_ff_macaron.bias-torch.Size([1280])-torch.float32
tensor([-0.2874, -0.1346,  0.0060,  ..., -0.0056, -0.1217, -0.0718])

encoder.encoders.15.norm_conv.weight-torch.Size([1280])-torch.float32
tensor([1.1271, 1.1236, 1.0607,  ..., 1.2295, 1.1977, 1.2087])

encoder.encoders.15.norm_conv.bias-torch.Size([1280])-torch.float32
tensor([-0.0915,  0.0231, -0.0820,  ..., -0.0794, -0.0107, -0.0717])

encoder.encoders.15.norm_final.weight-torch.Size([1280])-torch.float32
tensor([0.6705, 0.7086, 0.6973,  ..., 0.6836, 0.6640, 0.6913])

encoder.encoders.15.norm_final.bias-torch.Size([1280])-torch.float32
tensor([ 0.0019, -0.0434, -0.0445,  ..., -0.0097,  0.0054, -0.0127])

llm.base_model.model.model.embed_tokens.weight-torch.Size([152064, 3584])-torch.float32
tensor([[-3.0884e-02, -9.1553e-03,  2.9602e-03,  ...,  1.0681e-02,
          2.1851e-02, -8.5449e-03],
        [ 2.6398e-03,  4.1199e-03,  1.4160e-02,  ..., -3.9673e-03,
          1.4954e-02,  6.1340e-03],
        [-1.5015e-02, -8.5449e-03,  1.4771e-02,  ..., -6.4392e-03,
         -2.0386e-02,  3.1128e-03],
        ...,
        [-1.1755e-37,  1.1755e-37,  1.1755e-37,  ..., -1.1755e-37,
          1.1755e-37,  1.1755e-37],
        [ 1.1755e-37, -1.1755e-37,  1.1755e-37,  ..., -1.1755e-37,
          1.1755e-37, -1.1755e-37],
        [ 1.1755e-37, -1.1755e-37, -1.1755e-37,  ...,  1.1755e-37,
          1.1755e-37, -1.1755e-37]])

llm.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0040, -0.0041, -0.0090,  ...,  0.0070, -0.0069, -0.0025],
        [-0.0019, -0.0054, -0.0294,  ..., -0.0081,  0.0012, -0.0120],
        [-0.0081,  0.0027,  0.0097,  ...,  0.0044,  0.0059,  0.0140],
        ...,
        [ 0.0114,  0.0078,  0.0206,  ..., -0.0010,  0.0030,  0.0022],
        [ 0.0084, -0.0075, -0.0160,  ..., -0.0283, -0.0063,  0.0137],
        [ 0.0004, -0.0010,  0.0027,  ..., -0.0062,  0.0203,  0.0089]])

llm.base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.2520,  1.3359, -0.7891,  ...,  0.1719, -0.1807,  0.0737])

llm.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0127, -0.0460, -0.0074,  ..., -0.0431,  0.0538, -0.0036],
        [-0.0248, -0.0031, -0.0329,  ..., -0.0076,  0.0223,  0.0151],
        [-0.0195, -0.0026, -0.0014,  ...,  0.0332,  0.0409, -0.0140],
        ...,
        [-0.0420,  0.0054, -0.0120,  ...,  0.0794,  0.0285,  0.0314],
        [ 0.0314, -0.0346,  0.0081,  ..., -0.0692,  0.0075,  0.0234],
        [-0.0072, -0.0090,  0.0325,  ...,  0.0747, -0.0316, -0.0182]])

llm.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0128,  0.0191, -0.0136,  ..., -0.0471,  0.0144, -0.0054],
        [ 0.0213,  0.0390,  0.0265,  ..., -0.0042, -0.0382,  0.0387],
        [ 0.0015,  0.0208,  0.0017,  ..., -0.0147, -0.0250,  0.0187],
        ...,
        [ 0.0093, -0.0154,  0.0032,  ...,  0.0279, -0.0021,  0.0009],
        [-0.0045, -0.0118, -0.0042,  ..., -0.0063, -0.0032,  0.0122],
        [ 0.0095,  0.0179,  0.0277,  ...,  0.0226, -0.0019,  0.0118]])

llm.base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0024, -0.0062, -0.0016,  ..., -0.0044, -0.0027,  0.0189],
        [ 0.0488,  0.0159, -0.0306,  ..., -0.0059, -0.0071, -0.0061],
        [-0.0032,  0.0004,  0.0398,  ...,  0.0293, -0.0069,  0.0168],
        ...,
        [-0.0334, -0.0089, -0.0099,  ..., -0.0134,  0.0303,  0.0339],
        [-0.0126,  0.0093, -0.0204,  ..., -0.0071,  0.0486, -0.0031],
        [ 0.0569, -0.0092, -0.0049,  ...,  0.0269, -0.0364, -0.0364]])

llm.base_model.model.model.layers.0.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-7.4219e-01,  2.2969e+00,  5.6641e-01, -2.9297e-01,  5.3516e-01,
         4.8438e-01,  2.3906e+00,  1.0312e+00,  2.6367e-01, -6.6406e-01,
         6.2109e-01,  5.3516e-01,  3.4062e+00, -9.6094e-01, -8.3203e-01,
        -2.6758e-01, -1.0703e+00,  5.7812e-01,  8.3203e-01,  3.9648e-01,
         1.3965e-01,  8.6914e-02,  4.5625e+00, -1.9141e-01,  5.6641e-01,
         8.9844e-01,  1.0645e-01, -4.7656e-01, -3.5938e-01,  3.2617e-01,
         8.8281e-01,  3.5625e+00, -2.9883e-01, -4.6875e-01, -4.8438e-01,
         3.9648e-01, -5.1172e-01,  1.1406e+00, -1.6406e+00,  7.0312e-01,
         8.0078e-01, -8.0625e+00, -1.4766e+00, -1.0781e+00, -3.1250e+00,
        -1.3672e+00, -3.4766e-01, -1.5000e+00, -7.5391e-01,  9.7500e+00,
        -2.2070e-01, -1.4438e+01, -1.3250e+01, -2.0000e+01,  1.8000e+01,
        -6.7500e+01,  7.1500e+01,  1.0150e+02,  7.5000e+01,  6.8000e+01,
         7.4500e+01,  1.2700e+02, -1.0150e+02,  1.6600e+02, -4.4375e+00,
        -3.8086e-01,  8.5938e-01, -1.9219e+00, -1.2188e+00, -1.2344e+00,
         7.4219e-01, -1.1230e-02, -2.2754e-01,  5.0781e-01, -9.2969e-01,
         4.7266e-01,  7.6562e-01, -9.3750e-01, -3.8086e-01,  3.4668e-02,
        -3.9453e-01,  6.8359e-01,  1.6113e-02,  2.6406e+00,  5.5078e-01,
         2.0312e-01, -1.9141e+00,  6.6528e-03,  7.2656e-01, -6.3672e-01,
        -6.9336e-02,  2.5195e-01,  9.6875e-01, -1.0596e-01, -1.0781e+00,
         4.6875e+00,  8.6426e-02,  1.3438e+00, -7.5391e-01, -9.3750e-01,
         1.4062e+00, -9.9609e-01, -2.2070e-01, -4.1992e-01, -9.7656e-01,
         1.3000e+01, -9.0625e-01, -1.6250e+00,  3.4375e+00, -5.0000e-01,
        -1.4922e+00, -4.2500e+00, -3.0938e+00,  8.9375e+00, -1.7891e+00,
        -1.5000e+00,  1.4625e+01, -9.8750e+00, -4.4000e+01,  1.6125e+01,
         8.2000e+01,  6.7000e+01, -3.3750e+00,  1.5800e+02,  1.5500e+02,
        -1.1050e+02, -1.6000e+02, -1.0900e+02,  7.0312e+00, -2.1406e+00,
         3.4219e+00, -2.4062e+00, -2.9844e+00,  2.5781e-01,  7.1875e-01,
         1.3184e-01, -4.3750e-01, -2.2656e-01,  3.2344e+00,  7.3438e-01,
         8.0566e-03,  3.2031e+00, -5.1562e-01, -2.1191e-01,  1.5723e-01,
         1.9629e-01, -8.0469e-01,  5.6885e-02,  1.8594e+00,  2.6758e-01,
         6.5234e-01, -8.7500e-01, -3.5312e+00, -2.1387e-01, -4.3213e-02,
         3.8672e-01,  3.3203e-01,  1.6406e-01, -5.1953e-01, -1.1641e+00,
        -5.2490e-02,  4.6875e+00, -3.7031e+00,  3.8330e-02,  2.3242e-01,
        -1.1094e+00, -1.0205e-01,  4.5117e-01, -7.4219e-01,  1.6016e-01,
         9.0625e-01,  9.2578e-01,  9.5312e-01, -1.8750e-01, -1.2734e+00,
        -2.3730e-01,  8.2812e-01,  1.3203e+00, -8.4375e-01,  7.3828e-01,
         4.9805e-01, -4.6250e+00, -2.9062e+00, -6.1250e+01,  1.3672e+00,
        -1.2500e+01,  3.7250e+01,  2.9375e+01,  6.7000e+01,  1.0400e+02,
        -1.0250e+02,  3.6250e+01, -1.8906e+00,  1.1641e+00,  1.1816e-01,
         9.3750e-01,  1.7656e+00, -8.5547e-01,  6.0547e-01, -2.3438e+00,
         1.1953e+00,  1.5869e-02, -1.9141e-01, -5.1172e-01,  2.0020e-01,
        -7.0312e-01,  2.5586e-01, -6.1719e-01, -2.7734e-01, -3.6719e-01,
        -5.2734e-01, -4.3945e-02,  3.0000e+00,  1.5918e-01, -2.4219e-01,
        -5.0781e-01,  2.0781e+00,  1.6797e-01,  4.0039e-01, -7.0312e-01,
         7.2266e-01,  5.9766e-01, -6.6895e-02, -2.3535e-01, -3.3984e-01,
        -2.4688e+00, -3.2188e+00, -1.1865e-01, -5.7422e-01,  2.2754e-01,
        -1.1250e+00,  3.0518e-02, -7.2754e-02,  2.8516e-01,  9.4922e-01,
        -1.0469e+00, -1.5078e+00, -4.0938e+00,  1.4609e+00, -9.2773e-02,
        -2.0000e+00, -1.5234e+00, -1.6484e+00,  1.1133e-01, -8.6719e-01,
         1.4141e+00, -2.3750e+00, -1.9750e+01, -9.6250e+00,  5.4062e+00,
        -1.7500e+01, -5.4750e+01,  7.8500e+01, -3.0375e+01, -1.2200e+02,
         9.6000e+01,  2.5156e+00, -1.6641e+00,  1.9531e+00,  6.8359e-01,
         1.8906e+00, -1.4844e-01,  3.8086e-01, -1.6016e-01,  1.7676e-01,
        -6.9922e-01,  9.7656e-02, -7.0703e-01, -4.1406e-01,  3.0762e-02,
         6.2988e-02,  9.8633e-02, -2.4414e-01, -9.8145e-02,  2.5312e+00,
        -8.3984e-02,  6.1035e-03, -2.4512e-01, -4.5508e-01, -2.7539e-01,
         7.2266e-01,  2.3242e-01, -1.7656e+00, -6.7969e-01, -2.4414e-01,
         5.5078e-01,  9.9219e-01, -5.3125e-01, -4.6875e-02,  2.2070e-01,
        -1.0938e+00, -7.8906e-01,  2.3145e-01, -2.6172e-01,  5.8203e-01,
         2.3594e+00,  6.2500e-02,  2.0703e-01, -2.0781e+00, -3.2422e-01,
         7.1484e-01, -5.1562e-01, -1.1035e-01,  3.5547e-01, -2.0703e-01,
        -1.8066e-01, -3.4180e-02,  9.5625e+00,  7.5000e+00, -1.6504e-01,
         3.2031e-01, -1.6094e+00,  1.3047e+00, -1.0562e+01, -4.2250e+01,
        -8.0000e+00,  5.7250e+01,  2.5375e+01, -9.0000e+01, -6.6000e+01,
         1.9989e-03,  1.5078e+00, -7.1777e-02,  5.2344e-01, -4.2188e-01,
         3.0859e-01, -1.9375e+00, -1.5078e+00, -5.8984e-01,  3.5156e-01,
        -2.1562e+00,  9.0332e-02,  2.7466e-02,  1.0703e+00, -1.3672e+00,
        -2.9297e-01, -6.7969e-01,  1.9336e-01, -6.6406e-01, -7.2656e-01,
         1.1084e-01,  2.2656e-01, -5.3516e-01, -1.3359e+00, -2.4707e-01,
        -1.2256e-01, -2.9062e+00, -5.0391e-01, -8.5938e-01, -3.7109e-01,
        -8.9453e-01, -4.4336e-01, -3.8672e-01, -4.0625e-01, -4.6484e-01,
        -1.2891e+00,  4.0820e-01, -8.5449e-02,  3.7695e-01, -5.3438e+00,
         3.0518e-02, -8.9111e-03,  2.0625e+00, -4.2969e-02, -1.4551e-01,
        -4.9609e-01,  2.2266e-01,  7.5000e-01,  7.3730e-02, -2.3633e-01,
         9.2578e-01,  6.8750e+00,  1.2250e+01,  1.4219e+00, -5.5469e-01,
         9.7266e-01, -2.5938e+00,  4.8438e+00, -1.1562e+01,  7.8000e+01,
         1.8875e+01, -1.0300e+02,  7.9500e+01,  1.0400e+02, -3.6719e-01,
        -1.7944e-02, -1.4531e+00,  1.2578e+00,  1.6602e-02,  1.8359e+00,
         1.0498e-02,  1.7090e-02, -1.5234e+00, -2.6953e-01, -1.6406e-01,
         1.1035e-01,  6.4062e-01,  1.9238e-01, -2.1191e-01, -9.0332e-02,
        -6.9531e-01, -7.4707e-02,  7.9590e-02,  1.3977e-02, -2.1875e+00,
         6.9824e-02,  4.1504e-02,  9.0332e-02,  2.2266e-01,  1.7480e-01,
         1.0312e+00, -2.1484e-02,  1.7969e-01, -1.7578e-02, -1.0107e-01,
         3.9368e-03,  6.2188e+00,  3.4180e-02, -1.5723e-01,  4.2188e-01,
        -4.2480e-02,  7.9102e-02, -8.0078e-01,  1.5918e-01,  1.6211e-01,
         5.9375e-01,  1.6953e+00, -1.2344e+00,  2.7812e+00,  2.9375e+00,
        -1.3672e+00,  1.7188e+00,  5.0781e-01,  3.3750e+00, -7.7188e+00,
         1.0375e+01,  3.6719e-01,  7.9000e+01, -7.1562e+00,  2.1000e+01,
        -2.0156e+00,  6.6562e+00,  2.5938e+00, -4.5898e-01,  3.5000e+00,
         5.3125e+00, -8.0625e+00, -5.1562e+00,  1.8750e+00,  1.4219e+00,
         3.9062e-01, -1.2734e+00,  4.1748e-02,  8.4375e-01, -4.5410e-02,
         1.4258e-01, -7.5781e-01,  4.8584e-02, -6.8848e-02, -2.8906e-01,
         4.7656e-01,  1.5625e-01,  3.4570e-01, -1.0925e-02, -8.0078e-02,
        -3.2959e-02,  7.8613e-02,  1.0681e-02, -2.7812e+00, -3.8330e-02,
        -1.2256e-01,  1.6211e-01, -3.5156e-02, -1.6797e-01, -8.8672e-01,
        -1.0156e-01,  9.4727e-02, -3.0078e-01,  3.3691e-02, -1.6479e-02,
         5.1250e+00,  9.1309e-02, -1.8066e-01,  1.1426e-01,  2.1484e-01,
         3.8281e-01, -9.1016e-01,  8.2031e-01,  9.7656e-02, -2.0117e-01,
        -6.1719e-01, -2.9844e+00,  6.8359e-01,  7.8125e-01, -8.9844e-02,
         1.5938e+00, -3.4688e+00, -6.8438e+00,  6.1250e+00, -8.5000e+00,
        -2.1719e+00, -5.4500e+01,  1.6641e+00,  4.2812e+00, -1.7375e+01,
         2.9844e+00, -6.6250e+00, -2.9062e+00,  6.6250e+00,  2.2812e+00,
        -4.4062e+00,  7.2500e+00])

llm.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0387, -0.0256, -0.0165,  ..., -0.0170, -0.0123,  0.0054],
        [ 0.0498,  0.0064,  0.0389,  ...,  0.0261, -0.0179,  0.0175],
        [ 0.0411,  0.0024,  0.0183,  ..., -0.0281, -0.0218, -0.0127],
        ...,
        [ 0.0316,  0.0316, -0.0079,  ..., -0.0294, -0.0490,  0.0202],
        [ 0.0259,  0.0367, -0.0736,  ..., -0.0051, -0.0079, -0.0390],
        [ 0.0589,  0.0105, -0.0289,  ..., -0.0434, -0.0459, -0.0295]])

llm.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0579, -0.0414,  0.0358,  ...,  0.0514, -0.0173,  0.0071],
        [-0.0277, -0.0972, -0.1314,  ..., -0.1730, -0.1099, -0.1453],
        [-0.0809, -0.0634, -0.0728,  ..., -0.0504, -0.0970, -0.0562],
        ...,
        [ 0.0293, -0.0016, -0.0091,  ..., -0.0183,  0.0027, -0.0152],
        [-0.0058,  0.0053, -0.0064,  ..., -0.0025, -0.0071,  0.0164],
        [-0.0038, -0.0117, -0.0319,  ..., -0.0153, -0.0036, -0.0010]])

llm.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0084,  0.0037, -0.0032,  ..., -0.0021,  0.0067,  0.0033],
        [ 0.0111,  0.0103,  0.0044,  ...,  0.0077, -0.0070,  0.0123],
        [ 0.0118,  0.0027, -0.0070,  ..., -0.0066, -0.0041, -0.0018],
        ...,
        [ 0.0030, -0.0165, -0.0007,  ...,  0.0142,  0.0030,  0.0013],
        [-0.0029, -0.0179, -0.0004,  ..., -0.0052, -0.0204, -0.0119],
        [ 0.0166,  0.0029,  0.0044,  ...,  0.0021, -0.0103, -0.0008]])

llm.base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-5.2979e-02, -2.3071e-02, -7.0496e-03,  1.0315e-02, -2.6733e-02,
         1.1816e-01, -3.0640e-02, -4.3701e-02, -1.9043e-02,  3.6377e-02,
         7.6660e-02,  1.3367e-02, -8.3008e-02, -2.6489e-02, -2.3315e-02,
         4.1016e-02, -4.4434e-02, -7.4219e-02, -4.7607e-02, -1.0205e-01,
        -1.6479e-02, -8.7891e-03, -1.6357e-02, -4.1504e-02,  8.7402e-02,
         2.8320e-02,  2.6367e-02,  6.4453e-02,  4.3701e-02,  1.7212e-02,
         4.7363e-02,  2.0703e-01,  4.6387e-02, -3.8086e-02, -1.5259e-02,
         1.0376e-02, -3.6865e-02,  1.6235e-02, -1.6797e-01,  1.4404e-02,
         3.2471e-02,  5.2246e-02,  6.8848e-02,  4.5508e-01, -9.1797e-02,
         1.3794e-02, -1.3275e-03,  1.9287e-02, -2.8198e-02, -5.4443e-02,
        -1.0742e-02, -7.7637e-02,  5.5664e-02,  4.1504e-02, -1.9775e-02,
         1.7700e-02, -1.4404e-02,  3.1128e-02, -5.2246e-02,  4.5654e-02,
        -1.8188e-02,  2.9419e-02,  1.0254e-01, -6.6406e-02, -8.6426e-02,
         2.5879e-02, -1.0864e-02, -5.7373e-02,  5.5908e-02,  7.0953e-04,
        -1.9409e-02, -8.3984e-02, -4.1748e-02,  1.8311e-02, -5.2490e-02,
         7.6294e-03,  3.5400e-02, -5.9814e-03,  3.1738e-02, -1.0986e-03,
         8.3008e-02,  1.4551e-01, -1.1133e-01,  1.0071e-02, -5.3467e-02,
        -8.9844e-01, -3.1006e-02, -2.8442e-02, -2.7657e-04, -3.8086e-02,
         1.9409e-02, -3.5156e-02, -5.1758e-02,  3.2715e-02,  4.9072e-02,
         3.2196e-03, -4.3213e-02, -6.8359e-02, -2.4048e-02, -4.6875e-02,
        -4.3457e-02,  1.1572e-01,  6.3965e-02, -2.1240e-02,  6.5918e-02,
         1.3672e-01, -2.5000e-01,  4.7363e-02,  8.4229e-03, -1.2451e-02,
        -5.3955e-02,  3.2471e-02, -4.3457e-02,  3.4668e-02,  6.1340e-03,
         8.9645e-04,  4.9805e-02,  2.3633e-01,  1.1816e-01, -7.2327e-03,
         3.9795e-02,  4.2236e-02, -1.3379e-01,  6.8359e-02, -2.5000e-01,
        -8.7891e-03,  5.0293e-02,  2.7954e-02, -3.7109e-02, -1.3733e-02,
         4.6289e-01, -1.1182e-01, -2.4609e-01,  3.0640e-02, -4.4441e-04,
         1.5625e-02,  5.3406e-03,  8.6914e-02, -5.5176e-02, -4.7363e-02,
        -5.2979e-02,  4.3213e-02,  6.3965e-02,  7.8964e-04, -3.0640e-02,
        -5.9204e-03,  2.3926e-01,  3.4668e-02, -1.1816e-01,  8.0078e-02,
         1.2756e-02,  1.0205e-01, -7.6660e-02,  1.5259e-02,  3.5400e-02,
        -1.7578e-02,  1.7212e-02, -6.4453e-02, -4.2419e-03, -4.4922e-02,
         7.6660e-02,  2.2095e-02, -7.8125e-03,  8.0566e-03, -3.7231e-03,
         2.5781e-01, -1.3977e-02, -2.0264e-02, -1.2109e-01, -8.1055e-02,
        -1.6479e-03,  9.3750e-02,  3.0273e-02,  9.3994e-03,  1.0452e-03,
        -2.7954e-02,  2.1729e-02, -1.7334e-02,  2.2217e-02, -1.0693e-01,
        -8.7402e-02, -4.5166e-03, -6.5918e-02,  2.4170e-02, -1.3916e-02,
         4.4189e-02,  7.4219e-02, -7.1106e-03, -3.8330e-02, -3.0273e-02,
         8.3984e-02,  5.5847e-03,  5.4932e-02, -1.9653e-02, -1.2793e-01,
        -1.3306e-02, -4.5654e-02,  2.4170e-02,  4.4434e-02,  6.1035e-02,
        -6.6406e-02,  2.8931e-02,  6.0791e-02, -1.1328e-01,  4.3213e-02,
         1.7822e-02,  5.1025e-02, -6.7871e-02,  4.3701e-02, -2.8564e-02,
        -2.9907e-02, -2.7100e-02, -2.8687e-02, -3.2654e-03, -1.1768e-01,
        -1.9287e-02,  3.3691e-02, -5.6152e-02,  1.8066e-02, -8.3984e-02,
        -3.1250e-02,  6.9824e-02, -9.7046e-03, -4.2480e-02, -3.6621e-03,
         4.0283e-02,  2.4292e-02,  7.8735e-03, -2.8839e-03, -2.9175e-02,
         1.3550e-02,  1.0889e-01, -1.0498e-02,  5.8838e-02, -6.9580e-03,
        -3.5889e-02,  9.7046e-03,  5.8350e-02, -4.1260e-02,  2.0508e-02,
         1.3184e-02,  5.1270e-02,  3.2959e-02, -1.5137e-02, -2.6978e-02,
         5.2734e-02, -3.9795e-02,  2.9541e-02,  4.9561e-02,  2.8564e-02,
         2.1484e-02, -8.5449e-03,  1.0925e-02,  8.6914e-02,  4.1016e-02,
        -1.8188e-02,  2.7710e-02,  5.6396e-02,  4.2969e-02, -3.9795e-02,
         2.5781e+00,  1.0645e-01,  2.6367e-02, -7.4219e-02,  8.4473e-02,
        -4.3457e-02,  2.5269e-02,  4.4250e-03, -6.3965e-02,  3.3188e-04,
         3.6328e-01,  3.3569e-03,  1.0010e-02, -2.1484e-02,  3.1738e-03,
         4.9438e-03, -8.8867e-02,  1.3184e-02,  2.9907e-02,  6.8970e-03,
         4.3945e-02,  1.4587e-02,  4.8096e-02, -4.6692e-03,  8.9355e-02,
         4.0039e-02, -5.3467e-02, -5.4199e-02, -4.8584e-02, -3.6865e-02,
         4.6143e-02,  2.5391e-02, -6.7871e-02,  8.8501e-03,  2.2583e-02,
         1.6098e-03,  6.8665e-03, -8.0566e-03, -8.3008e-03, -2.3804e-03,
        -1.4709e-02, -2.5757e-02,  4.4189e-02, -2.6245e-02,  5.7129e-02,
         8.9355e-02, -4.2236e-02, -2.7954e-02,  2.6978e-02, -5.3955e-02,
         8.8501e-03,  2.0264e-02,  4.9561e-02, -9.2773e-02,  1.4099e-02,
        -1.5747e-02,  1.6968e-02, -9.5825e-03,  6.2988e-02,  4.1809e-03,
         7.6660e-02,  2.9907e-02, -4.5776e-03, -8.1177e-03, -1.3672e-01,
         4.6387e-02, -3.4180e-02, -1.3489e-02, -4.3945e-02, -1.0596e-01,
        -4.4189e-02,  4.2114e-03,  5.3955e-02,  1.7548e-03,  6.1523e-02,
         1.3916e-02, -1.6357e-02, -1.2256e-01,  4.9133e-03, -4.3555e-01,
        -5.8899e-03,  9.5825e-03,  2.4292e-02,  4.9561e-02,  1.2451e-02,
        -4.6631e-02, -1.0010e-02, -1.0889e-01, -1.0925e-02,  2.1606e-02,
        -1.0840e-01, -5.9509e-03,  3.1738e-02,  4.0771e-02, -8.5938e-02,
         5.6396e-02, -2.2827e-02,  3.0151e-02,  6.4453e-02, -2.0386e-02,
        -5.8899e-03,  3.3691e-02,  5.7068e-03,  2.5146e-02, -1.3867e-01,
        -3.6377e-02, -9.7656e-03, -4.3335e-03,  7.9346e-03,  1.7456e-02,
        -7.4707e-02, -2.4048e-02,  2.4872e-03, -1.5442e-02, -3.5645e-02,
         1.2329e-02, -4.5410e-02, -1.7188e-01,  2.9297e-02, -4.8584e-02,
         4.6997e-03, -1.1963e-02,  6.7444e-03, -6.4941e-02,  5.1514e-02,
         1.1780e-02, -1.1414e-02,  1.9165e-02, -2.8809e-02, -3.6621e-03,
         4.8828e-03, -6.1646e-03,  2.8076e-02, -1.0303e-01,  3.4912e-02,
        -2.6367e-02, -1.1536e-02,  2.3926e-02, -4.0039e-02, -2.9907e-02,
        -1.1084e-01, -1.2573e-02, -6.5430e-02, -2.7466e-02, -8.7402e-02,
         2.4109e-03,  3.8300e-03, -1.1084e-01, -1.1414e-02, -2.8125e-01,
         3.0518e-02, -2.7710e-02, -1.1719e-02, -3.1494e-02, -3.1006e-02,
         6.0547e-02, -2.1362e-02,  3.3203e-02,  3.0029e-02, -1.7188e+00,
         3.4668e-02,  2.1729e-02,  4.1504e-02, -1.2390e-02,  3.4668e-02,
         6.2866e-03,  1.0742e-02, -4.7119e-02,  7.0190e-03,  4.6875e-02,
         3.2715e-02, -5.1758e-02, -8.7402e-02, -6.9336e-02, -2.5635e-03,
         2.3682e-02, -1.8799e-02,  4.4823e-04, -2.6245e-03, -9.3842e-04,
        -7.7148e-02, -3.2425e-04, -2.9419e-02,  4.6997e-03,  4.9072e-02,
        -1.3062e-02, -6.3782e-03, -1.0693e-01, -4.6692e-03, -2.9297e-02,
        -8.4839e-03,  2.4902e-02, -5.5542e-03, -4.9438e-03,  3.6774e-03,
         2.0264e-02,  6.2500e-02, -2.5391e-02, -7.0312e-02, -2.4170e-02,
         2.1606e-02,  1.6403e-03,  5.6641e-01,  2.5269e-02,  5.1172e-01,
         2.5879e-02, -4.1260e-02,  1.8677e-02, -1.9165e-02,  3.8910e-03,
        -1.7090e-02, -4.7363e-02, -1.1963e-02,  2.0938e+00, -1.4832e-02,
         1.5991e-02, -1.2756e-02, -5.1498e-04, -1.8359e+00, -3.4424e-02,
         8.1055e-02, -9.0332e-03, -4.4434e-02, -4.2969e-02, -1.1292e-02,
        -1.1719e-02,  1.7471e-03, -2.5391e-02,  1.5332e-01,  4.6143e-02,
        -2.5330e-03,  7.8613e-02,  3.1738e-02,  5.0049e-03, -1.3359e+00,
         8.8379e-02, -4.9316e-02,  1.7334e-02,  3.3203e-02,  4.8340e-02,
        -1.7090e-02, -1.2500e+00,  5.6396e-02, -3.8818e-02, -3.1494e-02,
        -1.5076e-02, -2.6398e-03,  1.3580e-03,  4.2725e-02,  3.2196e-03,
        -4.1016e-01,  2.4292e-02])

llm.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0041, -0.0424, -0.0018,  ...,  0.0221,  0.0180, -0.0041],
        [-0.0219, -0.0063, -0.0124,  ..., -0.0371,  0.0042,  0.0106],
        [-0.0214,  0.0082, -0.0211,  ...,  0.0203,  0.0339, -0.0381],
        ...,
        [-0.0303, -0.0597,  0.0442,  ...,  0.0051, -0.0099, -0.0044],
        [-0.0125, -0.0198,  0.0271,  ...,  0.0111, -0.0229, -0.0077],
        [-0.0276, -0.0079, -0.0247,  ...,  0.0191, -0.0152,  0.0194]])

llm.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0020, -0.0168, -0.0104,  ...,  0.0104, -0.0184,  0.0158],
        [ 0.0112,  0.0038,  0.0064,  ...,  0.0052,  0.0334, -0.0007],
        [ 0.0059,  0.0076,  0.0088,  ...,  0.0094,  0.0214, -0.0157],
        ...,
        [ 0.0122,  0.0085,  0.0100,  ...,  0.0059, -0.0019, -0.0133],
        [ 0.0081,  0.0068,  0.0404,  ...,  0.0076,  0.0723, -0.0326],
        [ 0.0393,  0.0114,  0.0071,  ..., -0.0112,  0.0119, -0.0414]])

llm.base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0072, -0.0042, -0.0089,  ..., -0.0059,  0.0055, -0.0016],
        [ 0.0027,  0.0173,  0.0135,  ...,  0.0024,  0.0008, -0.0003],
        [-0.0057,  0.0074,  0.0092,  ..., -0.0087,  0.0022,  0.0071],
        ...,
        [-0.0129, -0.0031, -0.0098,  ..., -0.0022,  0.0008,  0.0079],
        [ 0.0063, -0.0070, -0.0090,  ...,  0.0025, -0.0094, -0.0032],
        [-0.0057, -0.0087,  0.0161,  ...,  0.0074,  0.0067, -0.0083]])

llm.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0317, -0.0046,  0.0206,  ...,  0.0309, -0.0035,  0.0145],
        [ 0.0041,  0.0172,  0.0007,  ..., -0.0125, -0.0053,  0.0081],
        [ 0.0113,  0.0331,  0.0560,  ...,  0.0061,  0.0320,  0.0033],
        ...,
        [ 0.0095, -0.0018,  0.0355,  ..., -0.0061, -0.0190, -0.0082],
        [-0.0069,  0.0247, -0.0268,  ...,  0.0165, -0.0167, -0.0240],
        [ 0.0181, -0.0096, -0.0026,  ..., -0.0100,  0.0138, -0.0422]])

llm.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 3.7853e-02,  4.5744e-02, -9.0440e-03,  ...,  8.8999e-03,
          4.5151e-02, -1.5929e-02],
        [-7.3793e-03, -2.2706e-02,  1.7138e-02,  ..., -1.4617e-02,
         -1.3851e-02,  3.0323e-02],
        [-8.6619e-03, -1.8883e-02,  3.6175e-02,  ...,  2.9437e-03,
          7.9129e-03,  2.8753e-02],
        ...,
        [-2.7281e-02,  2.7486e-02, -2.4675e-02,  ..., -3.2047e-02,
         -4.7931e-02,  2.2161e-02],
        [-1.5796e-02, -2.6323e-02, -1.8516e-02,  ...,  1.2466e-02,
          9.5462e-03,  5.5212e-02],
        [-2.9318e-02,  5.8210e-05, -2.0132e-02,  ..., -1.2383e-03,
          1.8650e-03,  3.8817e-03]])

llm.base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-1.9775e-02, -1.7456e-02, -4.6692e-03,  ...,  2.2583e-03,
          9.2163e-03,  4.3945e-03],
        [-1.9653e-02,  3.5667e-04,  8.3618e-03,  ..., -6.4392e-03,
          1.4160e-02, -1.2268e-02],
        [-1.4114e-03,  3.8605e-03,  5.1575e-03,  ...,  3.6774e-03,
          9.8877e-03,  2.3926e-02],
        ...,
        [ 2.8801e-04, -3.9864e-04, -1.4782e-04,  ...,  9.9659e-05,
          4.1008e-04,  4.0436e-04],
        [-3.8300e-03,  7.2632e-03,  1.0376e-03,  ..., -7.4768e-03,
         -6.3477e-03, -3.0884e-02],
        [-1.5869e-02,  1.7456e-02, -1.4343e-02,  ...,  2.0386e-02,
         -3.4790e-03,  3.8086e-02]])

llm.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0258, -0.0231,  0.0130,  ..., -0.0294, -0.0071, -0.0452],
        [ 0.0433,  0.0265,  0.0978,  ..., -0.0377, -0.0162, -0.0027],
        [-0.0463,  0.0484, -0.0052,  ..., -0.0145,  0.0294, -0.0205],
        ...,
        [-0.0252,  0.0763,  0.0457,  ...,  0.0133, -0.0039, -0.0723],
        [ 0.0364, -0.0247,  0.0993,  ..., -0.0304,  0.0303,  0.0128],
        [ 0.0276, -0.0045, -0.0937,  ..., -0.0390, -0.0836,  0.0271]])

llm.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0048, -0.0223,  0.0051,  ..., -0.0248,  0.0111, -0.0089],
        [ 0.0184, -0.0115, -0.0011,  ..., -0.0022,  0.0107, -0.0243],
        [ 0.0168, -0.0219, -0.0226,  ...,  0.0511, -0.0047,  0.0156],
        ...,
        [-0.0073, -0.0111,  0.0099,  ..., -0.0099,  0.0104,  0.0076],
        [-0.0216, -0.0220, -0.0078,  ...,  0.0258,  0.0319, -0.0083],
        [-0.0235,  0.0161,  0.0258,  ...,  0.0049, -0.0107,  0.0067]])

llm.base_model.model.model.layers.0.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0019, -0.0022,  0.0052,  ...,  0.0011,  0.0364,  0.0055],
        [ 0.0008,  0.0059, -0.0008,  ..., -0.0198,  0.0164, -0.0049],
        [ 0.0093,  0.0154, -0.0039,  ..., -0.0260, -0.0004, -0.0044],
        ...,
        [-0.0007, -0.0002,  0.0001,  ..., -0.0003, -0.0015, -0.0013],
        [ 0.0184,  0.0032,  0.0059,  ..., -0.0141, -0.0023,  0.0107],
        [-0.0018, -0.0025, -0.0254,  ..., -0.0259, -0.0164,  0.0113]])

llm.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0101,  0.0174,  0.0240,  ...,  0.0069, -0.0095, -0.0635],
        [ 0.0138, -0.0712, -0.0849,  ..., -0.0125,  0.0305,  0.0517],
        [ 0.0810, -0.0394,  0.0444,  ..., -0.0527, -0.0357, -0.0402],
        ...,
        [ 0.0418,  0.0004, -0.0085,  ...,  0.0110, -0.0178,  0.0345],
        [ 0.0654, -0.0518, -0.0296,  ..., -0.0124, -0.0231, -0.0243],
        [-0.0084, -0.0329, -0.0596,  ..., -0.0579, -0.0176,  0.0714]])

llm.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0055, -0.0124, -0.0028,  ...,  0.0126,  0.0133,  0.0090],
        [ 0.0116,  0.0061, -0.0052,  ...,  0.0462,  0.0104,  0.0021],
        [ 0.0177, -0.0064, -0.0334,  ...,  0.0019, -0.0210, -0.0252],
        ...,
        [-0.0079, -0.0103,  0.0076,  ...,  0.0219,  0.0050, -0.0036],
        [-0.0094, -0.0073,  0.0179,  ..., -0.0368, -0.0264,  0.0489],
        [-0.0089,  0.0256, -0.0206,  ...,  0.0099, -0.0333, -0.0048]])

llm.base_model.model.model.layers.0.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 5.0049e-03,  1.6098e-03, -8.4229e-03,  ...,  2.4796e-05,
          8.3923e-04, -1.2634e-02],
        [ 1.2573e-02,  1.2207e-02,  1.7456e-02,  ...,  6.1512e-05,
         -7.1716e-03, -4.1809e-03],
        [ 1.7262e-04,  9.3384e-03,  4.3335e-03,  ...,  1.4496e-03,
          1.2390e-02,  7.4158e-03],
        ...,
        [ 1.0315e-02,  2.6611e-02,  1.2207e-02,  ...,  1.2493e-04,
          9.2773e-03,  2.7161e-03],
        [ 2.1362e-02,  2.5024e-02,  6.9580e-03,  ..., -1.4343e-03,
          3.0518e-02,  1.4954e-02],
        [ 1.3672e-02, -9.8877e-03,  2.0874e-02,  ...,  2.0218e-04,
         -8.4839e-03,  1.9684e-03]])

llm.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0117,  0.0142,  0.0175,  ...,  0.0003, -0.0072,  0.0087],
        [ 0.0166, -0.0187, -0.0117,  ...,  0.0140,  0.0281, -0.0003],
        [-0.0189, -0.0124,  0.0398,  ...,  0.0111, -0.0158, -0.0148],
        ...,
        [-0.0301,  0.0118,  0.0279,  ..., -0.0033, -0.0136, -0.0024],
        [-0.0195,  0.0542, -0.0206,  ..., -0.0045, -0.0063,  0.0083],
        [-0.0041,  0.0001,  0.0208,  ...,  0.0003, -0.0230,  0.0343]])

llm.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0146, -0.0063,  0.0095,  ..., -0.0046,  0.0014, -0.0104],
        [ 0.0085,  0.0263,  0.0062,  ...,  0.0357,  0.0260, -0.0451],
        [-0.0083, -0.0050,  0.0187,  ...,  0.0045,  0.0143, -0.0125],
        ...,
        [-0.0286,  0.0044,  0.0078,  ..., -0.0077, -0.0131,  0.0047],
        [ 0.0146, -0.0114, -0.0034,  ...,  0.0327, -0.0247, -0.0017],
        [ 0.0183, -0.0178, -0.0280,  ...,  0.0166,  0.0253,  0.0498]])

llm.base_model.model.model.layers.0.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.3105, 0.3008, 0.2969,  ..., 0.2988, 0.2852, 0.2852])

llm.base_model.model.model.layers.0.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.3164, 0.3828, 0.2852,  ..., 0.2393, 0.3711, 0.3828])

llm.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-6.2866e-03,  6.5002e-03, -5.9814e-03,  ...,  1.6308e-04,
          2.1240e-02, -2.6550e-03],
        [ 3.1982e-02, -1.8799e-02, -3.4332e-03,  ..., -7.5150e-04,
          7.1716e-03, -6.8665e-03],
        [ 5.6458e-03,  1.4954e-02,  1.3885e-03,  ..., -5.7220e-04,
          1.0620e-02, -3.9978e-03],
        ...,
        [ 1.1139e-03, -8.6670e-03,  2.2705e-02,  ...,  5.9366e-05,
          1.7456e-02,  4.8523e-03],
        [-1.9043e-02, -3.5889e-02, -1.3504e-03,  ...,  5.5552e-05,
         -7.8125e-03,  2.7954e-02],
        [-1.7090e-02, -3.7842e-03, -2.1729e-02,  ...,  9.1076e-05,
         -2.2583e-03, -2.8229e-03]])

llm.base_model.model.model.layers.1.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 1.6016,  2.2188,  0.4141,  ...,  0.6328, -0.9609, -1.4219])

llm.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0181,  0.0209, -0.0240,  ...,  0.0083, -0.0407, -0.0437],
        [-0.0263, -0.0156, -0.0265,  ..., -0.0095, -0.0347,  0.0097],
        [ 0.0178, -0.0227,  0.0212,  ...,  0.0102,  0.0316, -0.0369],
        ...,
        [ 0.0239,  0.0206,  0.0128,  ..., -0.0152,  0.0412,  0.0004],
        [-0.0232,  0.0067, -0.0019,  ...,  0.0171,  0.0137,  0.0427],
        [-0.0486, -0.0045, -0.0003,  ..., -0.0090, -0.0379,  0.0027]])

llm.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0496, -0.0613, -0.0531,  ..., -0.0195, -0.0060,  0.0072],
        [ 0.0946,  0.0744,  0.0436,  ...,  0.0349, -0.0034, -0.0074],
        [-0.0020,  0.0080, -0.0401,  ...,  0.0004,  0.0146,  0.0176],
        ...,
        [ 0.0268, -0.0130,  0.0237,  ...,  0.0155, -0.0196, -0.0153],
        [ 0.0178,  0.0084, -0.0270,  ..., -0.0098,  0.0212,  0.0443],
        [ 0.0017, -0.0101, -0.0428,  ..., -0.0387,  0.0399,  0.0387]])

llm.base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0097,  0.0093,  0.0019,  ..., -0.0038,  0.0036, -0.0069],
        [-0.0209, -0.0132, -0.0081,  ..., -0.0019,  0.0176,  0.0278],
        [-0.0231,  0.0023,  0.0023,  ..., -0.0015, -0.0128, -0.0015],
        ...,
        [ 0.0080, -0.0214, -0.0084,  ...,  0.0003, -0.0026,  0.0251],
        [ 0.0097, -0.0325,  0.0239,  ...,  0.0015, -0.0236,  0.0198],
        [-0.0093, -0.0361, -0.0106,  ...,  0.0015,  0.0054,  0.0020]])

llm.base_model.model.model.layers.1.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 3.1641e-01, -6.8359e-01, -5.2344e-01, -9.7168e-02, -7.0312e-02,
         8.4375e-01, -4.4922e-01, -1.2158e-01, -4.0771e-02,  1.1328e+00,
        -4.1211e-01,  9.4531e-01,  4.2969e-02, -5.3438e+00, -9.8438e-01,
         2.2266e-01,  6.8359e-01,  3.4570e-01,  3.2812e-01, -8.6250e+00,
         4.2969e-02,  1.4531e+00,  1.6699e-01, -5.9375e-01,  8.0469e-01,
         5.8984e-01, -1.3562e+01,  7.8516e-01, -1.7578e-01, -2.1191e-01,
        -4.1797e-01,  3.1445e-01, -1.9043e-01, -1.8125e+01, -6.0547e-01,
         5.7812e-01, -1.3516e+00,  4.6289e-01,  1.6016e+00,  9.0234e-01,
         2.5250e+01, -8.5547e-01,  9.8047e-01, -2.5781e-01, -6.2891e-01,
         3.0938e+00, -1.7734e+00, -8.6250e+00,  3.7750e+01,  2.2125e+01,
        -1.6094e+00, -3.2750e+01, -8.3125e+00, -7.6875e+00,  1.6016e+00,
         5.4375e+00,  1.5875e+01,  3.6328e-01, -1.1625e+01,  1.8125e+01,
         4.9688e+00, -1.0688e+01, -8.0000e+00, -2.3594e+00,  2.2188e+00,
        -2.5781e-01,  3.0000e+00,  2.5391e-01,  6.1719e-01,  8.9844e-01,
        -1.6602e-01,  4.4434e-02, -3.7656e+00,  3.9844e-01,  2.4512e-01,
        -1.4062e+00,  2.1973e-01,  1.6406e-01,  5.5078e-01, -3.2812e-01,
        -1.2344e+00,  5.3906e-01,  9.8047e-01,  8.5938e-01,  7.5781e-01,
         1.3281e+00, -6.2012e-02,  5.1172e-01,  1.6846e-02, -1.9531e-01,
         3.9688e+00, -1.9824e-01,  4.9023e-01, -3.9453e-01,  1.7031e+00,
        -4.1016e-01,  9.2188e-01,  7.8125e+00,  8.2422e-01, -5.5469e-01,
        -2.3594e+00, -1.6719e+00, -1.8047e+00,  2.1250e+00, -1.1625e+01,
        -1.3047e+00, -1.0938e+00, -1.1250e+00, -7.7148e-02,  3.3125e+00,
        -1.0391e+00, -8.6250e+00,  3.9500e+01,  2.3000e+01, -7.9375e+00,
         2.4000e+01,  1.9000e+01, -5.7188e+00, -2.6406e+00,  1.2938e+01,
         9.8125e+00, -1.2062e+01, -1.0375e+01,  6.8125e+00,  8.3594e-01,
        -2.1750e+01,  6.7500e+00,  1.1500e+01, -5.5312e+00,  1.1094e+00,
         1.2031e+00,  1.2500e+00,  1.4160e-01,  1.2891e-01, -2.0938e+00,
        -1.7812e+00,  2.0898e-01,  2.7734e-01,  2.6094e+00, -8.3203e-01,
        -6.2891e-01, -5.7422e-01,  3.3203e-01,  2.8125e+00,  6.2500e-02,
         3.0078e-01,  2.3750e+00, -1.2500e+00,  8.3984e-01,  1.0938e+00,
        -3.7812e+00,  6.1719e-01,  5.0000e-01,  2.0000e+00, -1.6641e+00,
        -6.2891e-01,  6.7578e-01, -4.9219e-01, -5.6641e-01, -2.3125e+00,
        -1.6875e+00,  1.8047e+00,  6.6406e-01,  4.6094e-01, -6.3281e-01,
        -3.3789e-01,  1.2305e-01, -2.4316e-01, -1.1562e+00,  4.5508e-01,
         5.1875e+00,  2.8594e+00,  1.5781e+00,  5.8750e+00, -7.4688e+00,
         2.5156e+00, -5.5000e+00, -2.7344e-01,  8.2031e-01, -6.3750e+00,
        -5.8750e+00, -9.2500e+00,  1.2344e+00, -9.6875e-01, -7.1875e+00,
         7.8750e+00,  1.1953e+00, -4.4375e+00, -6.0312e+00, -1.2656e+00,
         3.4375e-01,  6.0312e+00, -3.3750e+00, -2.1250e+00, -2.9844e+00,
        -6.4062e-01, -2.0625e+00, -8.7891e-01, -8.1250e-01, -1.5332e-01,
        -7.6953e-01, -8.3984e-01,  2.4707e-01, -8.2031e-01, -2.2339e-02,
         9.2188e-01,  1.2734e+00, -5.1172e-01,  1.1562e+00,  4.7266e-01,
        -8.4375e-01, -6.7969e-01,  1.9238e-01, -9.3750e-02,  1.2344e+00,
        -1.4453e-01, -5.6396e-02, -4.3555e-01,  7.7344e-01, -1.1641e+00,
         8.9844e-01, -6.6406e-01, -2.9375e+00,  1.3516e+00, -3.7891e-01,
         2.0625e+00, -3.9688e+00, -1.2656e+00, -1.5312e+00,  2.2656e+00,
         4.0625e-01, -1.5625e+00,  3.0625e+00,  3.3398e-01, -5.2188e+00,
         3.7109e-01, -8.4375e-01,  1.6504e-01, -1.2062e+01,  2.3906e+00,
        -1.7578e+00, -2.8125e+00,  5.8750e+00,  4.9062e+00,  2.4121e-01,
         1.2188e+00,  4.0625e+00, -2.3594e+00,  4.0938e+00, -1.5391e+00,
         5.4062e+00,  1.0000e+01,  1.7109e+00, -1.7266e+00, -8.2500e+00,
        -1.9062e+00, -6.6875e+00,  2.6250e+00, -4.6250e+00,  2.7466e-02,
         5.6641e-01, -4.2188e-01, -3.4219e+00,  2.5000e+00,  5.9375e-01,
        -3.9062e-01,  1.4297e+00, -7.9688e-01, -2.4023e-01, -1.5723e-01,
        -1.6484e+00, -2.0938e+00, -1.2656e+00, -1.2500e-01,  3.6250e+00,
         9.9121e-02, -8.9355e-02,  1.3438e+00, -5.3516e-01, -2.0508e-01,
        -2.1289e-01, -1.0391e+00,  1.0312e+00,  7.1875e-01,  2.8594e+00,
        -5.2734e-01, -3.1006e-02,  6.5938e+00,  2.4414e-01, -1.0742e-01,
        -3.4375e-01, -2.1094e-01,  2.1118e-02, -4.7852e-01, -7.9297e-01,
         8.1875e+00, -2.1387e-01, -5.1172e-01, -1.6235e-02,  2.5156e+00,
         9.8438e-01, -1.4531e+00,  2.3125e+00, -8.3008e-02, -7.5000e+00,
        -5.3750e+00, -1.2812e+01, -8.5156e-01, -4.1562e+00,  4.9375e+00,
         4.2812e+00,  8.4375e+00,  1.5859e+00, -3.2812e+00, -1.5000e+00,
         1.3047e+00,  9.1875e+00, -6.9688e+00,  1.5250e+01,  8.1875e+00,
        -1.1375e+01,  8.9375e+00, -4.4062e+00,  1.1406e+00, -9.9219e-01,
         4.3125e+00, -7.1875e-01,  1.5078e+00, -6.1328e-01,  3.4688e+00,
         3.5352e-01, -3.3125e+00,  1.5527e-01,  2.5312e+00,  8.9062e-01,
        -5.2188e+00,  2.0312e-01, -2.2852e-01, -9.9219e-01, -1.1719e+00,
         9.9219e-01,  4.1875e+00, -3.4961e-01,  8.2520e-02,  7.2266e-01,
         3.2227e-01, -9.4531e-01,  7.3047e-01,  3.3594e+00,  7.9688e-01,
        -4.4531e-01, -4.3125e+00,  5.4297e-01,  1.3867e-01, -3.0273e-01,
         3.7891e-01, -4.3750e-01, -1.3184e-01,  1.2500e+00,  9.1250e+00,
         1.1719e+00, -2.5586e-01,  1.2402e-01,  2.3535e-01, -2.5312e+00,
        -5.3516e-01,  4.7500e+00,  1.6016e+00,  4.9688e+00, -1.5312e+01,
        -1.9750e+01, -5.0781e-02,  2.2344e+00,  2.1562e+00,  7.7500e+00,
         6.5625e-01, -3.4766e-01, -1.1641e+00,  2.0469e+00, -4.7812e+00,
         7.3125e+00,  6.0625e+00,  3.7031e+00, -8.5625e+00,  2.4844e+00,
        -3.9375e+00, -3.1719e+00, -1.2656e+00,  2.0781e+00,  2.1875e+00,
        -4.3945e-01,  2.8125e+00,  6.0547e-02,  1.3359e+00,  9.7168e-02,
         3.5742e-01, -1.0625e+00, -7.3438e-01, -1.6113e-02, -4.8633e-01,
        -3.7500e-01, -3.0640e-02,  1.1426e-01, -2.6406e+00, -2.0996e-01,
         3.0312e+00, -1.8164e-01,  2.8516e-01, -1.3438e+00, -3.0078e-01,
        -1.6895e-01,  2.1973e-01,  5.9375e-01,  4.2812e+00,  1.3867e-01,
         5.0781e-01,  3.6719e-01,  7.8906e-01, -1.9062e+00,  3.1055e-01,
        -2.5977e-01,  1.9238e-01,  1.2451e-01,  1.0469e+00,  2.5781e-01,
         5.4062e+00,  1.0312e+00, -5.9766e-01, -6.6797e-01,  3.6406e+00,
        -8.8281e-01,  1.9375e+00, -5.6875e+00, -1.2012e-01, -1.5938e+00,
        -4.3164e-01, -1.0469e+00, -9.1875e+00, -7.4609e-01,  6.2188e+00,
        -6.3750e+00,  1.2188e+00, -8.7500e-01,  2.0469e+00,  1.4766e+00,
        -7.8438e+00, -3.0469e+00, -4.0625e+00,  3.8594e+00, -9.4531e-01,
        -1.9453e+00,  1.0938e+00, -1.1768e-01, -4.6484e-01,  2.2070e-01,
        -1.0400e-01,  1.7285e-01, -1.4922e+00, -2.7188e+00,  2.4292e-02,
         1.0156e+00, -7.8516e-01,  5.9766e-01, -5.2812e+00, -2.1484e-01,
         1.7109e+00, -9.5215e-02, -3.3984e-01, -1.7090e-02,  1.0938e+00,
         3.7109e-02, -5.5469e-01, -3.4375e-01, -3.1094e+00, -3.6328e-01,
        -1.1230e-01,  8.3496e-02, -2.2188e+00,  5.7422e-01,  3.1641e-01,
         5.2734e-01,  2.3340e-01, -4.1562e+00, -8.5547e-01,  6.4453e-01,
        -2.1562e+00,  5.9375e-01,  5.9375e-01, -1.8359e+00, -2.3281e+00,
        -2.9492e-01,  3.3281e+00, -2.0312e-01, -1.0859e+00, -1.1172e+00,
         7.5312e+00, -3.6094e+00,  7.9688e-01,  9.8125e+00, -2.0312e+00,
         6.9531e-01, -5.1250e+00, -2.8438e+00,  2.4062e+00, -6.5000e+00,
         5.1562e+00,  3.1719e+00, -1.7734e+00, -3.5469e+00, -2.8281e+00,
         2.6562e+00,  4.0039e-01])

llm.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0683,  0.0204, -0.0213,  ..., -0.0034,  0.0582,  0.0557],
        [ 0.0393,  0.0229, -0.0291,  ...,  0.0088, -0.0171, -0.0273],
        [ 0.0384, -0.0455,  0.0394,  ...,  0.0064, -0.0376, -0.0375],
        ...,
        [ 0.0305, -0.0078,  0.0058,  ...,  0.0103, -0.0337, -0.0136],
        [ 0.0198,  0.0371, -0.0241,  ...,  0.0048, -0.0348, -0.0771],
        [ 0.0017, -0.0156,  0.0221,  ...,  0.0150, -0.0182, -0.0078]])

llm.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0304, -0.0083, -0.0331,  ..., -0.0950,  0.0110, -0.0569],
        [-0.0098, -0.0126,  0.0288,  ..., -0.0423,  0.0093, -0.0045],
        [ 0.0224,  0.0196, -0.0278,  ..., -0.0115,  0.0345, -0.0011],
        ...,
        [ 0.0221, -0.0112, -0.0127,  ..., -0.0011, -0.0125, -0.0050],
        [ 0.0304, -0.0239, -0.0093,  ...,  0.0227, -0.0125,  0.0004],
        [ 0.0156, -0.0470, -0.0345,  ...,  0.0065, -0.0301,  0.0048]])

llm.base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0119,  0.0063,  0.0236,  ...,  0.0006, -0.0129,  0.0112],
        [-0.0162, -0.0061, -0.0088,  ..., -0.0085, -0.0212,  0.0089],
        [ 0.0085, -0.0044,  0.0081,  ..., -0.0009,  0.0053,  0.0084],
        ...,
        [-0.0189,  0.0084, -0.0176,  ...,  0.0052,  0.0189, -0.0063],
        [-0.0020,  0.0040, -0.0139,  ...,  0.0081, -0.0064,  0.0009],
        [-0.0135,  0.0093,  0.0159,  ...,  0.0005,  0.0226,  0.0179]])

llm.base_model.model.model.layers.1.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 5.0537e-02, -1.4282e-02, -2.4658e-02,  2.5879e-02,  4.5898e-02,
        -2.7344e-02,  3.4912e-02,  6.3477e-03, -1.6861e-03, -1.3351e-03,
         5.0659e-03, -2.4292e-02, -1.3867e-01, -1.0193e-02,  3.0029e-02,
         8.6914e-02, -2.4902e-02,  3.8574e-02, -2.0020e-02,  6.0547e-02,
         5.0293e-02,  2.8687e-02, -4.9805e-02,  6.6833e-03, -9.7046e-03,
         3.7598e-02,  1.6235e-02, -1.2085e-02, -4.6387e-02, -4.4922e-02,
        -3.4180e-02,  1.2817e-03,  3.3936e-02,  3.8605e-03, -5.0781e-02,
         3.8574e-02, -3.6865e-02,  1.9165e-02,  6.5430e-02,  2.9663e-02,
         2.0996e-02, -1.0193e-02, -2.8809e-02,  6.5002e-03,  4.9744e-03,
        -3.5156e-02, -6.3477e-02, -3.1982e-02,  9.0332e-03, -3.1982e-02,
         3.1006e-02,  7.5989e-03,  2.4414e-02, -1.8921e-02, -2.2217e-02,
         6.6406e-02,  2.8442e-02, -1.5625e-02,  8.2016e-04,  1.1353e-02,
         1.0315e-02, -1.5137e-02,  2.2827e-02, -4.2969e-02,  3.0029e-02,
         4.2236e-02,  2.7954e-02, -1.0840e-01, -1.1536e-02,  2.7710e-02,
         2.5879e-02, -1.3611e-02, -1.1328e-01,  3.6621e-02,  5.5847e-03,
         4.5410e-02, -2.6367e-02, -1.0132e-02, -1.1768e-01, -3.9673e-03,
        -4.9072e-02, -1.4404e-02, -2.5513e-02, -1.4709e-02,  7.9102e-02,
        -1.0254e-02,  2.0752e-02,  4.6082e-03,  3.3203e-02, -7.9102e-02,
        -7.2266e-02, -7.6294e-03, -4.0039e-02, -2.8931e-02, -3.7598e-02,
         1.6357e-02,  1.4343e-02, -2.2339e-02, -3.8086e-02,  3.8086e-02,
        -1.7700e-03, -4.0039e-02, -1.9897e-02, -2.0752e-02, -1.5564e-02,
        -8.5449e-03, -4.9438e-03,  1.9741e-04, -1.7212e-02,  3.1494e-02,
         7.8125e-02, -1.3550e-02, -2.7313e-03, -1.0791e-01,  2.0386e-02,
         7.1289e-02, -5.8105e-02, -3.8330e-02,  2.7539e-01, -2.2070e-01,
         9.7168e-02,  2.7832e-02,  9.8877e-03,  1.9653e-02,  2.4902e-02,
        -8.3496e-02, -4.5776e-03, -5.7861e-02,  1.9043e-01,  4.6387e-02,
         3.6926e-03,  2.4658e-02, -1.3965e-01,  2.3340e-01,  8.0566e-02,
        -1.4355e-01, -1.5259e-02,  5.6396e-02,  7.1289e-02,  8.1177e-03,
         1.7383e-01,  6.8359e-02, -1.5234e-01, -7.1777e-02,  5.5664e-02,
         7.2754e-02,  1.4746e-01, -1.0254e-01, -9.5215e-02,  1.7188e-01,
        -6.2988e-02,  7.6660e-02,  1.9165e-02,  9.3460e-04, -5.2734e-02,
         1.5137e-01,  1.9336e-01,  4.8584e-02,  2.2363e-01, -7.9102e-02,
         4.5166e-02,  2.8320e-02, -1.0693e-01, -2.5269e-02, -2.2125e-03,
        -3.6377e-02,  1.1328e-01, -2.5024e-02, -1.6309e-01, -1.0834e-03,
         2.5024e-02, -1.0205e-01,  1.1621e-01,  1.5198e-02,  1.1621e-01,
        -4.0039e-02,  1.3672e-01, -4.7363e-02,  2.1851e-02,  3.5400e-02,
         7.9102e-02, -1.2158e-01,  3.0640e-02,  7.4219e-02, -3.6133e-02,
         3.8330e-02, -2.9102e-01,  1.0010e-01, -8.4473e-02,  2.3340e-01,
        -3.1738e-02,  4.4189e-02,  8.1055e-02, -6.8848e-02,  1.1865e-01,
        -6.6895e-02,  2.2095e-02, -1.1328e-01, -1.2817e-02, -7.8964e-04,
         2.0215e-01,  2.9175e-02,  6.7871e-02,  1.7676e-01,  6.4453e-02,
        -2.7832e-02, -1.8066e-02,  1.5039e-01, -9.8877e-03, -7.1777e-02,
         1.8066e-02, -4.3701e-02, -1.7578e-01, -1.0303e-01, -3.8281e-01,
         5.6152e-02,  1.4038e-02, -3.3875e-03,  9.2773e-03,  1.3086e-01,
         2.3438e-02,  2.7954e-02, -7.4219e-02,  4.1504e-02, -3.7695e-01,
        -3.7891e-01,  5.2246e-02, -1.9336e-01,  8.9844e-02, -1.2988e-01,
        -7.2266e-02, -2.8906e-01,  1.7944e-02, -4.4678e-02, -3.1982e-02,
        -2.9449e-03, -5.4688e-02, -1.2354e-01, -3.9307e-02,  1.6327e-03,
         1.1816e-01, -6.9824e-02,  2.3047e-01,  7.1777e-02,  8.7402e-02,
         7.0801e-02, -6.2988e-02, -1.1084e-01,  1.2817e-03,  2.3560e-02,
        -4.6875e-02, -5.3711e-02,  4.9072e-02,  1.2500e-01, -3.2227e-02,
        -7.1716e-03,  1.2402e-01, -5.0293e-02,  1.4355e-01,  6.7139e-03,
         5.2246e-02, -5.7861e-02,  4.0283e-03,  2.0447e-03, -2.6733e-02,
         3.4668e-02, -2.3438e-01,  3.2227e-02,  3.4912e-02,  1.4526e-02,
         9.1553e-03, -9.0332e-03, -5.5664e-02, -1.8921e-02, -4.3945e-02,
         5.7617e-02, -4.4434e-02, -4.7119e-02,  1.2878e-02,  8.0109e-04,
         2.7954e-02, -5.7861e-02,  7.6172e-02, -4.3457e-02,  1.9684e-03,
         2.2339e-02,  2.3560e-02,  2.4902e-02,  1.9043e-02, -6.0730e-03,
        -7.8613e-02,  2.6855e-02,  7.1289e-02,  2.7466e-02, -6.4453e-02,
         1.1047e-02,  9.1553e-03, -6.0547e-02, -3.5156e-02,  4.3213e-02,
        -7.0312e-02,  8.0872e-04, -2.2583e-02,  2.8931e-02,  2.6367e-02,
         7.6172e-02,  5.8105e-02,  5.3711e-02,  6.0059e-02, -9.5703e-02,
        -7.6660e-02,  1.7090e-02, -1.1169e-02,  5.7861e-02, -3.5400e-02,
         1.6113e-02, -7.1289e-02,  8.5449e-02,  2.8809e-02, -1.1475e-02,
         2.4780e-02, -2.0752e-02, -2.1851e-02,  1.4526e-02, -8.6914e-02,
         2.9663e-02, -5.6152e-03, -1.2634e-02, -7.8613e-02, -1.0559e-02,
        -1.5747e-02,  3.0762e-02, -9.8267e-03,  9.7046e-03,  3.3447e-02,
         9.5215e-02,  5.7617e-02, -4.1504e-02, -8.6060e-03, -3.1494e-02,
         5.0781e-02,  3.4424e-02,  1.5039e-01,  4.2969e-02,  4.0771e-02,
        -3.0273e-02,  1.3916e-02, -4.1016e-02, -1.7822e-02, -6.8359e-02,
        -3.5889e-02, -4.0527e-02, -9.1309e-02, -6.4697e-03,  1.8677e-02,
        -2.6398e-03, -1.4099e-02,  3.1006e-02, -2.0752e-02, -1.5793e-03,
        -1.2061e-01, -2.1851e-02,  4.1504e-02,  1.0254e-02,  3.2471e-02,
        -1.1035e-01,  1.9379e-03,  3.1738e-02, -3.8818e-02, -4.3701e-02,
        -2.8687e-02, -4.1992e-02,  8.2520e-02, -1.6846e-02, -6.6895e-02,
        -1.0620e-02, -3.5400e-02, -4.1260e-02,  1.1914e-01,  1.2817e-02,
         1.9775e-02, -2.1484e-02,  2.7832e-02,  9.5215e-02, -3.5889e-02,
        -5.8899e-03, -6.2866e-03, -4.7119e-02, -4.0771e-02, -3.7109e-02,
        -7.6904e-03,  4.7852e-02, -3.5156e-02,  8.1543e-02, -2.1606e-02,
         9.1309e-02, -1.8799e-02, -1.1084e-01, -6.2943e-04, -7.1777e-02,
         6.7383e-02, -3.9551e-02, -3.1494e-02,  3.8910e-04, -3.3936e-02,
        -4.9316e-02, -3.8086e-02, -9.7046e-03, -1.5747e-02,  2.1729e-02,
        -6.4392e-03,  2.2705e-02,  1.8311e-02, -1.4453e-01, -1.4160e-01,
         1.0925e-02, -1.1816e-01,  2.5586e-01,  3.6133e-02,  2.5635e-02,
         7.6660e-02,  4.0527e-02,  9.2773e-02,  7.5684e-03,  9.1309e-02,
        -6.7444e-03, -1.0681e-02, -2.1851e-02,  1.9775e-02,  6.8359e-02,
         1.9836e-04,  7.5684e-02, -1.9897e-02,  1.0059e-01,  3.6377e-02,
         3.2715e-02,  4.7119e-02, -4.2480e-02,  4.4189e-02,  6.6895e-02,
         6.5918e-02,  2.4780e-02,  8.6670e-03,  4.1992e-02,  2.8687e-02,
         1.5259e-02, -4.0039e-02,  6.6895e-02, -5.8594e-02,  4.1504e-02,
        -4.1016e-02, -8.1543e-02, -4.3213e-02, -9.2773e-03,  1.6251e-03,
         1.3184e-01,  7.9102e-02, -1.4160e-02,  1.6846e-02,  2.8687e-02,
        -2.8687e-02, -8.6914e-02, -6.8970e-03, -3.0640e-02,  5.3406e-03,
         7.8125e-03, -3.2471e-02,  1.4099e-02, -9.0332e-03, -5.1758e-02,
        -5.1025e-02, -4.5410e-02,  4.6875e-02, -1.7700e-02,  2.0874e-02,
         6.6528e-03, -4.8340e-02,  4.3945e-02,  6.3965e-02,  5.8594e-02,
         4.4922e-02, -8.0566e-02, -9.5215e-02, -5.1025e-02,  1.4954e-03,
        -8.8379e-02, -1.8188e-02,  3.4180e-02,  4.8828e-02, -4.3213e-02,
        -5.7861e-02, -1.3794e-02, -1.3867e-01, -4.3030e-03,  1.4221e-02,
         2.9419e-02, -7.8735e-03, -1.1963e-02,  2.6733e-02, -5.7373e-02,
         2.1729e-02,  1.3977e-02, -3.6377e-02,  4.0039e-02, -2.7832e-02,
        -3.7354e-02, -1.7929e-03,  1.1719e-02, -6.4453e-02,  6.1646e-03,
         2.2461e-02,  7.8125e-02])

llm.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0131,  0.0004,  0.0711,  ...,  0.0029, -0.0182,  0.0230],
        [ 0.0292,  0.0166, -0.0706,  ...,  0.0104,  0.0442, -0.0010],
        [ 0.0413, -0.0136, -0.0118,  ..., -0.0039, -0.0024,  0.0349],
        ...,
        [-0.0220, -0.0117,  0.0070,  ..., -0.0119, -0.0067,  0.0543],
        [ 0.0117, -0.0009, -0.0548,  ...,  0.0037,  0.0100, -0.0258],
        [-0.0100,  0.0038, -0.0013,  ...,  0.0015, -0.0120, -0.0134]])

llm.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-4.1337e-03,  1.5174e-02,  4.5147e-02,  ..., -3.4766e-02,
         -2.1156e-02,  1.0110e-02],
        [-1.0362e-05,  2.5788e-02, -2.5505e-02,  ...,  3.0588e-03,
          1.2951e-02,  1.5207e-02],
        [ 5.0089e-02, -1.0290e-02,  6.7036e-03,  ...,  2.5435e-02,
         -1.0849e-02, -3.0825e-02],
        ...,
        [ 2.4794e-03,  2.3534e-02, -6.5190e-03,  ..., -1.3408e-02,
          2.6119e-02, -2.5127e-03],
        [ 1.2776e-02,  3.9479e-03,  2.7499e-02,  ...,  1.2488e-02,
         -1.4375e-02, -3.3576e-02],
        [-1.9797e-02, -7.0274e-03, -3.3580e-02,  ...,  1.7540e-02,
          4.3321e-02,  6.4010e-03]])

llm.base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0073,  0.0036, -0.0031,  ..., -0.0143, -0.0041,  0.0104],
        [ 0.0076,  0.0116,  0.0047,  ..., -0.0112,  0.0118,  0.0123],
        [ 0.0220,  0.0011, -0.0153,  ..., -0.0052,  0.0113, -0.0087],
        ...,
        [ 0.0042, -0.0177,  0.0034,  ...,  0.0056,  0.0109, -0.0122],
        [ 0.0229,  0.0006, -0.0081,  ..., -0.0037, -0.0020,  0.0162],
        [-0.0086, -0.0187,  0.0031,  ..., -0.0015,  0.0142, -0.0132]])

llm.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0519,  0.0193, -0.0120,  ...,  0.0047,  0.0097, -0.0208],
        [-0.0210, -0.0236, -0.0086,  ..., -0.0692,  0.0156,  0.0052],
        [ 0.0303,  0.0264, -0.0295,  ..., -0.0140, -0.0195,  0.0033],
        ...,
        [-0.0100, -0.0133, -0.0015,  ..., -0.0092,  0.0289, -0.0125],
        [ 0.0200, -0.0319,  0.0396,  ...,  0.0255, -0.0101, -0.0006],
        [ 0.0083, -0.0345,  0.0276,  ...,  0.0065, -0.0122, -0.0144]])

llm.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0262,  0.0226, -0.0123,  ..., -0.0234, -0.0322, -0.0235],
        [-0.0137, -0.0158,  0.0419,  ..., -0.0220, -0.0060, -0.0386],
        [ 0.0226, -0.0450,  0.0238,  ...,  0.0106,  0.0116,  0.0138],
        ...,
        [-0.0096, -0.0205,  0.0097,  ..., -0.0073, -0.0331, -0.0199],
        [ 0.0047, -0.0283,  0.0077,  ..., -0.0014,  0.0315, -0.0380],
        [ 0.0237, -0.0256,  0.0166,  ..., -0.0074,  0.0224, -0.0092]])

llm.base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 1.9819e-06, -1.3638e-04, -3.6621e-04,  ...,  1.2493e-04,
         -1.3065e-04,  4.0817e-04],
        [-6.4087e-03,  2.5177e-03,  2.7466e-02,  ..., -7.5073e-03,
         -1.3123e-02, -4.0588e-03],
        [-4.6692e-03, -1.1444e-03, -1.8066e-02,  ..., -7.2632e-03,
         -1.6235e-02,  2.0447e-03],
        ...,
        [ 8.9722e-03,  2.9449e-03,  3.3379e-04,  ..., -1.0193e-02,
         -6.2256e-03, -2.1240e-02],
        [ 1.9043e-02,  7.8125e-03,  2.4109e-03,  ..., -2.3926e-02,
         -2.8198e-02,  1.8677e-02],
        [ 4.4441e-04,  4.1389e-04,  1.4877e-04,  ..., -1.8215e-04,
         -4.1580e-04, -3.2425e-04]])

llm.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 3.4222e-02,  3.7286e-02,  5.9358e-03,  ..., -1.5865e-03,
         -3.2439e-02,  3.3122e-02],
        [ 2.0538e-02, -4.0528e-02, -4.6471e-02,  ..., -1.0825e-01,
          2.3798e-02,  5.7267e-03],
        [ 4.3690e-02, -1.2240e-02,  1.7629e-02,  ..., -4.8298e-02,
          8.3340e-02,  7.2932e-03],
        ...,
        [-1.4786e-02,  1.0779e-04, -1.5297e-02,  ..., -3.7293e-02,
          4.3327e-02, -2.7192e-02],
        [-3.9786e-02,  1.5690e-02, -7.3290e-03,  ...,  1.2147e-02,
         -4.7456e-03,  7.3645e-03],
        [ 2.4032e-02, -1.1859e-02,  1.8092e-02,  ...,  7.8111e-02,
         -6.5440e-02,  2.2724e-03]])

llm.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0116,  0.0037,  0.0059,  ..., -0.0339, -0.0292,  0.0224],
        [ 0.0158, -0.0068, -0.0388,  ...,  0.0028,  0.0201, -0.0450],
        [ 0.0350,  0.0074,  0.0332,  ..., -0.0253, -0.0007,  0.0516],
        ...,
        [-0.0232, -0.0015, -0.0098,  ..., -0.0520,  0.0152, -0.0301],
        [ 0.0168, -0.0600, -0.0248,  ..., -0.0037, -0.0007,  0.0221],
        [ 0.0119, -0.0117, -0.0087,  ...,  0.0103, -0.0123, -0.0108]])

llm.base_model.model.model.layers.1.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0005,  0.0003,  0.0009,  ...,  0.0003, -0.0002, -0.0017],
        [ 0.0087,  0.0089,  0.0033,  ..., -0.0210,  0.0203, -0.0130],
        [ 0.0019,  0.0058,  0.0067,  ..., -0.0098,  0.0178, -0.0034],
        ...,
        [ 0.0172, -0.0098,  0.0195,  ..., -0.0193, -0.0102,  0.0010],
        [ 0.0033,  0.0099,  0.0006,  ...,  0.0111, -0.0067,  0.0201],
        [-0.0005,  0.0011,  0.0007,  ...,  0.0003, -0.0008, -0.0008]])

llm.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0340,  0.0143, -0.0006,  ..., -0.0181,  0.0185,  0.0138],
        [-0.0153, -0.0653,  0.0650,  ..., -0.0403,  0.0389,  0.0254],
        [-0.0060,  0.0231, -0.0069,  ...,  0.0097, -0.0441, -0.0248],
        ...,
        [ 0.0561, -0.0171,  0.0163,  ..., -0.0602,  0.0081,  0.0206],
        [ 0.0019,  0.0297,  0.0228,  ..., -0.0698,  0.0364, -0.0023],
        [-0.0223,  0.0243, -0.0064,  ..., -0.0130, -0.0197, -0.0226]])

llm.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0074, -0.0123, -0.0106,  ..., -0.0047,  0.0106,  0.0195],
        [-0.0254,  0.0021,  0.0201,  ..., -0.0163, -0.0209,  0.0002],
        [-0.0264,  0.0051, -0.0154,  ..., -0.0056, -0.0199,  0.0152],
        ...,
        [-0.0057,  0.0367, -0.0118,  ...,  0.1006, -0.0773,  0.0116],
        [-0.0109,  0.0150, -0.0223,  ..., -0.0206,  0.0370, -0.0362],
        [ 0.0240,  0.0229,  0.0071,  ..., -0.0122,  0.0207, -0.0203]])

llm.base_model.model.model.layers.1.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 1.0605e-03,  4.7207e-05, -5.1880e-03,  ..., -3.5400e-03,
         -3.6926e-03,  1.4400e-04],
        [-2.0905e-03,  2.5269e-02,  2.4414e-02,  ...,  2.3438e-02,
         -4.3640e-03,  5.7602e-04],
        [-6.2943e-04, -9.2773e-03,  6.8359e-03,  ..., -1.7090e-02,
          6.1340e-03,  4.2915e-04],
        ...,
        [ 1.1139e-03,  2.3193e-02, -1.0803e-02,  ..., -1.3504e-03,
          2.6855e-02,  1.7242e-03],
        [ 1.4496e-03, -3.1586e-03, -4.5654e-02,  ...,  4.7607e-03,
         -2.4414e-03, -7.5531e-04],
        [-4.8637e-04, -1.3184e-02,  2.9297e-03,  ...,  1.5381e-02,
          3.0884e-02,  6.9046e-04]])

llm.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0358, -0.0458, -0.0271,  ...,  0.0127,  0.0400, -0.0281],
        [-0.0178,  0.0155, -0.0162,  ..., -0.0284, -0.0593,  0.0125],
        [ 0.0026, -0.0036, -0.0343,  ...,  0.0050, -0.0323, -0.0051],
        ...,
        [-0.0021, -0.0261,  0.0073,  ...,  0.0046, -0.0086, -0.0254],
        [-0.0393, -0.0221,  0.0255,  ..., -0.0432, -0.0371,  0.0397],
        [ 0.0117,  0.0388,  0.0063,  ...,  0.0356, -0.0075,  0.0030]])

llm.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0344, -0.0004, -0.0320,  ..., -0.0287,  0.0066,  0.0058],
        [-0.0738,  0.0081, -0.0190,  ..., -0.0097, -0.0269,  0.0243],
        [ 0.0205,  0.0238,  0.0164,  ...,  0.0088, -0.0049,  0.0102],
        ...,
        [ 0.0386,  0.0406, -0.0258,  ...,  0.0058,  0.0260, -0.0351],
        [ 0.0176,  0.0166, -0.0079,  ..., -0.0179,  0.0054,  0.0214],
        [ 0.0201, -0.0399,  0.0436,  ...,  0.0182,  0.0203, -0.0453]])

llm.base_model.model.model.layers.1.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([ 1.8652e-01,  3.0078e-01,  1.6504e-01,  ..., -5.6505e-05,
         5.6396e-02,  3.6523e-01])

llm.base_model.model.model.layers.1.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.6289, 1.1094, 0.6875,  ..., 0.5703, 0.9570, 1.2812])

llm.base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0075,  0.0020, -0.0115,  ...,  0.0046,  0.0044, -0.0134],
        [-0.0029,  0.0092, -0.0030,  ...,  0.0146,  0.0039, -0.0045],
        [ 0.0066, -0.0009, -0.0038,  ..., -0.0222, -0.0060,  0.0042],
        ...,
        [ 0.0123, -0.0043,  0.0026,  ...,  0.0019,  0.0123, -0.0221],
        [-0.0026, -0.0053,  0.0020,  ..., -0.0297,  0.0157, -0.0037],
        [-0.0232,  0.0152,  0.0205,  ..., -0.0050, -0.0080,  0.0208]])

llm.base_model.model.model.layers.2.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([-1.1016, -0.7305,  2.7500,  ...,  0.0132,  0.0996,  0.0513])

llm.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0007,  0.0085, -0.0032,  ..., -0.0214, -0.0083, -0.0103],
        [ 0.0341,  0.0052,  0.0435,  ..., -0.0357, -0.0155,  0.0286],
        [-0.0195,  0.0082, -0.0317,  ...,  0.0333,  0.0003, -0.0061],
        ...,
        [-0.0206, -0.0109, -0.0277,  ..., -0.0016,  0.0459, -0.0358],
        [ 0.0168, -0.0014, -0.0404,  ...,  0.0342, -0.0362,  0.0239],
        [-0.0242,  0.0088,  0.0373,  ..., -0.0421,  0.0137,  0.0098]])

llm.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0327, -0.0087,  0.0209,  ..., -0.0376, -0.0109,  0.0033],
        [ 0.0208, -0.0024,  0.0153,  ..., -0.0035,  0.0274, -0.0219],
        [ 0.0029, -0.0075, -0.0202,  ...,  0.0173, -0.0428,  0.0417],
        ...,
        [ 0.0075, -0.0574, -0.0007,  ...,  0.0182, -0.0559,  0.0187],
        [-0.0038,  0.0079, -0.0193,  ..., -0.0267,  0.0357, -0.0147],
        [-0.0216, -0.0009,  0.0095,  ..., -0.0046,  0.0212, -0.0610]])

llm.base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0177, -0.0003, -0.0037,  ..., -0.0055, -0.0035,  0.0123],
        [ 0.0122, -0.0055,  0.0060,  ...,  0.0109,  0.0162, -0.0028],
        [-0.0096,  0.0022,  0.0026,  ..., -0.0007,  0.0073,  0.0123],
        ...,
        [ 0.0072,  0.0109, -0.0398,  ..., -0.0247, -0.0097,  0.0371],
        [ 0.0085, -0.0635,  0.0126,  ...,  0.0359, -0.0033,  0.0435],
        [ 0.0167,  0.0212, -0.0208,  ..., -0.0393,  0.0011, -0.0055]])

llm.base_model.model.model.layers.2.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-6.7188e-01,  1.6562e+00, -9.0625e-01,  1.0703e+00, -9.7275e-04,
         1.5547e+00, -5.8984e-01, -8.3594e-01,  1.1780e-02, -4.1016e-01,
        -2.5195e-01, -1.3203e+00,  1.3047e+00,  1.4062e-01,  1.2146e-02,
        -2.1875e-01,  7.6562e-01, -1.3359e+00, -1.4746e-01,  1.4746e-01,
        -1.4766e+00,  7.8613e-02, -1.5137e-01, -3.3398e-01, -1.3125e+00,
         2.8906e-01, -1.1621e-01, -2.5312e+00,  2.8564e-02, -2.6245e-02,
         3.7500e-01,  5.1953e-01, -1.6504e-01,  1.5234e+00, -1.4258e-01,
        -2.0801e-01,  4.1602e-01, -8.9844e-02, -3.4062e+00, -1.4746e-01,
        -2.2852e-01,  3.7500e-01,  3.3203e-01, -8.2031e-01,  1.7578e-01,
         9.4922e-01,  1.0938e+00,  9.6094e-01,  1.2012e-01, -4.5898e-01,
         1.0469e+00,  1.5859e+00, -7.9688e-01,  4.4531e-01, -5.9375e-01,
         1.3203e+00, -2.6875e+00,  2.0156e+00,  2.1875e+00, -3.4531e+00,
         2.0938e+00, -2.9219e+00,  5.7812e-01, -2.1250e+00, -2.1562e+00,
         3.9062e-01,  1.0469e+00, -9.6094e-01,  1.1562e+00,  1.2305e-01,
         1.0859e+00,  1.2734e+00,  2.3633e-01,  1.3203e+00,  9.2773e-02,
         1.4941e-01,  2.9688e-01, -7.1289e-02,  1.0391e+00,  1.2812e+00,
        -2.5781e-01, -4.0820e-01,  1.1172e+00,  3.0469e-01, -1.8359e-01,
        -2.6172e-01, -1.2891e+00, -2.7148e-01,  1.9531e-01,  1.2305e-01,
         1.6211e-01, -5.8594e-01,  1.1377e-01, -3.2031e-01, -1.0312e+00,
        -3.5352e-01,  6.2500e-01,  7.1875e-01,  4.3164e-01, -1.7383e-01,
         5.1025e-02, -6.9824e-02,  9.1797e-01,  2.8125e-01,  5.7373e-02,
        -3.9453e-01,  2.2217e-02, -1.9531e+00,  3.1406e+00,  1.2578e+00,
         1.0312e+00,  7.7734e-01, -8.2422e-01,  9.7266e-01,  5.1875e+00,
         4.1406e-01,  5.6641e-01, -5.9766e-01,  2.7031e+00,  5.9766e-01,
        -2.4844e+00, -4.7461e-01, -2.8125e+00, -2.6406e+00,  1.7188e+00,
        -2.4375e+00,  3.2812e+00, -4.2480e-02, -1.7773e-01, -6.3965e-02,
        -9.7656e-01,  6.7969e-01, -1.3184e-01, -1.5156e+00, -2.8320e-01,
         1.4648e-01, -6.2109e-01, -2.3535e-01,  1.2061e-01,  2.4121e-01,
        -3.3008e-01,  8.3496e-02, -2.1289e-01, -1.8164e-01,  1.7188e-01,
         6.5918e-02, -7.2656e-01, -2.1582e-01, -1.3184e-01,  3.6719e-01,
        -1.9062e+00,  8.3984e-02,  2.3047e-01,  2.7500e+00,  8.2520e-02,
        -7.1777e-02, -1.9688e+00, -3.1641e-01, -5.7678e-03,  2.5391e-01,
         3.7305e-01,  1.5039e-01,  6.3672e-01, -6.0156e-01, -1.5430e-01,
         2.7734e-01, -5.8125e+00,  4.3750e-01, -2.5391e-01,  1.7676e-01,
        -3.1738e-02,  3.6328e-01, -1.1572e-01,  1.2188e+00, -1.0234e+00,
         9.7266e-01, -8.1875e+00,  2.4121e-01, -4.5000e+00, -2.8564e-02,
        -1.5000e+00,  7.8438e+00, -8.5156e-01,  8.4375e+00, -2.6750e+01,
        -3.6406e+00,  4.8438e+00,  1.1312e+01, -3.7812e+00, -1.0500e+01,
         7.1875e+00,  8.0625e+00,  1.6953e+00,  7.3438e-01, -9.6484e-01,
        -1.0469e+00, -9.9121e-02, -1.4648e-01, -4.0234e-01, -3.6914e-01,
         2.0000e+00,  4.6875e-01, -2.5391e-01, -2.3594e+00,  4.2480e-02,
        -5.0537e-02, -1.7656e+00, -4.8438e-01, -1.7773e-01, -9.3750e-02,
        -4.0312e+00, -9.4727e-02, -7.3242e-02, -1.2939e-02,  3.1641e-01,
         2.0020e-01,  2.3633e-01,  1.4258e-01,  2.9492e-01,  9.4238e-02,
         4.7656e-01, -8.7891e-03, -1.3086e-01, -4.0234e-01, -8.7891e-02,
        -1.0352e-01,  8.5449e-02,  1.3477e-01, -5.5908e-02, -3.0273e-02,
        -2.4844e+00, -7.2937e-03,  2.7539e-01, -9.3262e-02,  2.8320e-01,
        -1.9336e-01,  6.5234e-01,  6.5234e-01, -1.5078e+00,  1.1641e+00,
        -8.5547e-01,  2.7031e+00, -3.2969e+00,  2.6562e+00, -5.9375e-01,
        -3.4688e+00,  7.3125e+00,  6.3438e+00,  2.8500e+01, -4.2969e-01,
         2.5469e+00, -7.0938e+00,  1.1812e+01,  5.6562e+00,  4.9062e+00,
        -7.4375e+00, -6.0156e-01, -1.7676e-01,  2.9102e-01, -7.2266e-01,
         5.0391e-01,  7.5781e-01, -3.2422e-01, -2.0117e-01,  7.5781e-01,
         4.9805e-01,  1.8677e-02,  4.0820e-01, -6.6895e-02, -4.3555e-01,
        -7.2266e-01,  5.1953e-01,  1.1641e+00, -8.4375e-01,  4.3555e-01,
         6.8359e-01,  7.7344e-01,  4.6875e-01,  1.6016e+00, -6.8750e-01,
        -1.1797e+00,  6.4453e-01,  7.6953e-01, -1.1572e-01, -1.2500e+00,
         2.0625e+00,  4.2383e-01, -7.1094e-01, -4.8242e-01,  8.8281e-01,
         4.4141e-01, -6.1768e-02, -1.8594e+00, -5.0781e-02, -1.6641e+00,
         2.3633e-01, -4.4336e-01,  8.9062e-01,  2.9688e+00, -2.1484e-01,
         1.0010e-02,  4.7266e-01, -8.5156e-01, -2.4531e+00,  3.9844e-01,
        -1.2562e+01,  2.6719e+00,  3.5156e+00,  1.9375e+00,  4.0625e+00,
         3.8281e+00,  1.0645e-01, -5.9062e+00,  1.7969e+00,  6.1562e+00,
        -4.0625e+00,  2.3281e+00,  3.8906e+00,  4.4375e+00, -3.4062e+00,
        -6.2891e-01, -5.0000e-01, -5.9375e-01, -1.7676e-01,  1.6211e-01,
         3.1494e-02, -6.9922e-01,  7.5000e-01,  2.8516e-01,  1.9727e-01,
        -9.1016e-01, -3.0859e-01, -7.1875e-01, -1.0547e+00, -7.1777e-02,
         7.9688e-01, -2.2852e-01, -1.8652e-01, -9.1309e-02,  1.1797e+00,
        -7.9346e-03, -5.0000e-01, -1.6211e-01,  6.2891e-01, -1.7109e+00,
        -7.3828e-01, -4.8096e-02,  3.5938e-01, -3.5156e-01,  2.2031e+00,
         1.3379e-01,  2.8125e-01,  2.3633e-01,  7.9688e-01, -2.3071e-02,
        -5.2979e-02,  4.1562e+00,  2.0996e-02,  1.8750e-01, -9.1797e-02,
         6.8750e-01,  4.1797e-01,  8.4766e-01, -1.1621e-01,  5.7812e-01,
        -8.2812e-01,  3.5547e-01,  1.5078e+00,  1.3359e+00, -4.8125e+00,
        -1.8203e+00, -9.4727e-02,  3.2031e+00,  1.5391e+00,  1.5000e+00,
         1.0547e+00,  1.8281e+00,  1.6406e+00, -5.7188e+00, -9.2969e-01,
        -7.1250e+00, -1.2656e+00,  1.2793e-01, -4.1562e+00,  9.0234e-01,
         5.5469e-01, -1.1816e-01, -1.2207e-01,  4.1797e-01,  4.1211e-01,
         1.1406e+00, -3.6328e-01, -3.2422e-01,  1.1621e-01,  3.0664e-01,
        -4.6680e-01,  2.2031e+00,  1.1133e-01, -7.2266e-01,  2.3594e+00,
        -9.0625e-01, -4.3164e-01, -1.2656e+00,  4.4727e-01,  1.7969e-01,
        -2.1562e+00,  7.5000e-01, -3.0078e-01, -3.5000e+00, -1.5723e-01,
         6.8359e-01, -9.5312e-01, -7.1875e-01, -7.5391e-01, -8.5938e-02,
         3.7305e-01, -1.6562e+00,  5.0293e-02,  9.3750e-02,  3.2617e-01,
         8.0859e-01, -1.1328e+00,  1.3306e-02,  4.0430e-01, -4.8096e-02,
        -5.1025e-02, -9.1309e-02,  1.0889e-01, -1.8082e-03, -5.7188e+00,
        -6.2561e-03,  8.9355e-02,  3.0273e-01, -1.9336e-01, -8.6328e-01,
        -4.2812e+00, -1.3906e+00,  4.2383e-01,  2.0605e-01,  3.4688e+00,
        -6.4844e-01,  1.3477e-01,  2.1719e+00,  6.5938e+00, -4.6250e+00,
        -5.5625e+00,  1.7109e+00, -3.6250e+00, -4.8438e-01, -5.3906e-01,
        -7.8125e-01, -1.2734e+00, -3.9453e-01, -7.3828e-01,  7.8125e-02,
         1.0781e+00,  2.0996e-01, -1.8906e+00,  9.8633e-02,  9.2969e-01,
        -1.7285e-01, -4.5117e-01,  6.4062e-01, -7.5391e-01,  4.9805e-02,
         1.2344e+00,  1.1621e-01,  1.3281e+00,  1.5781e+00,  5.0781e-01,
        -3.7305e-01, -6.0156e-01,  1.0938e+00, -5.8594e-02, -1.2266e+00,
         5.7812e-01,  2.9102e-01, -1.4766e+00,  2.0996e-01, -7.8906e-01,
        -3.6562e+00,  4.2188e-01, -4.1504e-02,  2.4219e-01, -2.1484e-01,
         9.1016e-01, -6.4062e-01,  2.8320e-01,  9.9609e-01,  3.7500e-01,
        -1.5547e+00,  1.6846e-02, -1.9287e-02, -4.8438e+00,  7.7209e-03,
        -5.3516e-01,  1.3867e-01, -1.2512e-02, -7.7637e-02,  1.5125e+01,
         1.0156e+00, -2.8125e-01,  1.5820e-01, -2.9219e+00, -1.0625e+00,
        -4.1250e+00, -2.1250e+00,  3.1250e+00,  4.1875e+00, -1.0562e+01,
        -5.5000e+00, -8.8750e+00])

llm.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0275, -0.0129, -0.0349,  ..., -0.0274,  0.0360, -0.0064],
        [-0.0360, -0.0191,  0.0337,  ...,  0.0387,  0.0216, -0.0072],
        [-0.0226, -0.0178,  0.0009,  ...,  0.0023,  0.0182, -0.0123],
        ...,
        [ 0.0092, -0.0111, -0.0326,  ..., -0.0185,  0.0325,  0.0216],
        [ 0.0495,  0.0091,  0.0090,  ..., -0.0320, -0.0432, -0.0126],
        [ 0.0022, -0.0034, -0.0086,  ..., -0.0064,  0.0266, -0.0172]])

llm.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0477, -0.0150,  0.0196,  ...,  0.0863, -0.0436,  0.1074],
        [-0.0351,  0.0488,  0.0043,  ..., -0.0579, -0.0213, -0.0309],
        [ 0.0206, -0.0282, -0.0202,  ...,  0.0274,  0.0280,  0.0132],
        ...,
        [-0.0096,  0.0257,  0.0170,  ..., -0.0390,  0.0073,  0.0067],
        [-0.0187, -0.0167, -0.0141,  ...,  0.0085,  0.0115, -0.0327],
        [ 0.0177, -0.0282, -0.0218,  ...,  0.0163,  0.0307,  0.0368]])

llm.base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-8.6060e-03, -2.1973e-02, -1.5503e-02,  ..., -1.0620e-02,
          6.4087e-03,  5.7678e-03],
        [-9.3384e-03,  3.8757e-03, -1.2024e-02,  ..., -1.3184e-02,
          4.8523e-03, -2.8229e-03],
        [-1.1475e-02,  1.0132e-02, -1.9684e-03,  ...,  1.5381e-02,
          1.1292e-02, -8.4839e-03],
        ...,
        [-2.2583e-03,  1.2390e-02, -6.2256e-03,  ...,  1.5717e-03,
          2.5558e-04,  1.1536e-02],
        [-1.7944e-02, -1.8677e-02, -7.6294e-03,  ...,  7.4158e-03,
          3.0975e-03,  7.1716e-03],
        [-2.8229e-03, -1.0742e-02, -6.6280e-05,  ..., -5.1575e-03,
         -8.4839e-03,  4.4250e-03]])

llm.base_model.model.model.layers.2.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-2.4109e-03,  1.1292e-02, -3.1433e-03,  2.9175e-02,  3.3447e-02,
        -1.1902e-02,  1.1658e-02, -3.5156e-02, -1.5564e-02,  8.9722e-03,
         2.8534e-03,  4.1748e-02,  2.7954e-02, -5.0354e-03, -1.3489e-02,
        -3.4668e-02,  7.1289e-02,  5.7373e-03, -2.0752e-02, -1.4343e-02,
         2.3438e-02,  9.5367e-05,  1.4648e-03,  1.0061e-04, -1.1414e-02,
        -1.9653e-02, -5.4932e-03, -3.0212e-03, -3.8574e-02, -3.8757e-03,
         3.2715e-02,  1.3367e-02, -8.0872e-04,  1.4832e-02,  1.2634e-02,
        -7.0801e-03,  1.0559e-02, -3.9673e-03, -4.7363e-02,  1.0498e-02,
         2.2583e-02, -1.2329e-02, -1.0223e-03, -1.3733e-03,  1.3828e-04,
         4.6387e-03,  2.9907e-03,  2.0508e-02, -2.7954e-02,  2.3682e-02,
         6.3477e-02,  8.2397e-03,  1.9684e-03, -1.0559e-02, -1.0625e+00,
        -1.5747e-02,  2.1515e-03,  7.0496e-03,  1.7944e-02, -1.4343e-02,
        -3.1250e-02, -5.1880e-03, -1.3351e-03, -1.7822e-02, -2.9175e-02,
         1.6235e-02, -1.6022e-03,  5.7678e-03,  4.7302e-03, -8.3618e-03,
         2.7832e-02,  1.7090e-02,  2.5635e-02, -1.2109e-01, -1.9287e-02,
        -2.3560e-02,  9.2163e-03,  1.1816e-01,  4.3701e-02,  1.0223e-03,
         2.2217e-02,  2.3560e-02,  2.3682e-02, -2.8198e-02,  2.3499e-03,
        -2.5757e-02, -6.6833e-03,  6.9336e-02,  1.6602e-02,  5.3024e-04,
        -2.3071e-02, -2.1606e-02, -4.4678e-02, -2.4902e-01,  6.8359e-03,
         1.5198e-02, -1.2512e-02,  3.0396e-02, -2.1667e-03,  3.0518e-02,
        -1.1536e-02,  6.4453e-02, -2.1606e-02,  7.3242e-03, -1.9775e-02,
         4.2969e-02, -2.9755e-03, -3.0060e-03, -9.9487e-03, -9.1553e-03,
        -2.0752e-02,  7.3547e-03,  1.2817e-02,  1.9043e-02, -6.6406e-02,
         2.1240e-02,  1.3000e-02, -1.3245e-02,  3.4180e-02,  4.0588e-03,
        -2.1484e-02,  1.9897e-02,  2.2705e-02, -1.1597e-02,  3.7354e-02,
         2.8931e-02, -4.0771e-02, -1.7334e-02,  3.2043e-04,  2.7344e-02,
         1.3256e-04,  6.1340e-03, -2.1118e-02, -1.3428e-02, -2.1851e-02,
        -2.7466e-02,  4.6387e-03, -1.8066e-02, -4.9591e-04,  1.1169e-02,
         8.4839e-03,  7.0801e-03, -2.8687e-03, -8.8501e-03, -6.1340e-03,
        -9.2773e-03, -2.7466e-03, -7.7057e-04,  1.4258e-01, -2.9373e-04,
         5.4321e-03, -2.6550e-03,  8.7891e-03,  1.3794e-02, -3.3417e-03,
         2.7832e-02,  8.2397e-03, -1.9141e-01, -5.1270e-03,  4.8828e-03,
        -1.1826e-03, -6.0120e-03,  6.9427e-04, -2.0752e-02, -6.0425e-03,
        -3.6133e-02, -4.4556e-03, -1.5442e-02, -1.4221e-02, -2.4719e-03,
        -1.2085e-02,  5.0354e-03,  1.2451e-02,  4.6082e-03,  6.8665e-03,
         1.0742e-02, -8.1787e-03, -2.4780e-02,  8.4229e-03,  9.4604e-03,
         1.7383e-01,  3.4912e-02, -3.2959e-02, -2.3804e-02,  1.6098e-03,
         2.7618e-03, -1.7929e-03,  3.9101e-04, -1.0071e-02, -2.6245e-02,
         7.8735e-03, -2.2705e-02,  2.3041e-03,  1.2207e-02, -1.9043e-02,
        -2.1851e-02,  1.7700e-02,  3.2349e-03, -1.8311e-02,  5.6152e-03,
        -1.6479e-02,  1.1780e-02, -3.6469e-03,  6.9046e-04, -1.2512e-02,
         1.4038e-02,  1.1658e-02,  1.0132e-02, -5.1270e-02,  1.7929e-03,
         1.4832e-02, -2.1553e-04,  1.1230e-02,  8.5831e-04,  3.3417e-03,
         2.3560e-02,  9.5825e-03, -4.7913e-03,  1.5335e-03, -1.1719e-02,
        -6.4697e-03,  1.5625e-02, -7.0801e-03, -6.2988e-02,  1.2207e-02,
         5.7983e-03,  1.4572e-03, -6.5918e-03,  3.6163e-03, -9.4604e-03,
        -2.0630e-02, -6.5002e-03,  6.2866e-03,  6.6528e-03,  3.2227e-02,
         5.2795e-03,  2.8931e-02,  2.1076e-04, -2.2461e-02,  6.0120e-03,
        -1.7212e-02,  1.1780e-02, -4.1260e-02,  1.0498e-02, -9.7046e-03,
        -3.8910e-03,  1.7578e-02,  1.2024e-02,  1.6113e-02,  1.4709e-02,
        -2.8320e-02, -6.4087e-04,  2.9907e-03, -4.3640e-03,  2.4414e-02,
         7.0190e-03,  3.0029e-02, -1.9775e-02, -2.6703e-03,  6.2180e-04,
        -1.2817e-02,  1.0437e-02,  1.4038e-03,  2.6093e-03, -8.3923e-05,
         2.0142e-02,  2.3926e-02,  9.3384e-03, -4.9744e-03, -5.2490e-03,
        -1.2878e-02,  3.7598e-02,  1.7578e-02, -3.8330e-02, -1.9775e-02,
        -8.7891e-03, -5.3406e-03, -3.3691e-02, -2.8839e-03, -1.3489e-02,
         1.1597e-02,  1.2756e-02, -1.0803e-02,  1.4587e-02, -2.1667e-03,
         7.1335e-04, -7.7209e-03,  1.6357e-02, -2.9663e-02,  4.8523e-03,
        -2.6550e-03, -6.5918e-03, -6.3782e-03,  1.2634e-02,  2.2507e-04,
        -8.2397e-03, -3.1494e-02,  7.7820e-03, -3.0273e-02,  4.1199e-03,
         1.5488e-03, -3.2227e-02,  1.4465e-02,  1.3275e-03, -1.4404e-02,
         1.2146e-02, -9.1553e-04,  3.0670e-03, -4.8523e-03, -1.4687e-04,
         9.8877e-03,  1.6113e-02, -2.3560e-02,  1.2268e-02, -3.6316e-03,
         2.8076e-02, -6.2561e-03, -4.9744e-03,  2.8320e-02, -1.2573e-02,
         3.4332e-03,  8.4229e-03, -1.3611e-02, -4.9805e-02,  1.5259e-02,
         2.5330e-03,  4.3457e-02, -2.5024e-03, -1.8921e-02, -1.2329e-02,
         2.1484e-02,  1.2207e-03,  1.9653e-02,  1.5991e-02, -7.1411e-03,
        -5.0964e-03,  6.5002e-03, -7.9346e-03, -5.5237e-03, -8.3618e-03,
         4.6387e-03, -4.9744e-03, -7.2021e-03,  4.8584e-02, -1.0742e-01,
         2.2278e-03, -2.3651e-04,  4.3335e-03, -3.3936e-02,  3.1250e-02,
        -2.1606e-02, -1.2268e-02,  3.8330e-02, -3.1738e-02, -1.4099e-02,
        -1.8616e-03,  4.9744e-03,  1.0925e-02, -9.8267e-03,  7.9956e-03,
         8.3618e-03, -3.5156e-02,  1.1536e-02,  3.5889e-02,  8.9111e-03,
        -1.5137e-02, -8.5449e-03, -1.7548e-03,  5.4688e-02,  1.4771e-02,
        -1.4941e-01,  1.2146e-02, -5.7129e-02, -2.2705e-02,  2.3315e-02,
        -3.3722e-03,  1.6235e-02, -3.2715e-02, -1.6479e-02,  6.2256e-03,
        -5.7678e-03,  1.1414e-02, -1.2634e-02,  1.0864e-02, -1.8066e-02,
        -4.6387e-03, -7.3853e-03,  1.8311e-03, -8.4839e-03, -7.7820e-03,
        -2.1118e-02,  1.1719e-01, -1.0132e-02, -4.0283e-02, -3.7842e-02,
        -6.3782e-03, -6.8359e-03, -1.5869e-02,  5.4626e-03,  7.4463e-03,
        -5.7983e-03,  1.0315e-02,  9.8877e-03, -2.0905e-03,  1.2085e-02,
        -5.0964e-03, -2.3682e-02,  2.8931e-02, -2.5024e-02, -6.0120e-03,
        -3.8910e-04,  9.9487e-03, -5.9204e-03, -2.7710e-02, -6.1951e-03,
         7.0801e-02, -1.3828e-04, -1.1047e-02,  2.2583e-02,  3.8330e-02,
        -1.6785e-03,  1.1414e-02, -1.1597e-02,  1.6113e-02,  7.6599e-03,
         2.1973e-02, -2.7710e-02,  9.4238e-02,  4.1260e-02, -2.7588e-02,
         1.4648e-03, -3.5645e-02,  1.6602e-02, -5.4321e-03, -5.7068e-03,
         2.3346e-03, -5.1270e-03, -1.1475e-02, -1.8387e-03,  8.3008e-03,
        -1.2939e-02,  1.7578e-02,  5.1880e-03,  1.0315e-02, -4.5471e-03,
        -3.1250e-02, -5.9814e-03,  8.4229e-03,  1.6479e-02, -1.3611e-02,
         3.6133e-02,  2.0996e-02, -3.4790e-03,  9.1553e-03, -9.4604e-03,
         9.0332e-03, -2.5146e-02,  2.2217e-02,  1.8768e-03,  7.1526e-05,
         1.5991e-02, -1.4099e-02,  5.6152e-03, -2.3560e-02,  1.2024e-02,
         1.7578e-02, -1.9165e-02, -8.7891e-02,  5.6763e-03,  8.3542e-04,
        -1.9409e-02,  2.3315e-02,  1.0986e-02,  2.6245e-02,  4.3030e-03,
         4.3640e-03, -1.9073e-03,  2.9175e-02, -1.8311e-02,  1.0193e-02,
        -5.1270e-03,  1.5076e-02, -6.2256e-02,  5.5908e-02,  1.8188e-02,
         1.6357e-02, -1.3977e-02, -1.3367e-02, -1.0986e-02,  3.2196e-03,
         1.6785e-03, -1.7822e-02, -1.7319e-03, -3.6865e-02,  1.0925e-02,
         1.1475e-02, -8.7280e-03, -2.1240e-02,  1.8311e-02,  2.9785e-02,
        -1.0925e-02,  2.8076e-02,  4.2969e-02,  5.5237e-03,  2.4536e-02,
        -2.0996e-02, -2.1515e-03,  2.7161e-03, -1.3062e-02, -1.0071e-02,
         1.0010e-02, -1.5747e-02])

llm.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0034, -0.0204, -0.0040,  ...,  0.0165, -0.0171,  0.0051],
        [-0.0035,  0.0143, -0.0349,  ..., -0.0359, -0.0017,  0.0185],
        [-0.0093,  0.0097,  0.0018,  ...,  0.0004, -0.0042, -0.0141],
        ...,
        [ 0.0443, -0.0061,  0.0255,  ...,  0.0064,  0.0236,  0.0021],
        [ 0.0182, -0.0194,  0.0474,  ..., -0.0186, -0.0018,  0.0027],
        [ 0.0123, -0.0128,  0.0001,  ..., -0.0055,  0.0415,  0.0216]])

llm.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0254, -0.0277,  0.0050,  ..., -0.0389, -0.0312,  0.0015],
        [ 0.0083,  0.0038,  0.0306,  ...,  0.0046, -0.0179, -0.0027],
        [-0.0108,  0.0016,  0.0095,  ...,  0.0288, -0.0196, -0.0275],
        ...,
        [-0.0020, -0.0083,  0.0042,  ...,  0.0309,  0.0089, -0.0170],
        [ 0.0332,  0.0062, -0.0038,  ..., -0.0430, -0.0324, -0.0472],
        [-0.0185,  0.0059, -0.0022,  ..., -0.0071,  0.0086, -0.0300]])

llm.base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0121,  0.0146, -0.0107,  ..., -0.0014,  0.0125, -0.0016],
        [ 0.0198, -0.0018,  0.0056,  ..., -0.0005, -0.0193, -0.0262],
        [ 0.0154, -0.0062, -0.0134,  ...,  0.0004, -0.0048, -0.0047],
        ...,
        [ 0.0006, -0.0015, -0.0056,  ...,  0.0167, -0.0125, -0.0168],
        [-0.0020, -0.0038, -0.0114,  ..., -0.0078, -0.0032, -0.0193],
        [ 0.0120,  0.0035,  0.0288,  ..., -0.0126,  0.0219,  0.0011]])

llm.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0261,  0.0113,  0.0075,  ...,  0.0140, -0.0241, -0.0507],
        [-0.0116,  0.0306, -0.0144,  ..., -0.0394, -0.0033, -0.0063],
        [ 0.0506, -0.0292,  0.0030,  ..., -0.0556, -0.0208, -0.0181],
        ...,
        [-0.0274, -0.0109, -0.0067,  ..., -0.0056,  0.0018, -0.0177],
        [ 0.0054, -0.0642,  0.0281,  ...,  0.0034,  0.0057,  0.0131],
        [ 0.0039, -0.0169,  0.0360,  ...,  0.0130,  0.0413,  0.0291]])

llm.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0018, -0.0272,  0.0169,  ..., -0.0331,  0.0623,  0.0144],
        [-0.0101,  0.0391, -0.0229,  ...,  0.0128, -0.0029,  0.0519],
        [ 0.0337,  0.0460, -0.0256,  ...,  0.0019, -0.0045,  0.0085],
        ...,
        [-0.0249,  0.0149, -0.0068,  ...,  0.0055, -0.0015,  0.0139],
        [ 0.0017, -0.0257, -0.0107,  ...,  0.0062, -0.0270, -0.0128],
        [-0.0617,  0.0192,  0.0341,  ...,  0.0036, -0.0030, -0.0106]])

llm.base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-1.3046e-03, -1.1902e-03, -1.2338e-05,  ..., -8.3160e-04,
         -9.9182e-04,  2.9297e-03],
        [ 1.3611e-02,  2.2949e-02, -1.5076e-02,  ..., -1.1658e-02,
         -1.6724e-02,  2.7954e-02],
        [-8.9111e-03, -2.3560e-02,  2.5391e-02,  ...,  1.0193e-02,
          1.6861e-03, -1.0834e-03],
        ...,
        [ 5.5313e-04,  4.0527e-02,  1.7456e-02,  ..., -8.6060e-03,
          2.3315e-02, -5.2734e-02],
        [-2.6855e-02,  1.7776e-03, -1.0559e-02,  ...,  1.7471e-03,
         -1.3245e-02, -2.1667e-03],
        [ 2.3556e-04, -8.0872e-04, -4.5395e-04,  ..., -7.3624e-04,
         -7.8201e-05,  1.8616e-03]])

llm.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0619, -0.0200,  0.0445,  ...,  0.0810,  0.0224,  0.0301],
        [-0.0440, -0.0654,  0.0806,  ..., -0.0337,  0.0310,  0.0004],
        [-0.0361,  0.0511,  0.0381,  ...,  0.0023,  0.0029,  0.0438],
        ...,
        [-0.0330, -0.0424, -0.0354,  ...,  0.0204, -0.0986,  0.0515],
        [-0.0042,  0.0986, -0.0941,  ...,  0.0847, -0.0689, -0.0395],
        [ 0.0166, -0.0017,  0.0404,  ..., -0.0030,  0.0533,  0.0500]])

llm.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0132,  0.0026, -0.0264,  ...,  0.0102,  0.0458,  0.0266],
        [-0.0444, -0.0695, -0.0348,  ...,  0.0093, -0.0017,  0.0216],
        [ 0.0002,  0.0010, -0.0416,  ...,  0.0059, -0.0173, -0.0084],
        ...,
        [-0.0368,  0.0122, -0.0567,  ..., -0.0151, -0.0059,  0.0249],
        [ 0.0161, -0.0208,  0.0231,  ...,  0.0029,  0.0114,  0.0326],
        [-0.0448,  0.0451, -0.0692,  ..., -0.0470, -0.0319, -0.0296]])

llm.base_model.model.model.layers.2.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0003, -0.0009,  0.0028,  ...,  0.0009, -0.0017, -0.0007],
        [ 0.0016,  0.0337, -0.0070,  ...,  0.0019, -0.0181,  0.0115],
        [-0.0181,  0.0070,  0.0128,  ..., -0.0014,  0.0038,  0.0088],
        ...,
        [ 0.0100,  0.0085, -0.0001,  ..., -0.0067, -0.0046, -0.0006],
        [-0.0056, -0.0096, -0.0262,  ..., -0.0162, -0.0039, -0.0037],
        [-0.0004,  0.0034,  0.0005,  ..., -0.0007,  0.0012, -0.0031]])

llm.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0439,  0.0207, -0.0058,  ..., -0.0441, -0.0664, -0.0142],
        [-0.0284,  0.0218, -0.0162,  ..., -0.0335, -0.0138,  0.0468],
        [-0.0267,  0.0168,  0.0831,  ..., -0.0178, -0.0166,  0.0274],
        ...,
        [-0.0154, -0.0267,  0.0087,  ...,  0.0127,  0.0199,  0.0351],
        [ 0.0311, -0.0044, -0.0091,  ...,  0.0110,  0.0066,  0.0330],
        [ 0.0011,  0.0794,  0.0024,  ..., -0.0211, -0.0052,  0.0035]])

llm.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0084,  0.0220,  0.0049,  ...,  0.0028, -0.0189,  0.0129],
        [-0.0332,  0.0351,  0.0321,  ...,  0.0224, -0.0008, -0.0208],
        [ 0.0358,  0.0420, -0.0530,  ..., -0.0394,  0.0171, -0.0074],
        ...,
        [-0.0304,  0.0070,  0.0192,  ...,  0.0445,  0.0083,  0.0560],
        [ 0.0090,  0.0419,  0.0213,  ...,  0.0271, -0.0091, -0.0369],
        [ 0.0287, -0.0201, -0.0148,  ..., -0.0060, -0.0329,  0.0326]])

llm.base_model.model.model.layers.2.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 7.5817e-05,  5.3711e-03, -6.7444e-03,  ...,  1.7395e-03,
          1.0147e-03, -5.4016e-03],
        [ 2.4567e-03, -2.8229e-03,  4.2725e-03,  ..., -4.9438e-03,
          4.4861e-03, -2.7275e-04],
        [-2.1820e-03, -1.5747e-02,  3.0365e-03,  ..., -5.4932e-03,
          1.4343e-02,  5.7983e-04],
        ...,
        [-4.3945e-03, -2.1820e-03, -7.8735e-03,  ...,  2.1118e-02,
         -3.0640e-02, -8.5449e-04],
        [-1.8463e-03,  7.1716e-03,  1.2085e-02,  ...,  1.1826e-03,
         -1.1353e-02, -2.1057e-03],
        [-3.6469e-03, -1.8387e-03,  7.0496e-03,  ..., -9.6436e-03,
          7.1716e-03, -1.4801e-03]])

llm.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0088,  0.0141,  0.0144,  ..., -0.0203,  0.0032,  0.0268],
        [ 0.0328,  0.0078, -0.0254,  ...,  0.0068, -0.0251, -0.0131],
        [ 0.0307,  0.0131, -0.0029,  ...,  0.0049,  0.0209, -0.0009],
        ...,
        [ 0.0186, -0.0392, -0.0025,  ...,  0.0295, -0.0202,  0.0070],
        [-0.0359, -0.0018,  0.0396,  ...,  0.0032,  0.0032, -0.0152],
        [-0.0054,  0.0132, -0.0110,  ..., -0.0296,  0.0102, -0.0140]])

llm.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0261, -0.0100,  0.0053,  ..., -0.0097,  0.0149, -0.0132],
        [-0.0266, -0.0154,  0.0043,  ..., -0.0262, -0.0371,  0.0045],
        [ 0.0253,  0.0004,  0.0122,  ...,  0.0033,  0.0043, -0.0075],
        ...,
        [-0.0290,  0.0033, -0.0096,  ..., -0.0142,  0.0196,  0.0492],
        [ 0.0241, -0.0138, -0.0147,  ..., -0.0143, -0.0222,  0.0355],
        [ 0.0072,  0.0257,  0.0087,  ..., -0.0278,  0.0129, -0.0387]])

llm.base_model.model.model.layers.2.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.5039, 0.6133, 0.5078,  ..., 0.3008, 0.3398, 0.7031])

llm.base_model.model.model.layers.2.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.7539, 0.9570, 0.9062,  ..., 0.7344, 0.9453, 1.1562])

llm.base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-1.6235e-02,  5.9204e-03, -7.2327e-03,  ..., -6.8665e-03,
          7.7820e-03,  7.0572e-04],
        [-9.0332e-03,  4.1199e-03, -5.0354e-03,  ..., -2.8229e-03,
          2.9907e-02, -6.9580e-03],
        [-1.6479e-02,  9.6436e-03,  1.2207e-02,  ..., -1.0986e-02,
         -1.4782e-04, -4.1723e-05],
        ...,
        [ 1.3809e-03,  1.6602e-02, -1.8158e-03,  ...,  1.2589e-03,
         -1.9531e-02,  5.3406e-03],
        [-6.1340e-03, -7.4463e-03,  1.2878e-02,  ..., -4.2419e-03,
          6.4697e-03,  1.3428e-02],
        [ 1.2573e-02, -1.3123e-02,  1.4709e-02,  ..., -2.5757e-02,
         -3.6621e-04,  1.4832e-02]])

llm.base_model.model.model.layers.3.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.3613,  0.1738,  0.8984,  ..., -0.2676, -0.6680,  0.0226])

llm.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0240, -0.0048, -0.0128,  ...,  0.0050, -0.0321, -0.0085],
        [-0.0053, -0.0155,  0.0292,  ...,  0.0233,  0.0198,  0.0301],
        [-0.0285,  0.0122, -0.0511,  ...,  0.0041, -0.0105, -0.0223],
        ...,
        [ 0.0273,  0.0181, -0.0150,  ...,  0.0293,  0.0069,  0.0027],
        [-0.0370,  0.0116,  0.0450,  ...,  0.0054, -0.0046, -0.0537],
        [ 0.0461,  0.0400,  0.0178,  ..., -0.0176,  0.0015, -0.0885]])

llm.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0871,  0.0778,  0.0171,  ...,  0.0665,  0.0681,  0.0132],
        [-0.0151,  0.0048,  0.0139,  ..., -0.0136, -0.0044, -0.0211],
        [ 0.0463, -0.0035, -0.0187,  ..., -0.0127, -0.0433, -0.0555],
        ...,
        [-0.0026, -0.0036, -0.0160,  ...,  0.0289, -0.0097,  0.0119],
        [ 0.0166,  0.0103,  0.0507,  ...,  0.0018,  0.0202,  0.0546],
        [-0.0149, -0.0286, -0.0073,  ..., -0.0326, -0.0089,  0.0096]])

llm.base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0197,  0.0190, -0.0298,  ...,  0.0061, -0.0143,  0.0208],
        [ 0.0273,  0.0400,  0.0175,  ...,  0.0037,  0.0020,  0.0067],
        [ 0.0109, -0.0177, -0.0072,  ...,  0.0190,  0.0096, -0.0114],
        ...,
        [-0.0084, -0.0356, -0.0099,  ...,  0.0010,  0.0330, -0.0148],
        [ 0.0146, -0.0124,  0.0110,  ..., -0.0162, -0.0400,  0.0208],
        [-0.0280,  0.0056,  0.0439,  ..., -0.0033,  0.0277,  0.0193]])

llm.base_model.model.model.layers.3.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-8.0469e-01,  1.4258e-01,  2.6250e+00,  5.5469e-01, -4.3945e-01,
        -1.7456e-02, -1.5000e+00, -1.7285e-01, -2.7812e+00,  3.5938e-01,
        -5.7812e-01,  1.5527e-01, -2.0020e-02,  2.6172e-01, -9.6094e-01,
        -5.1172e-01,  2.5391e-01,  8.2031e-01,  7.4158e-03, -2.6398e-03,
        -3.4375e-01, -6.0938e-01,  3.8281e+00, -6.9531e-01, -8.4766e-01,
        -6.9141e-01, -1.3477e-01,  5.0781e-01,  3.6719e+00,  8.7500e-01,
         1.0469e+00,  1.4062e-01, -7.7734e-01,  1.6016e-01, -1.9062e+00,
        -7.8125e-01, -2.7344e-01, -2.0938e+00, -5.5859e-01, -1.0781e+00,
         1.2188e+00, -4.4632e-04, -2.3125e+00,  2.4062e+00, -1.1406e+00,
        -7.1875e-01,  1.4688e+00,  1.4750e+01, -1.3281e+00, -8.1641e-01,
         3.3906e+00,  4.4189e-02, -3.8594e+00,  3.0938e+00, -3.5500e+01,
        -1.2188e+00,  3.5469e+00, -2.9062e+00,  2.1125e+01,  1.2250e+01,
        -4.0500e+01, -1.3562e+01,  2.5375e+01,  3.0469e+00, -3.4531e+00,
        -4.1406e-01,  5.0391e-01,  2.9883e-01, -3.9258e-01, -2.7812e+00,
         2.9688e-01, -3.2031e-01,  7.7344e-01,  2.2266e-01, -1.0156e-01,
        -3.1250e+00,  2.4512e-01,  7.1875e-01,  4.1406e-01,  1.5625e-01,
        -2.4414e-02,  3.7031e+00,  8.5547e-01, -5.3516e-01,  3.5352e-01,
         3.6621e-02, -9.0625e-01, -2.8906e-01,  3.2227e-01,  5.1953e-01,
         3.9844e-01, -8.2520e-02, -8.1055e-02,  7.2266e-01, -3.3008e-01,
        -8.3008e-02, -8.0078e-01, -1.1797e+00, -5.1875e+00, -5.4932e-03,
         4.6094e-01, -2.6406e+00, -1.7500e+00,  2.3906e+00, -1.0547e+00,
        -1.9219e+00,  6.4062e-01,  4.9414e-01,  1.1562e+00, -2.3125e+00,
        -1.3828e+00,  7.0312e+00, -1.9844e+00, -1.2188e+00, -4.6562e+00,
         3.6406e+00,  6.8125e+00, -1.3203e+00, -1.6375e+01, -2.0312e+00,
         5.2812e+00,  5.0000e+00,  7.1094e-01, -2.0625e+01, -2.1125e+01,
         6.7500e+00,  3.2656e+00,  6.8000e+01, -4.0430e-01,  1.5625e+00,
         1.0625e+00,  1.7734e+00,  2.5391e-01,  1.3125e+00,  4.9805e-02,
         4.9805e-01, -7.2656e-01,  3.3203e-01, -1.6172e+00, -2.5977e-01,
        -4.8633e-01,  2.1973e-01,  3.3203e-01, -5.3906e-01,  8.8672e-01,
        -2.3346e-03, -3.6523e-01,  7.7344e-01,  2.7148e-01, -6.0938e-01,
         3.2969e+00,  1.2109e+00, -1.7656e+00, -7.7734e-01,  1.4141e+00,
        -6.3672e-01, -1.1172e+00, -1.1094e+00, -1.4941e-01,  2.4062e+00,
         2.1777e-01, -1.7578e-02,  2.4062e+00,  9.7656e-02,  1.8066e-01,
        -1.2031e+00,  5.1172e-01,  9.3359e-01,  2.5469e+00,  3.1250e-01,
         3.9219e+00,  2.2705e-02, -5.9326e-02,  2.1289e-01,  2.1973e-02,
        -4.7461e-01,  3.4570e-01, -7.3047e-01,  2.7539e-01,  2.5195e-01,
         4.6484e-01,  4.2500e+00, -1.4648e-01,  1.3984e+00,  9.2188e-01,
        -5.5078e-01, -1.0156e+00, -7.2656e-01,  4.2383e-01,  5.2246e-02,
         2.4688e+00,  2.0469e+00,  3.1719e+00, -1.9336e-01, -1.2969e+00,
         5.8594e-02, -1.3281e+00,  6.5234e-01, -1.5547e+00, -1.3750e+00,
         1.5547e+00, -3.8672e-01, -1.6309e-01,  1.5859e+00, -4.1992e-02,
         2.0469e+00,  1.0938e+00, -1.9238e-01,  1.9141e+00,  2.5781e-01,
        -2.4062e+00,  3.6719e-01,  4.4189e-02, -6.0156e-01, -4.2773e-01,
        -6.6406e-01, -7.3828e-01, -3.0781e+00, -5.4688e-01,  1.0391e+00,
        -2.5977e-01,  1.9766e+00, -8.4961e-02, -1.9922e+00, -7.5684e-02,
        -8.0078e-02,  4.5000e+00,  2.9492e-01,  2.3730e-01,  7.8906e-01,
        -1.0625e+00, -6.3281e-01,  1.2969e+00,  7.6172e-02,  6.0000e+00,
         3.0273e-01,  3.5938e-01,  1.4844e-01,  1.7090e-02,  4.8047e-01,
        -2.2095e-02,  2.5469e+00, -3.1250e-01, -2.6367e-01, -6.4453e-02,
         1.4438e+01, -3.0029e-02,  2.8516e-01,  1.2109e+00, -5.6250e-01,
        -1.0938e+00, -5.1953e-01, -9.6484e-01, -2.9844e+00,  5.5859e-01,
         3.5156e-02, -1.7266e+00,  2.0469e+00,  1.4297e+00, -7.8125e-01,
        -1.7344e+00,  5.4297e-01,  7.9688e-01,  1.4688e+00, -1.3672e+00,
         9.2188e-01, -2.4805e-01,  4.4678e-02,  4.9561e-02,  8.8281e-01,
         3.3398e-01, -1.6309e-01, -5.6641e-02,  1.1172e+00, -2.4688e+00,
        -1.1172e+00,  3.3203e-01, -3.7109e-01,  3.3984e-01,  9.0234e-01,
         5.3906e-01, -5.6250e-01,  2.4414e-01,  5.2734e-01, -8.5156e-01,
        -5.2734e-01, -3.4844e+00, -1.8066e-01, -5.5664e-02, -8.2422e-01,
         1.7109e+00, -1.8125e+00, -1.1562e+00, -5.1953e-01,  8.6426e-02,
         2.1191e-01,  2.7656e+00,  6.1328e-01, -1.7266e+00, -4.3213e-02,
        -7.7637e-02,  3.6875e+00, -1.5781e+00, -1.5078e+00,  2.0938e+00,
        -4.8125e+00,  2.0625e+00, -1.3438e+00,  1.4188e+01,  5.8750e+00,
         5.8750e+00, -3.7188e+00, -2.2344e+00, -1.8164e-01, -3.7344e+00,
         6.2109e-01, -4.8750e+00, -9.3359e-01,  3.2812e+00,  6.1250e+00,
        -1.0625e+00, -1.6016e+00, -4.7266e-01,  2.0905e-03, -7.8906e-01,
        -1.3489e-02, -1.7500e+00,  5.2734e-01, -2.9663e-02,  1.6113e-01,
        -1.9453e+00,  5.6641e-01, -8.1641e-01, -1.4355e-01, -1.7891e+00,
        -5.3906e-01, -1.4688e+00,  7.3828e-01,  5.8594e-01, -4.4141e-01,
         9.0625e-01, -6.7188e-01, -3.2031e-01,  2.6406e+00,  1.8555e-01,
        -8.2812e-01, -7.9688e-01, -1.0469e+00, -1.5078e+00,  8.8672e-01,
         1.9688e+00, -1.1797e+00,  5.6250e-01,  1.6797e+00, -3.0029e-02,
         3.3984e-01, -9.5312e-01,  1.6641e+00, -1.8594e+00,  2.8809e-02,
        -6.0000e+00, -1.4844e+00,  5.9375e-01,  2.6562e-01,  2.2812e+00,
         1.1719e+00, -1.2656e+00, -1.7383e-01,  2.3438e+00, -6.6562e+00,
        -1.6875e+00,  1.6172e+00,  2.9375e+00,  1.5234e+00,  2.4023e-01,
        -2.4902e-01,  8.6328e-01, -4.5312e+00, -1.2344e+00,  5.3906e-01,
        -6.4062e-01,  3.4375e+00,  2.8125e+00, -4.5625e+00,  1.2656e+00,
        -3.0469e+00, -9.2188e-01, -2.3926e-01,  2.8281e+00,  1.4941e-01,
        -8.3984e-01, -4.4727e-01,  4.0039e-01,  7.8125e-01,  9.7656e-02,
         1.1035e-01,  5.3125e-01, -8.2031e-01,  2.8438e+00, -4.6875e-02,
         1.9775e-02, -4.5312e-01,  4.6680e-01, -2.3926e-01,  8.4766e-01,
        -3.5938e-01, -3.3203e-02,  4.1250e+00, -4.0283e-02, -2.2949e-01,
         5.4688e-01,  6.9531e-01,  3.2812e-01, -2.6172e-01, -1.2402e-01,
        -5.6152e-02, -1.6719e+00,  2.3047e-01, -2.0215e-01,  2.4805e-01,
        -1.8848e-01, -5.7373e-02, -5.8594e-01, -3.9551e-02,  6.4844e-01,
        -1.3594e+00, -3.0469e-01,  1.0625e+00,  2.2188e+00,  5.1562e-01,
         1.0312e+00, -6.7188e-01, -2.1250e+00, -2.5781e+00, -3.7695e-01,
         1.9062e+00, -7.2500e+00,  3.3203e-01, -8.3203e-01,  2.4375e+00,
         1.0625e+00, -2.0605e-01, -4.7500e+00, -3.3594e+00,  9.4531e-01,
        -2.1729e-02,  1.3594e+00, -3.8906e+00,  4.1250e+00, -6.4844e-01,
        -1.4062e+00, -2.0898e-01, -1.2969e+00, -5.7031e-01,  2.4688e+00,
        -5.4688e-01,  4.3125e+00, -1.0071e-02, -4.8438e-01,  4.6562e+00,
        -1.5918e-01, -3.0640e-02, -7.7734e-01, -3.4668e-02,  9.1797e-01,
        -4.8125e+00,  2.0117e-01,  2.4219e-01,  3.7188e+00,  5.2344e-01,
         1.1328e+00,  1.3672e-01,  1.3184e-01, -2.6758e-01,  3.2422e-01,
        -8.1055e-02, -2.0605e-01, -2.3340e-01, -2.9688e-01,  1.1523e-01,
        -3.9844e+00,  2.4414e-01,  4.9219e-01, -3.9062e-02, -4.3164e-01,
        -4.1406e-01,  1.7700e-02, -2.2949e-01, -1.4258e-01,  7.4609e-01,
         1.5234e-01,  2.9883e-01, -9.1553e-03,  1.5391e+00, -2.1562e+00,
         1.6562e+00, -1.0234e+00,  1.9375e+00,  4.3164e-01, -1.9531e-01,
         3.8250e+01,  2.4219e+00,  2.9844e+00,  3.2031e+00,  7.2266e-01,
        -2.0000e+00, -1.2000e+01, -5.3750e+00, -1.5234e+00, -7.9062e+00,
        -3.7344e+00,  6.5625e-01])

llm.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0289,  0.0500,  0.0564,  ...,  0.0875, -0.0030,  0.0100],
        [ 0.0079, -0.0491, -0.0086,  ..., -0.0254, -0.0188,  0.0081],
        [ 0.0108,  0.0413, -0.0362,  ..., -0.0099,  0.0169, -0.0053],
        ...,
        [ 0.0302,  0.0626, -0.0036,  ...,  0.0494, -0.0025, -0.0323],
        [ 0.0184,  0.0267,  0.0102,  ..., -0.0274,  0.0012,  0.0148],
        [ 0.0216,  0.0448,  0.0293,  ...,  0.0024,  0.0154,  0.0051]])

llm.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0410,  0.0721,  0.0497,  ..., -0.0286, -0.0618, -0.0169],
        [ 0.0382, -0.0339, -0.0262,  ...,  0.0289, -0.0845,  0.0186],
        [ 0.0150,  0.0022, -0.0389,  ..., -0.0020, -0.0015, -0.0097],
        ...,
        [ 0.0187,  0.0062, -0.0477,  ...,  0.0054, -0.0178, -0.0049],
        [ 0.0065, -0.0136,  0.0160,  ...,  0.0325, -0.0121, -0.0027],
        [-0.0121,  0.0330,  0.0020,  ...,  0.0192, -0.0304, -0.0121]])

llm.base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0038, -0.0166,  0.0172,  ...,  0.0018,  0.0125,  0.0278],
        [ 0.0088, -0.0008,  0.0023,  ...,  0.0083,  0.0001, -0.0007],
        [ 0.0071,  0.0115,  0.0067,  ...,  0.0027,  0.0006,  0.0179],
        ...,
        [ 0.0043,  0.0029, -0.0022,  ...,  0.0020, -0.0004,  0.0094],
        [ 0.0249, -0.0078,  0.0060,  ..., -0.0104, -0.0017, -0.0045],
        [ 0.0049,  0.0040, -0.0066,  ..., -0.0137,  0.0104, -0.0044]])

llm.base_model.model.model.layers.3.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-4.0588e-03, -7.8613e-02, -9.6680e-02, -2.5195e-01,  1.8164e-01,
         9.9121e-02,  5.0293e-02, -2.2266e-01,  5.5420e-02, -1.9043e-01,
        -2.6953e-01, -1.5039e-01, -5.1758e-02, -9.3262e-02, -4.1504e-02,
        -6.7383e-02, -3.3008e-01, -2.2852e-01, -1.0059e-01,  6.9336e-02,
        -1.2793e-01,  3.0273e-01, -9.5215e-03,  7.8964e-04, -1.1426e-01,
         1.2207e-01, -4.7852e-02,  3.5742e-01,  8.9844e-02,  7.9102e-02,
        -1.8262e-01, -1.0107e-01,  6.6016e-01,  6.2500e-01, -2.3340e-01,
        -1.6602e-01,  1.9531e-01,  5.7373e-02, -2.6758e-01,  2.3242e-01,
        -1.0986e-01,  5.4199e-02, -6.7383e-02,  5.4443e-02, -4.3701e-02,
        -4.6387e-02, -2.2559e-01, -5.1514e-02, -1.9629e-01, -7.9102e-02,
         2.1191e-01,  1.1133e-01, -2.7539e-01,  1.5820e-01, -5.3516e-01,
         6.4062e-01,  2.0996e-01,  1.6479e-02, -2.5781e-01,  1.0391e+00,
        -5.3125e-01,  2.0312e-01,  2.8125e-01,  9.0820e-02, -7.6660e-02,
         1.1230e-01, -1.6602e-02, -8.6426e-02, -1.3574e-01,  3.6523e-01,
        -1.7480e-01,  3.2227e-02, -5.5078e-01,  4.8584e-02, -4.4434e-02,
         2.6855e-03, -1.2451e-02,  1.4258e-01, -1.0742e-01,  3.8477e-01,
         2.5977e-01,  3.5547e-01,  1.6992e-01, -1.9531e-01, -4.2969e-02,
        -7.2266e-02,  2.7588e-02, -8.8867e-02,  3.2959e-02, -1.1426e-01,
        -1.9629e-01, -1.6406e-01, -3.9307e-02,  7.7148e-02,  4.3945e-01,
        -2.4902e-01, -3.4180e-01, -1.7014e-03, -3.0762e-02, -1.3672e-01,
         1.0498e-01,  4.6875e-02,  7.7637e-02, -2.7344e-01,  6.8359e-02,
         1.7773e-01,  2.8125e-01, -2.0117e-01,  1.6797e-01, -1.2891e-01,
         6.8359e-02, -1.7480e-01,  1.2146e-02, -1.0889e-01, -2.3730e-01,
        -4.6289e-01, -3.5547e-01, -9.9609e-02, -5.1953e-01,  2.9492e-01,
        -3.8086e-02, -3.3984e-01, -1.6113e-01,  1.5234e-01, -5.3467e-02,
         1.2756e-02, -8.7891e-02, -5.3906e-01, -6.7383e-02,  4.5654e-02,
        -1.2891e-01,  7.6172e-02, -5.9326e-02,  3.3594e-01, -1.2109e-01,
        -5.3467e-02,  6.2256e-02, -3.4766e-01, -1.7871e-01,  1.8359e-01,
        -1.4160e-01, -2.7539e-01,  4.0527e-02,  8.9111e-03,  1.7578e-01,
         3.5156e-02,  2.3804e-02, -1.3281e-01, -1.7969e-01,  3.0884e-02,
        -5.7617e-02,  2.6001e-02, -4.4189e-02,  9.6680e-02, -6.7871e-02,
         4.4434e-02, -6.3477e-02, -2.2363e-01,  1.5198e-02,  5.8350e-02,
         6.7871e-02,  1.9238e-01,  1.7212e-02,  4.9805e-02, -1.7090e-02,
        -1.5625e-01,  5.8105e-02,  5.9204e-03,  5.1270e-02, -3.0273e-01,
         1.1426e-01, -1.9141e-01, -3.1982e-02,  2.4658e-02,  1.2402e-01,
        -8.3984e-02, -1.1670e-01, -1.2354e-01, -1.5332e-01,  1.1914e-01,
        -9.2163e-03,  4.2969e-02,  7.5684e-02, -1.7188e-01, -4.1748e-02,
         1.6724e-02, -6.8359e-02,  1.5820e-01,  5.8289e-03, -8.2031e-02,
         4.1748e-02,  2.0117e-01,  1.2500e-01,  9.1797e-02, -1.9043e-02,
        -9.8633e-02,  7.4707e-02, -2.3804e-02, -1.4746e-01,  3.2617e-01,
         7.7637e-02,  4.1748e-02, -3.4180e-02,  4.9414e-01,  1.2305e-01,
        -2.2583e-02,  3.7354e-02, -1.9165e-02, -8.8379e-02,  4.8096e-02,
        -1.0645e-01, -2.2705e-02,  5.2246e-02,  4.0283e-02, -5.0293e-02,
         2.6733e-02, -9.6680e-02, -3.6865e-02,  2.7588e-02, -1.4746e-01,
         3.6377e-02,  6.8848e-02,  2.9492e-01, -1.9409e-02,  2.4170e-02,
         3.3936e-02, -3.7842e-02,  1.7578e-01, -7.5684e-02,  4.3945e-02,
         4.0894e-03, -2.2827e-02, -2.5781e-01,  2.2266e-01, -2.5781e-01,
         6.2500e-02, -1.0449e-01,  6.7188e-01, -1.1230e-01, -3.6163e-03,
        -1.4648e-03, -3.2806e-03,  1.4551e-01, -5.4932e-02, -2.1851e-02,
         8.3496e-02, -3.0273e-01,  4.1504e-02,  1.4062e-01,  1.9922e-01,
        -5.9570e-02, -4.2725e-02, -8.5938e-02, -7.3242e-03, -1.2878e-02,
         7.7820e-03, -4.3457e-02, -1.3965e-01,  2.4805e-01, -5.6250e-01,
        -4.6484e-01,  1.1475e-01,  1.2891e-01,  2.0905e-03,  1.9336e-01,
         1.9434e-01,  8.8379e-02,  7.8125e-02,  1.3281e-01,  2.8516e-01,
         2.8125e-01,  1.9238e-01,  2.1094e-01,  3.3594e-01, -1.3281e-01,
        -9.5703e-02, -1.7285e-01,  2.5977e-01, -1.5234e-01,  9.1309e-02,
         1.7700e-02,  2.7734e-01, -1.1865e-01, -1.1230e-01, -9.9487e-03,
        -5.6152e-03,  1.6113e-02,  1.0498e-01,  1.2012e-01,  2.3633e-01,
        -2.3560e-02, -1.0681e-02,  2.6367e-01,  5.9326e-02, -8.6426e-02,
        -1.4355e-01,  1.2695e-02,  6.0547e-02, -1.7676e-01,  2.8809e-02,
         9.5215e-03,  7.1289e-02,  1.0498e-01, -5.1758e-02,  1.6406e-01,
         2.5977e-01,  1.7578e-01,  1.5918e-01, -9.5703e-02, -2.0996e-01,
         1.0193e-02,  9.8145e-02,  1.3794e-02,  4.0771e-02, -4.7607e-02,
        -1.4355e-01, -9.6680e-02, -5.1562e-01, -9.0820e-02, -3.8910e-03,
         1.1572e-01, -9.4727e-02,  1.8457e-01,  1.1523e-01,  9.7168e-02,
        -2.7222e-02, -2.1680e-01, -6.8359e-02, -1.0107e-01, -1.6895e-01,
        -8.2520e-02, -9.5703e-02, -2.5586e-01, -9.9609e-02, -6.4392e-03,
         1.5625e-01,  4.6631e-02,  4.4922e-01, -6.8848e-02,  1.4648e-01,
         3.2959e-02,  2.0508e-02, -1.4453e-01, -5.7129e-02,  1.5332e-01,
        -1.4844e-01, -1.3574e-01, -5.4297e-01, -4.1992e-01, -1.1523e-01,
         3.5938e-01, -9.0820e-02,  2.0142e-03,  3.0029e-02, -1.9531e-01,
        -9.4238e-02,  2.2461e-01,  2.3828e-01,  8.6060e-03,  2.8992e-03,
        -1.9531e-01,  5.7373e-02,  2.6367e-01, -2.0508e-01, -6.4941e-02,
        -2.8906e-01, -2.5781e-01,  1.3477e-01, -3.9551e-02, -5.4932e-03,
        -2.7710e-02, -1.7383e-01,  6.4453e-02, -4.2236e-02, -6.3477e-02,
         7.4707e-02, -9.2773e-02, -4.1016e-02, -1.8066e-01,  1.8750e-01,
        -1.4258e-01, -6.7383e-02, -1.7090e-02,  1.0059e-01,  2.8076e-02,
         4.2419e-03,  9.2285e-02, -5.9570e-02,  4.9316e-02, -8.4473e-02,
         1.5430e-01,  2.2461e-02, -5.8350e-02, -7.9590e-02, -4.5471e-03,
        -2.4658e-02, -5.5908e-02, -4.9072e-02, -8.3984e-02, -4.5654e-02,
         5.7617e-02, -2.2583e-02,  1.5076e-02,  2.3560e-02, -2.1118e-02,
         2.9785e-02,  6.8848e-02, -2.4170e-02,  3.2471e-02, -1.2695e-01,
         1.4343e-02,  1.0059e-01, -1.1035e-01,  4.5166e-02,  1.7578e-02,
         3.0670e-03,  2.8992e-03, -3.6621e-02, -3.8574e-02,  5.7617e-02,
        -3.5400e-02,  5.5908e-02,  6.6895e-02,  3.2196e-03,  7.7148e-02,
         3.5889e-02,  3.1006e-02, -1.6846e-02,  7.4707e-02, -6.3965e-02,
         2.8809e-02,  1.3379e-01, -3.9307e-02,  7.9956e-03,  5.1270e-03,
        -4.4922e-02, -3.6621e-02,  2.2217e-02,  3.0151e-02, -2.0630e-02,
         4.0039e-02,  2.2583e-02,  4.7363e-02, -1.1475e-01, -8.4961e-02,
        -3.4180e-02, -4.3701e-02,  1.1963e-02, -2.6245e-02,  6.0059e-02,
         1.8997e-03,  1.8677e-02,  8.2031e-02,  5.1758e-02,  3.5156e-02,
         7.4219e-02, -8.4473e-02, -1.4404e-02, -4.4922e-02,  1.5991e-02,
        -2.5391e-02, -1.0315e-02, -4.0771e-02, -8.5449e-02, -2.6611e-02,
        -5.8594e-02, -4.1504e-02,  8.7402e-02,  3.0518e-02,  8.3008e-02,
        -2.2461e-02, -2.3438e-02,  1.1719e-02,  1.6846e-02,  2.6245e-02,
         1.5335e-03,  3.2959e-02,  3.9551e-02, -8.6426e-02,  1.1169e-02,
        -6.4392e-03, -6.9336e-02, -1.3733e-02,  2.1851e-02, -2.9449e-03,
         5.3955e-02, -6.0303e-02, -8.3008e-02,  3.0029e-02, -6.6895e-02,
        -1.6327e-03,  2.5391e-02, -8.0078e-02,  1.1768e-01,  6.6833e-03,
        -2.1973e-03, -7.0312e-02,  1.0205e-01, -1.7773e-01, -2.6855e-02,
        -1.4901e-05, -9.3994e-03,  6.2561e-03,  1.2012e-01, -3.2227e-02,
        -2.7222e-02,  7.0190e-04,  6.7871e-02,  2.2095e-02,  1.0840e-01,
        -3.7842e-02,  1.8311e-02])

llm.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0251, -0.0254, -0.0023,  ..., -0.0301,  0.0488, -0.0033],
        [-0.0609,  0.0136,  0.0029,  ..., -0.0128,  0.0319, -0.0193],
        [-0.0336,  0.0027,  0.0304,  ..., -0.0057,  0.0018, -0.0028],
        ...,
        [-0.0391,  0.0134,  0.0080,  ...,  0.0378, -0.0157, -0.0336],
        [-0.0222, -0.0048, -0.0237,  ...,  0.0413, -0.0170, -0.0013],
        [ 0.0334, -0.0223,  0.0021,  ...,  0.0013, -0.0006,  0.0321]])

llm.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0170,  0.0228, -0.0046,  ...,  0.0139, -0.0073, -0.0349],
        [-0.0005, -0.0032,  0.0113,  ..., -0.0029,  0.0044,  0.0063],
        [-0.0261,  0.0134, -0.0060,  ...,  0.0025, -0.0191, -0.0013],
        ...,
        [-0.0100, -0.0043, -0.0218,  ..., -0.0163,  0.0453,  0.0001],
        [ 0.0102, -0.0011, -0.0467,  ...,  0.0017,  0.0246, -0.0056],
        [-0.0379, -0.0127,  0.0083,  ..., -0.0132, -0.0265,  0.0197]])

llm.base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0024, -0.0095, -0.0065,  ..., -0.0129, -0.0189,  0.0034],
        [ 0.0117, -0.0225,  0.0035,  ...,  0.0178,  0.0081,  0.0031],
        [ 0.0064, -0.0079,  0.0037,  ...,  0.0048,  0.0025, -0.0009],
        ...,
        [ 0.0118, -0.0019,  0.0007,  ..., -0.0054,  0.0184,  0.0129],
        [ 0.0199, -0.0046, -0.0094,  ..., -0.0094, -0.0112,  0.0067],
        [ 0.0197, -0.0108,  0.0106,  ..., -0.0184, -0.0216,  0.0006]])

llm.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0258, -0.0072, -0.0312,  ..., -0.0300,  0.0357,  0.0281],
        [ 0.0310, -0.0278,  0.0158,  ...,  0.0002,  0.0592, -0.0165],
        [-0.0016, -0.0396,  0.0087,  ...,  0.0546, -0.0041,  0.0071],
        ...,
        [ 0.0465,  0.0291,  0.0336,  ..., -0.0282,  0.0321, -0.0007],
        [ 0.0525,  0.0093,  0.0111,  ...,  0.0126,  0.0378, -0.0127],
        [ 0.0044,  0.0099,  0.0221,  ...,  0.0126,  0.0079,  0.0219]])

llm.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0081, -0.0213,  0.0045,  ..., -0.0048,  0.0293, -0.0467],
        [-0.0078, -0.0094,  0.0275,  ..., -0.0092, -0.0185,  0.0542],
        [ 0.0060, -0.0148, -0.0158,  ..., -0.0280, -0.0582, -0.0251],
        ...,
        [ 0.0016, -0.0146, -0.0109,  ...,  0.0014,  0.0247,  0.0185],
        [ 0.0310, -0.0247,  0.0089,  ..., -0.0117, -0.0247,  0.0561],
        [-0.0523, -0.0488, -0.0108,  ..., -0.0215, -0.0308, -0.0447]])

llm.base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-1.9775e-02, -1.2756e-02,  9.3937e-05,  ...,  2.3560e-02,
         -1.3062e-02, -1.0376e-03],
        [-3.9368e-03, -7.3624e-04, -7.3853e-03,  ...,  1.9653e-02,
          3.2043e-03, -7.9956e-03],
        [ 7.5912e-04, -9.6436e-03,  3.7003e-04,  ...,  2.3804e-02,
         -5.9509e-03,  7.9346e-03],
        ...,
        [ 1.8066e-02,  1.3916e-02,  9.9487e-03,  ..., -7.0496e-03,
          2.4567e-03,  1.2779e-04],
        [-4.1504e-03, -1.3809e-03, -6.7139e-03,  ...,  2.2095e-02,
          7.7515e-03,  1.6968e-02],
        [-1.0925e-02, -1.0010e-02, -1.8799e-02,  ...,  9.9487e-03,
         -1.7334e-02,  1.7090e-02]])

llm.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0351,  0.0079,  0.0380,  ...,  0.0750,  0.0808,  0.0648],
        [-0.0242, -0.0046,  0.0229,  ..., -0.0012, -0.0176, -0.0550],
        [ 0.0038,  0.0677, -0.0089,  ..., -0.0227, -0.0340,  0.0009],
        ...,
        [ 0.0257, -0.0009, -0.0551,  ..., -0.0098, -0.0557, -0.0556],
        [ 0.0152,  0.0005,  0.0411,  ..., -0.0323,  0.0853, -0.0191],
        [-0.0098, -0.0145,  0.0436,  ..., -0.0109,  0.0310,  0.0485]])

llm.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0223, -0.0087, -0.0026,  ..., -0.0122, -0.0380,  0.0004],
        [-0.0121, -0.0072,  0.0166,  ..., -0.0056, -0.0135, -0.0030],
        [ 0.0084, -0.0265,  0.0342,  ...,  0.0078, -0.0310, -0.0007],
        ...,
        [ 0.0363, -0.0256,  0.0309,  ...,  0.0082, -0.0501, -0.0199],
        [-0.0317,  0.0009, -0.0072,  ...,  0.0120,  0.0218,  0.0021],
        [-0.0193,  0.0303, -0.0355,  ...,  0.0012,  0.0118,  0.0029]])

llm.base_model.model.model.layers.3.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0225,  0.0187, -0.0119,  ...,  0.0243, -0.0009, -0.0106],
        [ 0.0011,  0.0053, -0.0095,  ..., -0.0157,  0.0177,  0.0162],
        [ 0.0084, -0.0156,  0.0118,  ..., -0.0097, -0.0056, -0.0189],
        ...,
        [ 0.0095,  0.0063, -0.0142,  ...,  0.0192,  0.0026,  0.0049],
        [ 0.0208, -0.0204, -0.0054,  ..., -0.0199, -0.0243,  0.0157],
        [ 0.0079, -0.0400,  0.0099,  ..., -0.0019,  0.0057, -0.0077]])

llm.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0112,  0.0120, -0.0601,  ...,  0.0221, -0.0319,  0.0240],
        [-0.0326, -0.0446, -0.0227,  ..., -0.0321, -0.0093, -0.0236],
        [ 0.0103, -0.0417, -0.0827,  ...,  0.0119,  0.0025,  0.0136],
        ...,
        [-0.0125, -0.0246, -0.0570,  ...,  0.0899, -0.0444,  0.0075],
        [ 0.0083, -0.0075,  0.0151,  ..., -0.0152, -0.0057,  0.0125],
        [ 0.0242, -0.0360, -0.0293,  ...,  0.0212,  0.0326, -0.0314]])

llm.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0099, -0.0279, -0.0303,  ..., -0.0002,  0.0030,  0.0209],
        [ 0.0026, -0.0017, -0.0161,  ..., -0.0163, -0.0039,  0.0059],
        [ 0.0331,  0.0053,  0.0168,  ..., -0.0421, -0.0328, -0.0392],
        ...,
        [-0.0031,  0.0205, -0.0122,  ...,  0.0065,  0.0403, -0.0170],
        [ 0.0184,  0.0110,  0.0066,  ...,  0.0479,  0.0394, -0.0194],
        [ 0.0144, -0.0296,  0.0103,  ..., -0.0227, -0.0003, -0.0229]])

llm.base_model.model.model.layers.3.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[-0.0188, -0.0008, -0.0049,  ...,  0.0081,  0.0084,  0.0058],
        [ 0.0201,  0.0192, -0.0070,  ..., -0.0037, -0.0117, -0.0069],
        [-0.0110, -0.0075, -0.0007,  ...,  0.0019, -0.0032, -0.0006],
        ...,
        [ 0.0087, -0.0037, -0.0012,  ...,  0.0077, -0.0166,  0.0015],
        [-0.0117,  0.0010,  0.0189,  ..., -0.0045,  0.0037,  0.0063],
        [-0.0092,  0.0039, -0.0120,  ...,  0.0159, -0.0176, -0.0117]])

llm.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[-0.0197, -0.0253,  0.0273,  ...,  0.0293,  0.0355,  0.0013],
        [ 0.0095,  0.0095, -0.0061,  ..., -0.0218, -0.0085, -0.0036],
        [ 0.0263, -0.0163,  0.0585,  ...,  0.0085,  0.0192,  0.0196],
        ...,
        [ 0.0071, -0.0233, -0.0133,  ...,  0.0118, -0.0277, -0.0165],
        [-0.0640,  0.0213, -0.0365,  ...,  0.0006, -0.0524, -0.0008],
        [ 0.0095, -0.0062, -0.0219,  ..., -0.0104, -0.0053, -0.0336]])

llm.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0359, -0.0638,  0.0265,  ..., -0.0272, -0.0137, -0.0192],
        [ 0.0566,  0.0023, -0.0279,  ..., -0.0312,  0.0076,  0.0126],
        [-0.0016, -0.0201,  0.0335,  ...,  0.0306, -0.0168,  0.0095],
        ...,
        [-0.0256, -0.0029,  0.0165,  ...,  0.0274, -0.0350,  0.0060],
        [-0.0033, -0.0357,  0.0108,  ...,  0.0758, -0.0149,  0.0178],
        [-0.0303,  0.0303, -0.0103,  ...,  0.0344, -0.0188,  0.0219]])

llm.base_model.model.model.layers.3.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.3711, 0.4805, 0.4258,  ..., 0.3027, 0.2930, 0.6094])

llm.base_model.model.model.layers.3.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.2266, 1.3438, 1.3359,  ..., 1.1016, 1.2969, 1.4219])

llm.base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0051, -0.0430,  0.0058,  ..., -0.0017, -0.0236,  0.0002],
        [ 0.0172, -0.0160,  0.0020,  ..., -0.0244,  0.0019,  0.0083],
        [ 0.0135,  0.0065, -0.0060,  ...,  0.0118,  0.0010,  0.0003],
        ...,
        [-0.0247, -0.0074,  0.0104,  ..., -0.0098, -0.0060,  0.0171],
        [-0.0047,  0.0002, -0.0070,  ..., -0.0471,  0.0037,  0.0171],
        [-0.0182,  0.0107,  0.0165,  ...,  0.0171,  0.0143, -0.0078]])

llm.base_model.model.model.layers.4.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.4160,  0.1523, -0.3477,  ...,  0.2676, -0.1230, -0.2812])

llm.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0084,  0.0129, -0.0040,  ...,  0.0320, -0.0359, -0.0091],
        [-0.0004, -0.0227, -0.0254,  ..., -0.0025,  0.0166, -0.0418],
        [ 0.0332,  0.0362,  0.0061,  ...,  0.0667, -0.0269,  0.0155],
        ...,
        [ 0.0061, -0.0305,  0.0516,  ..., -0.0274,  0.0576,  0.0146],
        [ 0.0060, -0.0046,  0.0292,  ...,  0.0192,  0.0090, -0.0161],
        [-0.0067,  0.0143,  0.0342,  ...,  0.0152, -0.0474,  0.0398]])

llm.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0174, -0.0245, -0.0305,  ...,  0.0595,  0.0190, -0.0204],
        [-0.0237,  0.0276, -0.0622,  ...,  0.0878, -0.0157,  0.0238],
        [-0.0141,  0.0567, -0.0362,  ...,  0.0017, -0.0463,  0.0788],
        ...,
        [-0.0274, -0.0063, -0.0237,  ...,  0.0664, -0.0212, -0.0091],
        [ 0.0041,  0.0048,  0.0192,  ...,  0.0319,  0.0255, -0.0143],
        [ 0.0004,  0.0086,  0.0093,  ..., -0.0191,  0.0083, -0.0198]])

llm.base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0051, -0.0095,  0.0055,  ...,  0.0245, -0.0105,  0.0093],
        [-0.0299, -0.0121,  0.0077,  ...,  0.0255, -0.0157,  0.0166],
        [ 0.0126,  0.0176, -0.0315,  ..., -0.0078,  0.0128,  0.0079],
        ...,
        [ 0.0179, -0.0082, -0.0096,  ...,  0.0369, -0.0393, -0.0225],
        [ 0.0078,  0.0237, -0.0289,  ..., -0.0569,  0.0205, -0.0063],
        [-0.0150,  0.0255,  0.0061,  ..., -0.0410, -0.0178,  0.0309]])

llm.base_model.model.model.layers.4.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-7.1875e-01, -1.9165e-02,  2.6562e-01, -5.8594e-02, -4.2578e-01,
         6.4844e-01, -2.7734e-01, -8.9355e-02, -7.6172e-01, -1.7676e-01,
        -5.2246e-02,  8.2422e-01, -4.4336e-01,  4.2383e-01, -4.1406e-01,
        -2.0599e-03,  2.7466e-03,  3.6523e-01,  1.2793e-01, -1.0645e-01,
        -8.4766e-01, -1.4551e-01, -4.3701e-02,  1.5918e-01,  3.2812e-01,
         1.0193e-02,  6.7383e-02, -7.6172e-02, -2.7344e-02,  1.2598e-01,
        -4.3359e-01,  1.5198e-02,  5.9082e-02, -1.8848e-01,  1.5430e-01,
        -2.6978e-02,  8.3984e-02, -2.5781e-01,  3.6133e-02, -3.3984e-01,
        -1.8457e-01,  2.2070e-01, -3.5352e-01,  4.5117e-01, -1.5703e+00,
        -2.0020e-02,  3.0859e-01, -6.6833e-03, -8.7402e-02, -3.8672e-01,
         2.7930e-01,  6.3281e-01,  6.0156e-01,  2.7344e-01, -1.1035e-01,
         2.1973e-01, -1.9062e+00, -3.2031e-01, -6.6016e-01,  9.9219e-01,
         3.7109e-01, -1.2891e-01, -5.8984e-01,  1.0625e+00, -4.0234e-01,
        -3.5742e-01, -4.9414e-01,  7.1484e-01, -3.2959e-02,  2.1094e-01,
        -2.0703e-01,  7.5684e-02, -6.0791e-02,  1.5332e-01,  3.9844e-01,
         8.1177e-03, -6.0791e-02, -1.1719e-01,  1.7871e-01, -2.6562e-01,
         7.1484e-01,  3.5156e-01,  1.3379e-01, -3.0078e-01,  1.7188e-01,
        -7.2754e-02,  6.9336e-02,  7.6953e-01,  1.5137e-01,  5.0049e-02,
        -4.8047e-01, -1.3867e-01, -1.0449e-01, -1.3184e-01, -6.9141e-01,
         1.5234e-01, -4.0527e-02, -7.3730e-02, -9.0234e-01,  2.0703e-01,
         8.2031e-02, -2.2070e-01, -3.3203e-01, -6.1719e-01,  1.9409e-02,
         1.8799e-02,  6.8359e-01,  3.3398e-01,  2.0312e-01,  4.3750e-01,
         1.7969e-01, -1.9043e-01, -3.2812e-01, -2.5586e-01,  2.5781e-01,
        -6.2500e-01,  3.1250e-01, -5.2344e-01, -2.8125e-01,  3.7109e-02,
        -3.0000e+00,  6.2109e-01, -2.1484e-01, -8.5547e-01,  4.0625e-01,
         1.5527e-01, -6.7578e-01, -4.0039e-02, -4.1992e-02, -1.8677e-02,
         2.3804e-02, -1.3281e-01,  6.5430e-02, -1.8457e-01, -2.3047e-01,
        -2.9688e-01, -9.9609e-02, -7.3242e-02,  4.1406e-01,  5.9082e-02,
         7.6172e-02, -1.0303e-01, -8.3496e-02, -4.4678e-02,  5.5176e-02,
         9.8267e-03, -6.4453e-02,  5.4199e-02, -6.7383e-02,  1.4844e-01,
         2.6367e-02,  7.6172e-01, -5.7373e-03, -1.6602e-01,  4.8584e-02,
        -7.0312e-02,  1.8188e-02, -8.2422e-01, -1.0742e-01,  1.7383e-01,
        -6.7383e-02,  7.5391e-01, -3.2227e-02, -1.3672e-02, -7.8613e-02,
        -1.2451e-01,  9.7656e-02, -1.0872e-04, -8.3618e-03, -1.2500e-01,
         4.7913e-03, -1.1536e-02,  1.2988e-01, -2.6367e-01, -1.2329e-02,
         1.0840e-01, -2.1973e-03,  8.3984e-02, -2.2339e-02,  1.7969e-01,
         1.0059e-01, -8.0078e-02,  4.9316e-02,  5.0781e-02, -2.0410e-01,
         1.1865e-01,  2.3594e+00, -1.8066e-01, -9.0234e-01, -5.0049e-02,
        -5.8594e-01, -1.4844e-01,  2.1210e-03,  6.5430e-02, -9.1309e-02,
         5.2979e-02, -1.6113e-01,  1.1768e-01,  7.5684e-02,  8.7891e-02,
        -3.1641e-01, -3.7109e-01, -5.8838e-02,  1.1328e-01,  4.8242e-01,
         2.1362e-03, -5.2734e-01, -2.1289e-01,  5.1172e-01, -5.2490e-02,
        -5.6250e-01,  1.0498e-02, -6.0938e-01,  7.9102e-02,  1.1865e-01,
        -1.9824e-01, -1.8066e-01, -3.4668e-02, -6.2891e-01, -9.8633e-02,
        -3.4912e-02,  1.0791e-01,  1.5991e-02,  3.4424e-02, -1.7944e-02,
         5.0293e-02,  6.1035e-02,  8.6426e-02, -4.1504e-02,  3.8477e-01,
        -7.5989e-03,  1.0889e-01, -2.1240e-02, -8.1641e-01,  9.6436e-03,
         1.1523e-01, -4.2383e-01,  2.4805e-01,  2.6855e-02,  4.0771e-02,
         4.2236e-02, -7.9590e-02, -5.5664e-02,  1.3123e-02, -2.6489e-02,
        -8.0078e-02, -1.0303e-01, -3.4912e-02, -2.2583e-02,  6.3965e-02,
        -1.8750e+00,  4.0771e-02,  1.8516e+00, -1.6016e-01,  3.0469e-01,
         8.1055e-02,  7.2266e-01,  2.3926e-01,  1.5039e-01, -9.8633e-02,
        -1.6406e-01,  8.6426e-02, -2.8516e-01,  1.7676e-01, -2.9688e-01,
         7.8125e-03, -1.4648e-01,  4.1016e-02,  4.5898e-02,  3.9062e-02,
         1.9531e-01,  9.2969e-01, -1.5234e-01, -1.6797e-01, -5.2979e-02,
        -2.0508e-01,  4.4678e-02,  1.8945e-01, -5.2490e-02,  1.3867e-01,
        -1.0703e+00, -2.6562e-01,  1.3770e-01,  6.1798e-04,  1.3086e-01,
        -7.3242e-02, -3.2422e-01, -6.9336e-02,  8.9355e-02,  2.0410e-01,
        -1.1875e+00, -1.5234e-01,  1.1536e-02,  8.8379e-02, -1.8457e-01,
        -1.6895e-01,  6.8359e-02, -1.3281e-01, -1.1914e-01,  2.6953e-01,
        -1.2402e-01, -1.8262e-01, -1.2256e-01,  2.5781e-01, -5.9570e-02,
         1.0840e-01, -2.9297e-02, -2.5879e-02, -1.2012e-01,  7.4219e-02,
        -2.0996e-01, -6.4453e-01,  2.8906e-01, -3.5781e+00, -2.1094e-01,
         1.1914e-01,  2.0215e-01, -1.6250e+00,  2.1094e-01, -3.7344e+00,
        -4.8096e-02, -3.8477e-01, -3.4570e-01,  6.6797e-01, -2.8198e-02,
        -6.0938e-01, -3.0469e-01, -1.4746e-01,  2.7734e-01, -8.3984e-01,
         3.4943e-03,  9.3262e-02,  8.2812e-01, -2.6172e-01, -6.8359e-02,
        -7.2754e-02,  1.1902e-03,  9.8633e-02, -9.7656e-01, -1.2756e-02,
         8.8501e-03,  1.0078e+00, -3.4424e-02, -9.7168e-02,  1.4941e-01,
        -3.0078e-01,  9.2773e-02, -1.1016e+00,  6.3965e-02, -1.3574e-01,
         2.2363e-01, -6.0303e-02,  1.6113e-01, -1.6699e-01,  1.3086e-01,
        -1.4258e-01, -1.7578e-01,  8.8867e-02,  1.2988e-01,  4.7852e-02,
        -1.0254e-01,  2.9688e-01, -1.9141e-01,  2.3315e-02,  7.4707e-02,
        -3.8086e-02,  9.3750e-02, -5.4688e-01,  1.4572e-03, -1.7578e-01,
         2.8516e-01,  3.6328e-01, -1.2305e-01,  7.3242e-02, -1.2061e-01,
         8.7402e-02, -5.0391e-01,  1.9141e+00,  2.2363e-01,  4.9072e-02,
         3.9844e-01, -3.3594e+00,  6.2109e-01, -2.7539e-01, -1.4922e+00,
        -9.4727e-02, -2.4121e-01,  6.7969e-01,  1.9727e-01,  1.0938e+00,
        -7.5195e-02,  1.8750e-01, -1.1572e-01, -1.7090e-01, -4.6387e-02,
         1.2402e-01, -1.1094e+00,  2.5513e-02, -2.8516e-01, -4.6875e-01,
         2.2949e-01, -9.1797e-02, -1.1230e-01,  3.5645e-02, -3.0469e-01,
        -9.6680e-02, -8.3984e-02, -1.5430e-01,  4.6631e-02, -9.3750e-02,
        -2.8320e-02, -1.1484e+00,  1.0107e-01,  2.5195e-01,  2.1582e-01,
        -4.8096e-02,  3.8330e-02,  9.6094e-01,  2.5586e-01, -8.2520e-02,
         1.9238e-01,  8.4229e-03,  7.7148e-02, -2.0142e-02,  4.7607e-02,
        -5.0781e-02, -7.7637e-02,  1.7773e-01,  3.8281e-01, -3.5156e-01,
        -8.5938e-02,  2.1973e-02,  1.2024e-02,  5.3955e-02, -2.1362e-03,
         1.0693e-01,  1.9238e-01, -6.4844e-01,  2.4219e+00,  6.7383e-02,
        -2.7344e-01, -9.5215e-02, -9.8145e-02,  1.9141e-01,  1.5859e+00,
         7.3828e-01,  9.3262e-02,  1.8262e-01,  8.9844e-02, -1.0078e+00,
        -5.6641e-01, -3.9844e-01, -2.6953e-01,  3.1250e-01, -2.2754e-01,
        -1.0000e+00,  1.1816e-01, -2.2095e-02,  1.1016e+00,  5.2246e-02,
        -5.3711e-02, -1.5723e-01,  1.0449e-01,  7.5195e-02, -4.3213e-02,
         1.2734e+00,  9.5703e-02,  1.9531e-01, -7.2754e-02, -1.1094e+00,
         1.7871e-01, -4.4189e-02,  8.5547e-01, -3.9551e-02,  4.6387e-03,
         1.1426e-01,  4.9072e-02,  1.2891e-01, -1.6113e-02,  1.0400e-01,
         1.5137e-01, -8.3008e-02,  6.2988e-02,  2.6367e-02,  3.6914e-01,
         1.9824e-01, -1.4258e-01, -7.2754e-02,  4.0039e-02,  3.3936e-02,
        -5.7373e-02, -1.5918e-01, -1.3477e-01, -2.5000e-01, -6.8848e-02,
         1.3611e-02,  2.0386e-02,  1.6992e-01, -1.8945e-01, -1.4062e-01,
        -1.1523e-01,  1.2695e-01, -1.5312e+00,  4.2236e-02,  1.8311e-03,
        -7.9102e-02, -2.5977e-01, -5.7129e-02,  2.7148e-01,  5.3516e-01,
         2.4023e-01,  1.1572e-01])

llm.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0181,  0.0057,  0.0070,  ...,  0.0154, -0.0286,  0.0039],
        [ 0.0099,  0.0366,  0.0096,  ..., -0.0656, -0.0446,  0.0307],
        [ 0.0314, -0.0364,  0.0194,  ..., -0.0370,  0.0191,  0.0041],
        ...,
        [-0.0149, -0.0151,  0.0629,  ..., -0.0154,  0.0101,  0.0352],
        [-0.0311, -0.0317, -0.0182,  ..., -0.0234, -0.0659, -0.0463],
        [-0.0116, -0.0005,  0.0305,  ...,  0.0611, -0.0191,  0.0063]])

llm.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0680, -0.0172,  0.0635,  ...,  0.0574,  0.0432, -0.0178],
        [-0.0099,  0.0006,  0.0772,  ..., -0.0204,  0.0312, -0.0611],
        [ 0.0067,  0.0073,  0.0245,  ...,  0.0513,  0.0382, -0.0025],
        ...,
        [ 0.0095, -0.0328, -0.0287,  ...,  0.0134,  0.0214,  0.0018],
        [-0.0202, -0.0012, -0.0061,  ..., -0.0053,  0.0084,  0.0180],
        [-0.0226, -0.0007, -0.0016,  ..., -0.0252, -0.0151, -0.0026]])

llm.base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0092,  0.0071,  0.0393,  ..., -0.0192,  0.0288, -0.0155],
        [-0.0014, -0.0186,  0.0337,  ..., -0.0050,  0.0162, -0.0086],
        [-0.0018, -0.0016, -0.0005,  ...,  0.0107,  0.0231,  0.0107],
        ...,
        [ 0.0231, -0.0005,  0.0044,  ...,  0.0043,  0.0124, -0.0125],
        [ 0.0131, -0.0176,  0.0161,  ..., -0.0320, -0.0151, -0.0017],
        [-0.0052, -0.0184, -0.0045,  ...,  0.0199, -0.0039,  0.0006]])

llm.base_model.model.model.layers.4.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 9.0942e-03,  2.3926e-02, -1.1719e-02, -1.7334e-02,  1.0315e-02,
        -3.2471e-02, -1.5076e-02,  1.1475e-02, -2.8931e-02, -1.6937e-03,
         7.2266e-02, -4.7363e-02, -2.2705e-02, -1.3977e-02, -1.9409e-02,
        -4.7607e-02,  4.4678e-02,  3.1738e-02,  5.1514e-02, -8.6975e-04,
        -1.9073e-03, -2.9144e-03, -2.7588e-02, -1.4771e-02,  3.5400e-03,
        -5.8105e-02, -6.9824e-02, -1.0498e-02,  5.3467e-02,  1.9409e-02,
        -1.5198e-02,  1.8066e-02, -6.9885e-03,  2.0630e-02,  3.4180e-03,
         4.9316e-02, -4.1504e-03,  4.3335e-03,  1.2756e-02,  1.9287e-02,
         1.3123e-03, -1.5259e-02,  9.2285e-02, -4.3945e-02, -1.7822e-02,
         8.2397e-03, -1.4832e-02, -1.4404e-02,  3.3691e-02,  2.2827e-02,
         6.3965e-02, -5.6763e-03, -1.3867e-01, -1.2268e-02,  1.3916e-02,
        -2.5879e-02,  1.4465e-02,  5.4359e-05, -1.7456e-02,  2.8687e-02,
        -6.9885e-03, -3.9551e-02,  2.7100e-02,  2.5635e-02,  2.8320e-02,
        -1.8066e-02, -6.0791e-02, -2.0996e-02, -3.7537e-03, -1.1475e-02,
        -2.2095e-02,  6.5002e-03,  1.5259e-02, -4.4922e-02,  2.2339e-02,
        -2.4536e-02,  8.1787e-03, -5.3711e-02,  1.3245e-02, -1.5381e-02,
        -8.3618e-03,  1.8433e-02, -1.9043e-02,  1.6357e-02,  1.0437e-02,
        -4.6143e-02, -4.3701e-02,  9.8633e-02,  2.3438e-02,  1.6479e-03,
         3.4637e-03,  4.7302e-03,  2.2339e-02,  6.7383e-02,  8.8501e-03,
        -6.7871e-02, -6.1279e-02, -2.7710e-02,  6.3477e-02,  4.9210e-04,
        -2.6611e-02,  1.1230e-02,  2.3071e-02, -1.3733e-02,  4.4556e-03,
        -8.7891e-03,  3.3203e-02,  3.9062e-02,  7.9346e-03, -1.0376e-02,
        -6.4941e-02,  1.9073e-03, -2.9907e-02,  4.7302e-03, -1.6113e-02,
         4.3945e-03,  3.1738e-02, -3.1738e-02, -6.6223e-03, -4.4922e-02,
         1.8311e-02,  1.6556e-03,  2.3315e-02, -6.9427e-04,  1.4526e-02,
        -2.1851e-02, -5.9082e-02,  2.2461e-02,  2.9755e-03, -1.0376e-02,
         7.6294e-03, -4.1504e-03, -2.1667e-03,  6.4697e-03, -1.0986e-02,
        -2.0142e-02,  3.1982e-02,  6.1646e-03, -1.7319e-03, -8.8501e-03,
         3.6469e-03,  3.1836e-01,  1.2329e-02,  7.9155e-05, -4.4922e-02,
         4.1016e-02,  2.1484e-02, -1.0803e-02,  5.3955e-02, -1.8921e-02,
        -2.7466e-02, -5.3711e-02,  1.9165e-02,  2.3315e-02,  6.4697e-03,
        -1.3306e-02, -1.0620e-02,  7.3853e-03, -5.2795e-03, -2.3956e-03,
        -1.1230e-02,  1.8921e-03,  3.5400e-02,  8.7891e-03, -1.1597e-02,
         9.3994e-03,  2.2339e-02,  8.8867e-02,  3.6377e-02,  1.3306e-02,
         2.9144e-03, -1.5503e-02, -2.1606e-02, -7.9956e-03, -3.3417e-03,
         1.0010e-02, -4.0771e-02,  1.0742e-02, -7.0496e-03, -1.1230e-02,
         2.5940e-03, -3.7994e-03, -2.6855e-03, -5.1575e-03,  5.0659e-03,
        -4.8523e-03,  1.0986e-02,  1.8799e-02, -1.0925e-02, -1.1797e+00,
        -1.4526e-02,  6.4392e-03, -8.1177e-03,  8.6670e-03, -5.9204e-03,
         1.3275e-03, -6.2500e-02, -9.2163e-03, -8.6426e-02, -5.3711e-03,
        -9.5215e-03, -1.8799e-02,  1.9531e-02, -1.2451e-02,  6.5918e-03,
        -6.6528e-03, -5.0659e-03,  1.6251e-03, -9.2285e-02, -1.8311e-02,
         2.8320e-02,  1.1597e-02,  2.0752e-03,  3.5645e-02, -1.1841e-02,
         1.1169e-02,  2.4414e-02,  2.5879e-02,  3.1128e-02, -2.4261e-03,
        -6.3171e-03,  1.4709e-02,  9.9121e-02, -3.6133e-02, -4.9744e-03,
         4.9023e-01, -9.1553e-03, -2.1484e-02,  8.9844e-02, -1.2085e-02,
         2.1851e-02, -1.0437e-02,  7.1106e-03,  9.1553e-03, -7.6675e-04,
        -1.0864e-02, -1.7212e-02, -1.7212e-02, -3.7842e-02, -4.5471e-03,
         2.0312e-01, -1.0681e-02, -3.2422e-01,  1.9531e-02, -4.7493e-04,
         6.5613e-03,  4.1016e-02, -2.1240e-02, -8.0078e-02,  1.4526e-02,
         5.4443e-02,  1.7700e-03,  1.2024e-02, -2.1729e-02,  1.7166e-03,
        -7.2937e-03,  5.2002e-02, -2.4414e-02, -7.5195e-02,  1.1780e-02,
        -4.6387e-02, -3.8757e-03, -1.1780e-02, -4.5013e-04, -4.8828e-02,
        -1.8311e-02, -6.7749e-03, -3.2715e-02,  7.2327e-03, -2.2339e-02,
         5.8899e-03, -2.6611e-02, -1.8799e-02, -4.0771e-02, -6.5430e-02,
         1.3550e-02, -4.2480e-02,  2.9175e-02,  9.3994e-03,  1.4801e-03,
        -2.5635e-02, -6.0791e-02,  1.3086e-01, -3.6133e-02, -6.2256e-03,
         3.9795e-02,  5.6839e-04,  9.5367e-04,  1.9409e-02, -2.1240e-02,
         2.8076e-02, -1.3428e-02, -3.1128e-02,  2.1515e-03,  2.5269e-02,
        -1.4893e-02, -3.5286e-04,  1.3977e-02, -3.9307e-02, -3.2715e-02,
        -1.5503e-02,  3.7354e-02,  4.5898e-02, -5.0354e-03, -3.5889e-02,
         4.5013e-04, -1.3367e-02,  1.3428e-02,  2.1515e-03,  2.7832e-02,
         2.8320e-02, -1.5137e-02,  5.9509e-03, -1.6357e-02, -3.8574e-02,
        -3.8574e-02,  2.8931e-02,  2.7100e-02, -2.1851e-02,  2.6367e-02,
        -3.6621e-04,  2.5146e-02, -3.8330e-02,  2.5024e-02,  3.0273e-02,
         3.5095e-04, -1.0864e-02,  2.8442e-02,  1.1719e-02, -5.4932e-02,
        -1.7944e-02,  1.9836e-04,  1.7456e-02, -8.4839e-03,  6.7139e-03,
         9.1309e-02,  5.2002e-02,  4.2236e-02, -4.7852e-02, -3.2471e-02,
         2.0264e-02, -7.8735e-03,  4.2725e-02,  2.5391e-02, -3.0518e-02,
         1.6968e-02,  1.1230e-01,  4.5410e-02,  1.0803e-02, -1.1292e-02,
         1.7212e-02, -1.9775e-02,  2.1118e-02,  1.0315e-02, -3.8086e-02,
         4.0283e-03, -5.7129e-02, -4.3030e-03,  3.3936e-02,  1.2451e-02,
         2.8809e-02, -2.3071e-02, -5.2246e-02,  2.0508e-02, -5.5237e-03,
         7.0312e-02,  1.5259e-02,  1.6602e-02,  3.4790e-03, -6.4392e-03,
        -7.0190e-04,  3.2227e-02, -1.8234e-03,  1.6724e-02,  4.1260e-02,
        -1.8921e-02, -1.0986e-02,  1.8066e-02, -2.1973e-02,  1.0010e-02,
        -2.1973e-02,  2.0874e-02,  8.3618e-03,  6.9824e-02, -2.5757e-02,
         3.5645e-02, -4.3701e-02,  2.1973e-02,  5.0781e-02, -1.6113e-02,
         6.6528e-03, -3.9062e-02,  8.5938e-02, -3.8452e-03,  1.2756e-02,
        -2.5879e-02, -1.7334e-02,  4.9133e-03,  1.3123e-03, -1.1597e-02,
        -4.6631e-02, -1.6113e-02, -5.4688e-02, -4.0283e-03,  1.6235e-02,
         5.4932e-02,  2.5940e-03, -2.7832e-02, -9.5215e-02, -5.6641e-02,
         2.8564e-02, -1.2512e-02, -7.3242e-03, -4.4434e-02, -3.3691e-02,
        -2.3438e-02,  2.8320e-02, -9.8877e-03,  6.4941e-02, -2.1240e-02,
        -1.2207e-01, -2.1729e-02, -3.7689e-03, -2.1362e-04, -4.7119e-02,
         1.0376e-02,  3.4180e-02,  8.8501e-03, -1.4465e-02,  3.1006e-02,
         1.5945e-03,  1.2024e-02, -2.5513e-02,  5.2246e-02,  5.1880e-03,
         2.0874e-02, -7.2327e-03,  2.3315e-02,  1.3794e-02, -2.3804e-02,
         3.5400e-02,  2.1606e-02, -2.0752e-03,  1.1921e-04, -1.4941e-01,
         6.1523e-02,  5.5664e-02,  1.8066e-02, -3.7598e-02,  1.8311e-02,
        -2.7618e-03, -1.2085e-02,  2.1648e-04, -7.5684e-02, -1.2512e-02,
        -1.2939e-02, -1.0803e-02, -1.8433e-02,  2.2461e-02,  8.5449e-03,
        -3.9795e-02,  6.1523e-02,  1.1963e-01,  8.3984e-02, -4.1992e-02,
        -9.8145e-02, -1.0986e-03,  3.3203e-02,  5.1880e-03, -2.2095e-02,
        -2.6367e-02, -1.0315e-02,  8.2397e-03, -2.1484e-02, -1.4587e-02,
         4.3701e-02,  8.8379e-02, -2.9785e-02,  3.6621e-02,  3.3936e-02,
        -2.5635e-02,  4.8523e-03, -1.0742e-02,  3.8574e-02, -8.9111e-03,
         7.3242e-03,  3.6865e-02,  3.3936e-02,  4.7363e-02,  1.2665e-03,
        -4.6387e-03,  9.8877e-03, -1.4648e-02,  1.6235e-02, -2.8687e-02,
        -2.6245e-02,  4.7607e-02,  3.2715e-02, -6.3477e-02,  1.6968e-02,
        -6.5308e-03,  3.7354e-02, -3.5400e-02,  7.1106e-03,  3.1128e-02,
        -5.1758e-02, -2.3682e-02, -1.2598e-01, -1.8921e-02, -1.3855e-02,
         8.9722e-03,  5.5176e-02])

llm.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0215, -0.0141, -0.0111,  ...,  0.0126,  0.0609,  0.0011],
        [-0.0055, -0.0089,  0.0363,  ...,  0.0371, -0.0148, -0.0119],
        [ 0.0555,  0.0020, -0.0638,  ...,  0.0090,  0.0031,  0.0029],
        ...,
        [-0.0141,  0.0029,  0.0489,  ..., -0.0128, -0.0141, -0.0777],
        [-0.0384,  0.0097, -0.0215,  ...,  0.0131,  0.0560, -0.0583],
        [-0.0201, -0.0474,  0.0495,  ...,  0.0225,  0.0048, -0.0094]])

llm.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0196,  0.0031, -0.0200,  ..., -0.0188,  0.0119, -0.0027],
        [ 0.0209, -0.0165,  0.0166,  ..., -0.0075, -0.0100, -0.0325],
        [-0.0004, -0.0127, -0.0092,  ..., -0.0153, -0.0018, -0.0011],
        ...,
        [-0.0021,  0.0355, -0.0001,  ...,  0.0035, -0.0053, -0.0134],
        [ 0.0251,  0.0015, -0.0041,  ...,  0.0105,  0.0039,  0.0027],
        [-0.0389, -0.0223,  0.0205,  ..., -0.0221, -0.0476,  0.0111]])

llm.base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0100,  0.0261,  0.0060,  ..., -0.0065,  0.0056,  0.0135],
        [ 0.0073,  0.0249, -0.0195,  ..., -0.0142,  0.0115, -0.0118],
        [-0.0129,  0.0016,  0.0054,  ..., -0.0133,  0.0028,  0.0239],
        ...,
        [-0.0093,  0.0059,  0.0237,  ..., -0.0205,  0.0071,  0.0195],
        [-0.0175, -0.0023,  0.0070,  ..., -0.0006, -0.0132, -0.0001],
        [ 0.0033,  0.0010, -0.0017,  ...,  0.0058,  0.0030,  0.0049]])

llm.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0251,  0.0213, -0.0231,  ...,  0.0031, -0.0386,  0.0222],
        [-0.0170, -0.0036,  0.0095,  ..., -0.0097, -0.0514, -0.0038],
        [ 0.0483,  0.0023,  0.0575,  ..., -0.0173,  0.0069,  0.0305],
        ...,
        [-0.0412, -0.0083, -0.0150,  ...,  0.0056, -0.0032, -0.0453],
        [-0.0253, -0.0046, -0.0104,  ..., -0.0008, -0.0271, -0.0347],
        [-0.0286,  0.0158,  0.0449,  ...,  0.0169,  0.0153,  0.0077]])

llm.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0067, -0.0023,  0.0163,  ..., -0.0531,  0.0087, -0.0079],
        [-0.0563,  0.0274,  0.0059,  ..., -0.0305,  0.0155, -0.0286],
        [ 0.0132,  0.0162,  0.0217,  ...,  0.0182, -0.0025, -0.0035],
        ...,
        [-0.0002,  0.0064, -0.0467,  ..., -0.0004, -0.0172, -0.0229],
        [-0.0168,  0.0204, -0.0094,  ...,  0.0082, -0.0086, -0.0205],
        [ 0.0050, -0.0267, -0.0037,  ..., -0.0001, -0.0267, -0.0237]])

llm.base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0053,  0.0388, -0.0508,  ...,  0.0099,  0.0075,  0.0203],
        [-0.0068,  0.0232, -0.0098,  ...,  0.0108, -0.0104,  0.0089],
        [ 0.0383, -0.0374, -0.0002,  ...,  0.0061, -0.0269, -0.0352],
        ...,
        [-0.0053, -0.0160, -0.0054,  ..., -0.0141, -0.0047, -0.0198],
        [-0.0288, -0.0188,  0.0199,  ..., -0.0327, -0.0050, -0.0170],
        [ 0.0064, -0.0085, -0.0260,  ..., -0.0256, -0.0049, -0.0097]])

llm.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0424,  0.0043,  0.0237,  ...,  0.0634,  0.0497, -0.0252],
        [ 0.0177, -0.0201, -0.0159,  ...,  0.0027, -0.0573, -0.0364],
        [ 0.0507, -0.0169, -0.0610,  ..., -0.0049,  0.0154,  0.0124],
        ...,
        [ 0.0003,  0.0330, -0.0030,  ...,  0.0121, -0.0191,  0.0099],
        [ 0.0426, -0.0523, -0.0921,  ...,  0.0321,  0.0094, -0.0184],
        [ 0.0003,  0.0344,  0.0032,  ..., -0.0365, -0.0481,  0.0273]])

llm.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0209,  0.0061, -0.0090,  ...,  0.0104,  0.0042, -0.0028],
        [-0.0056,  0.0168,  0.0516,  ...,  0.0188,  0.0267, -0.0239],
        [ 0.0053,  0.0371,  0.0060,  ..., -0.0249,  0.0182, -0.0267],
        ...,
        [ 0.0263,  0.0199, -0.0069,  ..., -0.0593, -0.0023, -0.0170],
        [ 0.0191, -0.0334, -0.0104,  ..., -0.0485,  0.0052,  0.0125],
        [ 0.0219, -0.0056,  0.0157,  ..., -0.0097, -0.0341, -0.0321]])

llm.base_model.model.model.layers.4.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0161,  0.0054, -0.0161,  ..., -0.0041, -0.0089,  0.0138],
        [-0.0138, -0.0089, -0.0110,  ..., -0.0010, -0.0068,  0.0062],
        [-0.0020,  0.0215,  0.0204,  ...,  0.0146,  0.0061,  0.0043],
        ...,
        [ 0.0139,  0.0139, -0.0033,  ...,  0.0110,  0.0085, -0.0026],
        [ 0.0052, -0.0276, -0.0034,  ..., -0.0104,  0.0142, -0.0047],
        [ 0.0184, -0.0240, -0.0216,  ..., -0.0187, -0.0093, -0.0137]])

llm.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0138,  0.0437, -0.0187,  ..., -0.0481, -0.0021, -0.0136],
        [-0.0143,  0.0118, -0.0009,  ..., -0.0072,  0.0071,  0.0063],
        [-0.0404, -0.0094,  0.0266,  ...,  0.0055,  0.0342,  0.0066],
        ...,
        [-0.0188,  0.0196, -0.0220,  ...,  0.0112, -0.0587,  0.0566],
        [ 0.0088,  0.0413, -0.0055,  ...,  0.0081,  0.0114,  0.0379],
        [ 0.0005, -0.0240, -0.0161,  ..., -0.0050,  0.0475,  0.0212]])

llm.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0628, -0.0571, -0.0135,  ..., -0.0109,  0.0097,  0.0396],
        [-0.0135, -0.0092,  0.0083,  ...,  0.0063,  0.0261,  0.0117],
        [ 0.0309,  0.0302,  0.0028,  ..., -0.0274,  0.0345,  0.0194],
        ...,
        [-0.0004,  0.0289, -0.0034,  ..., -0.0086, -0.0265, -0.0326],
        [-0.0064, -0.0123, -0.0284,  ...,  0.0075, -0.0331,  0.0026],
        [ 0.0240,  0.0031,  0.0615,  ...,  0.0005,  0.0138,  0.0150]])

llm.base_model.model.model.layers.4.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 4.9438e-03, -1.0010e-02,  2.5024e-02,  ...,  1.1597e-02,
          1.1780e-02, -4.2419e-03],
        [-1.5259e-03, -1.4191e-03,  2.8320e-02,  ..., -1.4038e-02,
         -2.6123e-02,  1.7212e-02],
        [ 3.8300e-03, -1.3367e-02, -4.3030e-03,  ...,  1.2146e-02,
          7.2021e-03, -1.7578e-02],
        ...,
        [ 1.9684e-03, -2.5868e-05,  3.3447e-02,  ...,  1.3855e-02,
          3.3264e-03, -1.6602e-02],
        [ 2.1118e-02, -1.5381e-02, -4.8523e-03,  ..., -2.3926e-02,
          5.9204e-03,  4.0283e-03],
        [-4.9744e-03,  2.4719e-03,  1.0132e-02,  ..., -1.7456e-02,
          1.2512e-03, -1.9409e-02]])

llm.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0213, -0.0128,  0.0481,  ...,  0.0052, -0.0679,  0.0245],
        [ 0.0136, -0.0193,  0.0336,  ...,  0.0247,  0.0019, -0.0244],
        [-0.0451, -0.0104, -0.0279,  ..., -0.0523,  0.0264,  0.0183],
        ...,
        [ 0.0319,  0.0197,  0.0106,  ...,  0.0048, -0.0322,  0.0305],
        [ 0.0014, -0.0076, -0.0243,  ..., -0.0214, -0.0044,  0.0031],
        [-0.0163, -0.0186,  0.0114,  ..., -0.0079,  0.0160, -0.0272]])

llm.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0240,  0.0470,  0.0338,  ...,  0.0387,  0.0044, -0.0011],
        [-0.0327, -0.0327,  0.0217,  ...,  0.0029,  0.0179, -0.0143],
        [ 0.0191, -0.0146,  0.0036,  ...,  0.0017, -0.0280, -0.0206],
        ...,
        [ 0.0032, -0.0213, -0.0027,  ...,  0.0053,  0.0032, -0.0093],
        [-0.0089,  0.0062,  0.0173,  ..., -0.0252,  0.0118,  0.0021],
        [-0.0374,  0.0291, -0.0032,  ..., -0.0360,  0.0346,  0.0270]])

llm.base_model.model.model.layers.4.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.6133, 0.2754, 0.6992,  ..., 0.3945, 0.2715, 0.5352])

llm.base_model.model.model.layers.4.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.2188, 1.2578, 1.4062,  ..., 1.0859, 1.2031, 1.3125])

llm.base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0100, -0.0074,  0.0073,  ..., -0.0017,  0.0036, -0.0046],
        [-0.0028,  0.0031, -0.0060,  ..., -0.0023,  0.0002, -0.0182],
        [ 0.0008, -0.0020,  0.0049,  ...,  0.0027, -0.0003, -0.0143],
        ...,
        [-0.0057, -0.0114,  0.0216,  ..., -0.0093, -0.0007, -0.0184],
        [ 0.0071,  0.0177, -0.0168,  ...,  0.0012, -0.0356, -0.0315],
        [ 0.0176,  0.0068, -0.0004,  ..., -0.0082,  0.0061, -0.0232]])

llm.base_model.model.model.layers.5.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.5508, -0.6523, -0.6680,  ...,  0.2715, -0.3203, -0.0957])

llm.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0293, -0.0076, -0.0295,  ...,  0.0247, -0.0113,  0.0320],
        [ 0.0809,  0.0372,  0.0052,  ...,  0.0435,  0.0265,  0.0353],
        [ 0.0088, -0.0151, -0.0330,  ...,  0.0058, -0.0381, -0.0424],
        ...,
        [ 0.0587,  0.0069,  0.0194,  ...,  0.0814, -0.0438,  0.0222],
        [ 0.0306,  0.0599, -0.0219,  ...,  0.0716,  0.0060, -0.0088],
        [ 0.0212,  0.0178, -0.0037,  ...,  0.0251,  0.0547,  0.0773]])

llm.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0054,  0.0114, -0.0149,  ...,  0.0283,  0.0024, -0.0112],
        [ 0.0068, -0.0266,  0.0004,  ..., -0.0029, -0.0279, -0.0107],
        [-0.0073,  0.0223, -0.0315,  ...,  0.0281, -0.0147, -0.0171],
        ...,
        [ 0.0040, -0.0102,  0.0234,  ..., -0.0209,  0.0107, -0.0022],
        [ 0.0009,  0.0193, -0.0332,  ...,  0.0317, -0.0325,  0.0036],
        [-0.0116, -0.0058, -0.0273,  ...,  0.0112, -0.0194,  0.0076]])

llm.base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0005,  0.0022, -0.0006,  ...,  0.0120, -0.0143,  0.0037],
        [-0.0022, -0.0063, -0.0090,  ..., -0.0048, -0.0048,  0.0020],
        [ 0.0006,  0.0114, -0.0012,  ...,  0.0099, -0.0005, -0.0060],
        ...,
        [ 0.0361, -0.0206, -0.0283,  ..., -0.0208,  0.0107,  0.0205],
        [-0.0273, -0.0017, -0.0032,  ..., -0.0084, -0.0142,  0.0070],
        [ 0.0288,  0.0012, -0.0107,  ..., -0.0051,  0.0154, -0.0100]])

llm.base_model.model.model.layers.5.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-2.7539e-01,  1.0078e+00,  9.6484e-01,  8.2031e-01,  5.1172e-01,
        -8.7500e-01, -2.9297e-01, -1.0547e+00, -3.5352e-01,  5.9766e-01,
         1.5234e-01, -2.8906e-01, -5.3516e-01, -6.5430e-02, -2.8906e-01,
         2.1973e-01, -4.9072e-02,  3.4570e-01,  3.8574e-02, -8.5156e-01,
         8.8867e-02,  1.0986e-01,  7.7344e-01,  1.1047e-02, -9.3359e-01,
        -3.8605e-03,  1.1523e-01, -4.5166e-03,  1.7700e-03, -2.2559e-01,
         7.5195e-02, -1.0254e-01, -2.6001e-02, -5.3467e-02, -1.0469e+00,
         7.2754e-02,  2.1484e-02, -4.3701e-02,  7.2327e-03, -8.5449e-03,
        -3.5400e-02, -7.2632e-03, -1.9434e-01, -9.3750e-02, -8.2031e-02,
        -2.9102e-01,  9.5703e-02, -3.4180e-02, -2.4121e-01, -3.7354e-02,
         2.2559e-01,  2.7344e+00, -5.1270e-03,  5.3406e-03, -1.9531e-01,
         8.0078e-02, -3.2959e-02, -1.2573e-02,  6.1328e-01,  2.1094e-01,
         4.7852e-01,  1.6016e-01,  2.5879e-02,  2.9541e-02, -1.1094e+00,
        -3.8477e-01, -3.6914e-01, -4.7266e-01, -3.9062e-01, -7.8906e-01,
         3.0859e-01, -6.9922e-01,  1.8750e-01, -1.0000e+00, -2.0410e-01,
         1.1172e+00, -5.3125e-01, -1.2354e-01,  1.1094e+00,  3.5352e-01,
        -1.6602e-01, -9.5312e-01,  1.4160e-01, -2.8320e-01, -1.3477e-01,
         7.6172e-02,  1.3281e-01, -1.7871e-01, -3.7305e-01,  4.7607e-02,
        -3.1982e-02, -2.6758e-01,  7.8613e-02,  7.5000e-01,  7.1777e-02,
        -9.6680e-02, -9.2285e-02,  1.0315e-02, -7.7344e-01, -5.1880e-03,
         4.5654e-02, -1.0547e-01, -8.3496e-02,  1.1426e-01,  1.5430e-01,
         9.2285e-02, -7.8125e-01, -1.5625e-01, -1.1475e-01, -5.0781e-02,
         8.8379e-02, -1.0693e-01,  5.6641e-01,  6.0303e-02, -1.8262e-01,
         3.6094e+00,  1.8433e-02,  8.5938e-02,  1.2500e-01,  3.4332e-03,
         7.6172e-02,  4.8523e-03, -2.1240e-02,  3.2959e-02,  1.2891e-01,
         2.0312e-01, -5.8838e-02,  2.0142e-02, -5.5859e-01,  2.5000e-01,
        -5.0049e-02,  3.6328e-01, -4.3164e-01,  5.4443e-02, -6.7969e-01,
        -1.6992e-01,  7.1289e-02, -2.6855e-02, -3.3203e-01, -1.0059e-01,
         2.3315e-02, -3.4570e-01,  6.9824e-02,  2.7539e-01,  6.4453e-01,
        -1.5015e-02,  2.2070e-01, -2.2705e-02,  1.4160e-01, -2.0996e-01,
         1.9531e-01,  7.5195e-02, -1.1914e-01,  4.4336e-01, -6.9922e-01,
        -2.5391e-01,  6.3965e-02,  3.7109e-01, -1.6895e-01,  5.5078e-01,
         1.8555e-01, -2.0117e-01,  9.2773e-02,  5.7812e-01,  2.2559e-01,
        -2.1289e-01,  5.3125e-01, -4.4336e-01,  3.6328e-01,  6.8359e-02,
        -2.3828e-01,  1.4062e-01, -1.6016e-01, -6.2988e-02,  4.8828e-01,
         8.7891e-01,  5.0781e-01,  4.3555e-01,  1.9141e-01,  4.4189e-02,
        -7.0703e-01,  1.6504e-01,  3.9258e-01,  1.0156e-01,  1.4941e-01,
        -6.2109e-01,  5.9766e-01, -6.0547e-01, -1.9297e+00, -9.2188e-01,
         6.3750e+00,  2.5024e-02,  1.7285e-01, -4.0820e-01,  4.1016e-01,
        -4.9609e-01, -6.2988e-02,  1.0156e-01, -1.4771e-02,  3.1250e-01,
        -4.3164e-01,  4.0625e-01,  1.6016e-01, -3.7305e-01,  2.7930e-01,
        -3.1641e-01, -4.1797e-01, -1.3770e-01, -3.4424e-02,  1.8750e-01,
         2.0020e-01,  5.1562e-01,  2.9419e-02,  2.0215e-01,  7.6953e-01,
        -1.2988e-01,  3.3203e-02, -1.2500e-01,  1.8066e-01, -7.8125e-02,
         4.1797e-01, -2.0703e-01,  2.5195e-01, -2.5781e-01,  3.1055e-01,
        -7.3828e-01, -2.2266e-01, -6.2500e-01, -7.1777e-02, -1.1719e-01,
        -1.2402e-01,  4.3750e-01, -9.2969e-01,  1.9727e-01, -3.6328e-01,
         6.5625e-01,  1.7578e-02, -1.3906e+00,  1.9922e-01,  7.5391e-01,
        -2.6562e-01,  2.1973e-01, -5.5078e-01,  4.9219e-01, -1.3125e+00,
         1.3125e+00, -2.4707e-01,  7.4219e-01,  3.3594e-01, -1.2734e+00,
        -1.5391e+00, -4.7852e-01, -4.6680e-01,  1.2109e+00, -4.5938e+00,
        -1.9453e+00,  1.2500e-01,  1.5430e-01,  4.2773e-01,  3.1641e-01,
         4.3555e-01, -3.3203e-01, -8.7891e-02,  2.6172e-01, -5.6250e-01,
         4.6484e-01,  6.5430e-02,  5.0781e-01,  3.4375e-01, -1.9336e-01,
        -1.3574e-01,  2.9688e-01, -3.6719e-01, -1.5625e-01, -9.8145e-02,
         1.6602e-01, -7.5684e-02, -7.1777e-02, -3.7109e-01,  1.1475e-01,
        -6.9531e-01, -5.5859e-01, -4.7852e-02, -3.0273e-02,  1.0205e-01,
        -2.4609e-01,  4.2188e-01,  3.5400e-02, -9.4141e-01,  1.6235e-02,
        -1.2656e+00, -1.1719e-01,  1.0645e-01,  1.1406e+00,  2.1875e-01,
         2.7832e-02, -1.0010e-01, -2.7812e+00,  2.4023e-01, -9.5312e-01,
         3.7109e-01, -1.4453e-01,  1.9238e-01, -3.5742e-01,  1.6992e-01,
        -5.7031e-01,  3.2501e-03, -6.6406e-01, -4.1602e-01, -6.6406e-01,
        -2.8125e-01,  4.9609e-01,  3.4375e-01, -6.4941e-02, -1.1328e-01,
         1.6699e-01,  2.5635e-02, -2.9531e+00, -5.2344e-01, -2.1484e-01,
         6.3672e-01, -1.4551e-01, -1.5820e-01,  2.2949e-01,  9.3750e-02,
         3.6133e-01, -2.5781e-01,  9.5703e-02, -2.0898e-01,  1.0059e-01,
        -2.1680e-01,  2.3926e-01, -2.7344e-01,  5.3125e-01, -2.0996e-01,
        -5.7812e-01, -7.1289e-02, -6.4453e-01,  6.1719e-01,  1.0742e-01,
         7.9297e-01, -2.2461e-01, -8.1641e-01, -5.3516e-01, -4.5898e-01,
        -1.0303e-01,  6.5625e-01,  9.8047e-01, -4.2969e-01,  1.0156e+00,
         5.1172e-01, -8.2397e-03, -4.8828e-01, -1.9336e-01,  1.0859e+00,
         1.2207e-01,  1.3672e-01, -1.5039e-01, -4.1797e-01,  6.8848e-02,
        -2.5195e-01, -1.0010e-01, -4.8047e-01, -9.5703e-02,  1.6992e-01,
        -1.4453e-01,  4.5703e-01, -1.5859e+00, -9.0332e-02,  1.0234e+00,
         5.6152e-02,  1.8164e-01, -2.2852e-01, -3.2227e-02, -5.6250e-01,
        -3.9258e-01,  1.1328e-01,  2.9492e-01,  3.8672e-01, -2.5000e-01,
        -1.7188e-01, -5.1875e+00, -1.8164e-01,  4.7266e-01,  9.0625e-01,
         2.1729e-02,  8.6328e-01,  2.7222e-02, -8.0078e-01,  3.2715e-02,
         8.9453e-01, -8.6914e-02, -5.4297e-01,  1.5332e-01,  8.0859e-01,
        -7.4219e-02, -3.9844e-01,  5.9204e-03,  7.6953e-01, -1.5039e-01,
         3.1445e-01,  1.9302e-03,  5.8899e-03, -1.0391e+00, -1.6016e-01,
        -7.7209e-03, -8.2031e-01,  4.2480e-02,  1.7676e-01,  1.0234e+00,
        -1.3965e-01,  7.4707e-02,  6.5625e-01, -3.0029e-02,  9.6191e-02,
        -3.2471e-02, -5.8594e-02, -1.7212e-02,  6.8848e-02,  1.0596e-01,
         2.2827e-02, -1.9434e-01, -4.1016e-02,  1.4062e-01, -8.6914e-02,
         6.0303e-02, -5.6396e-02, -1.1719e-01, -1.5430e-01, -1.5918e-01,
         1.9165e-02, -1.4453e-01,  4.2480e-02,  6.4941e-02, -9.0942e-03,
        -1.2598e-01,  6.0547e-02, -2.7344e-01,  7.9590e-02,  2.0703e-01,
        -2.9062e+00,  7.5195e-02, -1.7090e-01,  7.3730e-02,  1.0840e-01,
         1.3281e-01,  2.4805e-01, -2.8125e-01, -2.3730e-01, -4.2188e-01,
        -4.4922e-01, -8.5449e-02,  6.2500e-01, -3.3203e-02,  4.7461e-01,
        -7.4219e-02,  8.2812e-01, -1.0986e-01,  4.6289e-01, -7.4707e-02,
         8.9062e-01, -9.6680e-02,  4.9805e-01,  4.4189e-02, -9.0234e-01,
        -9.6191e-02,  4.5654e-02, -3.8477e-01,  5.4932e-02,  8.5938e-02,
        -2.3242e-01,  3.4668e-02, -2.2339e-02,  1.0693e-01,  1.8066e-02,
        -1.9409e-02, -6.3965e-02,  5.7373e-03, -8.5156e-01,  6.4941e-02,
        -1.7319e-03,  9.6191e-02, -1.5430e-01,  1.5918e-01, -5.1562e-01,
        -1.6479e-02,  2.1851e-02, -1.5527e-01,  9.1309e-02, -1.3867e-01,
        -5.2734e-02, -7.4707e-02, -5.4199e-02, -1.1719e-02, -7.3242e-02,
         4.6143e-02, -4.0283e-02,  8.3496e-02,  5.6396e-02,  4.7119e-02,
        -1.7578e-01,  1.5869e-02,  1.8066e-01,  1.6211e-01,  1.6562e+00,
        -5.9570e-02, -1.6797e-01,  2.3438e-01,  2.2705e-02,  2.5000e-01,
        -2.1094e-01,  2.0996e-02])

llm.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0344,  0.0665,  0.0102,  ...,  0.0669, -0.0268, -0.0206],
        [-0.0308,  0.0215,  0.0291,  ...,  0.0333, -0.0355,  0.0258],
        [-0.0604, -0.0210,  0.0520,  ..., -0.0493,  0.0160,  0.0326],
        ...,
        [-0.0103, -0.0549,  0.0451,  ..., -0.0491, -0.0195,  0.0122],
        [-0.0036, -0.0381, -0.0455,  ..., -0.0684,  0.0093,  0.0477],
        [-0.0055, -0.0699,  0.0036,  ..., -0.0522,  0.0211,  0.0059]])

llm.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0422, -0.0728,  0.0092,  ..., -0.0056,  0.0310, -0.0032],
        [ 0.0226,  0.0112, -0.0090,  ...,  0.0024, -0.0272, -0.0172],
        [ 0.0003, -0.0113, -0.0061,  ..., -0.0088,  0.0031, -0.0006],
        ...,
        [ 0.0029,  0.0256,  0.0559,  ...,  0.0212, -0.0361,  0.0135],
        [-0.0077, -0.0134,  0.0470,  ...,  0.0052,  0.0033,  0.0063],
        [ 0.0044,  0.0197, -0.0022,  ...,  0.0009,  0.0098,  0.0042]])

llm.base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0170,  0.0074,  0.0189,  ..., -0.0036,  0.0050,  0.0043],
        [ 0.0012, -0.0189, -0.0079,  ..., -0.0023,  0.0032, -0.0148],
        [ 0.0031, -0.0052,  0.0045,  ...,  0.0168, -0.0044,  0.0089],
        ...,
        [ 0.0007,  0.0289,  0.0112,  ...,  0.0123,  0.0115, -0.0041],
        [-0.0065, -0.0018,  0.0188,  ...,  0.0334, -0.0020,  0.0066],
        [-0.0173, -0.0049,  0.0056,  ..., -0.0028,  0.0288, -0.0008]])

llm.base_model.model.model.layers.5.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 1.4453e-01,  7.7148e-02,  1.7212e-02, -2.1515e-03, -1.1230e-02,
         1.8799e-02, -3.1250e-02, -6.7383e-02, -1.2512e-02,  1.1230e-01,
         2.1973e-02,  3.8574e-02,  1.5717e-03, -6.4453e-02, -5.9082e-02,
         1.1780e-02, -1.3184e-02,  2.9419e-02,  8.1787e-03, -3.3447e-02,
         2.7344e-02,  1.0986e-02, -1.4453e-01, -1.3477e-01,  1.3916e-02,
         1.1475e-02, -6.9824e-02,  2.3438e-02,  7.9590e-02, -1.3504e-03,
        -4.3701e-02,  1.0437e-02,  2.2583e-02, -2.9541e-02, -3.5645e-02,
        -4.6875e-02,  6.3965e-02, -2.1362e-02, -6.1951e-03, -2.6131e-04,
        -2.5146e-02,  3.1494e-02,  3.6621e-02, -3.5645e-02, -2.1851e-02,
        -3.5706e-03, -3.6377e-02,  7.8735e-03,  4.0588e-03,  4.0039e-02,
         3.6133e-02, -1.6406e-01,  4.1504e-02, -6.3965e-02,  7.4707e-02,
        -4.8828e-03,  6.3171e-03,  8.6060e-03, -2.1362e-02, -6.8359e-02,
         5.6152e-03, -3.9062e-02,  2.3926e-02,  4.4250e-03, -3.5742e-01,
         1.1108e-02, -7.7820e-03, -1.2634e-02,  1.1719e-02, -4.1016e-02,
        -1.8066e-02,  7.0801e-02, -1.7212e-02,  2.8992e-03, -1.4709e-02,
        -3.9795e-02,  2.5146e-02,  5.6152e-02, -1.0910e-03,  1.3611e-02,
        -1.0889e-01,  8.5449e-03,  4.7913e-03,  2.5879e-02,  5.3467e-02,
         4.6387e-02, -1.7090e-02, -2.2217e-02, -2.6123e-02, -6.6528e-03,
        -6.1035e-03,  9.0820e-02, -3.4180e-02,  9.2163e-03, -2.7344e-02,
         3.6133e-02,  1.1963e-02, -2.7771e-03, -2.1362e-02, -3.8574e-02,
         6.8848e-02, -1.5430e-01,  3.1982e-02, -2.7466e-02, -2.5391e-02,
        -2.7466e-03,  4.5410e-02, -5.0781e-02, -8.6914e-02, -3.6865e-02,
        -2.1973e-02,  4.8340e-02,  2.4780e-02,  3.0273e-02, -3.8086e-02,
         8.0078e-02,  1.2695e-01, -3.9062e-02, -8.1543e-02, -9.1553e-03,
         6.3477e-02, -4.1748e-02,  2.8687e-02,  7.8735e-03,  2.8564e-02,
        -1.3885e-03,  1.9336e-01, -3.4180e-02, -1.9043e-02,  5.4443e-02,
         6.1279e-02, -4.5410e-02, -1.5564e-02, -2.1606e-02, -6.1340e-03,
         6.3965e-02, -8.4229e-03,  9.5825e-03,  3.5400e-02,  2.3926e-02,
         2.2705e-02, -2.9053e-02,  9.1309e-02,  1.2390e-02,  1.2146e-02,
        -1.9775e-02,  4.4250e-03,  8.4961e-02, -8.0078e-02,  4.7852e-02,
         7.8125e-02,  1.4221e-02,  2.1973e-02,  4.3457e-02,  8.8867e-02,
        -9.6680e-02, -9.2163e-03,  1.0156e-01,  5.5176e-02,  4.2969e-02,
        -1.2354e-01,  7.9102e-02,  2.7344e-02,  1.3281e-01,  5.5908e-02,
        -1.6479e-02,  9.5215e-02, -7.9590e-02, -6.6406e-02,  4.6631e-02,
         1.5381e-02, -3.1128e-03, -4.7852e-02,  2.1118e-02,  3.9551e-02,
         9.6436e-03, -5.3223e-02,  1.9360e-04, -8.2031e-02,  7.4219e-02,
        -7.8125e-02, -1.4355e-01, -1.0107e-01, -5.6152e-02, -3.1128e-03,
         4.8340e-02,  1.9775e-02, -1.5332e-01, -5.8105e-02, -7.1411e-03,
        -1.5869e-02, -4.5410e-02,  5.7068e-03, -8.8379e-02,  5.6641e-02,
         2.5391e-02, -4.7852e-02, -9.6512e-04,  4.1992e-02, -8.1543e-02,
        -1.0889e-01, -9.9121e-02,  6.0303e-02,  4.0588e-03, -2.9419e-02,
         5.5908e-02, -6.1035e-02,  5.7373e-03,  7.9590e-02, -9.2285e-02,
         1.8555e-02,  1.7212e-02, -4.7607e-02, -1.6895e-01,  3.9062e-02,
        -8.1543e-02, -6.3477e-02, -5.6250e-01, -5.4932e-02, -5.0049e-03,
         3.8330e-02,  6.2988e-02, -1.7456e-02,  3.1494e-02,  3.2617e-01,
        -9.9609e-02,  3.1738e-02,  1.1353e-02, -7.6172e-02,  1.3351e-04,
        -6.8359e-03, -2.4170e-02, -1.9775e-02, -3.8086e-02, -7.6904e-03,
        -3.2715e-02, -1.6016e-01, -5.4443e-02,  2.2697e-04, -1.0547e-01,
         5.9326e-02, -3.7598e-02,  2.7100e-02, -6.1523e-02, -4.9561e-02,
         1.1572e-01,  6.7383e-02,  2.2827e-02, -2.6367e-02, -3.7598e-02,
         5.9082e-02, -7.8735e-03,  5.7617e-02,  6.2500e-02, -1.3379e-01,
        -7.7637e-02, -1.9531e-03,  2.5635e-02, -4.7363e-02, -2.4414e-02,
         6.5308e-03, -1.5527e-01, -6.3965e-02,  7.2327e-03, -1.2085e-02,
         2.9755e-03, -1.7700e-02, -7.0312e-02, -3.4424e-02,  2.9419e-02,
        -2.5513e-02, -6.4392e-03,  3.6133e-02,  6.8359e-02, -2.6123e-02,
        -1.6846e-02,  5.4932e-02,  1.0986e-02,  2.4048e-02,  3.4668e-02,
        -4.9561e-02,  3.0708e-04,  9.3384e-03, -1.4282e-02, -3.9307e-02,
        -7.9956e-03, -5.5664e-02, -1.6797e-01,  2.3926e-02,  1.2024e-02,
        -1.6479e-02,  7.7148e-02,  9.3994e-03,  6.7383e-02,  6.7383e-02,
         6.1768e-02,  4.7302e-03, -1.1536e-02, -9.8145e-02,  3.0518e-02,
         2.3804e-02,  1.4062e-01, -2.4567e-03,  2.4414e-02, -4.5654e-02,
         7.3730e-02,  5.6763e-03, -2.6611e-02,  3.9551e-02, -5.7678e-03,
        -5.7617e-02, -2.5269e-02, -1.5332e-01,  5.0354e-03,  8.2397e-03,
        -1.2891e-01,  5.4443e-02,  7.4707e-02,  2.2461e-02, -8.0078e-02,
        -1.0803e-02,  1.1475e-02, -2.2278e-03,  3.0762e-02,  1.3733e-02,
         6.3477e-02, -7.5989e-03, -1.7090e-02,  1.4404e-02,  1.7334e-02,
         1.1292e-02, -1.2207e-01, -3.1494e-02,  1.1230e-02, -2.2095e-02,
        -2.0996e-01, -1.7944e-02,  1.9775e-02,  6.5430e-02, -3.1006e-02,
        -3.8574e-02,  1.5198e-02,  4.8523e-03, -1.1475e-01,  4.1016e-02,
        -8.5449e-02, -1.8188e-02, -1.2573e-02, -3.1982e-02,  5.1880e-03,
        -2.4170e-02, -1.7700e-02, -2.4048e-02,  4.5166e-02, -6.8848e-02,
        -1.1536e-02,  6.3965e-02,  1.9727e-01,  4.0894e-03,  2.6367e-02,
        -1.6235e-02, -3.7598e-02,  2.9755e-03,  1.0559e-02, -4.9805e-02,
         2.6978e-02, -7.1777e-02,  3.1006e-02,  3.4668e-02,  1.3962e-03,
         3.2959e-02,  8.8215e-05, -6.4941e-02,  4.3457e-02,  5.5176e-02,
         9.5703e-02,  1.3574e-01,  6.6895e-02,  3.1641e-01,  1.4587e-02,
         1.3965e-01, -2.4414e-02,  8.7891e-03,  1.5137e-02, -2.9785e-02,
        -9.1553e-03,  6.3171e-03,  2.9297e-02, -1.7944e-02, -2.7954e-02,
         3.0212e-03,  1.2500e-01, -1.0452e-03,  2.3682e-02,  2.6245e-02,
         3.3203e-02, -5.8594e-02,  9.1553e-03, -1.6113e-02, -4.4250e-03,
         4.9744e-03, -7.9346e-03,  9.1553e-03, -5.4932e-03,  3.2959e-02,
         1.3550e-02,  1.5137e-02, -2.8809e-02,  1.5320e-02,  4.5654e-02,
        -6.3477e-03,  1.6211e-01,  2.1484e-02,  2.5635e-02,  7.2479e-04,
         4.6875e-02, -1.5820e-01, -1.0059e-01,  8.7109e-01, -2.3560e-02,
        -2.0020e-02, -9.3460e-04,  1.4404e-02,  2.7100e-02, -2.8381e-03,
         9.9487e-03,  1.0437e-02,  6.4697e-03,  2.3071e-02, -2.8229e-03,
        -1.3672e-02, -4.9561e-02,  3.0396e-02, -1.4572e-03,  2.0752e-02,
        -2.0264e-02,  2.0386e-02, -1.0864e-02,  1.2329e-02,  2.7222e-02,
        -2.1362e-02, -5.0049e-02, -7.7515e-03, -8.3618e-03, -2.2461e-02,
        -3.0518e-03, -2.8564e-02,  3.3447e-02, -1.4453e-01,  3.5858e-03,
         1.1108e-02, -3.8330e-02, -1.5625e-02, -3.3112e-03, -6.8359e-03,
         1.9531e-02, -2.4414e-02,  1.1414e-02,  6.2012e-02,  3.1250e-02,
        -3.5156e-02, -2.0752e-02,  5.4688e-02, -2.9785e-02, -1.8677e-02,
        -6.8848e-02,  4.6631e-02,  1.7212e-02, -1.7480e-01, -4.8828e-02,
         1.3611e-02,  1.9043e-02,  2.7344e-02, -6.5613e-03, -8.8501e-03,
         6.5613e-03, -6.5430e-02,  1.0132e-02, -5.5313e-04,  5.8105e-02,
        -4.9072e-02,  3.5400e-02,  4.0283e-02, -2.3926e-02,  1.8555e-02,
         1.3611e-02, -8.3618e-03,  1.0559e-02,  1.7334e-02, -1.1047e-02,
         8.6426e-02,  1.1597e-02,  6.2561e-03, -3.5400e-02, -7.9346e-04,
         3.4668e-02, -8.7280e-03, -1.7090e-02, -6.8665e-03, -6.2256e-03,
        -2.3556e-04,  2.8564e-02,  4.5410e-02, -2.6855e-02, -3.4668e-02,
        -5.2490e-02, -3.6865e-02,  1.3428e-02,  1.8158e-03, -1.2573e-02,
        -3.1738e-03,  3.1006e-02])

llm.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0053, -0.0195,  0.0141,  ..., -0.0244,  0.0155, -0.0049],
        [ 0.0288,  0.0245,  0.0189,  ..., -0.0063,  0.0070, -0.0204],
        [-0.0134,  0.0082, -0.0226,  ..., -0.0078,  0.0124,  0.0077],
        ...,
        [ 0.0667,  0.0244,  0.0322,  ...,  0.0177,  0.0044,  0.0297],
        [-0.0163,  0.0052, -0.0155,  ..., -0.0302,  0.0228,  0.0112],
        [ 0.0472, -0.0467, -0.0325,  ..., -0.0328,  0.0053,  0.0218]])

llm.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0156,  0.0342,  0.0177,  ...,  0.0207, -0.0021, -0.0198],
        [-0.0072,  0.0003,  0.0065,  ...,  0.0097,  0.0066,  0.0274],
        [-0.0020,  0.0075,  0.0132,  ..., -0.0003,  0.0083, -0.0054],
        ...,
        [-0.0124, -0.0142,  0.0130,  ..., -0.0224,  0.0155,  0.0126],
        [-0.0097,  0.0317,  0.0145,  ..., -0.0100,  0.0232, -0.0415],
        [ 0.0007,  0.0175, -0.0303,  ..., -0.0133,  0.0428, -0.0034]])

llm.base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0071,  0.0177, -0.0099,  ...,  0.0021, -0.0149, -0.0160],
        [-0.0039, -0.0006,  0.0066,  ...,  0.0056,  0.0095,  0.0237],
        [-0.0233,  0.0014,  0.0162,  ...,  0.0096,  0.0082, -0.0107],
        ...,
        [ 0.0009, -0.0013, -0.0126,  ..., -0.0054,  0.0019, -0.0004],
        [-0.0071, -0.0107,  0.0114,  ...,  0.0129, -0.0172, -0.0214],
        [ 0.0167,  0.0212,  0.0129,  ..., -0.0078, -0.0173, -0.0094]])

llm.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0118,  0.0167, -0.0260,  ...,  0.0191,  0.0291, -0.0064],
        [ 0.0088,  0.0271, -0.0052,  ...,  0.0086,  0.0162,  0.0099],
        [-0.0470, -0.0202,  0.0196,  ...,  0.0178, -0.0205,  0.0129],
        ...,
        [-0.0485,  0.0178,  0.0208,  ..., -0.0114, -0.0125, -0.0027],
        [-0.0189,  0.0370, -0.0098,  ...,  0.0305,  0.0247, -0.0094],
        [-0.0247,  0.0710, -0.0130,  ...,  0.0290,  0.0066, -0.0012]])

llm.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0102,  0.0291,  0.0016,  ..., -0.0026, -0.0334,  0.0230],
        [-0.0466, -0.0231,  0.0026,  ..., -0.0360,  0.0230,  0.0151],
        [-0.0239,  0.0163,  0.0162,  ...,  0.0257, -0.0240, -0.0423],
        ...,
        [-0.0382, -0.0037,  0.0146,  ..., -0.0113,  0.0045, -0.0110],
        [ 0.0014,  0.0277, -0.0145,  ..., -0.0397, -0.0239,  0.0204],
        [ 0.0056, -0.0144, -0.0305,  ..., -0.0092, -0.0101,  0.0223]])

llm.base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0063,  0.0007, -0.0069,  ..., -0.0011, -0.0150, -0.0010],
        [-0.0168, -0.0222,  0.0058,  ...,  0.0275,  0.0041,  0.0157],
        [ 0.0327, -0.0071, -0.0124,  ...,  0.0101,  0.0032, -0.0050],
        ...,
        [-0.0078, -0.0349,  0.0071,  ...,  0.0083,  0.0040, -0.0130],
        [-0.0215, -0.0034,  0.0135,  ...,  0.0039, -0.0012, -0.0173],
        [-0.0289, -0.0056, -0.0112,  ...,  0.0046,  0.0095,  0.0065]])

llm.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0503, -0.0093, -0.0200,  ..., -0.0217, -0.0194, -0.0071],
        [-0.0219, -0.0394, -0.0374,  ..., -0.0347, -0.0088, -0.0317],
        [-0.0470, -0.0303, -0.0086,  ...,  0.0591, -0.0534,  0.0044],
        ...,
        [-0.0134, -0.0072, -0.0311,  ..., -0.0234, -0.0312,  0.0257],
        [-0.0297,  0.0219,  0.0873,  ..., -0.0120,  0.0823, -0.0519],
        [-0.0460,  0.0157, -0.0426,  ...,  0.0017, -0.0048,  0.0083]])

llm.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0039,  0.0105, -0.0288,  ...,  0.0422, -0.0373,  0.0302],
        [-0.0290,  0.0166, -0.0055,  ..., -0.0400,  0.0075,  0.0254],
        [ 0.0232, -0.0133,  0.0362,  ..., -0.0188,  0.0364, -0.0275],
        ...,
        [ 0.0206,  0.0193,  0.0047,  ...,  0.0031, -0.0121, -0.0048],
        [ 0.0055,  0.0024, -0.0477,  ..., -0.0383, -0.0053,  0.0296],
        [-0.0331, -0.0018, -0.0177,  ..., -0.0268,  0.0507,  0.0356]])

llm.base_model.model.model.layers.5.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0266,  0.0211,  0.0090,  ...,  0.0165, -0.0183,  0.0005],
        [ 0.0048, -0.0046,  0.0030,  ...,  0.0200,  0.0039, -0.0098],
        [ 0.0044, -0.0369,  0.0031,  ..., -0.0063,  0.0100, -0.0184],
        ...,
        [-0.0182, -0.0221, -0.0033,  ...,  0.0052, -0.0141, -0.0056],
        [ 0.0081, -0.0073, -0.0074,  ..., -0.0298,  0.0139,  0.0152],
        [ 0.0206, -0.0261, -0.0135,  ...,  0.0042, -0.0243, -0.0122]])

llm.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0090, -0.0174, -0.0075,  ..., -0.0089,  0.0093,  0.0297],
        [-0.0048, -0.0978,  0.0051,  ..., -0.0598, -0.0068,  0.0316],
        [-0.0130, -0.0318,  0.0153,  ...,  0.0275,  0.0288,  0.0155],
        ...,
        [ 0.0203, -0.0211,  0.0491,  ..., -0.0172,  0.0008,  0.0560],
        [-0.0010, -0.0166, -0.0321,  ..., -0.0351,  0.0226,  0.0060],
        [ 0.0671,  0.0063, -0.0207,  ..., -0.0470,  0.0163, -0.0036]])

llm.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 1.7564e-02, -3.2987e-02, -1.0700e-02,  ..., -4.1125e-03,
         -9.2101e-03,  9.4702e-04],
        [ 2.5980e-03,  5.2570e-02, -1.5707e-02,  ..., -3.5149e-03,
         -2.7219e-03,  7.9305e-03],
        [-9.8153e-03,  4.0807e-02,  4.3159e-02,  ..., -2.7128e-02,
          2.9906e-02, -2.6985e-02],
        ...,
        [-1.2873e-02, -1.1900e-02, -3.6240e-02,  ..., -9.6180e-05,
         -2.6256e-02, -1.3435e-02],
        [-2.1714e-03,  1.0197e-02, -1.2129e-02,  ..., -2.8245e-02,
         -1.0358e-02,  1.3877e-02],
        [-1.8642e-02, -1.9366e-03,  2.0601e-03,  ..., -1.1002e-02,
         -8.1506e-03,  2.9458e-02]])

llm.base_model.model.model.layers.5.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[-2.1667e-03,  1.9455e-03,  1.3199e-03,  ..., -9.7046e-03,
         -2.4414e-04, -1.0315e-02],
        [-5.8594e-03,  1.5991e-02, -1.4801e-03,  ...,  8.9722e-03,
          2.9907e-02, -8.8501e-03],
        [-4.1504e-03, -5.0354e-03, -1.5869e-02,  ..., -3.0823e-03,
         -5.6152e-03, -9.2030e-05],
        ...,
        [ 1.2268e-02,  7.9346e-03, -2.3346e-03,  ...,  3.4943e-03,
          1.5625e-02,  8.7891e-03],
        [-1.2390e-02,  1.3794e-02,  2.1851e-02,  ...,  1.4771e-02,
          5.0354e-03, -6.6223e-03],
        [ 1.8066e-02, -1.2634e-02, -1.4648e-02,  ...,  2.8229e-03,
          2.6978e-02, -1.3428e-03]])

llm.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[-0.0280,  0.0240, -0.0171,  ...,  0.0163, -0.0244, -0.0022],
        [-0.0287, -0.0010,  0.0211,  ..., -0.0344,  0.0685, -0.0711],
        [ 0.0171, -0.0282,  0.0146,  ...,  0.0298,  0.0450, -0.0237],
        ...,
        [ 0.0019,  0.0055, -0.0108,  ...,  0.0046, -0.0336,  0.0059],
        [-0.0063, -0.0029, -0.0423,  ...,  0.0273,  0.0179,  0.0010],
        [-0.0027,  0.0005, -0.0150,  ...,  0.0014,  0.0373, -0.0125]])

llm.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0452, -0.0137, -0.0339,  ..., -0.0069, -0.0168,  0.0039],
        [-0.0497,  0.0415, -0.0039,  ..., -0.0420, -0.0143,  0.0015],
        [ 0.0139, -0.0015,  0.0019,  ..., -0.0302,  0.0206,  0.0255],
        ...,
        [-0.0189, -0.0055, -0.0309,  ...,  0.0121,  0.0433, -0.0070],
        [ 0.0071, -0.0248, -0.0092,  ...,  0.0189,  0.0253,  0.0081],
        [ 0.0427, -0.0024, -0.0069,  ...,  0.0038, -0.0091, -0.0323]])

llm.base_model.model.model.layers.5.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.7461, 0.4316, 0.9609,  ..., 0.4688, 0.3828, 0.6836])

llm.base_model.model.model.layers.5.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.4219, 1.4688, 1.6875,  ..., 1.2578, 1.4219, 1.5312])

llm.base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-2.3804e-02, -1.2634e-02, -8.4229e-03,  ..., -6.9275e-03,
          1.2390e-02, -2.5787e-03],
        [-2.9907e-03, -1.1673e-03,  1.5747e-02,  ...,  3.9978e-03,
         -1.0803e-02, -1.4771e-02],
        [ 3.9368e-03,  3.2959e-03, -1.0529e-03,  ..., -2.5749e-04,
          7.6904e-03,  2.8992e-03],
        ...,
        [ 3.5858e-03, -2.3315e-02, -3.5095e-03,  ..., -1.1353e-02,
          1.0376e-03, -7.9956e-03],
        [-2.9419e-02,  4.1504e-02, -2.2339e-02,  ..., -8.3923e-05,
         -1.7578e-02,  4.1016e-02],
        [-3.2349e-03,  1.8921e-02,  3.8605e-03,  ...,  4.2114e-03,
          4.3640e-03,  1.5259e-02]])

llm.base_model.model.model.layers.6.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([-0.5273, -2.9844,  3.1094,  ...,  0.0503,  0.4961,  0.1147])

llm.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0255,  0.0258, -0.0457,  ...,  0.0661, -0.0125,  0.0171],
        [-0.0226, -0.0285,  0.0257,  ...,  0.0284, -0.0161, -0.0004],
        [ 0.0598, -0.0143, -0.0502,  ...,  0.0696,  0.0209,  0.0537],
        ...,
        [-0.0597,  0.0181,  0.0053,  ..., -0.0567,  0.0284,  0.0139],
        [ 0.0103,  0.0650, -0.0023,  ..., -0.0074,  0.0048, -0.0144],
        [-0.0467,  0.0375, -0.0297,  ..., -0.0100, -0.0222,  0.0252]])

llm.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 4.8360e-03,  3.5437e-03, -9.6411e-03,  ...,  2.1696e-02,
         -2.0042e-02,  1.9704e-03],
        [-6.7226e-03, -4.3763e-03,  1.9950e-03,  ...,  1.9660e-02,
         -1.4840e-02,  2.8610e-02],
        [ 8.0081e-03,  1.0847e-02, -8.1846e-03,  ..., -1.4751e-02,
          5.0777e-05, -1.0480e-02],
        ...,
        [ 3.4942e-02,  1.9197e-02,  1.4186e-02,  ..., -1.8499e-02,
         -1.5535e-02,  1.4881e-02],
        [-5.9715e-03,  1.8541e-03,  2.8182e-02,  ..., -1.5256e-02,
         -2.0090e-02, -2.1056e-02],
        [ 7.8413e-03,  9.5614e-03, -1.8419e-02,  ..., -3.7059e-02,
          2.1689e-02,  8.3842e-03]])

llm.base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0069,  0.0058, -0.0074,  ..., -0.0006,  0.0070, -0.0007],
        [-0.0168, -0.0278, -0.0305,  ..., -0.0125,  0.0096, -0.0078],
        [ 0.0042, -0.0020, -0.0002,  ..., -0.0036,  0.0064,  0.0068],
        ...,
        [ 0.0159,  0.0292,  0.0212,  ..., -0.0107,  0.0032,  0.0105],
        [-0.0273,  0.0089,  0.0077,  ...,  0.0146,  0.0098, -0.0178],
        [-0.0322, -0.0238, -0.0454,  ...,  0.0139,  0.0128, -0.0075]])

llm.base_model.model.model.layers.6.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-2.1719e+00, -3.7891e-01,  8.9844e-01,  1.0498e-01, -2.5024e-02,
         1.1094e+00, -1.2266e+00, -2.6758e-01, -1.2422e+00,  8.9355e-02,
        -3.8281e-01,  7.1289e-02,  1.0840e-01, -2.3535e-01,  9.2773e-02,
         9.7168e-02, -3.0859e-01,  1.6113e-01, -1.3770e-01,  2.3071e-02,
         6.7383e-02,  4.0771e-02, -4.5508e-01,  6.5918e-02,  3.0078e-01,
         2.2168e-01, -3.1738e-02,  2.8442e-02,  1.2695e-01,  1.0010e-01,
         2.1875e-01,  4.4189e-02, -3.1982e-02,  7.1289e-02,  1.1353e-02,
        -1.9165e-02,  5.8838e-02, -7.5195e-02,  1.9141e-01,  9.3750e-02,
        -1.5625e-01,  7.4219e-02,  7.2754e-02, -1.2256e-01,  2.2217e-02,
        -5.0049e-02,  4.6387e-02,  8.7891e-02,  1.3379e-01,  1.6992e-01,
         1.1230e-01, -2.7954e-02,  3.5625e+00,  4.9072e-02, -1.6211e-01,
        -7.9590e-02, -9.2285e-02,  2.6245e-02, -8.8379e-02,  1.1621e-01,
         1.1035e-01, -9.9609e-02, -7.6172e-02,  1.1841e-02,  6.0547e-01,
        -1.4062e+00,  8.7500e-01,  2.3340e-01, -1.0547e-01,  3.7695e-01,
        -5.0781e-01, -1.3867e-01, -3.5742e-01,  1.7773e-01,  1.2812e+00,
        -1.5625e-01,  5.1270e-02,  1.0703e+00,  2.5781e-01, -4.7852e-02,
         3.8818e-02, -2.9297e-01,  1.5430e-01,  2.1680e-01, -1.1016e+00,
         1.2598e-01, -4.6143e-02, -2.6953e-01,  1.7334e-02,  1.8164e-01,
         3.1494e-02,  6.0059e-02,  2.1289e-01, -9.0332e-03, -1.7480e-01,
        -4.6631e-02,  1.8677e-02, -1.1475e-02,  5.1514e-02,  3.4668e-02,
        -2.1680e-01, -1.4941e-01,  7.3730e-02, -8.0078e-02,  7.9346e-03,
        -6.3965e-02, -3.0762e-02,  6.6895e-02, -1.4099e-02, -2.5391e-02,
         5.0781e-02, -1.1621e-01, -1.3770e-01, -1.3281e-01,  4.0588e-03,
         3.8086e-02, -1.2422e+00,  6.0791e-02,  6.5430e-02,  6.6406e-02,
         7.9590e-02, -2.9541e-02,  6.8848e-02,  2.5513e-02, -2.0752e-03,
        -1.2109e-01,  9.7168e-02,  1.4160e-01, -2.3438e-01,  5.9766e-01,
         4.1211e-01, -5.9326e-02,  5.0000e-01, -3.8477e-01, -4.2773e-01,
         5.4688e-01,  2.1289e-01, -6.8750e-01,  1.1035e-01, -3.5938e-01,
         6.3672e-01, -6.3477e-02,  1.9922e-01, -1.2695e-01,  6.4453e-02,
         1.6504e-01,  6.2988e-02, -5.0391e-01,  3.6133e-02,  1.2012e-01,
         1.2061e-01, -4.3164e-01,  2.3145e-01, -5.6641e-02,  2.6953e-01,
        -1.6016e-01, -2.0703e-01,  4.7363e-02,  8.3496e-02, -1.9727e-01,
         5.1025e-02, -1.0391e+00,  3.9648e-01, -5.1025e-02,  8.1055e-02,
        -2.6562e-01,  6.2256e-03, -1.8652e-01,  1.4258e-01,  1.1797e+00,
        -4.7461e-01, -2.2266e-01,  2.6172e-01,  3.5156e-01,  5.4443e-02,
        -3.6133e-01,  1.0156e+00,  5.1514e-02,  3.2227e-01,  3.0078e-01,
         2.7734e-01, -1.2207e-01,  4.5117e-01,  1.4258e-01,  1.3125e+00,
        -4.9375e+00, -3.8672e-01,  1.0078e+00, -1.2012e-01, -1.6797e+00,
        -5.6562e+00,  4.5625e+00,  5.1562e-01, -9.0332e-02, -3.6914e-01,
        -6.2500e-01, -2.2949e-01, -2.8906e-01, -4.0820e-01,  4.9219e-01,
         1.5430e-01, -4.8096e-02, -3.9453e-01, -2.9492e-01,  1.1133e-01,
        -2.9297e-01,  5.2185e-03, -8.2031e-01,  1.0449e-01, -5.6250e-01,
        -3.3398e-01,  6.9141e-01,  5.4932e-02,  6.2109e-01, -4.4531e-01,
        -9.1406e-01, -1.4746e-01, -1.2256e-01,  8.7891e-01,  4.1797e-01,
        -3.7305e-01, -1.1094e+00,  1.0303e-01, -1.3281e-01, -2.3340e-01,
        -8.5547e-01,  2.2559e-01,  1.4160e-01, -4.1211e-01, -3.3203e-01,
        -1.1953e+00, -2.2949e-01, -3.4424e-02, -1.0449e-01,  1.1597e-02,
        -1.2451e-01,  9.3750e-02,  3.3789e-01,  7.0801e-03,  1.7578e-01,
         2.8906e+00, -2.2363e-01, -5.1562e-01, -4.1748e-02, -1.1230e-01,
         3.2422e-01, -4.6094e-01, -4.7070e-01,  1.2188e+00,  1.9062e+00,
         1.0312e+00,  1.3516e+00, -4.4688e+00,  2.7656e+00, -2.0156e+00,
         1.0071e-02, -4.8047e-01,  1.1016e+00,  2.4609e-01,  8.8867e-02,
        -8.5449e-02,  1.0742e-01,  1.1035e-01, -1.1914e-01, -2.4414e-01,
        -4.1016e-02,  2.7734e-01, -4.4727e-01, -1.0010e-01, -2.2278e-03,
        -3.1836e-01,  4.4141e-01,  1.5527e-01, -2.8931e-02,  5.5176e-02,
         3.2422e-01, -1.2793e-01,  2.5469e+00,  1.4258e-01,  1.2756e-02,
        -3.0469e-01,  3.3203e-01, -4.9414e-01,  2.8281e+00,  1.7090e-01,
        -2.4414e-01,  2.3047e-01,  8.0566e-02,  3.6328e-01,  6.2109e-01,
         1.4258e-01, -3.1982e-02, -1.3550e-02,  1.6689e-05, -5.7422e-01,
         1.1572e-01,  4.6631e-02,  2.0020e-01,  3.0000e+00, -2.0312e-01,
         8.4375e-01,  2.8711e-01,  2.3633e-01,  4.7656e-01, -4.0625e-01,
         8.3984e-02,  8.2520e-02, -1.9238e-01,  3.8818e-02, -3.4570e-01,
         2.0020e-02, -7.8125e-01, -1.3770e-01, -6.2109e-01, -3.2031e-01,
        -1.4648e-01,  2.3242e-01, -8.5625e+00, -1.4375e+00, -5.4375e+00,
         2.4536e-02, -7.0801e-02, -1.2500e-01,  2.5879e-02,  5.3467e-02,
        -1.4062e+00, -6.6833e-03, -4.2969e-02,  2.6611e-02,  1.5859e+00,
         2.3340e-01,  1.1670e-01,  1.8516e+00,  1.7383e-01, -4.3701e-02,
         2.5977e-01, -1.3672e-01,  2.0938e+00, -2.0020e-01,  6.1328e-01,
        -9.1797e-02, -8.6328e-01, -6.6895e-02, -2.7734e-01, -7.8125e-01,
        -5.6250e-01, -7.6294e-03, -5.4688e-01, -4.3945e-03, -1.5723e-01,
         2.9492e-01, -2.2949e-01, -1.9336e-01,  2.9844e+00,  8.3984e-02,
         3.5938e-01, -1.8848e-01,  5.6641e-02, -1.1719e-02,  5.9326e-02,
        -2.0605e-01, -4.8096e-02, -8.5156e-01,  1.7188e-01,  2.0117e-01,
        -2.5781e-01, -1.7578e-01,  5.5078e-01, -7.7500e+00, -1.8359e-01,
         3.3594e-01,  5.8984e-01,  2.0508e-01, -5.7812e-01,  6.6406e-01,
        -1.5938e+00, -2.7930e-01, -1.4844e-01,  1.1250e+00,  6.1328e-01,
        -1.6328e+00,  1.6250e+01,  2.8750e+00, -7.3047e-01, -3.8477e-01,
        -4.1016e-02, -5.7373e-02, -1.1133e-01,  1.4355e-01,  6.4062e-01,
        -1.9165e-02,  5.6250e-01, -1.4531e+00, -1.8262e-01,  1.4453e-01,
        -5.3125e-01,  1.5078e+00,  2.0605e-01,  1.9727e-01, -7.7148e-02,
        -2.9492e-01,  5.1025e-02, -2.1191e-01, -4.8242e-01,  1.0498e-01,
         1.0254e-01,  2.4121e-01,  3.6377e-02, -2.7734e-01, -1.3125e+00,
        -3.4766e-01, -2.0215e-01, -9.9609e-02, -4.8584e-02, -3.4180e-01,
        -1.6797e-01, -1.3855e-02, -1.0303e-01, -1.1328e-01,  4.4727e-01,
         9.1309e-02, -8.5547e-01,  7.4158e-03,  2.1562e+00,  1.6602e-01,
         1.3672e-01,  4.4922e-01,  2.3281e+00,  1.3477e-01,  1.7285e-01,
         3.3379e-04,  3.7231e-03,  5.8594e-02, -7.1094e-01, -8.3984e-02,
        -4.5703e-01,  5.9570e-02, -2.2363e-01,  2.0996e-01,  2.8750e+00,
        -5.8594e-01,  3.0212e-03,  7.5684e-02, -1.1797e+00, -3.9258e-01,
        -1.5703e+00, -1.4648e-01,  5.3906e-01, -1.3828e+00,  7.8906e-01,
        -1.1875e+00, -9.3262e-02,  9.0625e-01,  3.8867e-01,  1.1641e+00,
         7.4707e-02, -1.2012e-01,  2.2852e-01, -9.1797e-02, -3.0273e-01,
        -8.0566e-02, -7.2754e-02,  4.6094e-01,  1.5469e+00, -4.9414e-01,
         1.5156e+00, -1.3733e-02, -1.2891e-01,  1.5820e-01,  1.8555e-01,
         1.4141e+00, -2.5977e-01, -1.0254e-01,  2.1289e-01,  1.2012e-01,
        -1.1914e-01, -3.5938e-01,  3.7842e-02,  5.5664e-02, -9.9609e-02,
         2.9883e-01,  4.4141e-01, -8.7891e-02,  4.1602e-01, -2.8442e-02,
         1.0312e+00,  1.6211e-01, -3.3008e-01, -1.8066e-01, -4.9072e-02,
         4.2773e-01,  5.7812e-01,  2.1191e-01, -1.1035e-01, -2.5586e-01,
        -1.5820e-01,  1.1768e-01,  6.8359e-01,  1.1621e-01,  2.5977e-01,
         1.6406e-01, -1.3379e-01, -4.1016e-01,  1.4688e+00, -6.6797e-01,
         2.7539e-01, -3.0469e-01,  3.6865e-02, -1.3516e+00,  7.8125e-02,
         5.3125e-01, -1.3438e+00])

llm.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0157, -0.0158, -0.0216,  ...,  0.0020,  0.0125,  0.0163],
        [ 0.0198, -0.0437, -0.0352,  ..., -0.0231,  0.0168, -0.0301],
        [ 0.0070,  0.0058,  0.0042,  ...,  0.0210, -0.0071, -0.0277],
        ...,
        [-0.0338, -0.0222,  0.0345,  ..., -0.0056, -0.0612,  0.0013],
        [-0.0141, -0.0172,  0.0158,  ..., -0.0074, -0.0199,  0.0315],
        [ 0.0020,  0.0097,  0.0152,  ..., -0.0233,  0.0024,  0.0412]])

llm.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0149,  0.0092,  0.0338,  ..., -0.0129, -0.0202, -0.0043],
        [-0.0789,  0.0763,  0.0523,  ..., -0.0358,  0.0773,  0.0795],
        [ 0.0159, -0.0378, -0.0420,  ...,  0.0046,  0.0044, -0.0057],
        ...,
        [-0.0379,  0.0098,  0.0008,  ..., -0.0347, -0.0225,  0.0476],
        [-0.0214,  0.0056,  0.0210,  ...,  0.0120,  0.0764,  0.0094],
        [-0.0108,  0.0114, -0.0334,  ..., -0.0169,  0.0337,  0.0392]])

llm.base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0082, -0.0129,  0.0171,  ..., -0.0067, -0.0181, -0.0210],
        [ 0.0135,  0.0087,  0.0164,  ...,  0.0002, -0.0085,  0.0087],
        [-0.0083, -0.0227, -0.0233,  ..., -0.0055, -0.0075, -0.0080],
        ...,
        [-0.0206, -0.0425,  0.0247,  ..., -0.0092,  0.0211, -0.0061],
        [ 0.0031,  0.0181,  0.0165,  ...,  0.0172, -0.0096, -0.0052],
        [-0.0025, -0.0090,  0.0070,  ..., -0.0046,  0.0132,  0.0050]])

llm.base_model.model.model.layers.6.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 2.5513e-02, -1.8921e-02, -3.9062e-03, -4.6875e-02, -3.2959e-03,
         2.1851e-02,  7.8125e-03,  7.7637e-02, -7.5684e-02, -6.7871e-02,
         3.0273e-02,  3.1128e-02,  2.6733e-02, -7.3242e-02,  1.8652e-01,
         1.5991e-02,  9.2285e-02,  4.1211e-01, -6.4941e-02,  5.5859e-01,
         8.8379e-02, -1.2891e-01, -2.1729e-02,  4.5654e-02, -1.1816e-01,
         6.5430e-02, -3.3447e-02, -2.4414e-02,  5.9204e-03,  1.1475e-02,
        -6.8665e-03, -2.0142e-03, -3.3691e-02,  3.8330e-02,  3.0762e-02,
         2.4292e-02, -1.7285e-01, -3.3936e-02,  5.3711e-02, -5.5420e-02,
        -1.0986e-01,  3.4424e-02, -2.5635e-02,  8.3008e-02, -9.2163e-03,
        -5.7617e-02,  1.1963e-01,  4.4861e-03, -1.5259e-02, -5.2979e-02,
         5.3101e-03, -8.9111e-03,  1.4355e-01, -5.4932e-02, -4.3945e-02,
        -2.0508e-02,  1.8215e-04,  7.0312e-02, -9.3994e-03,  6.6406e-02,
        -8.4473e-02, -1.9775e-02, -2.3071e-02,  1.3161e-04,  6.8359e-02,
        -2.9297e-01, -3.9368e-03, -1.3477e-01, -1.3770e-01, -7.6172e-02,
         3.4766e-01,  6.3965e-02,  1.3916e-02,  2.8809e-02, -4.1504e-02,
        -6.1279e-02,  5.7373e-03,  3.6865e-02, -6.7383e-02, -6.7444e-03,
         1.1768e-01,  4.9316e-02,  2.0996e-02, -1.0596e-01, -7.2266e-02,
        -5.8594e-02,  8.7402e-02, -7.5684e-03,  2.9297e-02, -2.5635e-02,
         1.6968e-02, -6.0547e-02, -4.9072e-02, -4.0527e-02,  2.6611e-02,
        -7.4768e-03, -1.0986e-01,  2.9663e-02, -4.3213e-02,  1.3428e-02,
         2.5635e-02, -2.4780e-02,  1.0889e-01, -1.5234e-01, -1.5259e-02,
        -3.7354e-02,  6.2500e-02,  2.8809e-02,  2.9419e-02, -2.8381e-03,
         2.7539e-01, -4.3701e-02, -5.1025e-02, -1.0986e-02,  2.2827e-02,
         9.2163e-03, -1.4038e-02, -7.3730e-02, -5.3467e-02, -1.0498e-01,
        -4.8584e-02, -4.0283e-02,  4.9072e-02, -1.1523e-01, -2.2949e-02,
         1.1523e-01, -5.7678e-03,  7.4219e-02,  6.2012e-02, -1.1621e-01,
         1.9653e-02,  7.3730e-02, -2.0630e-02,  1.1426e-01,  3.0212e-03,
         2.5879e-02,  3.1738e-02, -4.7119e-02,  1.3916e-02,  6.1279e-02,
        -2.9297e-02,  2.9663e-02, -6.3477e-02, -7.5684e-02,  1.4099e-02,
         6.7871e-02, -6.6833e-03, -9.0332e-02,  2.0508e-02,  3.8147e-04,
         6.1523e-02,  4.9609e-01,  4.6631e-02,  1.6479e-02, -1.0400e-01,
        -6.1035e-03, -3.9062e-02,  1.5869e-02, -6.9336e-02,  3.8818e-02,
         6.3477e-02,  7.7148e-02, -1.6968e-02, -7.7637e-02, -3.7598e-02,
        -3.7231e-03, -2.1118e-02,  4.7119e-02,  3.1128e-02, -1.7822e-02,
        -1.2695e-02, -1.1230e-02,  2.0630e-02, -5.1025e-02, -5.5542e-03,
        -1.1230e-01, -4.1504e-03, -1.1084e-01,  6.1340e-03,  5.4688e-02,
        -1.0596e-01,  1.2512e-02, -1.0620e-02,  1.1108e-02,  1.0315e-02,
         9.8877e-03, -2.0874e-02,  2.7008e-03,  4.7363e-02, -2.0752e-03,
         1.9379e-03, -1.8845e-03,  8.2397e-03,  4.8828e-02, -1.5564e-03,
         1.7090e-02, -6.7871e-02,  1.2500e-01, -3.4912e-02,  6.4453e-02,
        -7.9102e-02, -2.8687e-02, -5.0659e-03, -3.6865e-02,  5.0781e-02,
         3.4668e-02,  3.7354e-02, -1.1902e-02, -1.6479e-02,  2.0312e-01,
        -2.5024e-02,  5.8350e-02, -1.3428e-02, -4.6387e-02, -1.9409e-02,
         1.6602e-02, -3.2501e-03, -8.7280e-03,  3.1006e-02,  2.5330e-03,
         6.5918e-02, -1.9409e-02, -2.8931e-02,  5.6152e-02,  3.7354e-02,
         4.6631e-02,  7.1411e-03,  2.3315e-02, -1.3379e-01,  2.0508e-02,
        -6.9824e-02,  1.7822e-02,  1.2329e-02, -6.5918e-02, -8.1787e-03,
        -6.3477e-02, -1.0437e-02,  6.6895e-02, -2.9175e-02,  2.5391e-02,
        -8.0566e-02, -2.6367e-02, -9.2163e-03, -1.4648e-02,  1.1536e-02,
        -2.4170e-02, -1.5723e-01,  6.0547e-02,  1.2878e-02,  4.9805e-02,
         2.7710e-02, -2.5879e-02,  1.4062e-01,  4.4556e-03,  2.9907e-02,
         4.4556e-03, -1.2329e-02,  4.5166e-02, -1.7548e-03,  6.7383e-02,
        -7.5073e-03, -6.5918e-02,  1.8616e-03,  3.1982e-02,  4.5166e-02,
         2.2827e-02,  1.4648e-03,  2.2217e-02, -1.8799e-02, -5.6885e-02,
        -5.8289e-03,  2.7924e-03,  1.7822e-02,  8.3008e-03,  1.0864e-02,
        -6.8359e-02,  4.8218e-03, -2.8198e-02,  2.8076e-02, -1.6602e-02,
         1.5869e-02, -4.6143e-02,  7.1289e-02,  4.0771e-02, -4.0283e-02,
         4.4678e-02, -2.8198e-02,  1.9531e-03,  1.7090e-03,  1.6846e-02,
        -7.2327e-03, -3.4668e-02,  2.6001e-02, -4.1504e-03, -3.4912e-02,
         9.3994e-03,  7.4219e-02, -4.5654e-02,  4.0283e-02,  5.1270e-02,
         1.6479e-02,  4.0039e-01, -4.8218e-03,  1.2329e-02,  2.1240e-02,
         3.5400e-02,  2.0020e-02,  3.5156e-02, -3.3379e-04, -2.1484e-02,
         1.4160e-02,  2.0386e-02,  1.0620e-02, -5.3955e-02, -3.0640e-02,
         1.2451e-02,  3.9062e-02,  1.7242e-03, -4.3457e-02,  1.6174e-03,
        -8.8867e-02, -1.0059e-01, -1.7578e-02, -1.7700e-02, -4.0771e-02,
         2.1606e-02, -5.8594e-03, -2.8198e-02, -4.8828e-02, -1.7456e-02,
         1.2988e-01,  3.1494e-02, -5.2246e-02,  1.9043e-02, -1.7212e-02,
         6.6528e-03, -6.4087e-03,  1.1719e-01,  5.2979e-02,  1.8921e-02,
        -1.3672e-02,  2.0874e-02, -6.2561e-03, -3.6377e-02,  4.4434e-02,
         1.1230e-02, -1.5991e-02, -2.0264e-02, -7.3730e-02, -8.9355e-02,
        -6.0059e-02,  3.8330e-02,  4.2969e-02,  5.5664e-02, -1.0059e-01,
        -1.3916e-02, -2.8253e-05, -5.3024e-04, -1.5182e-03,  2.7832e-02,
         5.2490e-02,  2.1851e-02, -2.4414e-02,  8.9722e-03, -3.9307e-02,
         2.7466e-02,  7.4158e-03,  2.0020e-02, -1.2573e-02,  2.2697e-04,
        -4.6387e-02,  2.3315e-02,  4.7119e-02, -6.3477e-03, -5.5176e-02,
        -5.0964e-03,  2.5635e-02, -2.5391e-02, -4.8584e-02, -4.7119e-02,
        -2.3804e-02,  5.8594e-03,  3.4180e-02, -1.2390e-02,  3.8867e-01,
        -5.2002e-02, -6.2500e-02,  7.2937e-03, -1.2207e-02,  6.8848e-02,
        -1.7334e-02,  8.0078e-02,  4.2725e-02,  8.7402e-02,  6.6895e-02,
        -1.4648e-02, -6.7871e-02, -2.8931e-02, -2.4292e-02, -6.3477e-02,
         9.5215e-02,  5.1025e-02, -1.3574e-01,  3.9551e-02, -4.4434e-02,
         5.1880e-03, -2.5513e-02, -4.4678e-02,  7.9102e-02, -1.1035e-01,
        -1.1292e-02,  1.0132e-02, -9.0332e-03,  6.2988e-02,  5.0537e-02,
         9.0820e-02,  5.9570e-02,  1.6211e-01, -8.4961e-02,  3.3691e-02,
        -7.6660e-02, -4.3213e-02,  3.7598e-02, -2.1667e-03,  1.5137e-01,
         4.3945e-02,  4.5898e-02, -4.3213e-02,  2.8198e-02, -1.3477e-01,
        -6.4941e-02,  1.9897e-02,  2.0386e-02, -7.0801e-02, -2.5513e-02,
         2.1851e-02,  1.9165e-02,  7.3242e-02, -9.5825e-03, -1.3550e-02,
         1.1328e-01,  7.4768e-03,  3.7193e-04, -8.3984e-02, -1.0840e-01,
        -4.1008e-04, -5.6152e-02,  4.3701e-02, -2.5391e-02,  4.0039e-02,
        -9.8267e-03,  5.2734e-02,  1.4221e-02,  5.3223e-02, -8.3496e-02,
         3.7193e-04,  1.6968e-02, -2.1362e-03,  2.2430e-03, -2.4414e-02,
         5.1514e-02,  5.5664e-02,  4.2236e-02,  8.3008e-02, -1.3962e-03,
        -3.8574e-02,  5.4199e-02,  2.8931e-02, -2.7100e-02,  2.6978e-02,
        -2.0020e-02, -1.0132e-02,  5.2734e-02,  6.6406e-02,  2.3560e-02,
        -1.5234e-01,  5.3955e-02,  9.3262e-02,  1.4465e-02, -4.9316e-02,
         5.7617e-02, -4.8340e-02, -9.4604e-04, -4.1260e-02, -6.9824e-02,
         1.8555e-02,  1.2207e-02, -5.9570e-02, -5.2734e-02, -1.4062e-01,
        -9.6191e-02,  7.6904e-03,  4.9805e-02, -1.0864e-02,  4.4434e-02,
         3.3936e-02,  9.1797e-02, -8.2031e-02, -3.2715e-02, -2.3438e-02,
        -9.1309e-02, -6.3965e-02,  4.0771e-02,  4.9561e-02, -2.8687e-02,
        -1.3184e-01, -5.8105e-02,  1.7822e-02,  6.8359e-02, -1.2512e-02,
        -1.7944e-02, -5.7068e-03])

llm.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0165, -0.0342, -0.0111,  ..., -0.0298,  0.0529, -0.0116],
        [ 0.0042,  0.0048, -0.0197,  ...,  0.0695, -0.0492,  0.0288],
        [ 0.0109,  0.0010, -0.0117,  ...,  0.0033, -0.0195, -0.0095],
        ...,
        [-0.0014, -0.0217,  0.0393,  ...,  0.0216,  0.0440, -0.0235],
        [ 0.0082,  0.0347,  0.0099,  ...,  0.0033, -0.0187,  0.0008],
        [-0.0031, -0.0091,  0.0177,  ...,  0.0237,  0.0031, -0.0286]])

llm.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0087,  0.0319,  0.0064,  ...,  0.0328, -0.0025, -0.0233],
        [ 0.0170, -0.0338, -0.0042,  ...,  0.0448,  0.0031,  0.0026],
        [-0.0029,  0.0299,  0.0098,  ...,  0.0118, -0.0618,  0.0324],
        ...,
        [-0.0177, -0.0216, -0.0060,  ..., -0.0047,  0.0204, -0.0211],
        [ 0.0051, -0.0071,  0.0124,  ...,  0.0337,  0.0128,  0.0048],
        [-0.0020,  0.0089,  0.0476,  ..., -0.0237, -0.0012,  0.0196]])

llm.base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0300,  0.0154, -0.0070,  ..., -0.0074, -0.0171,  0.0004],
        [ 0.0039, -0.0002, -0.0114,  ..., -0.0049, -0.0115,  0.0129],
        [-0.0162,  0.0069, -0.0171,  ...,  0.0277,  0.0008,  0.0049],
        ...,
        [-0.0001,  0.0099,  0.0273,  ...,  0.0147,  0.0184, -0.0101],
        [-0.0010,  0.0092,  0.0127,  ..., -0.0031,  0.0091,  0.0015],
        [-0.0151, -0.0096,  0.0039,  ..., -0.0042,  0.0106,  0.0067]])

llm.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0002, -0.0237, -0.0437,  ...,  0.0269,  0.0031, -0.0402],
        [ 0.0150,  0.0123,  0.0270,  ...,  0.0166, -0.0173, -0.0157],
        [ 0.0089,  0.0381, -0.0247,  ...,  0.0325, -0.0107, -0.0131],
        ...,
        [-0.0366, -0.0663,  0.0331,  ..., -0.0133, -0.0328, -0.0280],
        [ 0.0044,  0.0161,  0.0034,  ...,  0.0037,  0.0140,  0.0118],
        [-0.0065, -0.0061, -0.0312,  ...,  0.0142, -0.0388, -0.0247]])

llm.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0034,  0.0188,  0.0236,  ...,  0.0016, -0.0138,  0.0083],
        [ 0.0837,  0.0096,  0.0266,  ...,  0.0191,  0.0056,  0.0270],
        [ 0.0025,  0.0065,  0.0292,  ..., -0.0050, -0.0046,  0.0216],
        ...,
        [-0.0079,  0.0044, -0.0220,  ...,  0.0075,  0.0363, -0.0122],
        [ 0.0261,  0.0195,  0.0044,  ..., -0.0230,  0.0036, -0.0049],
        [-0.0036, -0.0080,  0.0336,  ..., -0.0082,  0.0271,  0.0240]])

llm.base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 6.8970e-03, -3.1586e-03, -2.3071e-02,  ...,  9.9182e-05,
          1.2512e-02,  4.5204e-04],
        [-1.3184e-02,  1.0559e-02, -1.1169e-02,  ...,  3.9673e-03,
          1.3367e-02, -3.6240e-04],
        [-4.4250e-03,  2.0447e-03, -4.7302e-03,  ..., -3.8910e-03,
          2.2339e-02, -7.0190e-03],
        ...,
        [ 1.8082e-03,  6.0730e-03,  7.0953e-04,  ..., -1.1597e-02,
          8.1787e-03,  2.1362e-02],
        [ 1.4282e-02,  3.1738e-02,  1.7944e-02,  ...,  3.2501e-03,
         -2.1820e-03, -2.0142e-02],
        [-8.6670e-03, -3.3112e-03,  2.2278e-03,  ...,  1.7944e-02,
          1.3428e-02, -1.6602e-02]])

llm.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0231, -0.0250, -0.0178,  ..., -0.0485, -0.0135,  0.0265],
        [-0.0159,  0.0299,  0.0298,  ...,  0.0159, -0.0312,  0.0047],
        [ 0.0265,  0.0601,  0.0596,  ..., -0.0439,  0.0528,  0.0069],
        ...,
        [-0.0257,  0.0193, -0.0085,  ...,  0.0111, -0.0391,  0.0059],
        [ 0.0545, -0.0263, -0.0336,  ..., -0.0444, -0.0007,  0.0166],
        [-0.0274, -0.0043,  0.0165,  ...,  0.0145, -0.0133,  0.0515]])

llm.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0146,  0.0183,  0.0339,  ..., -0.0047,  0.0233, -0.0143],
        [-0.0078, -0.0208, -0.0439,  ..., -0.0009,  0.0162,  0.0008],
        [-0.0741, -0.0340, -0.0156,  ...,  0.0197,  0.0038,  0.0087],
        ...,
        [ 0.0246, -0.0153, -0.0443,  ..., -0.0281, -0.0347, -0.0611],
        [ 0.0089,  0.0230,  0.1074,  ...,  0.0075, -0.0187, -0.0093],
        [ 0.0294,  0.0231,  0.0380,  ...,  0.0621,  0.0231, -0.0101]])

llm.base_model.model.model.layers.6.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0159,  0.0234,  0.0193,  ...,  0.0190,  0.0071,  0.0152],
        [ 0.0125,  0.0031,  0.0066,  ...,  0.0160, -0.0168,  0.0096],
        [ 0.0140,  0.0033, -0.0242,  ..., -0.0157,  0.0064,  0.0040],
        ...,
        [ 0.0199, -0.0108,  0.0165,  ..., -0.0022,  0.0012, -0.0025],
        [ 0.0077, -0.0094,  0.0018,  ...,  0.0047,  0.0014,  0.0044],
        [-0.0043,  0.0076,  0.0245,  ...,  0.0251,  0.0052, -0.0233]])

llm.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0098, -0.0640, -0.0308,  ..., -0.0736,  0.0522, -0.0325],
        [ 0.0042,  0.0401, -0.0132,  ..., -0.0227,  0.0066, -0.0019],
        [ 0.0281, -0.0195, -0.0236,  ..., -0.0481,  0.0031,  0.0477],
        ...,
        [ 0.0796, -0.0268, -0.0136,  ..., -0.0124,  0.0226, -0.0166],
        [ 0.0016, -0.0153, -0.0020,  ...,  0.0048, -0.0012,  0.0198],
        [-0.0073,  0.0152, -0.0247,  ..., -0.0050,  0.0403, -0.0017]])

llm.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0290, -0.0097, -0.0076,  ..., -0.0047,  0.0130, -0.0293],
        [-0.0508, -0.0023,  0.0147,  ..., -0.0070,  0.0355,  0.0098],
        [-0.0282,  0.0003, -0.0152,  ...,  0.0134,  0.0197, -0.0232],
        ...,
        [ 0.0517, -0.0126,  0.0303,  ...,  0.0201,  0.0768, -0.0399],
        [-0.0024,  0.0129, -0.0245,  ..., -0.0158, -0.0137,  0.0161],
        [-0.0245, -0.0142, -0.0181,  ...,  0.0076, -0.0039,  0.0258]])

llm.base_model.model.model.layers.6.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[-6.6528e-03,  2.8320e-02, -5.2490e-03,  ...,  1.3123e-02,
         -8.2397e-04, -6.9275e-03],
        [ 2.7222e-02, -4.0588e-03,  9.8267e-03,  ..., -1.5991e-02,
          1.2146e-02, -1.9150e-03],
        [ 8.7280e-03,  1.2634e-02, -1.4893e-02,  ...,  1.1414e-02,
          8.9722e-03,  2.7222e-02],
        ...,
        [-1.4648e-02, -3.6163e-03, -2.9144e-03,  ..., -8.2397e-03,
         -1.3916e-02, -1.3062e-02],
        [ 1.0010e-02, -1.3428e-02, -5.4626e-03,  ..., -5.8594e-03,
          2.3499e-03, -1.1108e-02],
        [-7.2937e-03, -4.8399e-05, -8.3618e-03,  ..., -4.7302e-03,
         -2.8076e-02, -1.8433e-02]])

llm.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0473,  0.0490, -0.0051,  ...,  0.0305, -0.0071, -0.0136],
        [ 0.0257, -0.0375,  0.0040,  ..., -0.0242, -0.0101, -0.0487],
        [-0.0071,  0.0461,  0.0029,  ...,  0.0604, -0.0184,  0.0281],
        ...,
        [ 0.0117, -0.0179, -0.0020,  ..., -0.0255,  0.0191, -0.0072],
        [-0.0114,  0.0068,  0.0019,  ...,  0.0102, -0.0218,  0.0058],
        [-0.0228,  0.0518,  0.0049,  ..., -0.0312, -0.0535, -0.0129]])

llm.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0021, -0.0438,  0.0089,  ...,  0.0393,  0.0038, -0.0138],
        [ 0.0122,  0.0050,  0.0255,  ...,  0.0049,  0.0328, -0.0028],
        [ 0.0104, -0.0053, -0.0141,  ..., -0.0047, -0.0214,  0.0007],
        ...,
        [-0.0208,  0.0024,  0.0037,  ...,  0.0063, -0.0123, -0.0071],
        [ 0.0338,  0.0271, -0.0205,  ..., -0.0174, -0.0219,  0.0182],
        [ 0.0100, -0.0169, -0.0351,  ...,  0.0058, -0.0314,  0.0479]])

llm.base_model.model.model.layers.6.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.7109, 0.3477, 1.0391,  ..., 0.4824, 0.3340, 0.6055])

llm.base_model.model.model.layers.6.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.8750, 0.8516, 1.0391,  ..., 0.7031, 0.8164, 0.8906])

llm.base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0022, -0.0006,  0.0051,  ...,  0.0193, -0.0002,  0.0066],
        [-0.0044,  0.0165,  0.0117,  ...,  0.0025, -0.0093, -0.0112],
        [ 0.0077, -0.0103, -0.0139,  ...,  0.0017,  0.0073, -0.0182],
        ...,
        [ 0.0042,  0.0126,  0.0093,  ...,  0.0168, -0.0002,  0.0095],
        [-0.0146, -0.0157,  0.0240,  ...,  0.0082, -0.0059,  0.0248],
        [-0.0074,  0.0025, -0.0240,  ..., -0.0146,  0.0122, -0.0098]])

llm.base_model.model.model.layers.7.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([-0.4844,  0.2344, -0.2930,  ..., -0.0449, -0.1934, -3.2656])

llm.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0533,  0.0140,  0.0170,  ..., -0.0035,  0.0331, -0.0035],
        [-0.0161,  0.0146, -0.0105,  ..., -0.0110,  0.0117, -0.0079],
        [ 0.0335, -0.0006, -0.0452,  ...,  0.0147,  0.0084,  0.0539],
        ...,
        [-0.0619, -0.0445,  0.0206,  ...,  0.0094, -0.0318, -0.0011],
        [ 0.0038,  0.0017,  0.0167,  ...,  0.0202,  0.0093,  0.0323],
        [-0.0073, -0.0077,  0.0168,  ..., -0.0149, -0.0061, -0.0289]])

llm.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0004,  0.0052,  0.0334,  ...,  0.0363,  0.0593, -0.0068],
        [ 0.0195, -0.0145, -0.0173,  ..., -0.0050, -0.0418, -0.0308],
        [ 0.0139,  0.0070, -0.0113,  ..., -0.0012, -0.0310, -0.0316],
        ...,
        [-0.0012, -0.0284, -0.0188,  ..., -0.0189, -0.0335,  0.0070],
        [ 0.0097,  0.0165, -0.0149,  ..., -0.0021, -0.0241, -0.0218],
        [ 0.0014,  0.0253, -0.0075,  ..., -0.0056,  0.0153, -0.0572]])

llm.base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0049,  0.0121, -0.0074,  ..., -0.0164,  0.0063,  0.0004],
        [-0.0026, -0.0061, -0.0018,  ..., -0.0096, -0.0036,  0.0085],
        [-0.0076,  0.0134,  0.0148,  ...,  0.0064,  0.0031,  0.0198],
        ...,
        [-0.0132,  0.0030, -0.0199,  ...,  0.0107, -0.0115,  0.0060],
        [ 0.0020, -0.0002,  0.0036,  ...,  0.0020,  0.0006,  0.0145],
        [-0.0038, -0.0145,  0.0049,  ...,  0.0298,  0.0481,  0.0048]])

llm.base_model.model.model.layers.7.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 1.2578e+00,  2.6562e-01, -3.1641e-01, -7.4219e-02, -5.4688e-02,
        -5.5078e-01, -2.2461e-01,  1.3359e+00, -1.1475e-01, -4.9414e-01,
         1.4648e-01,  1.0205e-01,  1.3184e-01, -3.3594e-01, -2.7344e-01,
         1.3867e-01, -6.9824e-02, -2.0625e+00,  1.2158e-01, -1.0498e-01,
        -1.4160e-01,  2.1719e+00,  2.1973e-02,  3.7354e-02,  2.8687e-02,
         2.2188e+00,  1.3574e-01,  2.4121e-01,  5.8984e-01, -5.5237e-03,
        -2.0781e+00, -5.0293e-02,  2.1094e-01, -1.5156e+00, -2.7930e-01,
        -1.4648e-01,  7.8516e-01, -2.8320e-01,  8.5938e-02,  2.6367e-01,
        -1.9922e-01,  6.2500e-02,  3.9844e-01,  2.7344e-01,  2.4609e-01,
        -1.5625e-01, -2.3804e-02,  2.4780e-02, -1.0693e-01,  1.1621e-01,
        -9.6436e-03, -1.6357e-02,  3.3936e-02,  2.6953e-01, -2.0781e+00,
        -2.3242e-01,  1.0840e-01,  1.4587e-02, -1.4160e-01,  2.9541e-02,
         6.9922e-01, -2.2949e-01,  3.3398e-01, -2.4609e-01,  2.8516e-01,
         5.3467e-02, -1.1169e-02,  1.1953e+00, -3.4570e-01, -5.3223e-02,
         2.9419e-02, -1.1914e-01,  1.5332e-01,  5.6641e-02,  1.3203e+00,
         8.9355e-02,  2.8516e-01, -1.7812e+00, -4.3335e-03, -9.7168e-02,
        -1.9409e-02,  3.9648e-01,  1.2793e-01,  6.3281e-01, -1.7285e-01,
        -2.8125e-01,  6.3672e-01,  1.1133e-01,  1.0059e-01, -2.9492e-01,
        -1.9287e-02,  1.0107e-01,  1.4648e-01, -3.1982e-02,  4.1406e-01,
         5.0537e-02, -1.4160e-01,  3.1641e-01, -5.2734e-02, -2.5391e-01,
        -9.6680e-02, -4.1016e-02,  1.4688e+00, -6.1035e-02, -3.7354e-02,
         1.1658e-02, -5.2002e-02,  7.3438e-01, -1.1816e-01,  2.2168e-01,
        -2.3438e-01,  5.6396e-02,  1.3770e-01,  2.1777e-01, -7.6172e-02,
         2.8442e-02,  1.5918e-01,  2.3828e-01, -6.1562e+00, -1.9824e-01,
        -1.4258e-01,  8.5449e-02, -2.6367e-02, -3.3936e-02, -1.2656e+00,
        -8.9844e-02,  2.4414e-01, -1.5723e-01, -7.0801e-02,  3.8086e-02,
        -5.7812e-01, -3.8281e-01,  2.3047e-01, -3.7305e-01,  1.9922e-01,
         7.3730e-02, -3.0664e-01,  1.2734e+00,  3.9307e-02, -1.5137e-01,
        -2.6172e-01, -1.4746e-01,  2.0117e-01,  2.6562e-01, -1.3867e-01,
         5.7129e-02,  1.4297e+00,  1.6602e-01,  1.0303e-01,  1.8750e-01,
        -2.2559e-01,  8.5449e-02, -8.5449e-02,  6.7139e-03, -2.7344e-01,
         1.0303e-01,  3.2227e-01,  7.5684e-02,  1.9922e-01, -9.3262e-02,
        -5.3906e-01, -1.0681e-02, -1.5312e+00, -1.7676e-01, -6.6895e-02,
        -2.0874e-02,  1.1377e-01,  4.7852e-02, -4.5508e-01,  3.3447e-02,
        -2.5977e-01,  1.3770e-01, -3.4570e-01,  4.3701e-02,  8.4961e-02,
        -8.9111e-03,  1.7090e-01,  9.6191e-02,  7.2754e-02, -2.1406e+00,
        -1.8164e-01, -2.5977e-01, -5.7373e-02, -7.2937e-03,  7.8906e-01,
        -5.4297e-01, -7.2632e-03, -3.3594e-01, -4.3945e-02, -1.2188e+00,
        -4.6094e-01, -2.3047e-01,  1.0547e+00, -6.6797e-01, -2.6367e-01,
         8.7891e-01,  5.6641e-01,  2.2754e-01,  1.0703e+00,  3.5938e-01,
         2.6367e-01, -8.3496e-02, -8.7891e-02, -2.8809e-02, -1.4141e+00,
         1.4160e-01, -3.3398e-01,  1.4531e+00,  6.2500e-02, -4.9438e-03,
        -3.4766e-01,  8.5938e-02, -4.8584e-02,  1.5859e+00,  2.6562e-01,
        -8.8867e-02,  1.6504e-01, -2.0898e-01, -1.4844e-01,  1.4648e-01,
         1.7578e+00, -1.7383e-01,  6.4062e-01,  5.0781e-02,  1.4258e-01,
         1.9922e-01,  5.9375e-01, -1.7578e-01, -5.4688e-01, -3.4375e-01,
         6.7188e-01,  4.0234e-01, -2.1973e-01, -3.1641e-01, -2.1484e-02,
        -5.2734e-02, -6.7969e-01, -3.5352e-01,  8.1787e-03, -1.7676e-01,
         4.0283e-02,  3.5156e-02, -1.8555e-01, -4.9375e+00,  1.3281e-01,
        -1.2354e-01, -2.9297e-01,  2.2217e-02,  5.1953e-01,  5.0391e-01,
         1.7285e-01,  2.0996e-01,  4.0625e-01, -2.4062e+00,  8.7109e-01,
        -2.0898e-01,  1.0938e+00,  4.6387e-03, -1.0625e+00, -1.7578e-01,
         9.2578e-01,  5.6641e-02,  1.3086e-01,  4.6082e-03,  9.1309e-02,
         1.7822e-02,  4.0039e-01,  1.1475e-01,  3.4375e-01, -1.3062e-02,
         1.4297e+00,  1.6504e-01, -1.3867e-01,  3.3447e-02,  1.5781e+00,
         4.1809e-03,  2.6562e-01,  7.6172e-01,  7.1777e-02, -1.6797e-01,
        -1.8047e+00, -2.2583e-02,  1.9922e-01, -2.9492e-01,  3.3447e-02,
        -1.4746e-01,  1.8125e+00,  1.7395e-03,  2.3633e-01,  5.7861e-02,
         2.8906e-01,  1.8359e-01,  2.1875e+00,  3.3691e-02, -3.0273e-01,
         2.9492e-01,  1.0596e-01, -2.0508e-01,  1.8750e-01,  1.8359e-01,
         8.0566e-02,  1.8984e+00, -8.6914e-02,  4.5654e-02,  1.8921e-02,
         9.2773e-02,  4.2236e-02,  7.4219e-02, -1.4844e-01,  4.3750e-01,
        -6.5625e+00,  3.4668e-02, -5.6250e-01, -3.6523e-01, -2.0020e-01,
         4.1406e-01, -5.1270e-02, -6.9141e-01, -8.1641e-01,  3.0312e+00,
         5.5420e-02, -2.5977e-01, -3.8086e-01,  1.2061e-01, -4.9316e-02,
         1.8945e-01, -6.2891e-01, -1.6602e-01, -1.2031e+00,  1.3281e-01,
        -1.2402e-01, -9.2578e-01,  1.9824e-01,  1.2500e-01, -1.6797e-01,
        -2.8687e-02, -2.2656e-01, -1.6211e-01, -2.5781e-01, -1.9824e-01,
         5.1270e-02, -1.8555e-01, -2.9102e-01,  1.4258e-01,  4.1016e-01,
        -1.3306e-02,  2.0996e-02, -1.0312e+00,  5.9570e-02,  2.8320e-01,
        -7.3438e-01, -9.1309e-02, -1.1182e-01,  3.4424e-02,  3.7891e-01,
         2.1973e-01, -5.6250e-01,  7.7820e-03, -6.4453e-02, -4.8828e-02,
        -1.5820e-01, -2.2070e-01,  1.5039e-01,  2.0117e-01,  7.4707e-02,
        -2.9541e-02,  1.3086e-01, -5.5176e-02,  2.5195e-01, -8.9355e-02,
        -6.2109e-01,  1.7700e-02,  6.5430e-02, -4.1211e-01,  3.0156e+00,
         1.6602e-01, -3.0859e-01,  8.7891e-01,  4.5117e-01, -9.5703e-01,
        -1.6250e+00, -3.7500e-01, -4.4336e-01, -6.9141e-01,  1.3281e-01,
        -7.5000e-01, -8.3984e-01,  1.5625e-01, -9.4922e-01,  8.8867e-02,
        -2.7344e-01, -1.2266e+00, -7.3828e-01,  3.7500e-01, -1.0889e-01,
        -1.4844e-01, -2.1362e-03, -5.4688e-01, -4.9805e-01, -5.8203e-01,
         2.5635e-02,  2.5781e-01, -7.1289e-02, -6.4062e-01, -6.5625e-01,
        -1.0156e+00,  1.9336e-01,  5.3467e-02,  6.7188e-01,  4.0820e-01,
         2.8906e-01, -1.3750e+00, -5.7373e-02,  4.5410e-02,  2.3535e-01,
        -7.3547e-03,  2.5781e-01,  8.9844e-01, -5.3906e-01,  1.8433e-02,
        -3.3203e-01, -3.0518e-03,  5.5469e-01, -4.4189e-02, -1.4355e-01,
         2.6367e-01, -3.9648e-01,  4.1211e-01,  7.2266e-02, -4.5312e-01,
         3.4668e-02, -1.8262e-01,  3.8281e-01, -6.1768e-02, -5.2812e+00,
        -4.6484e-01, -3.5547e-01, -8.1543e-02, -2.0996e-01, -6.0156e-01,
         4.0430e-01, -1.8652e-01, -1.4844e+00, -7.2812e+00,  2.6094e+00,
         2.4375e+00, -4.3750e+00, -5.0938e+00, -1.1094e+00,  7.3438e-01,
         4.5508e-01, -3.8086e-02,  8.9062e-01, -7.5781e-01,  1.4453e-01,
        -3.2227e-01, -3.9453e-01,  1.6113e-02,  1.4766e+00, -2.8125e-01,
        -5.8984e-01, -1.6250e+00, -5.7678e-03,  1.2188e+00,  1.0391e+00,
        -5.9375e-01,  9.5215e-02, -1.5703e+00, -8.0566e-03,  1.1621e-01,
         4.3555e-01,  7.3828e-01,  3.4424e-02,  1.0938e+00,  9.1309e-02,
         6.5625e-01, -5.8984e-01, -5.1514e-02,  1.1172e+00, -3.4766e-01,
        -4.1602e-01,  1.4297e+00,  3.8574e-02, -1.2891e-01,  3.5156e-01,
         3.6523e-01,  1.1875e+00, -1.4355e-01,  3.7891e-01,  1.5391e+00,
         2.4805e-01, -3.9062e-01,  9.1016e-01,  1.1768e-01, -2.5977e-01,
        -2.1680e-01,  1.5332e-01,  6.0303e-02,  3.4219e+00,  9.7168e-02,
         3.4180e-01,  4.8584e-02, -3.4766e-01,  1.2988e-01, -8.7109e-01,
        -2.9883e-01,  2.4219e+00,  5.6250e+00, -9.5000e+00, -3.8125e+00,
         5.8750e+00, -5.6250e+00])

llm.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0024, -0.0391,  0.0433,  ..., -0.0047,  0.0380,  0.0209],
        [ 0.0258,  0.0483, -0.0071,  ...,  0.0264,  0.0556, -0.0222],
        [-0.0012,  0.0866,  0.0462,  ...,  0.0396,  0.0377,  0.0057],
        ...,
        [ 0.0030,  0.0001,  0.0304,  ...,  0.0178,  0.0242, -0.0198],
        [-0.0488,  0.0219,  0.0196,  ..., -0.0016,  0.0094,  0.0087],
        [ 0.0137,  0.0355,  0.0185,  ...,  0.0453, -0.0626,  0.0365]])

llm.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0742,  0.0252,  0.0508,  ..., -0.0112, -0.0181,  0.0420],
        [-0.0014,  0.0092, -0.0194,  ..., -0.0313, -0.0315,  0.0006],
        [ 0.0409, -0.0098, -0.0187,  ...,  0.0086, -0.0018,  0.0188],
        ...,
        [-0.0557, -0.0191,  0.0038,  ...,  0.0191,  0.0167,  0.0102],
        [-0.0342,  0.0150,  0.0433,  ...,  0.0175, -0.0090,  0.0527],
        [ 0.0454,  0.0237, -0.0136,  ...,  0.0201, -0.0130,  0.0032]])

llm.base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0300,  0.0078,  0.0108,  ...,  0.0071, -0.0107, -0.0151],
        [-0.0189,  0.0089,  0.0081,  ..., -0.0013,  0.0105,  0.0067],
        [-0.0157, -0.0057,  0.0036,  ..., -0.0098,  0.0193, -0.0161],
        ...,
        [ 0.0007, -0.0031,  0.0204,  ..., -0.0095,  0.0061,  0.0078],
        [ 0.0149, -0.0005,  0.0005,  ..., -0.0449, -0.0058,  0.0197],
        [-0.0161,  0.0145, -0.0289,  ...,  0.0129,  0.0269,  0.0097]])

llm.base_model.model.model.layers.7.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-1.0193e-02,  7.5195e-02, -3.1982e-02, -7.1289e-02,  4.5166e-02,
        -8.7280e-03,  4.1260e-02, -3.1250e-02,  4.7363e-02, -6.2988e-02,
         6.1035e-02, -4.8340e-02, -3.1586e-03, -8.8867e-02,  2.1582e-01,
        -1.8433e-02, -8.6426e-02, -4.0527e-02, -4.3869e-04, -6.0791e-02,
         6.2012e-02, -2.2827e-02, -2.9053e-02, -9.2773e-03,  8.2520e-02,
         3.0212e-03,  7.3853e-03,  3.1006e-02, -6.4941e-02,  7.3730e-02,
        -6.8848e-02,  1.1902e-02,  5.4932e-02,  3.0396e-02,  1.1963e-02,
         5.4932e-02,  5.6396e-02,  9.3994e-03, -1.7548e-03, -5.2246e-02,
        -1.5320e-02, -1.7822e-02,  1.0071e-02, -2.9297e-02,  5.1270e-02,
        -6.3171e-03,  1.5991e-02,  4.3701e-02,  5.1117e-04,  1.3367e-02,
         4.7363e-02,  8.9844e-02, -1.0925e-02, -6.2561e-03, -9.5215e-02,
         2.3926e-02,  8.3496e-02,  2.8687e-02,  7.5684e-02,  6.0303e-02,
         1.0132e-02, -7.2266e-02,  4.2419e-03,  4.3945e-02, -4.8828e-02,
        -1.5869e-02, -5.4443e-02, -3.8574e-02, -1.3794e-02, -2.2656e-01,
        -3.4912e-02,  7.3730e-02, -6.8665e-04, -6.9824e-02,  1.7212e-02,
         5.8838e-02,  1.1444e-03,  1.9653e-02,  2.0508e-02,  1.7090e-02,
        -2.8320e-02, -4.4678e-02,  2.3682e-02,  3.7109e-02,  4.3457e-02,
        -2.0142e-03,  6.1035e-02,  5.1270e-02, -1.5564e-02,  1.1230e-02,
         2.9419e-02, -2.1729e-02, -4.1016e-02, -9.3384e-03, -1.1426e-01,
         8.9844e-02, -3.5156e-02, -1.9897e-02, -9.7656e-03,  7.6172e-02,
        -3.6621e-03, -5.8899e-03, -4.5166e-02, -7.9590e-02, -1.0071e-02,
         4.6387e-02, -2.0142e-02,  6.9336e-02, -7.1289e-02, -7.5378e-03,
         8.3008e-02, -8.5449e-02, -2.6978e-02,  1.2939e-02,  1.8188e-02,
         3.1738e-02,  4.8584e-02,  2.9175e-02,  1.4343e-02,  5.7068e-03,
        -5.9814e-02, -1.0193e-02, -1.9043e-02, -3.0396e-02,  1.2451e-02,
         6.8665e-03, -2.3193e-02,  9.5215e-03, -5.9082e-02, -1.1414e-02,
         1.1230e-01, -3.8757e-03, -2.6123e-02, -1.4771e-02, -6.4941e-02,
         1.6968e-02, -8.4686e-04,  3.2227e-02,  3.4332e-03, -1.5381e-02,
         1.2695e-02, -2.8198e-02, -3.6774e-03,  4.0527e-02, -1.0376e-02,
        -3.9795e-02, -6.3324e-04, -1.5137e-02,  2.6855e-02,  1.0400e-01,
        -2.6001e-02, -2.3346e-03, -9.5215e-03, -5.7617e-02, -9.2163e-03,
        -5.8594e-03,  2.1851e-02, -2.3071e-02, -5.2734e-02,  3.6621e-02,
        -1.1658e-02,  1.7285e-01, -7.9956e-03,  1.6724e-02,  3.5400e-03,
        -6.4941e-02,  3.7842e-02, -3.4180e-02, -2.5513e-02,  3.0518e-02,
        -3.1494e-02, -2.5635e-02, -4.1504e-02,  1.7944e-02,  9.7275e-05,
        -4.0527e-02,  1.3794e-02,  5.9082e-02, -2.3438e-02, -1.3977e-02,
        -1.8677e-02, -2.7313e-03, -1.0925e-02, -1.7700e-02,  2.8320e-02,
        -2.7954e-02,  3.9307e-02,  1.7383e-01, -2.2278e-03, -2.7100e-02,
         1.1902e-02, -2.3560e-02,  1.0742e-02, -3.3203e-02, -2.4414e-03,
        -1.7822e-02,  1.8311e-02,  3.7354e-02,  2.5879e-02, -1.6504e-01,
        -2.1973e-02,  2.7710e-02,  4.6387e-02,  2.6978e-02, -3.9307e-02,
        -1.7822e-02, -3.7842e-02,  1.1963e-02, -2.6245e-03, -9.1797e-02,
        -2.9907e-02,  2.8381e-03, -1.7700e-02, -1.2207e-02,  2.3193e-02,
         9.8267e-03, -9.6436e-03, -3.3112e-03, -2.0630e-02, -2.6489e-02,
        -1.5625e-01, -8.8867e-02,  4.8584e-02,  3.0029e-02,  3.3875e-03,
         8.5449e-03,  1.9653e-02,  7.6294e-03,  4.4250e-03,  2.6093e-03,
        -1.4221e-02, -3.6133e-02,  2.1851e-02,  2.9663e-02, -2.4414e-03,
         1.4709e-02,  1.7014e-03,  2.6855e-02, -2.1667e-03,  4.1504e-02,
        -4.0771e-02, -1.5747e-02,  1.1780e-02,  1.7578e-02,  1.3184e-02,
         3.1250e-02, -2.3651e-03, -2.1362e-02,  8.8501e-03, -1.8555e-02,
         3.2959e-03,  1.3184e-02,  2.1851e-02, -1.5259e-02, -1.5564e-02,
         3.5645e-02, -9.0942e-03,  3.7842e-02,  2.6855e-02,  7.5195e-02,
         2.8198e-02, -3.4912e-02,  3.7354e-02,  1.6479e-02, -2.1851e-02,
        -2.3804e-02,  9.6130e-04, -4.8096e-02,  5.7373e-02,  2.9785e-02,
         3.5095e-03,  1.0986e-02,  6.5430e-02,  1.6968e-02, -2.0264e-02,
         1.1169e-02, -6.0547e-02, -3.4180e-02,  5.1758e-02, -3.7598e-02,
         5.9326e-02,  2.7466e-02,  5.3406e-04, -5.8594e-03, -7.7148e-02,
         2.4780e-02, -8.9844e-02,  4.6082e-03,  1.1475e-02,  3.2959e-03,
        -4.2725e-04, -5.2795e-03, -1.7776e-03, -3.2959e-02, -3.7354e-02,
        -5.2795e-03,  1.1536e-02,  1.0400e-01,  4.6875e-02, -1.5869e-02,
         3.5645e-02, -4.0771e-02, -1.8555e-02, -2.9053e-02, -2.5269e-02,
        -1.7212e-02, -1.1292e-02,  2.9449e-03, -5.2246e-02,  3.8330e-02,
         2.9419e-02,  5.5664e-02, -6.8054e-03, -3.6621e-02,  7.4707e-02,
        -2.8687e-02,  1.3245e-02, -5.7129e-02, -1.8433e-02,  1.5335e-03,
        -9.3262e-02, -2.4780e-02, -3.6621e-02, -4.2969e-02,  4.6143e-02,
        -5.1270e-03, -9.2773e-03,  1.0254e-01,  1.0315e-02, -8.5449e-03,
        -7.4463e-03, -2.6001e-02,  1.1963e-02, -6.0059e-02,  7.2266e-02,
        -4.1504e-02,  3.0273e-02,  7.1875e-01, -3.1250e-02,  8.4961e-02,
        -3.1128e-03, -3.2227e-02,  5.2734e-02,  3.2806e-03,  1.6357e-02,
        -3.2471e-02,  2.9175e-02,  3.9795e-02,  1.5430e-01, -4.7852e-02,
        -9.0942e-03,  1.5503e-02,  3.3264e-03, -4.8828e-02, -3.3691e-02,
         1.4648e-02,  1.2878e-02, -6.4392e-03,  2.0020e-02, -1.2085e-02,
         3.6621e-04,  1.2024e-02, -7.2327e-03,  1.3550e-02, -1.4038e-02,
        -1.5564e-03,  6.9336e-02,  1.5381e-02, -2.9297e-02, -1.6846e-02,
        -4.4434e-02,  2.8809e-02, -4.5654e-02, -7.2266e-02,  1.7700e-02,
         3.4912e-02, -4.1992e-02, -1.9043e-02,  7.2754e-02,  5.2734e-02,
        -4.8340e-02,  4.8828e-02, -1.3367e-02, -1.3794e-02, -2.5513e-02,
         2.4170e-02, -1.8677e-02, -2.0020e-02, -1.0205e-01,  2.1729e-02,
        -4.1809e-03, -2.5635e-02,  2.9755e-03,  1.9409e-02,  1.4282e-02,
        -1.3977e-02,  4.5410e-02, -4.6387e-02,  3.1006e-02, -9.5703e-02,
         6.9275e-03, -1.9836e-03,  1.8433e-02, -1.6846e-02, -5.8899e-03,
        -1.6724e-02, -5.5542e-03, -8.4839e-03,  2.5024e-02, -3.7354e-02,
        -2.3682e-02, -1.2268e-02,  8.0566e-03, -7.7438e-04, -3.3936e-02,
         2.6367e-02, -1.5869e-02, -1.6708e-03, -1.0452e-03, -3.5400e-02,
        -5.0354e-04, -1.5015e-02, -2.8687e-03, -3.3691e-02, -2.0874e-02,
         5.1575e-03, -2.0752e-02, -3.9062e-03,  2.4872e-03,  5.3101e-03,
         4.5410e-02,  2.5146e-02, -3.1128e-02,  4.9316e-02, -1.7334e-02,
        -1.9989e-03,  1.4099e-02, -1.9775e-02,  3.7842e-02, -3.0975e-03,
         3.5400e-03,  4.0527e-02, -3.9551e-02, -4.3945e-02,  3.2501e-03,
        -4.6387e-02,  8.3618e-03, -1.8997e-03, -4.2236e-02, -4.9072e-02,
        -2.1973e-02, -1.2939e-02,  1.7578e-02,  2.2827e-02,  1.6479e-02,
        -3.4424e-02,  6.8665e-03,  3.5645e-02,  4.0039e-02, -6.0120e-03,
         1.9043e-02, -6.3965e-02, -6.0425e-03, -6.0547e-02,  5.3955e-02,
        -6.9336e-02,  2.6245e-03,  1.0437e-02,  3.6865e-02,  6.0120e-03,
         8.3008e-03, -2.0996e-02,  1.9897e-02, -1.6602e-02, -3.2227e-02,
        -6.8054e-03, -8.9844e-02,  3.0762e-02,  1.3672e-02,  6.3171e-03,
         2.4414e-02, -1.4221e-02,  3.0151e-02, -3.4485e-03,  5.8350e-02,
        -2.3071e-02,  2.9297e-03,  8.7280e-03,  1.6113e-02,  1.3000e-02,
        -6.3782e-03, -3.3691e-02,  9.4604e-03, -6.5430e-02,  1.6235e-02,
        -4.3701e-02,  9.2163e-03, -1.2634e-02, -1.1475e-02, -1.1414e-02,
         6.4392e-03, -8.7891e-03, -9.4238e-02, -2.2827e-02,  3.2654e-03,
         8.4229e-03, -7.1335e-04, -2.5146e-02,  5.6641e-02, -5.5695e-04,
         2.3804e-03, -4.4189e-02])

llm.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0113,  0.0142,  0.0091,  ..., -0.0257, -0.0051,  0.0438],
        [-0.0649, -0.0045,  0.0369,  ...,  0.0170,  0.0332, -0.0218],
        [-0.0157,  0.0109,  0.0122,  ..., -0.0056, -0.0135, -0.0391],
        ...,
        [-0.0019,  0.0042, -0.0246,  ..., -0.0024, -0.0217,  0.0031],
        [ 0.0540, -0.0417, -0.0103,  ..., -0.0494,  0.0188,  0.0235],
        [-0.0047, -0.0171, -0.0053,  ..., -0.0025, -0.0412, -0.0096]])

llm.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0202,  0.0339, -0.0878,  ..., -0.0076,  0.0353, -0.0418],
        [-0.0031, -0.0035,  0.0077,  ...,  0.0010, -0.0003, -0.0353],
        [ 0.0095,  0.0218,  0.0348,  ...,  0.0158, -0.0102,  0.0496],
        ...,
        [ 0.0273, -0.0154, -0.0353,  ...,  0.0042,  0.0055, -0.0054],
        [-0.0057, -0.0124,  0.0015,  ..., -0.0117, -0.0210, -0.0377],
        [-0.0094, -0.0428, -0.0011,  ...,  0.0043, -0.0210,  0.0110]])

llm.base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 1.8616e-03, -1.7700e-02,  4.0894e-03,  ...,  7.0572e-05,
         -5.4016e-03,  2.9541e-02],
        [-6.7749e-03, -2.5513e-02, -5.9814e-03,  ..., -1.3123e-02,
          1.0986e-02, -7.5989e-03],
        [-9.2163e-03,  2.5024e-02, -2.1973e-02,  ..., -1.0864e-02,
          8.3008e-03,  2.6978e-02],
        ...,
        [ 4.0588e-03, -4.8523e-03,  4.9133e-03,  ...,  9.5215e-03,
          3.6133e-02, -1.3489e-02],
        [ 4.5776e-03, -4.9744e-03, -6.6528e-03,  ..., -5.4321e-03,
         -4.3030e-03,  1.2085e-02],
        [ 5.8594e-03,  1.1063e-03, -2.3560e-02,  ...,  3.2654e-03,
         -1.7822e-02, -1.4771e-02]])

llm.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0271, -0.0053,  0.0362,  ...,  0.0017,  0.0022,  0.0117],
        [ 0.0261, -0.0111,  0.0102,  ..., -0.0098, -0.0627,  0.0245],
        [-0.0226,  0.0278, -0.0215,  ..., -0.0142,  0.0242,  0.0024],
        ...,
        [ 0.0237,  0.0058,  0.0339,  ...,  0.0167,  0.0076, -0.0113],
        [-0.0115,  0.0257, -0.0380,  ...,  0.0154, -0.0115, -0.0330],
        [ 0.0082,  0.0528,  0.0057,  ...,  0.0204,  0.0307,  0.0163]])

llm.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0050,  0.0289, -0.0193,  ..., -0.0061,  0.0431, -0.0076],
        [ 0.0156, -0.0307, -0.0349,  ..., -0.0159, -0.0161, -0.0137],
        [-0.0126,  0.0269,  0.0144,  ...,  0.0463,  0.0144, -0.0116],
        ...,
        [-0.0219, -0.0087,  0.0116,  ...,  0.0321, -0.0032, -0.0239],
        [ 0.0031, -0.0148, -0.0152,  ...,  0.0150, -0.0095,  0.0076],
        [ 0.0284, -0.0090, -0.0328,  ..., -0.0071, -0.0063,  0.0282]])

llm.base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0078, -0.0073, -0.0076,  ...,  0.0039,  0.0178, -0.0147],
        [ 0.0042,  0.0369, -0.0074,  ..., -0.0084, -0.0052,  0.0020],
        [-0.0049, -0.0068, -0.0035,  ...,  0.0138, -0.0152,  0.0305],
        ...,
        [ 0.0062,  0.0170,  0.0170,  ...,  0.0287,  0.0099, -0.0255],
        [ 0.0264,  0.0084, -0.0014,  ...,  0.0291, -0.0039, -0.0113],
        [ 0.0294,  0.0131, -0.0087,  ..., -0.0005, -0.0138,  0.0016]])

llm.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 2.2985e-02, -1.3205e-02,  2.0213e-05,  ..., -2.7522e-02,
          3.4736e-02,  5.3957e-02],
        [ 4.5777e-03,  1.6288e-02,  2.8418e-02,  ...,  4.0146e-02,
          2.7803e-02,  2.1395e-02],
        [ 1.2188e-02,  1.1397e-02,  2.5013e-02,  ..., -1.9932e-02,
          4.3751e-02, -3.8484e-02],
        ...,
        [ 5.4679e-02, -6.7381e-02, -4.8377e-02,  ..., -2.3419e-02,
         -2.0876e-02,  9.5405e-03],
        [ 3.6044e-02,  1.9084e-02,  5.0493e-02,  ..., -3.5134e-02,
         -1.1101e-02,  1.2092e-03],
        [ 2.7757e-02, -3.1088e-02, -1.3144e-02,  ...,  7.0110e-02,
         -3.0927e-02,  6.9840e-03]])

llm.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0254, -0.0307, -0.0099,  ...,  0.0307, -0.0248,  0.0091],
        [-0.0080, -0.0423, -0.0169,  ...,  0.0517, -0.0402,  0.0317],
        [ 0.0128, -0.0232, -0.0247,  ...,  0.0323,  0.0083, -0.0088],
        ...,
        [-0.0049, -0.0209, -0.0243,  ..., -0.0139,  0.0062,  0.0276],
        [ 0.0288,  0.0301,  0.0412,  ...,  0.0430, -0.0505, -0.0166],
        [ 0.0179, -0.0172, -0.0216,  ..., -0.0332,  0.0190,  0.0304]])

llm.base_model.model.model.layers.7.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0028, -0.0070, -0.0198,  ..., -0.0115, -0.0131,  0.0156],
        [-0.0157,  0.0089,  0.0104,  ...,  0.0091,  0.0053, -0.0193],
        [ 0.0036, -0.0176,  0.0283,  ...,  0.0167, -0.0042, -0.0272],
        ...,
        [-0.0046,  0.0179, -0.0020,  ..., -0.0182, -0.0077,  0.0065],
        [ 0.0143,  0.0010, -0.0005,  ...,  0.0105, -0.0159, -0.0094],
        [-0.0140,  0.0085,  0.0022,  ...,  0.0165, -0.0089,  0.0089]])

llm.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0161, -0.0301, -0.0093,  ..., -0.0631, -0.0059, -0.0127],
        [ 0.0142,  0.0488,  0.0059,  ...,  0.0190, -0.0501, -0.0343],
        [-0.0287,  0.0212,  0.0467,  ..., -0.0262,  0.0326,  0.0116],
        ...,
        [ 0.0390,  0.0167, -0.0058,  ..., -0.0415,  0.0710,  0.0125],
        [ 0.0071,  0.0287,  0.0951,  ...,  0.0114, -0.0041,  0.0470],
        [ 0.0023, -0.0240,  0.0245,  ..., -0.0280,  0.0098, -0.0122]])

llm.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0063, -0.0113,  0.0170,  ..., -0.0375, -0.0080,  0.0018],
        [ 0.0020, -0.0349, -0.0187,  ..., -0.0068, -0.0226,  0.0106],
        [-0.0046, -0.0023, -0.0290,  ..., -0.0041, -0.0250, -0.0222],
        ...,
        [-0.0114,  0.0129, -0.0256,  ...,  0.0066,  0.0287,  0.0083],
        [ 0.0499,  0.0019, -0.0249,  ..., -0.0218,  0.0192,  0.0333],
        [-0.0359, -0.0413, -0.0156,  ...,  0.0302, -0.0109, -0.0013]])

llm.base_model.model.model.layers.7.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[-0.0040, -0.0225, -0.0022,  ..., -0.0098,  0.0035,  0.0027],
        [-0.0206, -0.0062, -0.0053,  ...,  0.0108, -0.0152,  0.0152],
        [-0.0015,  0.0002, -0.0017,  ..., -0.0016, -0.0048,  0.0024],
        ...,
        [-0.0003,  0.0243, -0.0071,  ..., -0.0038, -0.0066,  0.0322],
        [-0.0253,  0.0059, -0.0145,  ..., -0.0030, -0.0165,  0.0019],
        [ 0.0200,  0.0165, -0.0072,  ..., -0.0128,  0.0020,  0.0073]])

llm.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0083, -0.0041, -0.0139,  ..., -0.0149, -0.0121,  0.0003],
        [ 0.0390,  0.0302,  0.0448,  ..., -0.0110,  0.0013, -0.0230],
        [-0.0087, -0.0128,  0.0120,  ...,  0.0340,  0.0013, -0.0041],
        ...,
        [-0.0425, -0.0183, -0.0008,  ...,  0.0193,  0.0289, -0.0474],
        [-0.0018,  0.0157, -0.0111,  ..., -0.0565,  0.0040, -0.0424],
        [-0.0023, -0.0179,  0.0247,  ...,  0.0091,  0.0072, -0.0371]])

llm.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0208, -0.0003, -0.0248,  ...,  0.0073, -0.0072, -0.0444],
        [ 0.0441, -0.0253,  0.0215,  ..., -0.0346,  0.0062, -0.0562],
        [-0.0072, -0.0056, -0.0077,  ..., -0.0177, -0.0158, -0.0075],
        ...,
        [-0.0075, -0.0009,  0.0288,  ..., -0.0458, -0.0180, -0.0300],
        [ 0.0217,  0.0220,  0.0309,  ..., -0.0016,  0.0009,  0.0026],
        [-0.0246,  0.0229, -0.0024,  ..., -0.0027,  0.0010,  0.0289]])

llm.base_model.model.model.layers.7.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.9531, 0.4453, 1.3125,  ..., 0.5938, 0.4609, 0.7031])

llm.base_model.model.model.layers.7.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.8477, 0.7734, 0.9922,  ..., 0.6406, 0.7656, 0.8164])

llm.base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0114,  0.0071, -0.0041,  ..., -0.0126, -0.0242, -0.0096],
        [ 0.0018,  0.0175,  0.0017,  ...,  0.0023,  0.0001,  0.0039],
        [-0.0103, -0.0223, -0.0035,  ..., -0.0056,  0.0061,  0.0012],
        ...,
        [ 0.0056,  0.0128, -0.0236,  ...,  0.0254,  0.0052, -0.0020],
        [ 0.0244,  0.0007,  0.0117,  ...,  0.0248,  0.0064, -0.0203],
        [ 0.0417, -0.0203,  0.0079,  ..., -0.0156,  0.0072, -0.0049]])

llm.base_model.model.model.layers.8.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([-0.1738,  0.2441, -0.1846,  ..., -0.7812,  0.2031,  0.1895])

llm.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0846,  0.0032, -0.0301,  ..., -0.0123, -0.0060, -0.0041],
        [ 0.0109,  0.0452,  0.0562,  ...,  0.0125,  0.0459, -0.0406],
        [ 0.0103,  0.0151,  0.0107,  ..., -0.0008,  0.0259, -0.0085],
        ...,
        [-0.0108, -0.0697, -0.0072,  ..., -0.0406,  0.0058, -0.0117],
        [ 0.0014, -0.0193,  0.0055,  ...,  0.0484, -0.0359,  0.0310],
        [ 0.0242,  0.0326,  0.0041,  ...,  0.0036, -0.0094,  0.0104]])

llm.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0120, -0.0324, -0.0089,  ...,  0.0634,  0.0079, -0.0177],
        [-0.0048,  0.0148,  0.0114,  ..., -0.0255, -0.0222,  0.0107],
        [ 0.0060, -0.0233, -0.0410,  ...,  0.0254,  0.0228, -0.0165],
        ...,
        [-0.0092, -0.0228, -0.0346,  ..., -0.0246,  0.0236,  0.0144],
        [-0.0269, -0.0245, -0.0325,  ...,  0.0147, -0.0045, -0.0361],
        [ 0.0341, -0.0022, -0.0143,  ...,  0.0236,  0.0250,  0.0021]])

llm.base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 8.2397e-03, -2.9907e-03,  3.5706e-03,  ...,  7.5817e-05,
          4.6997e-03,  1.1902e-03],
        [-3.8757e-03, -1.5076e-02, -1.1536e-02,  ...,  2.8229e-03,
          2.8229e-03, -5.0049e-03],
        [ 4.3030e-03, -5.2490e-03,  1.0193e-02,  ...,  4.0588e-03,
          7.1411e-03, -8.1787e-03],
        ...,
        [ 9.2163e-03,  1.2451e-02, -1.1841e-02,  ...,  2.0142e-02,
          1.0498e-02,  3.9978e-03],
        [ 7.9956e-03,  1.0559e-02,  1.3000e-02,  ...,  2.1729e-02,
          1.1414e-02, -3.0151e-02],
        [ 8.7280e-03, -1.8677e-02,  2.2461e-02,  ..., -2.4048e-02,
          1.3367e-02,  1.0193e-02]])

llm.base_model.model.model.layers.8.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-9.6094e-01,  4.9805e-01, -1.9824e-01, -5.5664e-02, -1.0010e-01,
        -5.1562e-01,  5.8984e-01,  1.8164e-01,  1.9238e-01,  1.1133e-01,
        -8.8672e-01, -4.7607e-02,  2.8516e-01,  1.6357e-02, -1.1406e+00,
        -4.7852e-02,  9.5703e-02,  5.3223e-02, -3.9453e-01,  1.1172e+00,
         1.8188e-02, -3.2617e-01, -1.0498e-01,  1.5198e-02,  6.2988e-02,
         6.4087e-03,  1.3574e-01, -4.4141e-01, -8.2520e-02,  1.5234e-01,
         1.5820e-01,  2.1582e-01,  9.5215e-02, -1.0391e+00,  1.2988e-01,
         7.6294e-03,  1.5527e-01,  8.3008e-02,  6.4453e-02, -1.0059e-01,
        -5.4199e-02,  9.6680e-02, -1.4941e-01, -3.9978e-03, -1.7700e-02,
         5.2979e-02,  6.5430e-02, -4.2725e-02,  6.7383e-02,  3.9795e-02,
         3.7598e-02,  2.0752e-02,  6.4941e-02, -9.2285e-02, -6.3965e-02,
        -1.9775e-02,  1.9043e-02,  6.3672e-01,  6.5308e-03, -5.0781e-01,
        -3.9551e-02,  1.9043e-02, -1.6875e+00,  7.8125e-02,  4.1504e-02,
         5.8984e-01,  5.9814e-02,  1.0156e+00, -4.5117e-01,  7.2266e-02,
         3.2422e-01, -1.1230e-01,  7.5000e-01,  2.0898e-01, -9.7656e-02,
        -1.8262e-01, -1.2158e-01,  5.2246e-02,  4.6875e-02, -1.1475e-01,
        -9.8145e-02, -1.8945e-01,  2.5000e-01, -7.3547e-03,  1.0938e-01,
         9.6680e-02,  1.2891e-01,  1.2031e+00,  1.1133e-01, -1.7480e-01,
         4.7461e-01,  1.2061e-01,  4.1260e-02,  1.1953e+00, -6.3324e-04,
        -6.2866e-03, -9.6680e-02, -1.2305e-01,  1.1719e-02, -1.1475e-01,
         1.5430e-01,  1.2598e-01, -9.0625e-01,  2.7466e-02, -5.3711e-02,
         1.3062e-02, -3.4912e-02,  4.0039e-02, -1.2634e-02, -3.4912e-02,
        -7.6904e-03,  2.3926e-02,  2.3906e+00, -2.1484e-02,  3.3936e-02,
        -4.4250e-03,  4.4434e-02,  4.2480e-02,  4.0283e-03, -4.0039e-02,
         4.2236e-02, -1.3184e-01,  3.0762e-02,  1.2793e-01,  2.7008e-03,
        -1.8066e-01, -3.0469e+00, -1.9922e-01,  9.3750e-01, -3.1055e-01,
         3.8086e-02, -1.2988e-01,  7.3438e-01,  1.6699e-01,  2.0703e-01,
        -1.2988e-01, -4.9561e-02, -7.3438e-01, -1.8066e-01,  5.0781e-02,
        -5.7422e-01,  1.1230e-02,  4.0039e-02, -1.9238e-01, -1.2109e+00,
        -1.8164e-01,  2.6733e-02,  5.4688e-02,  1.4297e+00, -1.1914e-01,
        -9.5825e-03,  1.4551e-01, -1.1536e-02, -2.8125e-01,  2.4609e-01,
        -2.1191e-01, -2.3828e-01,  1.9653e-02, -3.8086e-02, -1.8750e-01,
         3.3789e-01,  6.2988e-02, -1.3438e+00, -3.0859e-01, -3.6011e-03,
         2.6758e-01,  4.5898e-02, -3.9368e-03,  4.2725e-02, -1.1084e-01,
        -5.3223e-02, -1.3184e-01, -5.9375e-01,  1.2598e-01, -1.0315e-02,
         3.8574e-02,  1.2512e-02,  1.1279e-01, -5.0000e+00,  1.2988e-01,
         3.2959e-03,  1.8164e-01,  1.3086e-01,  3.1055e-01,  4.0039e-01,
        -1.4160e-01,  1.0303e-01, -4.8584e-02,  4.3701e-02, -1.9043e-02,
         8.6426e-02, -3.1445e-01, -4.3701e-02,  4.8047e-01,  8.2422e-01,
        -2.5391e-01,  2.3730e-01, -8.9844e-02, -9.1406e-01,  5.5176e-02,
        -1.5625e-01, -5.0000e-01,  8.7500e-01,  1.9922e-01,  1.2354e-01,
        -1.8848e-01,  9.4141e-01, -1.0059e-01,  3.7354e-02, -1.1328e-01,
        -7.1289e-02,  2.6172e-01, -1.3184e-01,  1.2793e-01, -8.3984e-02,
         1.0703e+00, -3.0212e-03,  5.3467e-02, -1.4258e-01, -1.8047e+00,
         3.8818e-02,  2.1387e-01,  3.2422e-01, -1.6211e-01,  2.3340e-01,
        -2.3242e-01, -1.4941e-01,  3.6621e-02, -9.6191e-02,  3.7891e-01,
        -1.5137e-01,  1.1084e-01,  4.2915e-05, -8.7738e-05,  4.0771e-02,
         7.4609e-01,  3.2969e+00,  4.2480e-02, -5.0293e-02,  1.5442e-02,
         1.6098e-03, -8.0078e-02,  6.0156e-01, -9.6191e-02, -3.9258e-01,
        -3.1250e-01,  1.3965e-01,  6.5430e-02,  5.2734e-01, -5.6152e-02,
        -3.9551e-02,  2.1973e-02,  9.1309e-02, -2.6562e-01, -1.5747e-02,
        -1.3855e-02, -1.1094e+00,  5.1953e-01,  9.7656e-01,  1.5625e-01,
        -2.0312e-01, -4.2725e-02,  1.2891e-01,  1.2793e-01, -1.4453e-01,
        -8.8281e-01,  3.0469e-01,  1.7090e-01,  4.3701e-02, -1.7578e-01,
         1.0352e-01,  2.1680e-01,  1.3965e-01,  4.4250e-03,  2.9102e-01,
         3.2227e-02, -2.0996e-01,  1.1641e+00, -5.2734e-02, -1.2451e-01,
        -1.0010e-01,  1.7676e-01, -5.2979e-02, -4.1748e-02, -1.1768e-01,
        -2.8442e-02, -8.6914e-02, -1.5156e+00,  1.9824e-01, -7.3242e-02,
         3.9978e-03, -1.8066e-01,  4.2236e-02,  1.2158e-01, -3.1128e-02,
        -6.3965e-02,  1.9238e-01, -1.3379e-01,  8.4473e-02, -5.0781e-02,
        -1.0437e-02, -1.9141e-01,  2.2461e-02,  5.4443e-02, -3.9551e-02,
         9.1553e-03, -1.0645e-01,  2.2583e-02, -2.8076e-02,  3.9531e+00,
         6.9824e-02,  8.3008e-02,  2.7588e-02,  9.2285e-02, -5.4932e-02,
        -1.2695e-01, -6.8359e-02,  4.0527e-02,  4.2969e-02, -2.4170e-02,
         4.1797e-01,  2.5977e-01,  1.0693e-01,  5.4016e-03, -1.0000e+00,
         3.1641e-01, -2.2852e-01, -1.1875e+00, -1.4551e-01, -3.9062e-01,
        -1.2500e+00,  9.7168e-02, -1.0889e-01,  1.5156e+00, -2.7466e-02,
         1.6797e-01,  1.4922e+00,  1.8945e-01, -1.3047e+00, -9.2773e-02,
         7.2754e-02, -9.0820e-02,  5.5908e-02, -1.1035e-01, -1.8125e+00,
         1.9238e-01,  1.6602e-01,  3.9844e-01,  3.3984e-01, -8.8379e-02,
        -1.1963e-01,  2.3047e-01,  2.2266e-01, -6.7871e-02,  2.6953e-01,
        -6.8359e-02,  8.8867e-02, -1.4453e-01, -9.0332e-02, -2.7148e-01,
         2.1484e-02,  1.0303e-01, -9.5703e-02, -9.2188e-01, -2.3926e-02,
        -1.0010e-01,  6.1798e-04, -1.8555e-02,  8.7891e-02,  8.3984e-02,
         7.7148e-02, -4.0283e-02, -7.3242e-02, -2.6406e+00, -9.2773e-02,
        -5.1758e-02, -1.6479e-02,  2.1210e-03, -1.4355e-01,  1.1426e-01,
         6.0120e-03, -1.1377e-01,  1.7578e-01,  9.3750e-02,  2.7734e-01,
         4.3750e-01, -4.8438e-01, -5.3906e-01,  5.6641e-01, -6.0156e-01,
         6.0156e-01,  5.7031e-01, -3.6719e-01,  7.0703e-01,  5.8984e-01,
         6.2500e-01,  5.5078e-01,  7.3047e-01, -4.4922e-01,  2.9492e-01,
        -4.8340e-02, -5.0781e-01, -6.6016e-01,  5.6641e-01,  1.0303e-01,
         5.0000e-01,  9.5215e-02, -7.3047e-01, -1.4062e-01,  5.5469e-01,
         7.9297e-01,  2.3828e-01,  6.7969e-01, -2.9492e-01,  1.0938e-01,
         1.2695e-01,  7.9688e-01,  4.3750e-01,  2.4048e-02, -3.6865e-02,
        -3.3691e-02,  1.4062e+00,  2.8687e-02, -7.5195e-02,  2.4902e-02,
        -1.0071e-02, -5.9204e-03, -2.2656e+00,  7.5195e-02, -1.1279e-01,
         2.6562e-01,  6.3965e-02, -4.4922e-02,  5.7861e-02,  3.2471e-02,
        -2.9175e-02,  4.9561e-02,  3.2959e-02,  1.9043e-02,  1.1572e-01,
        -3.1641e-01,  1.8750e-01,  6.3477e-02, -8.4961e-02,  3.1641e-01,
        -1.5391e+00,  3.4668e-02, -5.1953e-01, -2.4805e-01,  2.0996e-02,
         1.7090e-02,  7.4219e-02, -5.6641e-02, -1.6504e-01, -7.8125e-02,
        -3.2227e-01,  4.5312e-01, -3.1738e-02, -1.9336e-01, -1.7480e-01,
        -3.5742e-01,  5.2490e-02, -1.7578e-01,  7.5781e-01,  1.0840e-01,
        -6.6406e-01,  1.2634e-02,  6.7188e-01, -1.1865e-01, -7.2266e-01,
         1.2451e-01, -5.7812e-01,  5.4688e-01,  6.0156e-01,  6.6895e-02,
        -8.6719e-01, -1.5430e-01,  7.5391e-01, -6.3281e-01, -1.4219e+00,
        -3.9307e-02, -4.5898e-02,  1.9287e-02,  8.7891e-02, -2.8076e-03,
         1.0938e-01,  1.8005e-03,  1.3906e+00,  9.8267e-03, -8.4839e-03,
        -6.8359e-02, -7.5195e-02,  4.1748e-02,  7.6660e-02,  3.3984e-01,
        -6.4087e-03,  7.0801e-02,  5.2002e-02,  5.4199e-02, -1.3000e-02,
         2.3438e-02, -7.7637e-02,  7.0801e-02, -4.7119e-02, -3.2031e+00,
         4.8096e-02,  2.9907e-02, -1.6016e-01, -3.1055e-01, -6.7578e-01,
         1.1816e-01,  5.1514e-02])

llm.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0064, -0.0208, -0.0074,  ...,  0.0194, -0.0460,  0.0154],
        [-0.0007, -0.0290,  0.0207,  ..., -0.0532,  0.0370,  0.0021],
        [ 0.0103, -0.0117, -0.0140,  ..., -0.0328, -0.0156, -0.0087],
        ...,
        [ 0.0543,  0.0308, -0.0016,  ..., -0.0418,  0.0326,  0.0385],
        [ 0.0093, -0.0173,  0.0155,  ..., -0.0191,  0.0092, -0.0316],
        [ 0.0266, -0.0032,  0.0564,  ...,  0.0056, -0.0129,  0.0222]])

llm.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0214,  0.0156, -0.0185,  ..., -0.0279, -0.0154,  0.0167],
        [ 0.0216, -0.0123,  0.0321,  ...,  0.0462,  0.0362, -0.0058],
        [-0.0398,  0.0180, -0.0167,  ...,  0.0083, -0.0386, -0.0078],
        ...,
        [ 0.0360, -0.0395,  0.0088,  ...,  0.0272,  0.0994,  0.0199],
        [-0.0373,  0.0273,  0.0231,  ..., -0.0027,  0.0071, -0.0154],
        [ 0.0033, -0.0132,  0.0018,  ...,  0.0513, -0.0171,  0.0191]])

llm.base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0160,  0.0018, -0.0322,  ..., -0.0009,  0.0165, -0.0167],
        [ 0.0062, -0.0067, -0.0204,  ...,  0.0067, -0.0123,  0.0106],
        [ 0.0046, -0.0075,  0.0074,  ..., -0.0073,  0.0003, -0.0064],
        ...,
        [ 0.0291,  0.0210, -0.0063,  ..., -0.0023, -0.0057,  0.0123],
        [-0.0089, -0.0004,  0.0069,  ...,  0.0140, -0.0033,  0.0177],
        [-0.0150,  0.0104, -0.0006,  ..., -0.0018,  0.0005, -0.0102]])

llm.base_model.model.model.layers.8.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 2.1484e-02,  2.6855e-02,  9.9487e-03,  2.5787e-03, -4.8828e-02,
        -5.8594e-02, -7.2021e-03,  2.0874e-02,  7.5195e-02, -1.6211e-01,
        -7.4707e-02,  1.6846e-02,  6.5308e-03, -4.0527e-02, -7.0312e-02,
         3.1128e-02, -2.0386e-02, -2.9053e-02,  4.0527e-02,  4.1016e-02,
         4.5654e-02, -1.5320e-02, -3.8574e-02, -4.8340e-02,  8.5449e-03,
        -1.2891e-01, -5.4626e-03,  3.8574e-02, -7.5378e-03, -9.9609e-02,
         2.5513e-02, -2.3682e-02,  4.7302e-03,  5.7129e-02,  2.8076e-02,
        -3.3447e-02,  9.5215e-02,  9.1309e-02,  9.9487e-03, -3.7842e-02,
        -3.5889e-02,  1.5039e-01,  4.3457e-02,  2.3560e-02,  6.5613e-03,
        -1.0986e-02, -3.1982e-02,  1.5564e-02,  3.3203e-02,  3.3203e-02,
        -3.5400e-03, -3.9062e-02, -5.5664e-02,  4.5654e-02, -2.8931e-02,
         9.8267e-03,  1.3855e-02,  5.9326e-02,  5.2002e-02,  1.3489e-02,
        -1.4832e-02, -4.3457e-02, -3.7109e-02,  5.5176e-02,  3.2715e-02,
        -4.2480e-02,  2.9175e-02, -2.8320e-02, -7.7820e-04, -3.1128e-02,
         7.3242e-02,  4.8340e-02, -4.0771e-02,  2.9907e-02,  2.4536e-02,
         2.1973e-02, -1.3977e-02, -9.1553e-04,  2.7008e-03,  2.1973e-02,
         1.2146e-02,  5.2002e-02, -5.7983e-04, -4.2725e-02,  1.9409e-02,
        -1.9409e-02, -3.5889e-02,  4.6875e-02,  5.6885e-02,  5.4443e-02,
        -7.4219e-02,  8.6914e-02,  2.1362e-02,  2.3804e-02,  8.1543e-02,
         5.4199e-02, -7.9956e-03,  1.2398e-04, -5.3711e-02, -3.9551e-02,
         8.1055e-02, -2.3071e-02, -3.8086e-02,  8.7280e-03,  6.2500e-02,
         9.0332e-03, -1.9287e-02, -3.2227e-02, -1.8311e-02, -1.5381e-02,
         7.3730e-02, -1.3855e-02, -1.1780e-02,  4.6387e-03,  1.2695e-02,
         3.7994e-03,  1.3855e-02, -5.2734e-02,  6.0791e-02, -4.1992e-02,
        -5.7373e-02, -4.9561e-02,  1.6937e-03,  2.9755e-03, -5.7373e-02,
        -5.7617e-02,  2.7588e-02,  1.0071e-02, -6.2561e-03,  6.8359e-02,
         1.2512e-02, -2.4902e-02, -1.1902e-02, -3.6377e-02, -5.7129e-02,
         3.5400e-02,  2.6001e-02, -2.0264e-02, -5.2734e-02,  2.4292e-02,
         2.7466e-03, -2.6611e-02, -2.7832e-02, -1.6357e-02,  5.1025e-02,
         9.0820e-02, -2.0020e-02, -1.8311e-02,  1.6699e-01, -3.7384e-03,
         3.6865e-02, -3.4180e-02, -4.1748e-02,  4.3457e-02, -3.2959e-03,
        -5.2490e-03, -1.9653e-02,  4.1260e-02, -5.9570e-02,  5.2246e-02,
        -7.5989e-03, -2.1973e-02,  1.0620e-02, -3.2196e-03, -8.5449e-04,
         2.2095e-02, -3.8574e-02, -5.8289e-03, -3.2227e-02, -2.9663e-02,
         6.9336e-02,  1.8311e-02,  2.1118e-02, -1.5430e-01, -1.6968e-02,
        -3.5553e-03, -3.4180e-02, -3.6133e-02,  6.1035e-02, -2.9175e-02,
         2.0630e-02,  3.3447e-02, -3.4424e-02,  3.4424e-02, -3.1982e-02,
        -2.1118e-02,  1.8311e-02,  1.5381e-02,  4.4922e-02,  2.5757e-02,
        -3.0518e-02,  1.1414e-02, -2.2125e-03,  5.2734e-02,  5.4688e-02,
         2.7832e-02, -2.2949e-02,  3.6430e-04,  5.0964e-03,  2.6611e-02,
         9.2773e-02,  3.9062e-03,  9.4604e-03, -1.1353e-02,  4.3335e-03,
         2.8320e-02, -1.9531e-02, -5.2979e-02,  1.5503e-02,  2.0996e-02,
        -6.8665e-03, -1.7383e-01,  1.8921e-02, -2.8711e-01, -2.5757e-02,
        -3.2227e-02,  2.2339e-02, -2.2095e-02,  6.2256e-02,  1.7822e-02,
        -6.6833e-03, -3.8574e-02,  3.7598e-02, -6.6833e-03,  8.2397e-04,
         2.2949e-02,  2.0294e-03, -4.3213e-02,  1.8311e-02,  2.9297e-02,
        -1.0498e-02,  9.5215e-02,  1.0107e-01,  1.5918e-01,  1.5869e-02,
         6.9580e-03, -9.3994e-03, -1.9897e-02, -1.1047e-02,  1.8677e-02,
        -6.4087e-03, -3.4180e-02, -5.0354e-03,  4.9072e-02, -2.5177e-03,
         1.9653e-02, -2.8931e-02,  5.7861e-02,  2.5757e-02,  5.2490e-02,
        -4.0527e-02,  2.0996e-02,  2.6398e-03, -3.1738e-02, -2.8198e-02,
        -1.4160e-02, -1.2451e-02,  1.7334e-02,  2.5146e-02, -3.4180e-02,
        -5.1514e-02,  1.0156e-01, -5.4016e-03,  4.8584e-02,  1.5723e-01,
        -1.4709e-02,  3.4180e-02, -1.1902e-02,  1.6235e-02,  4.9805e-02,
         1.8921e-02,  1.6479e-02, -7.4158e-03,  4.1504e-03,  4.7852e-02,
        -2.2125e-03, -9.1797e-02,  2.1729e-02, -3.8330e-02,  5.9814e-03,
        -8.8501e-03,  6.4941e-02,  5.6396e-02, -1.3123e-02, -6.1035e-02,
        -2.5391e-02, -2.5757e-02,  6.8848e-02,  3.3691e-02,  8.1177e-03,
        -8.3618e-03,  1.2207e-02,  2.8516e-01, -4.7913e-03,  2.1362e-02,
         1.7334e-02,  6.1523e-02,  1.5198e-02, -2.2583e-02,  3.4424e-02,
         5.5664e-02,  3.2471e-02,  8.1787e-03, -3.3936e-02, -6.8848e-02,
         7.7820e-03, -1.6846e-02, -2.8564e-02,  7.8613e-02, -2.4414e-02,
         6.5918e-02, -3.6621e-03,  1.0681e-02,  3.2959e-02, -3.5889e-02,
        -2.3193e-02, -5.7373e-02, -1.8555e-02, -2.0752e-02,  1.0303e-01,
        -5.2734e-02, -1.1597e-02,  3.4180e-02,  1.6113e-02, -3.1982e-02,
        -2.3346e-03,  1.0193e-02, -3.9795e-02, -1.9531e-01, -1.5625e-02,
        -2.2430e-03, -3.8281e-01, -1.1658e-02, -3.8605e-03,  2.9785e-02,
         1.0803e-02, -3.7109e-02,  1.6556e-03, -3.0670e-03, -1.8921e-02,
         6.1523e-02,  1.8677e-02,  6.1035e-03, -1.3245e-02,  7.8201e-04,
        -1.7578e-02, -3.7354e-02,  1.7578e-02,  9.1797e-02, -3.5400e-02,
        -6.6223e-03, -2.2583e-02, -3.5400e-02,  2.2461e-02,  3.7354e-02,
        -3.2715e-02, -5.0537e-02, -1.6479e-02,  1.4465e-02, -3.6133e-02,
        -2.1240e-02,  4.2236e-02, -3.7842e-03,  2.1606e-02, -3.6133e-02,
         6.4697e-03,  2.8564e-02,  5.3711e-02, -1.3794e-02,  3.2349e-03,
         6.1035e-03,  1.9287e-02, -5.0354e-03, -5.9204e-03, -5.1514e-02,
         2.6733e-02,  1.2305e-01, -2.6398e-03, -5.7068e-03,  3.4668e-02,
         4.1199e-03, -9.9487e-03,  1.9165e-02, -5.0293e-02,  1.1826e-03,
         1.5381e-02,  9.7046e-03,  9.8877e-03,  3.4424e-02, -1.3611e-02,
        -7.7515e-03,  1.6479e-02, -1.3550e-02,  3.9795e-02,  5.3955e-02,
         1.0437e-02,  2.5024e-02,  2.5024e-02, -6.8054e-03, -8.9264e-04,
         9.4531e-01, -1.3489e-02, -3.1982e-02,  4.4861e-03,  2.1820e-03,
         1.9531e-03, -3.3691e-02, -3.1128e-03,  3.0975e-03, -4.0527e-02,
        -4.2969e-02,  1.7944e-02,  1.7944e-02,  3.7079e-03,  3.8574e-02,
         3.3112e-03, -4.8828e-02, -2.1973e-02, -2.4414e-02,  1.4221e-02,
         1.3306e-02, -3.1250e-02,  2.3926e-02, -4.4678e-02,  1.2512e-02,
        -3.0151e-02, -1.7578e-02,  2.5269e-02,  6.7749e-03,  2.3560e-02,
         1.8677e-02,  2.3926e-02, -3.0762e-02,  1.3672e-01,  1.2268e-02,
        -6.1523e-02, -1.3574e-01, -1.7212e-02,  2.2217e-02,  2.1973e-03,
         7.3242e-03, -1.1658e-02, -6.4697e-03, -9.5825e-03, -1.1826e-03,
        -4.2969e-02, -1.9653e-02, -8.9111e-03, -7.9956e-03, -6.5002e-03,
         8.6670e-03, -1.0547e-01,  3.2715e-02,  2.5513e-02,  1.2024e-02,
        -1.4160e-02, -3.9795e-02, -1.1292e-02, -1.9531e-02, -1.6327e-03,
         3.3112e-03, -4.8523e-03,  6.6016e-01,  3.0670e-03,  1.4832e-02,
        -5.0964e-03, -2.2217e-02,  1.0352e-01, -7.8735e-03, -2.1729e-02,
         2.1118e-02, -9.2163e-03, -3.2715e-02,  1.3046e-03, -1.1658e-02,
        -7.9956e-03,  7.0312e-02, -2.3926e-02,  2.7161e-03, -8.6670e-03,
         2.2339e-02, -1.2512e-02, -1.5503e-02,  5.1270e-03, -3.0762e-02,
         1.7212e-02, -1.5137e-01, -1.6022e-03,  6.8665e-04,  8.7402e-02,
        -2.2217e-02,  1.6235e-02,  3.8757e-03, -2.2339e-02,  1.0014e-04,
         1.6968e-02, -2.6245e-03,  2.8198e-02,  3.2959e-02,  4.4556e-03,
        -6.1951e-03,  4.3640e-03,  3.0762e-02,  1.4343e-02,  4.1016e-02,
        -2.2888e-03, -3.4485e-03,  1.8677e-02,  7.1289e-02, -4.9805e-02,
        -1.5625e-02, -1.9897e-02])

llm.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0007, -0.0098, -0.0187,  ...,  0.0420, -0.0065, -0.0483],
        [-0.0013,  0.0261, -0.0157,  ...,  0.0153, -0.0408,  0.0139],
        [-0.0078, -0.0301, -0.0152,  ...,  0.0376,  0.0101,  0.0147],
        ...,
        [ 0.0259,  0.0221,  0.0245,  ...,  0.0305,  0.0114,  0.0820],
        [ 0.0139,  0.0366, -0.0176,  ...,  0.0321,  0.0011,  0.0055],
        [-0.0195,  0.0214,  0.0022,  ..., -0.0113, -0.0274, -0.0222]])

llm.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0323,  0.0062, -0.0109,  ..., -0.0316, -0.0024, -0.0191],
        [-0.0247, -0.0037, -0.0038,  ..., -0.0183, -0.0126,  0.0184],
        [-0.0065, -0.0432,  0.0007,  ..., -0.0032,  0.0081, -0.0405],
        ...,
        [ 0.0015, -0.0117, -0.0134,  ..., -0.0362, -0.0141,  0.0119],
        [-0.0437, -0.0290,  0.0157,  ..., -0.0025, -0.0316,  0.0058],
        [-0.0049, -0.0121, -0.0063,  ..., -0.0181,  0.0378, -0.0328]])

llm.base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0075,  0.0049, -0.0026,  ..., -0.0320,  0.0128, -0.0034],
        [ 0.0214, -0.0064, -0.0077,  ...,  0.0063, -0.0086, -0.0265],
        [-0.0031, -0.0189, -0.0261,  ..., -0.0037,  0.0084,  0.0164],
        ...,
        [-0.0054,  0.0215, -0.0149,  ..., -0.0054,  0.0060, -0.0049],
        [ 0.0005, -0.0036,  0.0008,  ..., -0.0025, -0.0034, -0.0183],
        [-0.0049,  0.0201,  0.0192,  ...,  0.0039,  0.0214,  0.0017]])

llm.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0031,  0.0209,  0.0029,  ...,  0.0500, -0.0203,  0.0324],
        [ 0.0100,  0.0165,  0.0649,  ...,  0.0027, -0.0954,  0.0244],
        [ 0.0097, -0.0292,  0.0317,  ...,  0.0100,  0.0072,  0.0287],
        ...,
        [ 0.0678,  0.0030,  0.0135,  ..., -0.0146,  0.0738, -0.0566],
        [-0.0119, -0.0086,  0.0194,  ..., -0.0127, -0.0031,  0.0432],
        [ 0.0153,  0.0113, -0.0042,  ...,  0.0008,  0.0279, -0.0262]])

llm.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0328,  0.0116,  0.0002,  ..., -0.0064, -0.0299, -0.0118],
        [ 0.0271, -0.0005,  0.0025,  ..., -0.0002,  0.0171, -0.0208],
        [-0.0303, -0.0015,  0.0013,  ...,  0.0301, -0.0089,  0.0077],
        ...,
        [ 0.0524,  0.0292, -0.0077,  ..., -0.0270, -0.0123,  0.0364],
        [-0.0495, -0.0082,  0.0192,  ..., -0.0425, -0.0122,  0.0064],
        [-0.0062, -0.0226,  0.0205,  ..., -0.0205, -0.0213, -0.0129]])

llm.base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-1.0376e-02, -3.4424e-02, -2.2583e-02,  ..., -1.1719e-02,
         -1.5564e-02, -3.8086e-02],
        [-1.8463e-03, -2.0630e-02,  7.9346e-03,  ..., -1.0620e-02,
         -9.2773e-03, -1.8555e-02],
        [-1.8921e-02, -1.5991e-02,  1.4160e-02,  ...,  1.5625e-02,
          1.2302e-04,  1.1292e-03],
        ...,
        [-1.0193e-02,  3.6774e-03,  3.7689e-03,  ...,  9.0942e-03,
          2.5635e-03,  1.7090e-03],
        [-8.2397e-03,  5.6152e-03,  2.3804e-02,  ...,  1.6968e-02,
         -3.5248e-03,  5.9366e-05],
        [-1.1658e-02,  1.2146e-02, -1.0315e-02,  ..., -1.1047e-02,
          1.2146e-02, -8.5449e-03]])

llm.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0424, -0.0216,  0.0066,  ..., -0.0588, -0.0563, -0.0158],
        [-0.0331,  0.0202,  0.0005,  ..., -0.0266,  0.0156, -0.0643],
        [-0.0193, -0.0408, -0.0298,  ..., -0.0171,  0.0102,  0.0168],
        ...,
        [-0.0312, -0.0234, -0.0508,  ..., -0.0198,  0.0165,  0.0192],
        [ 0.0570, -0.0276, -0.0284,  ...,  0.0427, -0.0670, -0.0080],
        [-0.0213, -0.0128,  0.0542,  ...,  0.0745,  0.0568,  0.0279]])

llm.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0291, -0.0167, -0.0446,  ...,  0.0030,  0.0120, -0.0291],
        [-0.0180, -0.0290, -0.0182,  ..., -0.0300, -0.0382,  0.0200],
        [ 0.0157,  0.0316, -0.0139,  ..., -0.0357,  0.0377, -0.0245],
        ...,
        [ 0.0306,  0.0390, -0.0087,  ...,  0.0328,  0.0257, -0.0177],
        [ 0.0401,  0.0109,  0.0241,  ...,  0.0234,  0.0094, -0.0023],
        [ 0.0207, -0.0282,  0.0145,  ..., -0.0014,  0.0231,  0.0309]])

llm.base_model.model.model.layers.8.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0089,  0.0210, -0.0260,  ..., -0.0046, -0.0245, -0.0184],
        [-0.0146, -0.0164,  0.0219,  ..., -0.0120,  0.0135, -0.0067],
        [ 0.0114, -0.0261,  0.0084,  ...,  0.0260, -0.0100,  0.0019],
        ...,
        [-0.0101, -0.0216,  0.0248,  ...,  0.0095,  0.0006, -0.0104],
        [ 0.0164,  0.0080, -0.0037,  ..., -0.0125, -0.0220,  0.0076],
        [ 0.0076,  0.0064,  0.0049,  ...,  0.0171,  0.0110, -0.0125]])

llm.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0629,  0.0339,  0.0004,  ...,  0.0168, -0.0198,  0.0262],
        [ 0.0330, -0.0295,  0.0127,  ..., -0.0008, -0.0482, -0.0359],
        [ 0.0533,  0.0294, -0.0764,  ..., -0.0200,  0.0026,  0.0048],
        ...,
        [ 0.0236, -0.0515,  0.0362,  ..., -0.0252, -0.0365, -0.0410],
        [ 0.0971, -0.0382, -0.0069,  ..., -0.0215, -0.0497, -0.0358],
        [ 0.0323,  0.0747, -0.0175,  ...,  0.0572, -0.0083,  0.0371]])

llm.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0284,  0.0019, -0.0428,  ..., -0.0157,  0.0014,  0.0494],
        [-0.0115, -0.0243,  0.0167,  ...,  0.0187,  0.0228,  0.0067],
        [-0.0428, -0.0073,  0.0676,  ..., -0.0005, -0.0473, -0.0330],
        ...,
        [-0.0331, -0.0437, -0.0087,  ...,  0.0182, -0.0450,  0.0188],
        [-0.0225,  0.0295,  0.0427,  ..., -0.0080, -0.0298,  0.0308],
        [ 0.0069, -0.0048, -0.0016,  ...,  0.0268,  0.0128, -0.0042]])

llm.base_model.model.model.layers.8.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[-0.0044, -0.0033,  0.0292,  ..., -0.0056,  0.0171,  0.0087],
        [ 0.0201, -0.0048, -0.0064,  ..., -0.0081, -0.0190,  0.0051],
        [ 0.0018,  0.0121,  0.0002,  ...,  0.0011, -0.0034, -0.0067],
        ...,
        [-0.0117,  0.0210,  0.0018,  ...,  0.0115, -0.0056,  0.0105],
        [-0.0194,  0.0342, -0.0344,  ...,  0.0081, -0.0154,  0.0022],
        [ 0.0010, -0.0036,  0.0076,  ..., -0.0068, -0.0036, -0.0139]])

llm.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0482, -0.0331, -0.0008,  ..., -0.0028,  0.0127, -0.0192],
        [-0.0214,  0.0003, -0.0251,  ...,  0.0189, -0.0202,  0.0052],
        [-0.0167, -0.0222, -0.0192,  ..., -0.0147,  0.0204,  0.0227],
        ...,
        [ 0.0059, -0.0013,  0.0104,  ..., -0.0383, -0.0264,  0.0359],
        [-0.0108,  0.0219,  0.0526,  ..., -0.0187,  0.0152,  0.0231],
        [-0.0274,  0.0455,  0.0105,  ..., -0.0299, -0.0389,  0.0032]])

llm.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0061,  0.0273, -0.0122,  ...,  0.0100, -0.0309, -0.0617],
        [ 0.0112,  0.0216,  0.0010,  ...,  0.0126, -0.0081, -0.0454],
        [-0.0214,  0.0242, -0.0066,  ...,  0.0156,  0.0029,  0.0291],
        ...,
        [ 0.0520,  0.0397, -0.0234,  ...,  0.0239, -0.0394,  0.0287],
        [-0.0358,  0.0224, -0.0448,  ...,  0.0096,  0.0029,  0.0245],
        [ 0.0092,  0.0287, -0.0173,  ...,  0.0351,  0.0221, -0.0230]])

llm.base_model.model.model.layers.8.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.0547, 0.4648, 1.3906,  ..., 0.5234, 0.4551, 0.7500])

llm.base_model.model.model.layers.8.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.9297, 0.8438, 1.0859,  ..., 0.6367, 0.8398, 0.8945])

llm.base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-5.8289e-03, -1.4465e-02, -7.5073e-03,  ...,  3.4180e-03,
         -9.0332e-03,  7.2937e-03],
        [-1.8005e-03, -1.2875e-04,  8.6670e-03,  ...,  5.2795e-03,
          9.0332e-03,  1.1368e-03],
        [ 4.7302e-03, -6.1646e-03,  4.0588e-03,  ..., -1.2451e-02,
         -1.5793e-03, -1.3672e-02],
        ...,
        [-1.3672e-02, -1.0864e-02,  1.3855e-02,  ...,  1.5855e-05,
          3.4790e-03,  1.5503e-02],
        [ 1.8066e-02,  6.4392e-03,  1.5625e-02,  ..., -2.2125e-03,
          1.5442e-02,  8.3618e-03],
        [-1.2390e-02, -9.2773e-03, -1.7929e-04,  ..., -4.1199e-03,
          1.2268e-02, -2.2095e-02]])

llm.base_model.model.model.layers.9.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.3887,  0.1562, -0.0815,  ...,  0.3066, -0.4355,  0.5195])

llm.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0110, -0.0149, -0.0019,  ...,  0.0295, -0.0271, -0.0092],
        [ 0.0371,  0.0081, -0.0420,  ...,  0.0028,  0.0043, -0.0180],
        [-0.0149, -0.0011, -0.0185,  ..., -0.0121, -0.0268, -0.0344],
        ...,
        [-0.0115, -0.0054,  0.0037,  ...,  0.0245,  0.0204,  0.0445],
        [-0.0082, -0.0158, -0.0014,  ..., -0.0130,  0.0056,  0.0229],
        [ 0.0328,  0.0029, -0.0281,  ..., -0.0252, -0.0383, -0.0550]])

llm.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0181,  0.0104,  0.0271,  ..., -0.0525, -0.0569,  0.0184],
        [ 0.0007, -0.0152, -0.0785,  ...,  0.0511,  0.0319, -0.0093],
        [-0.0052, -0.0178,  0.0462,  ..., -0.0258, -0.0173, -0.0084],
        ...,
        [ 0.0144,  0.0024,  0.0013,  ..., -0.0029,  0.0018, -0.0194],
        [ 0.0107,  0.0125,  0.0190,  ...,  0.0031,  0.0106,  0.0211],
        [ 0.0487, -0.0005,  0.0084,  ..., -0.0053,  0.0041,  0.0151]])

llm.base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-6.9580e-03, -7.6904e-03,  4.4861e-03,  ..., -3.5400e-03,
         -2.1057e-03,  6.6376e-04],
        [-4.9133e-03, -1.5182e-03, -8.0566e-03,  ..., -1.2817e-03,
         -4.7302e-03,  4.2419e-03],
        [-8.1177e-03, -9.8228e-05, -8.4839e-03,  ...,  2.3071e-02,
          1.2268e-02,  4.0894e-03],
        ...,
        [-3.1982e-02,  1.2085e-02,  4.3945e-03,  ...,  1.1841e-02,
          9.5825e-03,  3.7766e-04],
        [ 1.5625e-02, -3.6621e-02,  3.6133e-02,  ..., -6.5308e-03,
         -1.4832e-02,  1.3550e-02],
        [-4.7607e-03, -1.2756e-02,  1.0254e-02,  ...,  6.7139e-04,
          1.0254e-02,  1.3550e-02]])

llm.base_model.model.model.layers.9.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 1.1406e+00, -1.2891e+00,  3.6719e-01, -1.2891e+00, -4.6143e-02,
        -1.1484e+00, -2.5513e-02, -4.3359e-01,  4.2969e-01, -1.2012e-01,
        -2.5781e-01, -5.2344e-01,  1.8555e-01,  1.0859e+00, -7.3730e-02,
         1.8945e-01,  1.1562e+00, -7.3242e-02,  3.3789e-01,  1.3428e-02,
         2.6367e-01, -1.4258e-01, -1.2031e+00,  3.4424e-02, -2.2754e-01,
        -1.1475e-01, -3.6719e-01, -1.5332e-01,  4.5312e-01,  7.4707e-02,
        -2.9102e-01,  1.0625e+00, -2.9688e-01,  1.1670e-01, -8.1543e-02,
         4.1748e-02,  4.9561e-02, -2.2949e-01, -5.0781e-01,  3.6523e-01,
        -4.8047e-01, -6.7383e-02, -1.0938e+00,  2.2852e-01, -3.2227e-01,
         4.2188e-01, -5.5469e-01, -8.6670e-03,  3.5156e-01,  5.8594e-01,
         5.3711e-02, -5.4688e-01, -2.2754e-01, -4.3555e-01, -3.4180e-01,
         9.2969e-01, -4.1504e-03,  5.0781e-01,  4.6082e-03, -4.2500e+00,
         1.0703e+00, -2.7734e-01,  1.7480e-01,  8.0078e-01,  9.2969e-01,
        -8.0469e-01,  1.1523e-01, -5.3125e-01, -1.2891e-01, -6.4844e-01,
         8.7402e-02,  5.4688e-01, -1.1484e+00,  3.1250e-01,  9.1406e-01,
         8.7891e-01,  4.0430e-01,  3.4375e-01, -2.0410e-01,  4.1992e-01,
         3.0469e-01, -1.6479e-02, -9.4727e-02,  1.1875e+00, -1.1902e-03,
         3.1250e-02, -2.8076e-02, -1.6113e-01,  1.9824e-01, -1.2109e+00,
         1.4160e-01,  2.7539e-01,  1.0312e+00, -6.6406e-02, -2.2949e-01,
         8.0566e-02, -1.3125e+00, -8.7402e-02,  2.2656e-01, -2.4805e-01,
        -2.1289e-01, -1.9062e+00,  2.8906e-01,  5.7617e-02, -9.9121e-02,
         2.8516e-01,  2.0605e-01,  5.1172e-01,  1.6016e-01,  2.5391e-01,
         3.1641e-01,  2.6172e-01, -2.7734e-01,  3.1562e+00,  1.5820e-01,
         3.7305e-01, -1.3086e-01,  2.3242e-01,  5.5469e-01, -6.1719e-01,
         1.2451e-01, -3.5352e-01, -2.8711e-01, -4.1875e+00, -1.3828e+00,
         7.4219e-01, -2.1094e-01, -1.0938e+00, -9.4922e-01, -2.0703e-01,
         2.6172e-01, -5.0781e-02,  9.2969e-01,  1.5039e-01,  3.1250e-01,
         3.5352e-01,  1.1414e-02, -1.1875e+00,  1.6406e-01, -1.8066e-01,
         1.2109e+00,  1.7090e-01,  3.0273e-01, -1.3828e+00,  2.1191e-01,
        -4.2773e-01, -6.4941e-02, -3.1055e-01,  3.4424e-02,  8.3984e-02,
         1.5820e-01,  4.4922e-01, -2.3438e-01,  2.4609e-01,  6.4062e-01,
         6.7188e-01,  2.2559e-01,  1.1816e-01,  1.5820e-01, -1.7734e+00,
        -7.1777e-02,  9.1016e-01,  1.3000e-02, -1.2012e-01,  4.9609e-01,
        -1.4688e+00, -1.0791e-01, -6.6406e-02,  7.3828e-01, -1.9165e-02,
         3.8672e-01, -2.0156e+00,  2.0142e-03,  5.0293e-02, -4.2773e-01,
         2.3242e-01, -6.4453e-02,  6.2500e-02, -7.3047e-01, -1.5442e-02,
         1.8262e-01, -4.7607e-02, -7.9102e-02,  2.1777e-01,  2.0142e-02,
        -3.7109e-01,  1.0469e+00, -9.8145e-02, -3.5938e-01,  1.1035e-01,
        -8.5938e-02,  1.7188e+00, -6.4062e-01, -2.0996e-01, -1.0000e+00,
         3.3594e-01,  2.0508e-01, -3.8477e-01, -8.7109e-01,  2.0020e-01,
        -2.7148e-01, -2.2168e-01,  1.2305e-01, -8.8867e-02,  9.0332e-02,
         9.3750e-02,  8.5449e-02, -2.8198e-02,  1.6895e-01, -7.4219e-02,
        -1.0781e+00, -1.3770e-01, -3.5742e-01,  1.4844e+00,  8.3984e-02,
         6.2500e-02, -4.1406e-01,  1.4453e+00,  4.6143e-02,  2.7539e-01,
        -1.6992e-01, -3.1250e-01,  2.4219e-01,  3.1836e-01, -2.1362e-02,
        -1.8359e-01, -1.4160e-01, -1.8359e-01, -1.1572e-01, -1.1816e-01,
         1.2354e-01, -1.1914e-01,  1.1230e-01,  1.9922e-01,  1.1169e-02,
         2.9883e-01, -1.3770e-01,  7.0496e-03,  1.4832e-02, -7.3547e-03,
         2.8125e-01,  3.1836e-01, -4.0938e+00, -1.0254e-01, -4.6143e-02,
        -8.8867e-02, -4.9561e-02,  2.0142e-02,  5.4688e-01, -3.5645e-02,
        -2.4023e-01,  2.8516e-01,  5.0293e-02,  7.1484e-01,  1.8047e+00,
        -2.0938e+00, -1.1875e+00,  5.7031e-01,  2.5781e-01,  1.0312e+00,
        -5.2490e-02,  2.1289e-01, -6.6406e-01,  2.5586e-01, -2.0312e-01,
        -2.5977e-01,  1.3770e-01,  1.3867e-01, -1.2146e-02, -6.2256e-02,
         1.8164e-01, -4.5410e-02, -3.7598e-02, -1.0469e+00, -6.5918e-02,
        -8.5449e-02, -1.3594e+00,  1.3574e-01, -8.8867e-02,  2.0142e-02,
         5.2344e-01, -5.1514e-02,  1.4587e-02,  3.2617e-01,  6.6895e-02,
        -2.5146e-02,  3.0859e-01, -1.6797e+00,  1.4258e-01, -8.7891e-02,
         4.3359e-01, -7.1289e-02, -5.6250e-01, -1.0000e+00, -4.0527e-02,
        -1.5625e-02,  7.0801e-02,  1.5918e-01, -1.0938e-01, -1.5625e+00,
        -4.0527e-02,  1.2793e-01,  1.8066e-01, -2.3340e-01,  1.0547e-01,
        -9.0820e-02, -2.0215e-01,  5.5938e+00,  4.8633e-01, -4.1797e-01,
        -6.0938e-01, -1.0205e-01, -4.3359e-01, -2.8125e-01,  5.8838e-02,
        -2.1250e+00,  2.9102e-01, -1.6406e+00,  2.7812e+00, -1.2812e+00,
         8.5938e-01,  2.1777e-01, -5.1562e-01, -4.8633e-01,  3.2227e-02,
         7.4219e-01,  3.1641e-01,  1.8555e-01,  1.1016e+00, -5.5176e-02,
         2.1484e-01, -5.2979e-02,  1.4141e+00, -9.3262e-02, -3.0273e-02,
         1.2061e-01, -1.2891e-01,  8.0566e-02, -8.7402e-02, -8.6426e-02,
         5.5469e-01, -2.0117e-01,  4.2236e-02,  2.3926e-01,  1.5703e+00,
        -6.3965e-02, -2.8125e-01, -2.3242e-01,  7.8613e-02, -1.6992e-01,
         1.0352e-01,  8.6328e-01, -1.4893e-02, -1.6895e-01,  1.9043e-01,
        -7.1777e-02,  5.9082e-02, -6.6406e-01,  1.6797e-01,  2.2461e-02,
        -1.6992e-01,  8.6914e-02,  2.4512e-01,  6.8359e-01,  2.1582e-01,
        -6.1328e-01,  1.3770e-01, -5.4932e-02, -2.0801e-01, -6.5918e-02,
         6.2891e-01, -2.0000e+00,  9.8145e-02,  1.3086e-01, -2.5977e-01,
         1.5703e+00,  6.2891e-01,  4.3164e-01, -1.7344e+00,  2.0312e+00,
         1.0703e+00, -1.3203e+00,  1.4531e+00, -1.5078e+00, -1.2734e+00,
        -3.0664e-01,  6.6016e-01, -4.0820e-01,  7.3438e-01,  6.6016e-01,
         1.2344e+00, -2.6855e-02,  1.6113e-01, -8.5938e-01, -2.4414e-01,
        -6.4453e-02, -1.5137e-01,  2.8516e-01, -1.0156e-01,  2.0117e-01,
         2.6953e-01, -2.9883e-01,  3.5156e-01,  1.6211e-01, -2.7710e-02,
         8.6328e-01, -9.8145e-02, -1.9409e-02,  1.3867e-01,  2.6758e-01,
        -1.0781e+00, -1.2207e-01,  2.4121e-01,  3.9453e-01,  1.4648e-01,
        -2.2949e-01, -6.7578e-01,  2.6367e-02,  2.8711e-01,  1.1133e-01,
         4.5117e-01, -8.9355e-02,  1.8281e+00,  2.8320e-01, -2.0898e-01,
        -1.6406e-01, -4.2578e-01,  4.8047e-01,  5.7031e-01,  6.0938e-01,
        -1.7090e-01, -3.0078e-01, -1.4941e-01, -2.7344e-01, -5.0000e+00,
        -6.5430e-02, -6.0938e-01,  3.8086e-01, -3.7305e-01,  8.4229e-03,
         2.8906e-01,  6.4062e-01,  1.0400e-01,  3.9062e-01,  9.4922e-01,
        -2.3633e-01,  2.8906e-01, -3.0884e-02,  7.2656e-01,  1.1016e+00,
         1.2969e+00, -7.6660e-02, -1.0391e+00,  3.7354e-02,  6.8750e-01,
        -1.1780e-02,  2.5391e-01,  8.2031e-01,  4.1748e-02, -2.3438e-01,
         1.2500e+00, -6.5613e-03, -2.6367e-01,  1.0000e+00,  2.7148e-01,
        -3.5938e-01, -1.2344e+00,  4.9805e-02,  6.6406e-02, -3.4766e-01,
        -3.1250e-01,  2.5977e-01,  1.1953e+00, -4.9072e-02,  4.5312e-01,
         9.2773e-03, -2.7148e-01,  6.0156e-01, -1.7188e-01, -1.0234e+00,
        -1.1406e+00, -2.2656e-01, -6.4941e-02, -1.0681e-03,  2.7734e-01,
        -3.9844e-01, -2.6953e-01, -1.5430e-01,  7.1289e-02,  3.1445e-01,
        -3.5156e-02, -6.5625e-01,  1.7266e+00, -1.7480e-01, -3.0273e-01,
        -3.3203e-01,  1.2109e-01, -4.1504e-03,  1.4766e+00,  9.6680e-02,
         5.1514e-02,  3.9844e-01,  2.3926e-01,  3.8574e-02, -1.4375e+00,
        -1.2500e+00, -1.3438e+00,  1.1084e-01, -2.5977e-01,  6.0938e-01,
         9.5703e-02, -1.8125e+00])

llm.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0102,  0.0027, -0.0308,  ...,  0.0397, -0.0118,  0.0155],
        [-0.0120, -0.0020, -0.0014,  ...,  0.0324, -0.0207,  0.0164],
        [ 0.0241,  0.0526, -0.0106,  ..., -0.0186,  0.0112,  0.0263],
        ...,
        [ 0.0116,  0.0650, -0.0013,  ..., -0.0038,  0.0182, -0.0302],
        [-0.0483,  0.0010, -0.0458,  ..., -0.0111, -0.0454,  0.0174],
        [-0.0241, -0.0071,  0.0018,  ...,  0.0221, -0.0096,  0.0613]])

llm.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.1050, -0.0879, -0.0071,  ...,  0.0931, -0.0069, -0.0848],
        [-0.0009, -0.0242,  0.0016,  ..., -0.0125, -0.0352,  0.0100],
        [-0.0334, -0.0153,  0.0226,  ...,  0.0417,  0.0030, -0.0313],
        ...,
        [-0.0442, -0.0050,  0.0124,  ...,  0.0370, -0.0287, -0.0496],
        [-0.0176, -0.0271, -0.0232,  ...,  0.0262, -0.0308,  0.0169],
        [-0.0347, -0.0108, -0.0179,  ...,  0.0025,  0.0068, -0.0152]])

llm.base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0031, -0.0019,  0.0082,  ..., -0.0020, -0.0173,  0.0053],
        [-0.0225, -0.0186, -0.0096,  ...,  0.0098, -0.0311,  0.0278],
        [-0.0019,  0.0145,  0.0031,  ...,  0.0239,  0.0134, -0.0123],
        ...,
        [ 0.0031,  0.0056, -0.0076,  ...,  0.0046,  0.0028,  0.0265],
        [-0.0100, -0.0013,  0.0151,  ..., -0.0045,  0.0065, -0.0249],
        [ 0.0062, -0.0182,  0.0154,  ..., -0.0147, -0.0190,  0.0030]])

llm.base_model.model.model.layers.9.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-5.9082e-02, -1.6016e-01,  1.2207e-02, -7.3730e-02, -9.5215e-02,
         5.6396e-02, -8.3496e-02, -1.0889e-01, -6.1035e-02, -1.1816e-01,
         9.0820e-02,  1.2695e-01,  2.3560e-02,  1.1963e-02, -4.5395e-04,
         6.1768e-02,  4.0039e-02,  6.5430e-02, -4.3945e-02, -1.4160e-01,
        -1.2695e-01,  5.6458e-03,  2.8442e-02, -7.9346e-03, -1.2695e-01,
         5.6396e-02,  1.2109e-01, -1.2402e-01, -1.0791e-01, -1.1230e-02,
        -6.9824e-02,  2.8442e-02, -6.1768e-02, -3.2471e-02, -1.6895e-01,
         5.0659e-03, -1.7871e-01,  7.8125e-02, -1.2109e-01, -2.1777e-01,
        -2.1973e-02,  9.9121e-02,  2.2583e-02,  7.1716e-03,  1.2061e-01,
        -7.6172e-02,  1.0596e-01,  4.5898e-02,  5.3223e-02,  1.3184e-01,
        -5.5176e-02,  1.2891e-01,  1.7969e-01, -6.4941e-02,  3.8867e-01,
        -8.6426e-02,  4.0771e-02,  8.4839e-03,  3.5889e-02,  6.9824e-02,
         1.0071e-02, -1.1523e-01,  9.3384e-03,  2.5391e-02, -8.6914e-02,
        -8.8379e-02, -1.4709e-02,  9.2773e-02, -7.4219e-02, -1.2012e-01,
         3.1738e-03,  6.1279e-02,  4.3457e-02,  1.1816e-01,  7.8735e-03,
        -5.7617e-02, -1.7871e-01, -2.2266e-01,  1.5039e-01, -3.6865e-02,
        -1.7773e-01, -3.1128e-02, -5.7617e-02,  8.3496e-02,  2.1191e-01,
         1.0156e-01, -7.6172e-02, -1.0107e-01,  9.6191e-02,  1.0791e-01,
        -9.8145e-02,  9.2285e-02, -5.7861e-02,  9.6191e-02,  4.3457e-02,
         5.5908e-02,  1.3379e-01,  1.5820e-01,  8.3008e-02, -7.0801e-02,
         1.1328e-01,  1.4160e-01, -7.5195e-02,  2.1973e-02, -4.1748e-02,
         3.0151e-02,  1.0156e-01,  3.7354e-02,  3.6865e-02, -4.2725e-02,
         3.9062e-02, -4.8633e-01, -3.6133e-02, -3.2715e-02, -8.8379e-02,
         2.2339e-02,  4.1992e-02, -1.5918e-01,  1.2500e-01,  4.5898e-02,
        -1.7871e-01, -8.4961e-02,  1.6602e-02,  5.9082e-02, -3.5156e-02,
        -6.9824e-02, -1.0156e-01,  7.1289e-02,  2.4780e-02, -1.6113e-02,
        -1.0986e-02,  5.2490e-03, -4.2236e-02,  2.4316e-01, -2.6367e-02,
         3.3203e-02, -1.8158e-03, -6.8359e-02,  7.7148e-02,  1.8433e-02,
        -7.8613e-02, -1.7334e-02, -4.0527e-02, -2.6367e-02, -1.9897e-02,
        -1.0193e-02,  4.4922e-02,  2.9907e-02, -3.1128e-02,  3.6133e-02,
         6.1279e-02,  2.0020e-02,  3.9062e-02,  3.3691e-02,  2.0752e-02,
         4.5898e-02, -9.5703e-02,  1.4587e-02,  3.7109e-02, -4.2725e-03,
         1.8799e-02, -4.6082e-03, -4.8828e-02, -1.5991e-02, -5.2490e-02,
         8.2779e-04, -9.3384e-03,  1.1475e-02, -1.8311e-02, -1.5198e-02,
        -1.3489e-02, -7.2956e-05,  6.9809e-04, -6.6406e-02,  9.0027e-04,
         2.3926e-02, -1.2329e-02,  3.4424e-02,  5.2490e-02,  4.4189e-02,
         1.2390e-02, -4.0588e-03, -4.4922e-02, -6.1340e-03,  3.5889e-02,
        -2.6978e-02, -1.4832e-02,  2.5586e-01,  5.0781e-02, -1.4526e-02,
         4.0771e-02,  7.1106e-03,  7.2632e-03, -2.1118e-02, -4.3213e-02,
         1.6602e-02, -1.3489e-02,  4.2725e-02,  4.0771e-02,  1.7944e-02,
        -1.4221e-02, -1.0071e-03, -1.9287e-02,  3.0151e-02, -2.9541e-02,
         1.9531e-02,  4.6387e-03,  3.1738e-03,  2.1606e-02,  2.6367e-02,
         3.4912e-02,  1.6479e-02, -2.0386e-02, -6.6895e-02,  4.7119e-02,
        -1.5747e-02,  6.8848e-02, -3.3447e-02,  1.8921e-02,  3.1494e-02,
        -1.2684e-04,  3.6377e-02, -1.0681e-02, -1.6968e-02,  2.5024e-02,
        -1.7700e-02, -2.8839e-03,  8.9722e-03, -5.5542e-03, -3.6621e-02,
         1.2451e-02, -1.9531e-02, -2.7710e-02,  3.5156e-02, -2.6611e-02,
         4.2969e-02, -1.1902e-02, -2.7710e-02,  2.6855e-02, -2.7618e-03,
         1.3123e-02,  4.6082e-03,  2.6855e-02,  5.1270e-02,  8.7891e-03,
         4.6143e-02, -1.4648e-02, -3.0518e-02, -1.0986e-01,  4.6387e-02,
        -6.8054e-03,  7.2021e-03,  1.4343e-02, -1.3977e-02,  4.9316e-02,
         2.7344e-02,  1.0840e-01,  2.1851e-02,  8.7402e-02,  1.8311e-02,
        -4.1809e-03,  4.2969e-02,  1.9409e-02,  1.3855e-02, -1.0791e-01,
         8.3984e-02, -3.0060e-03, -1.6602e-02,  1.6992e-01,  1.5747e-02,
         5.1025e-02, -3.7842e-02,  1.0107e-01,  5.7373e-03,  1.0938e-01,
         5.8838e-02,  1.7944e-02,  1.6406e-01, -9.2285e-02, -9.0820e-02,
        -2.7954e-02, -9.1553e-03,  9.1553e-03, -4.1992e-02,  4.5410e-02,
         4.5898e-02,  4.4434e-02,  2.3804e-02,  1.2500e-01, -1.1426e-01,
         1.1377e-01, -7.6660e-02,  1.7676e-01,  6.0303e-02,  1.3184e-02,
         1.1914e-01,  5.2490e-02,  8.6426e-02, -1.7334e-02, -8.5938e-02,
         9.8267e-03, -5.2246e-02, -1.6113e-02,  1.0596e-01, -7.0312e-02,
         2.7954e-02, -1.5332e-01,  1.0376e-02, -8.4473e-02, -8.2520e-02,
         5.3955e-02, -8.7402e-02,  9.2285e-02,  6.6895e-02, -1.7456e-02,
         6.0791e-02, -6.4453e-02,  1.2988e-01,  4.9316e-02, -2.7100e-02,
        -1.0791e-01, -5.8594e-02, -9.3262e-02, -1.8433e-02, -1.2207e-02,
        -1.2305e-01, -1.0205e-01,  1.6846e-02,  7.4219e-02, -6.9336e-02,
        -1.5869e-02, -1.9897e-02,  4.7119e-02,  6.9336e-02,  2.3682e-02,
        -1.8463e-03,  9.9487e-03,  6.9824e-02,  3.6377e-02, -7.9102e-02,
        -4.9316e-02,  1.6113e-01, -2.0020e-02, -3.7109e-02,  1.1133e-01,
        -1.0938e-01,  2.8931e-02,  1.3379e-01, -6.8359e-02, -6.6528e-03,
        -9.2285e-02, -2.0264e-02,  1.0645e-01, -4.8340e-02,  6.3965e-02,
        -3.0823e-03, -1.7822e-02, -7.9102e-02, -8.0566e-02, -6.1279e-02,
        -2.1648e-04,  2.6953e-01, -5.6763e-03, -3.5645e-02,  1.6797e-01,
        -1.0791e-01, -1.0620e-02, -8.2031e-02,  1.8799e-02,  6.2012e-02,
         2.0508e-01, -8.6914e-02,  5.0781e-02, -4.1992e-02, -8.4961e-02,
         1.1572e-01, -1.8359e-01, -2.7954e-02, -1.6724e-02, -2.4902e-02,
        -1.3086e-01, -2.8931e-02,  5.1758e-02, -1.2024e-02, -6.5430e-02,
        -5.4688e-02,  3.3112e-03, -3.9307e-02, -4.4250e-03, -3.2806e-03,
        -5.1758e-02, -1.6968e-02,  3.4668e-02,  3.6133e-02,  6.8359e-03,
        -2.6093e-03, -4.1992e-02,  2.0020e-02,  6.1768e-02,  4.3945e-02,
        -1.4404e-02, -1.8555e-02, -6.5613e-03, -1.0059e-01,  8.7891e-03,
         2.4986e-04,  5.6458e-03, -1.6602e-02, -9.3384e-03, -1.1536e-02,
         6.3477e-02, -1.3428e-02,  3.5645e-02,  6.8359e-02, -5.3406e-03,
        -1.5015e-02, -1.7456e-02,  2.4902e-02, -2.3041e-03,  5.5847e-03,
        -3.9307e-02,  8.3008e-02,  4.7119e-02, -2.0630e-02, -3.2227e-02,
         4.1260e-02,  1.1047e-02, -5.1270e-02, -1.6113e-02,  2.5513e-02,
        -1.1353e-02,  2.5177e-03,  1.0107e-01,  2.0142e-02, -3.1738e-02,
        -3.3447e-02, -7.4158e-03,  5.3711e-02,  2.2125e-03,  3.6133e-02,
        -5.3955e-02,  3.2959e-02,  2.1729e-02, -2.9602e-03, -4.2419e-03,
        -5.5420e-02,  7.0801e-02,  1.1719e-02, -1.0071e-03, -1.6357e-02,
        -2.4316e-01, -2.1851e-02,  2.0508e-02,  4.1504e-02,  2.8809e-02,
        -1.7090e-02,  8.1055e-02, -2.6855e-02, -7.9590e-02, -4.0771e-02,
         4.1211e-01,  1.6699e-01, -2.8381e-03, -3.1250e-02,  5.2246e-02,
        -3.4668e-02, -2.7344e-02, -8.0566e-02, -4.7302e-03,  3.1982e-02,
         1.9287e-02,  5.1880e-03,  3.7354e-02,  2.7466e-02,  1.2146e-02,
         7.4768e-03,  2.4780e-02,  6.4941e-02,  4.6875e-02, -2.5024e-02,
         1.1902e-02, -3.9062e-03,  7.5989e-03,  1.3000e-02, -3.9307e-02,
        -1.2878e-02,  4.3457e-02,  6.8054e-03,  1.4941e-01,  4.1504e-02,
         1.8652e-01,  3.8574e-02, -4.6387e-02,  6.3477e-02, -1.2793e-01,
         2.0386e-02, -1.9287e-02, -3.4637e-03, -5.3101e-03, -7.7820e-03,
         4.3213e-02,  3.4668e-02, -3.2715e-02,  2.9785e-02,  5.9326e-02,
         4.7363e-02, -4.4678e-02,  3.8086e-02, -2.3438e-02, -1.7212e-02,
        -2.0447e-03,  1.4282e-02])

llm.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0155,  0.0158, -0.0030,  ..., -0.0219, -0.0007, -0.0002],
        [ 0.0143, -0.0468, -0.0027,  ..., -0.0466,  0.0002,  0.0168],
        [ 0.0443,  0.0645, -0.0139,  ...,  0.0132,  0.0119, -0.0174],
        ...,
        [ 0.0087, -0.0024,  0.0321,  ...,  0.0248, -0.0182, -0.0173],
        [ 0.0081,  0.0647,  0.0160,  ...,  0.0188,  0.0338,  0.0088],
        [-0.0233, -0.0493, -0.0152,  ...,  0.0067,  0.0117,  0.0334]])

llm.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0012, -0.0055,  0.0122,  ..., -0.0372, -0.0006, -0.0065],
        [ 0.0050,  0.0023, -0.0070,  ...,  0.0068, -0.0197, -0.0091],
        [-0.0251, -0.0086, -0.0143,  ...,  0.0140, -0.0080,  0.0152],
        ...,
        [ 0.0274, -0.0111,  0.0031,  ...,  0.0190,  0.0023,  0.0150],
        [-0.0005,  0.0212,  0.0032,  ...,  0.0148,  0.0108, -0.0201],
        [ 0.0315,  0.0240, -0.0292,  ..., -0.0245,  0.0236,  0.0210]])

llm.base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0072, -0.0037,  0.0063,  ..., -0.0050,  0.0211, -0.0052],
        [ 0.0208, -0.0023, -0.0168,  ...,  0.0028,  0.0024,  0.0253],
        [ 0.0364, -0.0217,  0.0203,  ...,  0.0128, -0.0165, -0.0123],
        ...,
        [-0.0035, -0.0118,  0.0452,  ...,  0.0009,  0.0048,  0.0144],
        [ 0.0342, -0.0156, -0.0137,  ..., -0.0028, -0.0120,  0.0081],
        [ 0.0087,  0.0146, -0.0025,  ..., -0.0291,  0.0232, -0.0071]])

llm.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0300, -0.0266, -0.0020,  ..., -0.0081,  0.0259,  0.0433],
        [ 0.0051,  0.0118, -0.0290,  ..., -0.0357,  0.0178,  0.0204],
        [ 0.0458, -0.0038, -0.0156,  ..., -0.0497, -0.0206, -0.0253],
        ...,
        [ 0.0125, -0.0148, -0.0447,  ..., -0.0208, -0.0175,  0.0231],
        [-0.0553,  0.0112, -0.0439,  ...,  0.0015,  0.0185,  0.0049],
        [-0.0665,  0.0047,  0.0218,  ..., -0.0161,  0.0141, -0.0379]])

llm.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0376,  0.0425, -0.0117,  ..., -0.0412,  0.0088, -0.0222],
        [ 0.0408, -0.0098, -0.0164,  ...,  0.0131, -0.0004,  0.0058],
        [-0.0223,  0.0733,  0.0087,  ...,  0.0074,  0.0364, -0.0335],
        ...,
        [-0.0183,  0.0132,  0.0312,  ..., -0.0391,  0.0135, -0.0214],
        [-0.0046,  0.0359, -0.0154,  ...,  0.0195, -0.0049,  0.0451],
        [ 0.0101,  0.0368,  0.0134,  ..., -0.0183, -0.0325, -0.0213]])

llm.base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0095,  0.0144,  0.0099,  ...,  0.0181, -0.0025,  0.0150],
        [ 0.0020,  0.0153, -0.0162,  ..., -0.0001, -0.0177, -0.0339],
        [ 0.0049,  0.0089, -0.0055,  ...,  0.0156, -0.0011, -0.0115],
        ...,
        [ 0.0430, -0.0161, -0.0090,  ..., -0.0193, -0.0289,  0.0167],
        [ 0.0178, -0.0034, -0.0110,  ...,  0.0050, -0.0075, -0.0198],
        [-0.0148,  0.0066, -0.0201,  ..., -0.0048, -0.0083,  0.0095]])

llm.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0046, -0.0516,  0.0211,  ..., -0.0518,  0.0174, -0.0170],
        [ 0.0112,  0.0532, -0.0326,  ...,  0.0483, -0.0505, -0.0651],
        [ 0.0124, -0.0105, -0.0869,  ..., -0.0588,  0.0191,  0.0045],
        ...,
        [-0.0336,  0.0150, -0.0025,  ..., -0.0194, -0.0220, -0.0374],
        [-0.0028, -0.0626,  0.0097,  ...,  0.0105, -0.0048,  0.0326],
        [-0.0565,  0.0065,  0.0371,  ..., -0.0375,  0.0152, -0.0209]])

llm.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0078, -0.0037,  0.0166,  ...,  0.0005, -0.0190,  0.0143],
        [ 0.0513,  0.0064, -0.0138,  ...,  0.0954, -0.0016, -0.0026],
        [ 0.0173, -0.0111, -0.0214,  ...,  0.0146,  0.0034,  0.0087],
        ...,
        [-0.0403, -0.0149, -0.0072,  ..., -0.0234, -0.0354,  0.0090],
        [ 0.0161,  0.0404, -0.0195,  ..., -0.0170,  0.0250, -0.0078],
        [-0.0223, -0.0060, -0.0211,  ..., -0.0167,  0.0137,  0.0043]])

llm.base_model.model.model.layers.9.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0137,  0.0084,  0.0047,  ...,  0.0135,  0.0071, -0.0114],
        [ 0.0164, -0.0044, -0.0121,  ..., -0.0064,  0.0020, -0.0019],
        [ 0.0032, -0.0040,  0.0154,  ..., -0.0270, -0.0022,  0.0045],
        ...,
        [ 0.0074, -0.0233,  0.0028,  ..., -0.0128, -0.0194, -0.0019],
        [ 0.0117,  0.0017, -0.0058,  ...,  0.0110, -0.0131,  0.0170],
        [ 0.0192, -0.0039,  0.0137,  ...,  0.0012,  0.0030,  0.0108]])

llm.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0330, -0.0043,  0.0392,  ...,  0.0303, -0.0105, -0.0084],
        [ 0.0487, -0.0425,  0.0193,  ...,  0.0320, -0.0018, -0.0406],
        [ 0.0182, -0.0140, -0.0349,  ..., -0.0366, -0.0160,  0.0646],
        ...,
        [-0.0545, -0.0272, -0.0061,  ..., -0.0263,  0.0704,  0.0115],
        [ 0.0463,  0.0220, -0.0009,  ..., -0.0399,  0.0217, -0.0029],
        [ 0.0463, -0.0218,  0.0151,  ..., -0.0571,  0.0413, -0.0084]])

llm.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0299, -0.0131,  0.0429,  ...,  0.0279, -0.0080,  0.0282],
        [-0.0002, -0.0114,  0.0429,  ..., -0.0248, -0.0470,  0.0067],
        [ 0.0180, -0.0455,  0.0096,  ...,  0.0211,  0.0248, -0.0068],
        ...,
        [-0.0082, -0.0440, -0.0005,  ...,  0.0342, -0.0099, -0.0226],
        [-0.0309, -0.0010, -0.0048,  ..., -0.0431,  0.0157,  0.0226],
        [ 0.0030,  0.0037, -0.0032,  ...,  0.0052, -0.0189, -0.0129]])

llm.base_model.model.model.layers.9.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 0.0110,  0.0031, -0.0045,  ...,  0.0160,  0.0014,  0.0147],
        [-0.0016, -0.0087,  0.0091,  ..., -0.0210, -0.0008, -0.0047],
        [ 0.0124, -0.0073,  0.0026,  ..., -0.0170,  0.0071,  0.0198],
        ...,
        [ 0.0143, -0.0086,  0.0009,  ..., -0.0393, -0.0029, -0.0160],
        [ 0.0154,  0.0106,  0.0049,  ..., -0.0312, -0.0092, -0.0073],
        [-0.0097,  0.0089,  0.0110,  ..., -0.0153,  0.0165,  0.0084]])

llm.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0380,  0.0262,  0.0060,  ...,  0.0293,  0.0175, -0.0523],
        [-0.0398, -0.0717, -0.0232,  ...,  0.0263,  0.0795, -0.0128],
        [-0.0088, -0.0010,  0.0091,  ..., -0.0143,  0.0094,  0.0034],
        ...,
        [-0.0018,  0.0051,  0.0137,  ..., -0.0136,  0.0129, -0.0432],
        [ 0.0280, -0.0185, -0.0226,  ..., -0.0245,  0.0160, -0.0084],
        [-0.0080, -0.0503,  0.0123,  ..., -0.0264,  0.0051,  0.0044]])

llm.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0120, -0.0144, -0.0075,  ...,  0.0149,  0.0100, -0.0010],
        [ 0.0198, -0.0084,  0.0320,  ...,  0.0603, -0.0010, -0.0031],
        [-0.0107, -0.0149, -0.0284,  ..., -0.0333,  0.0185,  0.0009],
        ...,
        [-0.0172,  0.0500,  0.0096,  ..., -0.0148,  0.0060,  0.0120],
        [ 0.0232, -0.0164, -0.0179,  ..., -0.0099,  0.0389, -0.0780],
        [-0.0131, -0.0153, -0.0571,  ...,  0.0314, -0.0122,  0.0265]])

llm.base_model.model.model.layers.9.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.3281, 0.5703, 1.6406,  ..., 0.4629, 0.6562, 0.8555])

llm.base_model.model.model.layers.9.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.6328, 1.3359, 1.9531,  ..., 0.8516, 1.4141, 1.4375])

llm.base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0016, -0.0012,  0.0007,  ...,  0.0029, -0.0148, -0.0125],
        [ 0.0166,  0.0117, -0.0033,  ...,  0.0006,  0.0118, -0.0113],
        [-0.0038, -0.0150, -0.0038,  ...,  0.0003, -0.0108, -0.0093],
        ...,
        [ 0.0060,  0.0151, -0.0106,  ...,  0.0133, -0.0034,  0.0177],
        [ 0.0043,  0.0271, -0.0045,  ..., -0.0084,  0.0140, -0.0040],
        [ 0.0110, -0.0105, -0.0009,  ..., -0.0001, -0.0006, -0.0042]])

llm.base_model.model.model.layers.10.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.0111,  0.0244,  0.0688,  ...,  0.1523,  0.2969, -0.1963])

llm.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-9.2175e-03, -1.4522e-02, -2.5705e-03,  ..., -9.4010e-03,
          1.7711e-02,  1.5993e-02],
        [ 2.4868e-02,  1.0170e-02, -2.5828e-02,  ..., -1.4835e-02,
          6.2678e-03,  2.1041e-02],
        [-1.5329e-02,  9.5484e-03,  4.0624e-02,  ..., -2.6248e-02,
          8.7070e-03, -3.1403e-02],
        ...,
        [ 2.0289e-02,  6.4688e-03, -3.5760e-02,  ...,  3.7722e-03,
         -2.6441e-03, -2.2038e-02],
        [ 4.1451e-02,  5.8363e-02, -9.6480e-03,  ...,  2.4636e-02,
         -9.4217e-03, -1.2037e-02],
        [-3.5148e-02,  2.6249e-02, -9.6316e-05,  ...,  8.4442e-03,
          1.3433e-04, -3.5457e-02]])

llm.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0143, -0.0212, -0.0217,  ...,  0.0441,  0.0223,  0.0276],
        [ 0.0127,  0.0045,  0.0070,  ..., -0.0172,  0.0167,  0.0115],
        [-0.0405, -0.0237, -0.0194,  ...,  0.0008,  0.0082,  0.0128],
        ...,
        [ 0.0734,  0.0198,  0.0039,  ..., -0.0472, -0.0779,  0.0020],
        [-0.0483,  0.0202, -0.0089,  ..., -0.0300,  0.0083,  0.0137],
        [ 0.0254,  0.0054,  0.0036,  ..., -0.0118, -0.0441, -0.0508]])

llm.base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0007, -0.0003,  0.0020,  ..., -0.0096,  0.0012, -0.0013],
        [ 0.0021,  0.0016, -0.0103,  ..., -0.0129, -0.0125, -0.0035],
        [-0.0090, -0.0219,  0.0286,  ...,  0.0031, -0.0099, -0.0190],
        ...,
        [ 0.0103,  0.0098,  0.0040,  ..., -0.0168,  0.0012,  0.0111],
        [-0.0294, -0.0014, -0.0059,  ...,  0.0488, -0.0029,  0.0034],
        [ 0.0237, -0.0008,  0.0354,  ..., -0.0124, -0.0540, -0.0229]])

llm.base_model.model.model.layers.10.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-1.9922e+00,  9.1016e-01, -3.9795e-02, -1.2109e-01,  1.5234e+00,
        -8.7891e-03,  8.7280e-03, -3.1982e-02,  1.1426e-01, -3.9062e-02,
        -1.7676e-01, -1.3867e-01, -2.7148e-01,  2.7734e-01, -1.3203e+00,
         1.6992e-01, -2.3633e-01,  1.2812e+00, -4.7607e-02,  1.1475e-01,
        -2.0508e-02,  3.0469e-01, -7.5000e-01, -6.9824e-02, -9.9609e-02,
         6.8848e-02,  7.5684e-02,  6.3477e-02, -1.7578e-01,  1.2500e-01,
        -7.4219e-02, -1.0303e-01,  3.7354e-02,  4.1797e-01, -1.9727e-01,
        -1.7090e-01,  1.7578e-02, -1.2012e-01,  2.6758e-01, -3.3398e-01,
        -2.9492e-01,  1.9727e-01,  7.5195e-02,  2.1582e-01, -1.2878e-02,
         1.3965e-01,  2.5391e-01,  2.2461e-01, -2.4414e-01, -2.8516e-01,
        -3.9375e+00,  4.3359e-01,  1.7578e-01,  2.9297e-01,  6.3965e-02,
        -7.8125e-02, -1.9043e-01, -1.0452e-03, -1.3379e-01,  3.2031e-01,
         1.2500e-01,  7.9688e-01, -3.3789e-01, -8.8125e+00,  1.0376e-02,
        -3.3594e-01,  3.2617e-01,  9.8145e-02,  2.1094e-01,  6.7383e-02,
         2.3145e-01,  1.3203e+00, -1.3672e-01, -1.6895e-01,  1.3750e+00,
         1.7285e-01, -3.1738e-02,  1.1768e-01, -9.8633e-02,  1.6211e-01,
         3.8672e-01,  1.2939e-02,  5.3906e-01, -6.7871e-02,  1.3359e+00,
         2.5146e-02, -3.1641e-01, -3.7695e-01, -1.1875e+00,  3.3875e-03,
        -9.5703e-01,  9.8828e-01,  1.7188e-01, -2.1973e-01,  1.7871e-01,
         1.6797e+00,  6.7871e-02, -1.2354e-01,  2.4512e-01,  1.2188e+00,
        -3.4375e-01,  2.2266e-01, -3.6133e-01, -1.5625e-01,  1.6016e-01,
         3.0078e-01,  2.1094e+00, -4.5410e-02,  1.8555e-01, -6.6016e-01,
        -3.6523e-01,  1.5625e-01, -2.2266e-01, -2.0410e-01,  7.5000e-01,
         1.3086e-01, -2.0312e-01,  2.2949e-01,  6.3672e-01, -3.5352e-01,
        -2.2852e-01, -1.8750e-01,  6.8359e-01, -3.0273e-01,  2.8320e-01,
         4.8047e-01,  2.0625e+00,  1.7383e-01, -2.4219e-01,  1.2598e-01,
        -3.8086e-02, -7.4158e-03,  5.5469e-01,  6.2891e-01, -1.0234e+00,
         1.5820e-01, -1.0681e-02, -9.7656e-01,  3.9648e-01, -5.2002e-02,
         3.5742e-01,  1.5234e-01, -8.7891e-02, -9.8145e-02, -3.4180e-01,
         1.6406e-01,  3.5156e-01, -1.3281e-01, -3.6377e-02,  1.6562e+00,
         9.9945e-04, -5.2734e-02,  2.9883e-01, -5.8350e-02, -1.2988e-01,
         2.5195e-01,  1.5747e-02,  1.3047e+00,  3.3691e-02, -3.2422e-01,
        -9.2285e-02,  1.5547e+00, -5.8203e-01,  2.0508e-01,  6.2988e-02,
        -2.2754e-01,  2.5586e-01,  1.0986e-02, -4.3750e-01,  9.6191e-02,
         2.1875e-01, -1.8457e-01, -2.3750e+00,  3.7842e-02,  3.0859e-01,
         3.5645e-02,  5.3223e-02, -4.1016e-01, -3.1250e-01,  4.1406e-01,
        -2.8516e-01, -2.9688e-01, -6.3477e-02,  9.6680e-02,  6.4453e-02,
        -2.4902e-01, -3.2422e-01,  7.6953e-01, -7.5684e-02, -2.6250e+00,
        -1.9922e-01,  9.8633e-02,  1.0625e+00,  2.8125e-01,  8.9453e-01,
         5.4297e-01,  1.5820e-01,  8.2520e-02, -3.0518e-02,  1.1133e-01,
         1.1047e-02, -6.8359e-02, -1.7090e-01, -2.9492e-01, -1.6895e-01,
         1.2422e+00,  1.5625e-01, -8.5449e-02,  1.6406e-01,  1.4453e+00,
        -1.3477e-01, -5.5176e-02,  1.4160e-01, -2.5391e-01,  2.1484e-01,
         4.6631e-02,  2.8711e-01, -8.3008e-02,  4.5410e-02,  1.8828e+00,
         2.1484e-01,  2.8076e-02, -2.2949e-02,  3.8281e-01, -5.6641e-02,
        -1.5234e-01, -1.9922e-01,  2.4805e-01, -1.2891e-01, -2.2266e-01,
        -6.6895e-02,  1.5234e-01, -2.3926e-01,  5.1025e-02,  4.0039e-02,
        -3.0884e-02, -1.2578e+00, -4.0039e-02,  5.9570e-02, -6.5918e-02,
         2.2852e-01, -4.1562e+00, -3.9551e-02, -1.6992e-01, -8.7402e-02,
         2.1191e-01, -9.5215e-02, -9.4727e-02, -4.2969e-02,  8.9844e-02,
         2.3340e-01, -4.5117e-01,  1.2695e-01, -2.1250e+00, -1.2891e-01,
         3.7695e-01, -4.1992e-01, -5.4688e-01, -2.7344e-01,  2.4023e-01,
         1.2988e-01,  1.0469e+00, -8.0078e-02, -4.6631e-02,  5.1172e-01,
        -1.1719e+00,  9.2285e-02, -3.5352e-01,  3.4766e-01,  2.6367e-01,
         3.2031e-01,  9.2773e-02, -1.8945e-01,  1.6309e-01, -1.5234e-01,
        -1.5918e-01, -9.2285e-02, -1.3047e+00,  3.0640e-02,  2.3047e-01,
        -2.5977e-01, -1.3770e-01, -6.0156e-01, -6.5918e-02, -1.7480e-01,
        -1.6992e-01,  1.2970e-04, -6.7383e-02,  1.0010e-02,  1.5625e-01,
         1.6846e-02,  1.7871e-01,  1.0645e-01, -5.2734e-02,  2.4805e-01,
         3.4424e-02,  4.2969e-02,  1.0107e-01,  2.3560e-02,  1.3477e-01,
         1.4844e-01, -2.8516e-01, -2.7734e-01,  1.5234e-01, -7.0801e-02,
        -1.2061e-01,  1.8555e-01, -3.1982e-02,  4.5312e+00, -4.5898e-01,
         5.9082e-02, -5.0781e-01, -8.8867e-02, -8.4375e-01, -3.1836e-01,
        -4.7363e-02, -1.3574e-01,  4.0625e-01,  7.2266e-02, -4.9219e-01,
         9.7266e-01,  4.4336e-01,  4.2578e-01, -1.0078e+00, -4.6143e-02,
         1.9531e-02, -6.7188e-01, -1.8311e-02,  1.7188e-01, -4.0283e-02,
        -1.6211e-01,  1.9434e-01,  1.3867e-01, -7.7515e-03, -2.0117e-01,
         1.2188e+00,  5.3955e-02,  1.5945e-03, -1.1484e+00, -3.5858e-03,
         1.2085e-02,  1.1377e-01,  1.1182e-01, -1.6992e-01, -1.1816e-01,
        -3.3398e-01, -7.6904e-03,  7.9102e-02, -3.4180e-01, -1.5156e+00,
         7.0312e-02,  1.1035e-01,  7.1777e-02, -9.7168e-02,  1.6309e-01,
        -1.2500e-01, -5.9570e-02,  2.5391e-01,  1.9688e+00,  4.6143e-02,
        -2.4219e-01,  8.2031e-02, -2.2852e-01, -1.5430e-01, -5.9766e-01,
         1.9727e-01,  9.2773e-02,  8.9844e-02, -1.5442e-02, -1.6797e-01,
         1.3477e-01,  1.4844e-01, -1.5859e+00, -1.1621e-01, -8.9722e-03,
         2.3193e-02, -4.6143e-02, -2.3926e-01,  4.3750e-01, -1.6406e-01,
         1.5820e-01, -1.6602e-01, -5.6250e-01,  9.9121e-02,  9.9609e-02,
         2.2559e-01,  7.7637e-02, -1.4453e+00,  8.0078e-02,  6.1035e-02,
         1.5820e-01,  4.0234e-01, -2.5781e-01, -5.9570e-02,  1.3574e-01,
         8.9844e-02, -1.5859e+00, -4.7363e-02, -1.1292e-02,  4.6875e-02,
         4.8096e-02,  3.4668e-02, -1.6797e-01, -8.4473e-02,  1.2256e-01,
         1.7500e+00,  3.7109e-02,  1.1963e-01, -2.6978e-02,  9.4727e-02,
         4.3213e-02, -1.4941e-01, -1.2109e+00,  1.0059e-01,  2.9663e-02,
        -3.1738e-02, -1.0303e-01,  2.5781e-01,  1.6602e-01,  6.0547e-01,
         3.9062e-01,  3.7193e-04, -1.8066e-01,  2.0020e-01,  3.9978e-03,
        -5.0293e-02, -1.1279e-01,  1.5625e-01, -2.5469e+00, -1.1670e-01,
         1.4258e-01, -1.2598e-01,  4.4189e-02, -6.9336e-02, -6.4062e-01,
         3.8330e-02,  1.5918e-01, -2.0312e-01,  7.5684e-02,  1.4160e-01,
         1.5137e-01, -1.0352e-01,  1.5332e-01, -2.4707e-01, -1.1719e-01,
         1.6797e-01,  1.4258e-01, -1.0742e-01,  1.7344e+00,  5.8350e-02,
        -2.3340e-01, -4.8828e-02,  3.4766e-01, -1.3594e+00,  8.7402e-02,
        -2.7344e-01, -9.9121e-02,  1.6562e+00,  1.1279e-01,  1.1035e-01,
         6.1279e-02,  2.1289e-01,  9.2773e-02,  1.4219e+00,  2.6367e-01,
        -2.6953e-01, -1.5078e+00, -1.1377e-01,  2.0142e-02, -1.9922e-01,
        -5.5908e-02, -7.0312e-02, -1.3984e+00,  8.0566e-02,  2.6367e-01,
         8.7891e-02,  1.0791e-01,  1.0254e-01, -1.4453e-01, -8.5938e-02,
         4.9316e-02,  6.3477e-02, -3.6719e-01, -2.5391e-01,  4.7461e-01,
         2.2363e-01, -4.0771e-02, -4.3640e-03,  1.6992e-01,  4.6484e-01,
        -4.9805e-02, -2.2827e-02, -2.4688e+00, -1.9238e-01, -8.7402e-02,
        -1.8750e-01, -4.8340e-02, -1.2390e-02, -5.1562e+00, -1.4941e-01,
        -2.0630e-02, -5.5176e-02, -3.1494e-02,  1.3770e-01, -1.2793e-01,
        -7.0312e-02,  2.0996e-01,  1.9043e-01,  1.1047e-02, -1.6504e-01,
        -3.0273e-01, -3.3008e-01])

llm.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0083, -0.0294, -0.0183,  ..., -0.0162, -0.0104,  0.0105],
        [-0.0087,  0.0161, -0.0228,  ...,  0.0318, -0.0274,  0.0176],
        [-0.0402,  0.0053, -0.0247,  ..., -0.0443,  0.0523,  0.0151],
        ...,
        [ 0.0012,  0.0090, -0.0022,  ...,  0.0267,  0.0023,  0.0277],
        [-0.0340, -0.0028,  0.0108,  ...,  0.0082, -0.0255,  0.0471],
        [-0.0008, -0.0034, -0.0474,  ...,  0.0588, -0.0414,  0.0420]])

llm.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0264, -0.0054, -0.0050,  ..., -0.0077,  0.0029, -0.0334],
        [-0.0106,  0.0250,  0.0092,  ...,  0.0097, -0.0018,  0.0340],
        [-0.0273, -0.0053,  0.0177,  ..., -0.0285, -0.0105, -0.0098],
        ...,
        [ 0.0125, -0.0193,  0.0252,  ..., -0.0004,  0.0062,  0.0230],
        [-0.0008, -0.0158,  0.0141,  ..., -0.0068, -0.0169,  0.0663],
        [-0.0114,  0.0231,  0.0453,  ..., -0.0351, -0.0326, -0.0324]])

llm.base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0356,  0.0031,  0.0155,  ...,  0.0044,  0.0124,  0.0197],
        [ 0.0093, -0.0104,  0.0066,  ...,  0.0103, -0.0208,  0.0045],
        [-0.0160,  0.0110, -0.0046,  ...,  0.0126, -0.0143,  0.0103],
        ...,
        [ 0.0038,  0.0093,  0.0151,  ...,  0.0015, -0.0129,  0.0115],
        [ 0.0098, -0.0044, -0.0059,  ..., -0.0199,  0.0021, -0.0028],
        [ 0.0217, -0.0187,  0.0356,  ..., -0.0063,  0.0067,  0.0188]])

llm.base_model.model.model.layers.10.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-1.3867e-01, -1.6211e-01, -4.3701e-02,  2.9492e-01, -5.1025e-02,
        -1.4355e-01, -7.1289e-02,  3.3447e-02, -9.4238e-02,  2.2852e-01,
        -1.5442e-02, -1.9336e-01,  4.8340e-02,  7.3730e-02,  7.9590e-02,
         2.4780e-02, -1.4771e-02, -4.1406e-01,  2.5977e-01, -1.1426e-01,
        -7.0801e-02, -7.2754e-02,  1.3281e-01, -5.2246e-02,  1.8066e-02,
        -6.5430e-02,  7.6172e-02, -1.2012e-01, -1.0547e-01,  8.7891e-02,
        -1.0059e-01, -9.6094e-01, -1.9824e-01, -2.4109e-03, -4.7363e-02,
        -1.4648e-01, -4.9805e-02,  5.7129e-02,  9.7168e-02, -1.3086e-01,
         2.3828e-01, -3.5400e-02,  1.7334e-02, -8.7891e-02,  1.1328e-01,
         8.7402e-02,  2.2583e-02,  1.4954e-02, -1.3489e-02, -2.8125e-01,
        -6.1523e-02, -9.2285e-02, -2.4414e-03, -1.0986e-02,  5.3406e-03,
         4.1992e-02,  2.1484e-02, -1.8555e-01,  2.8809e-02,  1.8311e-02,
        -6.1523e-02,  2.0898e-01, -3.9307e-02,  2.5513e-02, -6.2500e-02,
        -4.1602e-01,  1.3000e-02,  1.9287e-02, -1.5332e-01,  2.3560e-02,
         2.8809e-02, -1.2109e-01,  1.1414e-02, -7.3242e-02, -1.9629e-01,
        -9.4238e-02, -1.8848e-01,  2.4292e-02,  5.2979e-02, -9.2773e-02,
        -6.1035e-02,  1.4160e-01,  1.1865e-01, -2.3633e-01, -3.1836e-01,
         1.3184e-01,  7.9590e-02, -3.5889e-02, -3.1982e-02, -1.9434e-01,
        -5.4688e-02, -4.2969e-02,  3.0151e-02,  1.6846e-02, -2.3730e-01,
         1.0791e-01, -2.9541e-02, -8.6426e-02,  2.0874e-02, -2.6953e-01,
        -5.9570e-02, -2.0508e-02, -8.4473e-02,  6.6895e-02,  4.3457e-02,
        -1.0449e-01,  1.2451e-01,  1.5332e-01, -9.5215e-02, -1.4062e-01,
        -2.4902e-02, -9.8877e-03, -1.7212e-02,  3.7109e-01, -2.0020e-01,
         2.7222e-02,  2.6978e-02,  4.8096e-02,  1.7700e-02,  1.3245e-02,
        -4.3213e-02,  4.1992e-02, -8.1787e-03, -7.4219e-02,  7.8125e-02,
         7.9590e-02,  7.3730e-02,  7.6172e-02, -4.5776e-03, -3.5889e-02,
         2.2705e-02, -3.1738e-03, -7.2327e-03,  4.1504e-02, -2.8076e-02,
         9.2773e-02, -1.0156e-01, -9.7046e-03, -2.0630e-02, -3.9795e-02,
        -2.1729e-02, -3.6865e-02,  1.3962e-03,  1.0071e-02,  2.8320e-02,
         2.5146e-02, -4.2236e-02,  2.0630e-02,  5.4688e-02,  8.6670e-03,
         1.9684e-03,  8.9355e-02, -3.4027e-03, -2.5757e-02, -5.3711e-02,
         3.8147e-03, -2.7539e-01, -1.0193e-02, -2.0996e-02,  1.6602e-02,
        -5.4932e-02, -3.4424e-02, -2.7588e-02, -2.9907e-02, -4.9561e-02,
         1.3351e-03,  6.6406e-02, -1.2939e-02, -2.1851e-02, -6.3965e-02,
        -3.3447e-02,  9.8419e-04, -1.3367e-02, -2.2430e-03,  9.8633e-02,
        -2.7222e-02,  1.0010e-02, -5.6152e-02, -2.2095e-02, -2.7657e-04,
        -2.5146e-02, -8.2397e-03, -1.6113e-02,  1.2390e-02,  1.8921e-02,
        -8.6670e-03, -2.7954e-02,  2.1118e-02, -6.5918e-02, -4.1504e-02,
         9.4238e-02, -3.2616e-04, -3.9062e-02,  1.0010e-02,  7.2021e-03,
        -6.9824e-02, -5.6152e-02,  2.3193e-02,  1.0498e-01,  5.2734e-02,
         9.2163e-03,  3.3691e-02,  4.3213e-02, -1.5320e-02, -1.4709e-02,
         1.2512e-02, -2.9449e-03,  2.0264e-02, -2.9755e-04,  5.2246e-02,
         8.9111e-03,  1.4893e-02,  9.0332e-03,  9.1553e-03,  5.7678e-03,
        -4.0039e-02, -2.0752e-02,  4.2480e-02, -4.7852e-02,  8.7280e-03,
         2.8381e-03,  2.1362e-02, -3.5889e-02,  4.3213e-02, -1.0889e-01,
         1.8555e-02, -3.1494e-02,  6.2866e-03, -5.5664e-02,  1.4954e-02,
        -2.5879e-02,  1.4404e-02,  4.2480e-02,  3.2422e-01,  5.3223e-02,
        -1.3062e-02, -4.5166e-02,  3.8330e-02, -7.0801e-02, -2.6001e-02,
        -8.9111e-03,  2.9907e-02,  2.7710e-02, -5.5420e-02,  1.1673e-03,
        -2.9755e-03,  2.6123e-02,  1.5869e-02, -2.8442e-02,  2.9907e-02,
        -1.3123e-02, -3.0762e-02, -5.7068e-03, -2.2430e-03,  6.0547e-02,
         1.2085e-02,  3.8147e-03,  6.6406e-02,  5.8594e-02,  1.9165e-02,
         6.6833e-03, -5.3711e-02,  2.8931e-02, -4.2725e-02, -4.7363e-02,
         2.4170e-02,  8.1177e-03,  4.0283e-02, -9.0332e-02, -1.0742e-01,
         2.4048e-02, -4.5654e-02, -4.1260e-02, -5.7373e-02, -1.0681e-02,
         1.3977e-02,  9.8633e-02, -4.8340e-02, -5.5420e-02,  7.8735e-03,
         7.9346e-03,  6.0791e-02, -3.7354e-02, -6.4453e-02, -3.6377e-02,
        -1.4258e-01,  2.3560e-02, -2.9419e-02, -1.7456e-02,  1.6357e-02,
         1.1963e-02, -1.3046e-03,  5.0049e-02,  3.0396e-02, -7.1484e-01,
         6.5918e-02, -1.3574e-01, -4.6143e-02,  2.8076e-02, -8.9844e-02,
        -7.4707e-02, -9.2285e-02, -1.6699e-01, -1.8978e-04, -6.6406e-02,
        -3.7354e-02,  5.7129e-02, -4.3030e-03, -5.2490e-02,  6.2988e-02,
        -5.3223e-02, -1.2988e-01, -2.3560e-02, -2.0508e-02, -1.2131e-03,
         3.8086e-02,  1.1658e-02, -1.9824e-01, -3.2471e-02,  1.4954e-02,
        -2.8931e-02,  2.9419e-02,  3.3936e-02,  4.9561e-02, -9.5703e-02,
         2.1515e-03,  3.4912e-02, -8.5938e-02, -1.6602e-02,  9.8267e-03,
        -3.0029e-02,  8.9722e-03,  5.6396e-02, -1.8066e-02, -1.2939e-02,
        -1.1963e-02, -7.0801e-02,  6.5918e-02,  3.7305e-01, -5.7617e-02,
        -5.4932e-02,  6.6895e-02,  7.4219e-02, -2.3560e-02, -6.7444e-03,
         3.3691e-02, -5.8105e-02, -3.1982e-02,  7.7148e-02,  7.6172e-02,
         1.9043e-02,  2.0874e-02, -1.0547e-01,  9.5825e-03, -1.5564e-02,
         1.6479e-02,  3.6377e-02,  2.4780e-02,  2.1362e-02,  4.4922e-02,
         1.4062e-01,  5.2979e-02,  6.8848e-02,  4.0039e-02,  8.1543e-02,
        -8.4839e-03, -6.5918e-02, -1.1328e-01, -1.0742e-01, -8.2031e-02,
         8.7402e-02, -2.1240e-02,  7.0801e-02,  1.6992e-01,  9.0332e-02,
        -3.5645e-02, -4.7607e-02, -2.8442e-02, -1.2756e-02,  1.2817e-02,
         7.2266e-02, -4.4250e-03,  7.0190e-03,  1.2207e-02, -2.1484e-02,
        -2.1362e-02,  3.7842e-02, -1.0803e-02,  5.6152e-02,  9.1797e-02,
        -1.2695e-01,  3.9307e-02,  4.3945e-03,  3.1982e-02, -1.7334e-02,
         3.6133e-02,  6.7383e-02,  3.5400e-02,  1.9531e-02,  6.3965e-02,
        -4.7363e-02, -1.7456e-02, -2.5024e-02, -2.0508e-01, -3.8086e-02,
        -1.0693e-01,  5.8350e-02, -8.7280e-03, -5.3467e-02, -2.0386e-02,
         9.5703e-02,  3.1738e-02, -2.7344e-02, -3.1494e-02,  4.9805e-02,
         3.1982e-02,  7.7820e-03,  2.7734e-01, -1.5747e-02,  7.1106e-03,
        -3.5553e-03, -1.4258e-01, -1.6235e-02,  6.4453e-02,  1.6724e-02,
         2.0508e-01, -3.1250e-02, -6.9336e-02,  5.1514e-02, -1.8433e-02,
        -3.4668e-02, -4.5410e-02,  2.1484e-01, -7.5684e-02,  5.5420e-02,
         4.2969e-02,  6.2500e-02, -1.3855e-02, -3.1738e-02, -5.8899e-03,
         5.8105e-02, -3.5858e-03,  9.6436e-03,  2.5024e-03,  5.4443e-02,
         5.2734e-02,  8.5449e-04,  1.8677e-02, -3.7109e-02,  1.6235e-02,
         2.6855e-02,  1.1353e-02,  5.8838e-02,  2.6978e-02, -6.8359e-02,
        -2.0752e-02, -1.5625e-02, -2.6855e-03, -4.1748e-02,  7.1411e-03,
        -9.6680e-02,  6.6895e-02, -2.6978e-02, -9.8877e-03,  2.2461e-02,
         6.3477e-02, -2.0874e-02, -1.5332e-01, -3.0518e-02, -3.1982e-02,
        -4.5166e-02, -5.0537e-02,  3.9795e-02, -1.1475e-02, -5.6152e-02,
        -9.0408e-04,  5.0781e-02, -6.1340e-03, -1.6602e-02, -1.2329e-02,
         1.6968e-02,  1.2109e-01,  4.4189e-02, -6.1035e-02, -4.1016e-02,
         4.6631e-02, -3.4668e-02,  5.0293e-02, -2.7222e-02, -4.1809e-03,
         4.9561e-02,  9.4604e-03,  1.9073e-03,  9.9609e-02,  1.9775e-02,
         3.1006e-02,  5.7678e-03, -1.0193e-02,  3.2715e-02, -5.8105e-02,
         3.4637e-03, -3.1738e-02, -4.2725e-02, -2.0142e-02,  3.1738e-02,
        -6.8848e-02,  6.6406e-02, -2.8534e-03, -2.9297e-03, -5.2734e-02,
        -5.9128e-04,  3.1250e-02])

llm.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0297, -0.0329, -0.0072,  ..., -0.0318, -0.0071,  0.0002],
        [-0.0035, -0.0163,  0.0187,  ...,  0.0218,  0.0282, -0.0213],
        [-0.0133,  0.0181,  0.0372,  ..., -0.0090, -0.0071,  0.0629],
        ...,
        [ 0.0015, -0.0114, -0.0001,  ..., -0.0426,  0.0213,  0.0286],
        [-0.0086, -0.0331,  0.0248,  ...,  0.0052,  0.0065,  0.0105],
        [-0.0175, -0.0408,  0.0008,  ..., -0.0028,  0.0477, -0.0285]])

llm.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0107,  0.0136, -0.0102,  ...,  0.0126,  0.0006,  0.0304],
        [-0.0023,  0.0544, -0.0084,  ...,  0.0486,  0.0301,  0.0243],
        [-0.0015, -0.0128, -0.0205,  ...,  0.0284, -0.0004, -0.0132],
        ...,
        [ 0.0060,  0.0118,  0.0092,  ...,  0.0300, -0.0102,  0.0045],
        [ 0.0230, -0.0021, -0.0200,  ...,  0.0048,  0.0120, -0.0084],
        [ 0.0144, -0.0158, -0.0256,  ...,  0.0138,  0.0199,  0.0243]])

llm.base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0009, -0.0001, -0.0141,  ...,  0.0040,  0.0065, -0.0084],
        [-0.0062, -0.0061,  0.0080,  ..., -0.0068,  0.0311,  0.0002],
        [ 0.0031, -0.0013, -0.0197,  ..., -0.0079, -0.0110,  0.0092],
        ...,
        [-0.0162,  0.0072,  0.0014,  ...,  0.0074, -0.0098, -0.0012],
        [ 0.0391, -0.0030, -0.0039,  ...,  0.0160,  0.0006,  0.0030],
        [-0.0153,  0.0073,  0.0165,  ...,  0.0136,  0.0019, -0.0053]])

llm.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0024,  0.0072,  0.0094,  ...,  0.0222, -0.0257, -0.0043],
        [-0.0294, -0.0695,  0.0487,  ..., -0.0279,  0.0175, -0.0261],
        [ 0.0106,  0.0340,  0.0740,  ..., -0.0211,  0.0169,  0.0426],
        ...,
        [ 0.0341, -0.0204, -0.0039,  ..., -0.0022,  0.0078, -0.0105],
        [-0.0404,  0.0002, -0.0184,  ...,  0.0159,  0.0283,  0.0397],
        [-0.0034,  0.0327, -0.0123,  ...,  0.0264,  0.0391,  0.0149]])

llm.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0335, -0.0179,  0.0327,  ..., -0.0008, -0.0235, -0.0387],
        [-0.0060,  0.0214,  0.0142,  ...,  0.0200, -0.0232, -0.0199],
        [-0.0081,  0.0471, -0.0214,  ..., -0.0280, -0.0005,  0.0278],
        ...,
        [-0.0136,  0.0131,  0.0043,  ..., -0.0187, -0.0246, -0.0113],
        [-0.0174, -0.0069, -0.0092,  ..., -0.0205, -0.0061, -0.0093],
        [-0.0066, -0.0142, -0.0110,  ...,  0.0134,  0.0234,  0.0033]])

llm.base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0160,  0.0123, -0.0273,  ..., -0.0064, -0.0029,  0.0038],
        [ 0.0075,  0.0131, -0.0444,  ..., -0.0065, -0.0119,  0.0211],
        [ 0.0046, -0.0261,  0.0309,  ...,  0.0024,  0.0058, -0.0288],
        ...,
        [-0.0065, -0.0082, -0.0160,  ...,  0.0106,  0.0128,  0.0221],
        [ 0.0194, -0.0019, -0.0029,  ...,  0.0256,  0.0097, -0.0037],
        [ 0.0179, -0.0116,  0.0113,  ..., -0.0162,  0.0161,  0.0220]])

llm.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0189, -0.0083, -0.0185,  ...,  0.0374, -0.0093, -0.0193],
        [-0.0230,  0.0056,  0.0428,  ...,  0.0173, -0.0306, -0.0130],
        [-0.0605, -0.0114, -0.0580,  ...,  0.0118,  0.0019, -0.0017],
        ...,
        [-0.0319,  0.0374,  0.0208,  ..., -0.0426,  0.0330, -0.0100],
        [-0.0286, -0.0152, -0.0610,  ...,  0.0288, -0.0128,  0.0152],
        [-0.0263, -0.0166,  0.0459,  ..., -0.0008, -0.0327,  0.0222]])

llm.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0018,  0.0015,  0.0059,  ...,  0.0367,  0.0379,  0.0329],
        [ 0.0483,  0.0044,  0.0318,  ...,  0.0052,  0.0053,  0.0017],
        [-0.0107,  0.0012, -0.0088,  ..., -0.0093, -0.0078,  0.0245],
        ...,
        [-0.0168, -0.0124, -0.0589,  ...,  0.0089, -0.0371,  0.0260],
        [-0.0149, -0.0022, -0.0410,  ..., -0.0097, -0.0223,  0.0062],
        [-0.0102, -0.0443, -0.0001,  ..., -0.0562,  0.0100,  0.0204]])

llm.base_model.model.model.layers.10.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0082,  0.0025,  0.0007,  ...,  0.0137, -0.0019, -0.0050],
        [-0.0198,  0.0193, -0.0289,  ...,  0.0325, -0.0028,  0.0058],
        [ 0.0117,  0.0033, -0.0008,  ...,  0.0145,  0.0278, -0.0041],
        ...,
        [ 0.0095,  0.0122, -0.0076,  ...,  0.0017,  0.0016, -0.0048],
        [-0.0153,  0.0055,  0.0150,  ..., -0.0010,  0.0006, -0.0002],
        [-0.0013,  0.0081,  0.0027,  ...,  0.0148, -0.0108, -0.0011]])

llm.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0290, -0.0258, -0.0445,  ...,  0.0214, -0.0367,  0.0208],
        [-0.0122, -0.0356, -0.0086,  ...,  0.0101, -0.0260,  0.0382],
        [-0.0309, -0.0192,  0.0082,  ...,  0.0097, -0.0617, -0.0156],
        ...,
        [-0.0376, -0.0086, -0.0186,  ...,  0.0398, -0.0671,  0.0354],
        [-0.0107, -0.0189,  0.0020,  ..., -0.0434,  0.0275, -0.0493],
        [ 0.0302, -0.0493, -0.0067,  ...,  0.0249,  0.0056, -0.0195]])

llm.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0453,  0.0187, -0.0028,  ..., -0.0103, -0.0229, -0.0132],
        [-0.0159, -0.0039, -0.0040,  ...,  0.0362, -0.0143, -0.0022],
        [-0.0267,  0.0295,  0.0047,  ...,  0.0646, -0.0064, -0.0441],
        ...,
        [-0.0287,  0.0497,  0.0299,  ...,  0.0142, -0.0225,  0.0603],
        [-0.0228,  0.0143,  0.0132,  ..., -0.0386, -0.0140, -0.0159],
        [-0.0332,  0.0474,  0.0029,  ...,  0.0199,  0.0446,  0.0389]])

llm.base_model.model.model.layers.10.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 0.0064, -0.0087,  0.0005,  ...,  0.0089, -0.0026, -0.0173],
        [ 0.0006,  0.0254,  0.0094,  ...,  0.0023,  0.0083,  0.0081],
        [-0.0109, -0.0003,  0.0031,  ..., -0.0016,  0.0053, -0.0167],
        ...,
        [ 0.0134,  0.0120,  0.0175,  ...,  0.0073,  0.0149,  0.0087],
        [-0.0208, -0.0173,  0.0381,  ...,  0.0123,  0.0089,  0.0122],
        [ 0.0126,  0.0171, -0.0076,  ...,  0.0142,  0.0210, -0.0178]])

llm.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 2.8247e-02,  2.0065e-03, -9.7938e-03,  ...,  2.5869e-02,
         -3.9520e-02, -7.3970e-03],
        [ 7.1414e-04, -2.4175e-02, -4.0279e-02,  ..., -1.0126e-02,
         -1.1110e-02,  1.5132e-02],
        [-5.7448e-04, -1.0668e-05, -1.9682e-02,  ...,  2.4082e-03,
          1.5952e-02, -1.8576e-03],
        ...,
        [-7.3803e-03, -1.9501e-03,  1.6892e-03,  ...,  2.5712e-02,
          2.4889e-02, -1.3328e-03],
        [-5.6070e-03,  8.2293e-03,  2.7290e-03,  ...,  3.1941e-02,
          1.3078e-02,  7.2480e-03],
        [-2.6195e-03, -4.0234e-02, -2.9967e-02,  ..., -6.5159e-02,
          6.2252e-03, -3.5532e-03]])

llm.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0167,  0.0175,  0.0314,  ...,  0.0333,  0.0455, -0.0027],
        [ 0.0102, -0.0007,  0.0167,  ..., -0.0152, -0.0053, -0.0021],
        [-0.0279,  0.0027,  0.0012,  ...,  0.0154,  0.0395, -0.0121],
        ...,
        [-0.0374,  0.0223,  0.0194,  ..., -0.0405, -0.0034,  0.0188],
        [-0.0100, -0.0108, -0.0387,  ..., -0.0041, -0.0355,  0.0576],
        [ 0.0076,  0.0230,  0.0107,  ...,  0.0503,  0.0574, -0.0037]])

llm.base_model.model.model.layers.10.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.1562, 0.5508, 1.6484,  ..., 0.4531, 0.6328, 0.7969])

llm.base_model.model.model.layers.10.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.1719, 0.9844, 1.4609,  ..., 0.5859, 1.0625, 1.0938])

llm.base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0032, -0.0061, -0.0030,  ..., -0.0029, -0.0065,  0.0024],
        [-0.0081, -0.0033,  0.0103,  ..., -0.0124,  0.0090,  0.0111],
        [-0.0130, -0.0008,  0.0093,  ..., -0.0007,  0.0086,  0.0076],
        ...,
        [-0.0031,  0.0378, -0.0082,  ..., -0.0071,  0.0053, -0.0098],
        [-0.0091,  0.0206, -0.0229,  ...,  0.0183, -0.0178, -0.0549],
        [-0.0047,  0.0094,  0.0298,  ...,  0.0164,  0.0057, -0.0134]])

llm.base_model.model.model.layers.11.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([-0.0132,  0.3457,  0.2969,  ...,  0.2559, -0.7461,  0.5469])

llm.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0209, -0.0305, -0.0124,  ..., -0.0009, -0.0411, -0.0082],
        [ 0.0679, -0.0579,  0.0024,  ...,  0.0089, -0.0045,  0.0251],
        [-0.0160,  0.0022,  0.0338,  ...,  0.0161,  0.0052, -0.0192],
        ...,
        [-0.0324,  0.0238, -0.0245,  ...,  0.0305,  0.0084, -0.0292],
        [ 0.0259,  0.0156,  0.0428,  ..., -0.0088, -0.0557, -0.0044],
        [ 0.0018, -0.0262,  0.0242,  ..., -0.0345, -0.0356, -0.0346]])

llm.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0249,  0.0656,  0.0037,  ..., -0.0366, -0.0070, -0.0221],
        [-0.0260, -0.0021,  0.0103,  ..., -0.0055, -0.0215,  0.0104],
        [-0.0086, -0.0101, -0.0344,  ...,  0.0316, -0.0004,  0.0047],
        ...,
        [ 0.0012,  0.0385, -0.0057,  ...,  0.0439,  0.0075, -0.0197],
        [ 0.0136, -0.0014,  0.0008,  ...,  0.0024,  0.0063,  0.0176],
        [ 0.0046,  0.0095,  0.0214,  ..., -0.0039, -0.0016, -0.0084]])

llm.base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0062, -0.0109,  0.0081,  ..., -0.0024,  0.0002,  0.0055],
        [-0.0006,  0.0023, -0.0091,  ...,  0.0061,  0.0041, -0.0073],
        [-0.0088,  0.0168, -0.0124,  ...,  0.0044, -0.0177, -0.0004],
        ...,
        [-0.0131,  0.0437, -0.0659,  ...,  0.0067, -0.0058, -0.0120],
        [-0.0089, -0.0226,  0.0258,  ...,  0.0115,  0.0059,  0.0115],
        [-0.0240,  0.0073,  0.0053,  ..., -0.0049, -0.0027,  0.0197]])

llm.base_model.model.model.layers.11.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 8.5547e-01, -8.5156e-01, -8.1250e-01,  1.9824e-01, -9.3359e-01,
        -8.3984e-02,  1.0781e+00, -5.4932e-02,  1.0938e+00,  1.0352e-01,
        -4.3945e-02, -1.1250e+00, -1.7871e-01, -1.1953e+00,  7.6172e-01,
        -6.2988e-02,  1.0469e+00, -1.9043e-02, -1.6992e-01,  1.5918e-01,
        -3.6133e-01,  4.5654e-02, -1.1572e-01,  1.9531e-02, -1.2695e-01,
         2.0508e-01,  2.0386e-02,  1.9824e-01, -2.8125e-01, -7.4219e-02,
        -4.2969e-02,  7.2754e-02,  7.7637e-02, -1.0132e-02, -5.5078e-01,
        -9.5215e-02,  1.3611e-02, -5.2979e-02,  6.9824e-02, -1.4709e-02,
        -3.4180e-02,  2.6978e-02, -2.1729e-02,  4.2236e-02,  1.5869e-02,
         1.4160e-02,  4.2114e-03,  2.2217e-02,  2.2095e-02,  1.6479e-02,
         1.9653e-02, -1.2988e-01, -4.4375e+00,  4.1260e-02,  6.5918e-03,
         2.0508e-02, -5.7678e-03, -1.4221e-02,  3.3447e-02, -3.1982e-02,
         2.9419e-02, -6.9824e-02,  8.8867e-02, -3.2959e-02, -2.0938e+00,
        -2.2070e-01,  1.1328e-01,  3.5742e-01, -1.6992e-01,  2.0020e-01,
         2.2363e-01,  2.6978e-02,  3.2812e-01,  5.9082e-02,  4.1016e-01,
        -1.0742e-01,  5.1562e-01, -1.8262e-01,  2.2168e-01,  2.6245e-02,
         1.4062e-01,  2.0630e-02,  1.3047e+00, -1.7188e-01,  8.2031e-01,
        -3.1982e-02,  1.2734e+00,  4.1504e-02,  1.3574e-01, -9.3750e-01,
         3.2471e-02, -7.1777e-02,  1.5547e+00, -1.0147e-03, -2.9785e-02,
        -1.1719e-02, -3.9673e-03,  1.2109e-01,  1.0156e+00, -2.4292e-02,
        -3.2806e-03, -3.5400e-02, -2.5195e-01, -4.2480e-02,  2.5513e-02,
         3.1641e-01, -3.6316e-03, -6.2988e-02, -1.7700e-03,  4.5967e-04,
        -2.7954e-02, -1.1108e-02,  1.1230e-02,  1.9653e-02,  2.5146e-02,
        -6.8848e-02,  8.3984e-01,  4.8584e-02,  5.9204e-03, -2.2461e-02,
         2.0874e-02, -4.0588e-03,  1.0596e-01, -4.7363e-02,  4.5166e-02,
         3.6865e-02, -9.8877e-03, -1.0071e-03,  4.8047e-01,  6.0547e-01,
         6.0938e-01,  2.9102e-01,  2.0117e-01,  7.7734e-01,  2.8516e-01,
        -7.3438e-01,  8.1055e-02, -4.0820e-01, -8.4766e-01,  3.4912e-02,
         1.0859e+00, -5.9814e-02,  2.6562e-01,  1.0156e+00, -4.5703e-01,
        -1.7188e-01, -1.0840e-01,  7.3242e-02,  1.1641e+00,  9.7168e-02,
        -8.0859e-01,  1.2402e-01,  2.4121e-01,  6.9824e-02, -3.9844e-01,
        -1.8311e-02,  1.6250e+00,  1.0742e-01, -1.9141e-01,  7.2266e-02,
        -1.8047e+00,  5.0293e-02,  6.5918e-02, -9.3750e-02, -6.0547e-02,
        -1.2402e-01,  7.3853e-03,  2.4609e-01, -1.9336e-01,  1.0703e+00,
         6.8848e-02, -7.9102e-02, -1.5039e-01, -1.4648e-01,  1.2734e+00,
         3.6865e-02,  2.5635e-02,  1.1475e-01,  9.6680e-02,  5.4932e-03,
        -8.9111e-03, -4.2812e+00,  5.9326e-02, -1.0010e-01, -2.2583e-02,
        -3.4180e-02, -1.2061e-01,  4.9438e-03, -3.6316e-03, -2.0605e-01,
         1.4954e-02, -1.2793e-01,  4.4531e-01, -2.8516e-01, -2.8906e-01,
        -5.2344e-01, -5.7422e-01,  5.8594e-02, -4.4922e-01, -4.1797e-01,
         7.1094e-01, -2.4707e-01, -3.3984e-01, -2.7344e-01,  2.8906e-01,
        -4.2188e-01, -5.2734e-01, -1.3379e-01,  6.6797e-01,  3.7689e-03,
         1.1172e+00,  1.9922e-01,  5.6250e-01,  1.6113e-01,  6.8359e-02,
        -7.7148e-02, -1.5469e+00, -6.6528e-03,  2.2754e-01, -2.7148e-01,
         2.7734e-01, -1.1292e-02, -2.3535e-01,  3.5400e-02, -2.8125e-01,
         1.1426e-01,  2.2827e-02,  7.3047e-01,  2.2559e-01,  1.6699e-01,
         7.4707e-02, -4.5410e-02, -9.9609e-02,  3.1836e-01,  1.6479e-02,
        -3.6377e-02,  1.6113e-02, -5.5237e-03,  2.5391e-01,  3.5156e-02,
        -3.7109e-02,  2.0996e-02, -8.7891e-02,  6.1768e-02, -1.8555e-01,
         1.8047e+00, -5.1270e-02, -6.4941e-02,  9.0820e-02,  7.9102e-02,
         5.4626e-03, -2.3633e-01,  4.7363e-02,  7.4707e-02, -3.2471e-02,
         6.8848e-02, -1.1562e+00,  4.8438e-01, -5.1514e-02, -1.0352e-01,
        -1.4453e-01,  1.6797e-01,  7.7148e-02, -2.7148e-01, -7.9297e-01,
         1.0938e-01, -2.6489e-02,  4.1406e-01,  9.9609e-02,  2.2461e-01,
        -1.6211e-01, -3.2422e-01,  1.9336e-01,  2.6562e-01,  2.7148e-01,
        -3.1494e-02, -6.6406e-02, -2.6562e-01,  4.4189e-02, -1.1133e-01,
        -1.8125e+00, -1.8457e-01, -1.6992e-01,  2.0781e+00, -1.1523e-01,
         8.5449e-02, -8.3203e-01,  8.2520e-02, -4.7119e-02,  4.7363e-02,
         4.8242e-01,  3.5889e-02, -8.1543e-02,  4.7119e-02, -2.0215e-01,
        -6.7383e-02, -3.5400e-02, -1.1084e-01,  2.9663e-02,  1.2268e-02,
         1.2256e-01, -5.1025e-02, -4.1504e-02,  3.3203e-02,  4.2236e-02,
         6.7520e-04,  3.1982e-02,  3.6865e-02,  3.5400e-03, -4.4062e+00,
        -5.2979e-02,  7.5684e-02, -1.0889e-01,  1.1523e-01, -3.2959e-02,
         9.2285e-02, -8.6670e-03, -3.1738e-02,  3.9307e-02, -9.0820e-02,
         3.9453e-01, -2.7734e-01,  2.5391e-01,  1.2500e+00,  1.1670e-01,
        -7.9297e-01,  1.2812e+00,  1.5430e-01, -1.6211e-01,  6.1523e-02,
        -9.2163e-03,  1.4297e+00,  1.2891e+00,  9.0820e-02, -3.0273e-01,
        -1.8516e+00, -2.4902e-01, -6.2988e-02,  1.9609e+00,  1.6968e-02,
         8.3984e-02, -1.5703e+00,  4.6387e-03,  1.1182e-01,  2.0312e-01,
        -3.8086e-02,  2.2461e-02, -6.2988e-02, -1.0156e-01,  2.8320e-01,
         4.8340e-02, -7.4707e-02, -7.8516e-01, -8.1177e-03, -4.1504e-02,
         2.0117e-01, -9.1553e-03, -1.0469e+00,  4.7656e-01,  8.9844e-02,
        -1.5234e-01, -3.0884e-02, -2.3438e-02, -1.8555e-01,  1.1816e-01,
         1.2634e-02, -4.8340e-02,  2.7466e-02, -2.2583e-02, -1.6113e-02,
        -2.3346e-03,  1.5430e-01, -6.2988e-02,  2.5156e+00, -6.6406e-02,
         2.6855e-02, -8.9844e-02, -4.3213e-02,  8.3594e-01,  4.4434e-02,
        -1.2305e-01,  1.2305e-01,  3.0640e-02, -2.9883e-01,  4.6387e-03,
         7.8906e-01, -3.4180e-01,  9.1309e-02,  8.0078e-01,  7.1777e-02,
         1.8164e-01, -7.5000e-01, -5.3516e-01,  1.8848e-01,  2.2168e-01,
         6.5625e-01,  3.2471e-02,  5.5908e-02,  2.6562e-01, -4.6289e-01,
         4.1406e-01,  9.4922e-01,  7.5684e-02,  4.8047e-01, -8.4961e-02,
        -2.2656e-01, -8.5938e-02, -5.3906e-01,  1.2500e+00,  2.7344e-01,
         9.6191e-02, -3.7354e-02, -2.3242e-01,  2.1680e-01, -5.7812e-01,
        -1.9219e+00, -3.0859e-01,  2.6758e-01, -7.0312e-02, -2.1289e-01,
         1.7383e-01,  3.1250e-01,  1.7969e-01,  8.7402e-02,  2.4707e-01,
         1.2305e-01, -9.2285e-02, -1.5137e-01, -3.6865e-02,  3.6328e-01,
        -1.0498e-02, -1.7700e-02, -7.9102e-02,  1.2891e-01,  2.1973e-02,
        -6.5430e-02,  1.9375e+00,  1.3965e-01,  5.6152e-03,  6.4062e-01,
        -3.3398e-01,  6.1035e-02, -2.1777e-01,  2.8320e-01,  9.3262e-02,
         3.8867e-01,  2.6367e-01, -2.2754e-01,  6.6406e-01,  2.8320e-01,
         4.3945e-01, -2.7344e-01,  2.7930e-01, -3.4766e-01,  1.3281e-01,
        -8.3008e-02, -4.2725e-02,  3.5547e-01,  3.6621e-02,  2.9492e-01,
         3.1836e-01, -8.0078e-01,  2.0801e-01, -1.2891e-01,  4.6631e-02,
         9.6191e-02, -3.8086e-01,  4.8523e-03, -7.2266e-01, -1.2451e-01,
         1.0452e-03, -8.0566e-02, -6.1523e-02,  8.5449e-02,  6.7969e-01,
         7.7344e-01, -2.0020e-01,  2.5781e-01,  7.9102e-02,  2.9175e-02,
         3.7109e-01, -4.8584e-02, -7.1094e-01, -8.8379e-02,  8.6426e-02,
         8.3984e-01,  8.4473e-02,  8.5938e-02,  2.2500e+00, -1.0596e-01,
         1.2146e-02,  1.7285e-01,  1.5625e-01, -3.2031e+00, -1.6504e-01,
        -2.2461e-01,  2.6367e-02,  3.0518e-02, -5.6641e-02, -1.7285e-01,
         5.0625e+00, -2.0410e-01,  4.2480e-02,  8.4375e-01,  2.2217e-02,
         3.5156e-01, -5.3516e-01,  1.8066e-01, -1.7773e-01, -1.1797e+00,
         1.5938e+00,  5.0781e-01])

llm.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0099,  0.0434, -0.0353,  ..., -0.0363, -0.0171,  0.0454],
        [-0.0149, -0.0276, -0.0238,  ...,  0.0074, -0.0259,  0.0256],
        [ 0.0245,  0.0673, -0.0529,  ..., -0.0016,  0.0144,  0.0288],
        ...,
        [-0.0295,  0.0173,  0.0530,  ..., -0.0249, -0.0118,  0.0011],
        [-0.0062,  0.0430, -0.0080,  ..., -0.0057, -0.0074,  0.0016],
        [ 0.0401, -0.0363, -0.0390,  ...,  0.0064,  0.0070, -0.0065]])

llm.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0050,  0.0320, -0.0125,  ..., -0.0036, -0.0248,  0.0167],
        [ 0.0136, -0.0123,  0.0082,  ...,  0.0231, -0.0005,  0.0123],
        [ 0.0141, -0.0156,  0.0337,  ..., -0.0033,  0.0482,  0.0040],
        ...,
        [ 0.0242,  0.0266, -0.0331,  ..., -0.0236,  0.0215, -0.0095],
        [ 0.0013,  0.0027, -0.0259,  ..., -0.0008,  0.0181, -0.0237],
        [-0.0110,  0.0434, -0.0145,  ..., -0.0057, -0.0020,  0.0033]])

llm.base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-9.5825e-03,  8.9722e-03, -6.1951e-03,  ..., -7.1411e-03,
         -4.6082e-03, -1.2695e-02],
        [ 4.4823e-04, -1.4801e-03, -5.0659e-03,  ..., -8.6060e-03,
         -1.1780e-02, -9.3994e-03],
        [-4.0283e-03, -1.7212e-02, -1.4587e-02,  ..., -3.4485e-03,
         -6.1989e-05, -2.2217e-02],
        ...,
        [ 2.3560e-02,  1.8768e-03,  2.6733e-02,  ...,  5.3101e-03,
          1.3428e-02, -1.1063e-03],
        [-1.7700e-03, -3.2043e-03,  2.5635e-03,  ...,  1.3916e-02,
         -7.5684e-03,  4.6921e-04],
        [ 1.2512e-02, -1.4114e-03, -6.1646e-03,  ...,  1.0864e-02,
         -2.0752e-03,  4.9438e-03]])

llm.base_model.model.model.layers.11.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 7.9102e-02, -3.9978e-03, -1.2695e-01,  3.1128e-02,  1.2402e-01,
         2.2705e-02, -9.8633e-02, -1.2793e-01, -6.1951e-03,  6.4453e-02,
         2.0264e-02,  1.1475e-01,  1.5332e-01, -4.1016e-02,  1.2012e-01,
        -9.6893e-04, -5.9814e-02,  4.8096e-02,  9.2773e-02,  8.2031e-02,
        -4.3457e-02, -6.5918e-02,  3.8330e-02,  7.7820e-03,  9.1797e-02,
         6.6895e-02,  5.5859e-01, -7.1777e-02, -2.4719e-03, -1.6309e-01,
        -1.4343e-02,  7.3242e-02, -1.3733e-02,  9.2285e-02,  7.0312e-02,
         6.4453e-02, -9.9182e-04, -1.1963e-02,  7.9102e-02, -6.8848e-02,
        -9.4238e-02, -4.6875e-02, -6.1768e-02, -1.1621e-01, -1.4258e-01,
        -1.2695e-01,  4.6387e-02, -2.3828e-01,  8.3984e-02, -7.2754e-02,
         6.3477e-02,  2.5757e-02, -4.4922e-02, -9.7656e-03,  5.1270e-02,
         1.3184e-02,  2.0508e-02,  6.8359e-02, -1.3965e-01,  5.4932e-02,
         4.0039e-02, -1.8066e-01, -6.9824e-02, -6.8359e-02,  1.0059e-01,
        -1.8921e-02,  9.7168e-02,  1.0059e-01,  2.2949e-02, -4.0039e-02,
        -4.4678e-02, -4.5410e-02,  3.4027e-03, -1.9531e-01, -6.5430e-02,
         3.6865e-02, -9.2773e-02,  2.4658e-02, -6.4453e-02, -1.8066e-02,
         1.0254e-01, -2.4170e-02, -4.1809e-03,  7.1289e-02,  2.4414e-03,
        -6.0059e-02, -9.0820e-02, -3.7994e-03, -1.3477e-01,  4.1992e-02,
        -1.4062e-01,  1.0352e-01,  6.4453e-02,  8.3496e-02, -8.0566e-02,
        -1.7319e-03,  8.9355e-02,  3.1836e-01,  4.7119e-02,  5.9570e-02,
        -1.1230e-01,  4.2725e-02, -2.6367e-02, -1.1914e-01, -6.3477e-02,
         3.7109e-02,  1.3379e-01,  4.2969e-02,  6.3477e-02,  1.6113e-01,
         1.1841e-02,  3.9062e-02, -1.2305e-01, -3.4668e-02, -6.9824e-02,
        -4.4434e-02,  2.2411e-05, -1.8359e-01,  2.1289e-01,  5.2002e-02,
        -2.0020e-01,  1.0596e-01,  3.6133e-02,  2.9883e-01,  3.6621e-02,
         1.1978e-03, -1.0107e-01, -3.8818e-02, -1.6357e-02, -2.2339e-02,
         2.5635e-02,  3.2715e-02,  2.1606e-02,  3.7109e-02,  1.4420e-03,
        -1.1475e-02, -2.3484e-05, -8.7280e-03,  4.3213e-02, -3.0884e-02,
         7.1106e-03, -9.7275e-04,  4.6387e-03, -2.6550e-03, -2.8381e-03,
        -1.5869e-02,  2.1240e-02, -8.0078e-02,  1.0925e-02, -2.5635e-03,
         3.1982e-02, -4.1016e-02, -1.5717e-03, -5.3101e-03, -2.2949e-02,
         2.3804e-02,  4.4861e-03,  1.0559e-02,  2.0874e-02, -3.6865e-02,
         2.5635e-02, -5.0049e-02,  1.2573e-02, -3.9978e-03, -2.3174e-04,
        -2.0752e-03,  1.6357e-02, -1.0107e-01,  1.1841e-02, -2.0264e-02,
        -1.4038e-03, -6.1951e-03, -1.1841e-02, -2.9419e-02,  2.0905e-03,
        -1.7188e-01, -1.7090e-02,  1.2268e-02,  7.0312e-02, -1.2634e-02,
        -5.2979e-02,  2.8809e-02,  8.4229e-03, -3.1494e-02, -2.0752e-02,
         3.6133e-02,  1.6479e-02,  1.8066e-02, -1.9287e-02,  2.0142e-02,
        -4.8447e-04, -1.1084e-01,  6.1768e-02, -1.6846e-02, -1.7578e-02,
         2.1606e-02, -2.8809e-02,  1.6846e-02, -7.8735e-03, -7.5989e-03,
        -1.3367e-02, -1.2451e-02,  1.1292e-02,  8.7891e-03, -2.0752e-02,
         3.1281e-03,  2.2339e-02, -2.0508e-02,  7.8125e-03,  4.9805e-02,
         3.3691e-02,  3.6133e-02, -1.6113e-02,  1.9775e-02,  4.0283e-02,
        -9.7656e-03,  2.0020e-02,  5.3406e-03,  3.3203e-02,  4.5166e-03,
         3.4912e-02, -8.8501e-03,  3.9307e-02,  2.3193e-02,  5.3406e-03,
        -7.7820e-03,  3.8574e-02,  4.3945e-03, -8.6670e-03, -3.5645e-02,
         1.9989e-03, -8.0078e-02,  3.2227e-02,  4.3701e-02, -2.6367e-02,
         2.7954e-02,  3.6377e-02,  1.3367e-02, -1.3733e-02, -8.5449e-03,
         8.4473e-02, -8.1177e-03, -8.6060e-03,  2.0447e-03, -2.5269e-02,
         2.8992e-03,  4.1504e-02,  6.7969e-01, -2.5146e-02,  4.8096e-02,
         1.5015e-02,  5.0781e-02, -1.6968e-02, -3.7354e-02,  1.6235e-02,
         6.5430e-02, -3.1128e-02, -5.4688e-02,  2.2583e-02, -7.1289e-02,
         1.7700e-02, -7.8735e-03, -1.0498e-02,  4.1748e-02,  1.0132e-02,
         2.2461e-02,  2.4609e-01, -2.1851e-02,  4.7119e-02,  4.0039e-02,
         1.2146e-02,  9.5825e-03,  3.8605e-03,  2.6245e-03, -5.3223e-02,
        -6.4453e-02, -1.3794e-02, -3.3691e-02,  4.0039e-02,  7.6904e-03,
        -5.3711e-02, -1.8433e-02, -1.4114e-03,  3.9551e-02, -2.1680e-01,
        -4.3945e-02, -1.7578e-02, -8.1635e-04, -2.4170e-02, -2.5635e-02,
         1.1963e-02,  3.1494e-02,  2.7954e-02,  4.2236e-02,  6.7749e-03,
         5.1172e-01,  4.0283e-03, -2.0264e-02,  1.0864e-02, -7.2327e-03,
         1.6785e-03,  9.7046e-03,  2.8687e-03, -3.3447e-02, -8.3008e-02,
         5.1880e-03,  1.0938e-01,  1.3611e-02,  1.4160e-02, -5.4199e-02,
        -1.3794e-02,  2.1729e-02,  2.6123e-02, -3.1738e-02,  3.1641e-01,
        -8.7280e-03,  1.3184e-02, -5.1025e-02, -6.6406e-02, -4.4678e-02,
        -2.2339e-02,  2.8229e-03, -4.0527e-02, -5.5695e-04,  1.3184e-02,
        -3.7598e-02, -1.8555e-02,  8.0490e-04,  3.2471e-02, -4.2236e-02,
        -2.8931e-02,  2.5513e-02,  9.9487e-03, -2.6489e-02, -8.6060e-03,
        -1.1658e-02,  8.3618e-03, -3.3722e-03, -2.9541e-02, -5.3711e-02,
        -4.9591e-04, -4.8828e-02, -4.2725e-02,  8.1177e-03,  2.9297e-03,
        -3.8818e-02, -2.4414e-02, -1.2512e-02, -4.2236e-02, -3.9307e-02,
        -1.9775e-02,  2.3193e-02,  2.8198e-02, -1.5869e-02, -1.6479e-02,
         5.8350e-02,  2.9053e-02,  2.3682e-02, -3.4790e-03,  2.7954e-02,
        -2.1851e-02,  1.1536e-02, -1.8188e-02,  3.3936e-02, -2.4902e-02,
         3.7842e-02,  9.9945e-04, -2.1118e-02, -1.9897e-02,  8.5449e-03,
         6.9336e-02,  6.8665e-03, -1.6708e-03,  1.2207e-02,  1.2024e-02,
        -2.5757e-02, -3.9062e-02, -1.6357e-02,  4.9133e-03,  1.1292e-02,
        -3.0273e-02, -7.5195e-02, -7.4768e-03,  2.7710e-02,  1.4465e-02,
        -2.7832e-02, -2.8198e-02, -1.3245e-02,  4.5898e-02,  3.5889e-02,
         1.1426e-01,  3.5889e-02, -8.4839e-03,  5.5420e-02, -3.7598e-02,
        -1.2109e-01,  2.4658e-02, -9.0942e-03, -3.3936e-02,  5.2734e-02,
         1.9409e-02, -5.4321e-03,  3.6133e-02,  4.6875e-02, -3.1738e-02,
         1.1377e-01,  9.8267e-03,  1.7944e-02, -1.0605e-03, -3.2227e-02,
         1.3611e-02, -9.3994e-03, -9.0332e-03, -1.3550e-02, -7.2632e-03,
         9.0942e-03,  7.9956e-03,  7.2266e-02, -3.6865e-02,  6.5308e-03,
         1.1230e-01,  1.3504e-03, -2.9541e-02, -1.6357e-02, -6.9336e-02,
         4.1260e-02,  3.4668e-02,  4.0283e-02, -2.0752e-02, -8.4473e-02,
        -2.4536e-02, -2.5024e-02,  1.4526e-02, -1.9824e-01,  1.4062e-01,
        -3.8086e-02,  3.9062e-02,  2.5024e-03, -4.1199e-03,  6.5308e-03,
         3.3691e-02,  6.0547e-02, -6.3477e-02, -2.6245e-03,  1.1414e-02,
        -9.4238e-02,  1.5747e-02,  1.0300e-03,  7.8125e-02, -3.6133e-02,
         3.7842e-02,  6.0547e-02, -3.7598e-02,  3.1128e-02,  5.2979e-02,
         1.8433e-02, -1.9653e-02,  1.3086e-01, -3.2227e-02,  2.3682e-02,
        -9.0820e-02,  3.9551e-02, -5.7373e-02,  8.3496e-02,  2.0801e-01,
         4.2725e-02,  2.9175e-02,  4.8096e-02,  2.2266e-01, -1.2012e-01,
         2.1851e-02, -6.3965e-02, -1.7090e-02,  3.1738e-03, -1.3489e-02,
         1.2207e-02, -1.8750e-01,  6.8848e-02, -7.0801e-02,  9.8877e-03,
        -1.3672e-01,  1.4709e-02, -2.5146e-02, -1.5747e-02, -5.5176e-02,
         8.4839e-03,  1.1353e-02,  6.0081e-05, -2.1484e-01, -3.2715e-02,
        -1.4648e-02, -3.7537e-03, -6.0059e-02,  2.6001e-02, -1.8799e-02,
         6.5613e-03, -1.0605e-03, -5.3711e-02,  2.7954e-02,  1.6235e-02,
         3.1586e-03, -1.0791e-01,  3.4912e-02, -3.5889e-02, -4.4922e-02,
         3.2715e-02, -2.1362e-02,  2.8687e-02, -2.2095e-02, -4.2480e-02,
        -7.2754e-02,  2.1240e-02])

llm.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0167, -0.0093, -0.0034,  ...,  0.0224,  0.0255, -0.0198],
        [ 0.0184, -0.0179, -0.0445,  ..., -0.0271, -0.0070, -0.0012],
        [ 0.0184,  0.0156,  0.0306,  ...,  0.0086, -0.0407, -0.0064],
        ...,
        [ 0.0147, -0.0292,  0.0057,  ..., -0.0131, -0.0123,  0.0035],
        [ 0.0171, -0.0036, -0.0198,  ..., -0.0038,  0.0122,  0.0151],
        [ 0.0290, -0.0222,  0.0256,  ...,  0.0349, -0.0351, -0.0669]])

llm.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0075,  0.0284,  0.0067,  ...,  0.0215, -0.0113,  0.0228],
        [ 0.0021,  0.0408, -0.0629,  ...,  0.0395, -0.0039,  0.0089],
        [ 0.0099,  0.0289, -0.0040,  ..., -0.0071, -0.0184, -0.0038],
        ...,
        [ 0.0063,  0.0094,  0.0256,  ...,  0.0186,  0.0184,  0.0071],
        [-0.0014,  0.0076, -0.0337,  ..., -0.0123, -0.0269, -0.0272],
        [-0.0022,  0.0173,  0.0270,  ...,  0.0170,  0.0182, -0.0123]])

llm.base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0057, -0.0063, -0.0208,  ..., -0.0278, -0.0347,  0.0216],
        [-0.0271, -0.0078,  0.0024,  ..., -0.0118, -0.0089,  0.0181],
        [-0.0003,  0.0261, -0.0123,  ...,  0.0134, -0.0054, -0.0022],
        ...,
        [-0.0050,  0.0014, -0.0017,  ..., -0.0117,  0.0101, -0.0002],
        [ 0.0171, -0.0076,  0.0167,  ..., -0.0131, -0.0007,  0.0048],
        [ 0.0049,  0.0074, -0.0042,  ...,  0.0070,  0.0184,  0.0193]])

llm.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-5.2570e-03,  2.2611e-02,  4.7938e-02,  ...,  1.1425e-02,
          2.1469e-02, -6.7572e-03],
        [-5.0151e-02, -7.8507e-03, -7.0744e-03,  ..., -2.5909e-02,
          6.4234e-04, -1.7195e-02],
        [ 4.9997e-02, -3.5072e-02, -2.0199e-03,  ...,  1.6382e-02,
          4.7794e-02, -2.2904e-02],
        ...,
        [ 8.8563e-04,  2.8281e-02,  2.1378e-02,  ...,  2.2349e-02,
         -1.9931e-02,  1.0659e-02],
        [-7.1621e-05,  1.3106e-02,  2.5655e-03,  ..., -3.0708e-02,
          3.6085e-02,  1.4515e-02],
        [ 4.4691e-02, -3.3128e-02,  6.2739e-02,  ...,  2.7693e-02,
         -4.3192e-03, -4.1052e-02]])

llm.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0043,  0.0111,  0.0301,  ..., -0.0599, -0.0089, -0.0100],
        [ 0.0075, -0.0019,  0.0291,  ..., -0.0149,  0.0021, -0.0078],
        [ 0.0005, -0.0274, -0.0145,  ...,  0.0123, -0.0178,  0.0109],
        ...,
        [ 0.0174, -0.0353,  0.0042,  ..., -0.0285, -0.0033, -0.0217],
        [-0.0101, -0.0093, -0.0160,  ..., -0.0019, -0.0339, -0.0239],
        [ 0.0036,  0.0017,  0.0061,  ..., -0.0063,  0.0012,  0.0253]])

llm.base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0081,  0.0044, -0.0168,  ...,  0.0035,  0.0090, -0.0130],
        [ 0.0354, -0.0121, -0.0047,  ..., -0.0048,  0.0031, -0.0027],
        [-0.0149,  0.0123,  0.0371,  ..., -0.0289,  0.0061, -0.0084],
        ...,
        [ 0.0098, -0.0025,  0.0069,  ..., -0.0049,  0.0208,  0.0030],
        [-0.0159, -0.0109,  0.0228,  ...,  0.0239,  0.0087,  0.0192],
        [-0.0150, -0.0004,  0.0065,  ...,  0.0102, -0.0122,  0.0198]])

llm.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0039, -0.0136,  0.0031,  ...,  0.0228,  0.0203,  0.0320],
        [-0.0107, -0.0331, -0.0303,  ..., -0.0070,  0.0305, -0.0292],
        [ 0.0642, -0.0123, -0.0088,  ..., -0.0180, -0.0430, -0.0027],
        ...,
        [-0.0038, -0.0801,  0.0312,  ...,  0.0256,  0.0571,  0.0043],
        [-0.0294, -0.0104, -0.0292,  ...,  0.0209,  0.0072,  0.0580],
        [ 0.0140,  0.0433,  0.0634,  ..., -0.0046,  0.0170,  0.0030]])

llm.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0130,  0.0257, -0.0285,  ...,  0.0066,  0.0287,  0.0370],
        [-0.0210,  0.0153,  0.0445,  ...,  0.0446,  0.0551,  0.0254],
        [-0.0189,  0.0204, -0.0074,  ...,  0.0143,  0.0053,  0.0018],
        ...,
        [-0.0103,  0.0174,  0.0158,  ...,  0.0059,  0.0128, -0.0445],
        [-0.0018,  0.0088,  0.0833,  ...,  0.0179,  0.0294, -0.0332],
        [ 0.0070,  0.0035,  0.0273,  ...,  0.0006,  0.0465,  0.0016]])

llm.base_model.model.model.layers.11.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-2.9755e-04,  1.0071e-02, -9.5215e-03,  ...,  1.7700e-02,
          1.5625e-02, -7.6904e-03],
        [ 3.9062e-03, -1.0681e-02, -2.6703e-03,  ...,  4.3030e-03,
         -1.4587e-02, -3.8818e-02],
        [-1.6479e-02,  2.7100e-02, -6.9885e-03,  ..., -7.5378e-03,
         -1.2024e-02,  1.4648e-02],
        ...,
        [ 4.6692e-03,  1.8188e-02, -7.4158e-03,  ..., -2.6398e-03,
         -2.9907e-02,  4.4861e-03],
        [-1.3504e-03,  5.5075e-05, -1.7822e-02,  ..., -1.4954e-02,
          4.0894e-03, -2.8229e-03],
        [-8.9722e-03, -4.0283e-03,  1.5869e-02,  ...,  2.0508e-02,
         -1.5259e-02,  1.0559e-02]])

llm.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0269,  0.0116,  0.0222,  ..., -0.0347,  0.0400,  0.0084],
        [-0.0734,  0.0266,  0.0575,  ..., -0.0339,  0.0279,  0.0394],
        [-0.0120, -0.0327, -0.0178,  ...,  0.0195, -0.0596, -0.0269],
        ...,
        [-0.0868,  0.0205,  0.0217,  ...,  0.0269, -0.0201, -0.0079],
        [-0.0243, -0.0018,  0.0070,  ...,  0.0301,  0.0428,  0.0017],
        [-0.0255, -0.0316,  0.1039,  ...,  0.0654, -0.0509, -0.0315]])

llm.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0359, -0.0361, -0.0556,  ..., -0.0253,  0.0008,  0.0357],
        [-0.0082, -0.0006, -0.0046,  ..., -0.0008, -0.0102,  0.0098],
        [-0.0321,  0.0200,  0.0418,  ..., -0.0350, -0.0232, -0.0115],
        ...,
        [-0.0340, -0.0409, -0.0171,  ..., -0.0080, -0.0382,  0.0267],
        [ 0.0096,  0.0189,  0.0059,  ..., -0.0064,  0.0166, -0.0082],
        [-0.0238,  0.0067, -0.0303,  ..., -0.0183, -0.0124,  0.0217]])

llm.base_model.model.model.layers.11.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[-0.0059, -0.0089, -0.0193,  ...,  0.0271,  0.0089, -0.0152],
        [ 0.0273, -0.0139,  0.0233,  ...,  0.0337, -0.0147, -0.0206],
        [ 0.0017,  0.0101,  0.0085,  ...,  0.0155, -0.0025, -0.0053],
        ...,
        [-0.0003,  0.0277, -0.0098,  ..., -0.0112,  0.0194,  0.0153],
        [ 0.0295,  0.0012, -0.0082,  ..., -0.0036, -0.0048,  0.0044],
        [ 0.0237, -0.0211,  0.0051,  ...,  0.0021,  0.0019,  0.0251]])

llm.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0474,  0.0118, -0.0598,  ...,  0.0224,  0.0021, -0.0203],
        [ 0.0160, -0.0117, -0.0215,  ..., -0.0204, -0.0143,  0.0174],
        [-0.0066, -0.0134, -0.0056,  ...,  0.0289,  0.0559,  0.0256],
        ...,
        [-0.0460, -0.0127, -0.0278,  ...,  0.0253, -0.0117, -0.0196],
        [ 0.0195,  0.0294,  0.0180,  ..., -0.0259,  0.0463, -0.0191],
        [ 0.0383,  0.0352, -0.0080,  ...,  0.0132,  0.0094,  0.0635]])

llm.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0332, -0.0445, -0.0070,  ..., -0.0453,  0.0189, -0.0366],
        [ 0.0122,  0.0141,  0.0194,  ..., -0.0204,  0.0002, -0.0485],
        [-0.0034, -0.0059, -0.0205,  ...,  0.0191,  0.0296, -0.0083],
        ...,
        [ 0.0197, -0.0119, -0.0053,  ...,  0.0106, -0.0356, -0.0082],
        [-0.0089, -0.0025, -0.0059,  ..., -0.0120, -0.0015,  0.0375],
        [ 0.0469, -0.0384,  0.0342,  ...,  0.0091, -0.0174, -0.0124]])

llm.base_model.model.model.layers.11.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.2812, 0.5156, 1.7266,  ..., 0.4199, 0.6289, 0.8125])

llm.base_model.model.model.layers.11.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.1641, 1.0078, 1.6641,  ..., 0.5430, 1.1094, 1.1250])

llm.base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0012,  0.0097,  0.0090,  ..., -0.0167, -0.0170,  0.0013],
        [ 0.0017,  0.0007,  0.0483,  ..., -0.0063,  0.0267, -0.0052],
        [ 0.0138, -0.0085,  0.0168,  ..., -0.0171,  0.0031,  0.0071],
        ...,
        [ 0.0232, -0.0308,  0.0317,  ...,  0.0091, -0.0041, -0.0261],
        [-0.0106,  0.0056, -0.0244,  ..., -0.0134,  0.0039, -0.0225],
        [-0.0002,  0.0020,  0.0086,  ...,  0.0070, -0.0160, -0.0352]])

llm.base_model.model.model.layers.12.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([-9.5000,  1.1328, -1.4141,  ...,  0.5195, -0.2344,  0.0928])

llm.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0437,  0.0267,  0.0343,  ..., -0.0125, -0.0171,  0.0404],
        [-0.0072,  0.0656,  0.0092,  ..., -0.0071, -0.0318,  0.0155],
        [-0.0054,  0.0206,  0.0425,  ..., -0.0231, -0.0191,  0.0368],
        ...,
        [-0.0179, -0.0202,  0.0114,  ..., -0.0052,  0.0018, -0.0186],
        [ 0.0156, -0.0419, -0.0262,  ...,  0.0389, -0.0376, -0.0473],
        [-0.0436,  0.0219,  0.0154,  ..., -0.0014, -0.0074,  0.0465]])

llm.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0267,  0.0141,  0.0778,  ...,  0.0321, -0.0696,  0.0364],
        [ 0.0121,  0.0299,  0.0059,  ...,  0.0364, -0.0482, -0.0130],
        [ 0.0203, -0.0055,  0.0497,  ...,  0.0495, -0.0240,  0.0444],
        ...,
        [-0.0323,  0.0117,  0.0123,  ..., -0.0224,  0.0235, -0.0161],
        [ 0.0308,  0.0337,  0.0146,  ..., -0.0092, -0.0297,  0.0352],
        [ 0.0120,  0.0194,  0.0056,  ..., -0.0077,  0.0176,  0.0036]])

llm.base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0015,  0.0006,  0.0008,  ..., -0.0142, -0.0043, -0.0004],
        [ 0.0003,  0.0012,  0.0082,  ..., -0.0081,  0.0120, -0.0115],
        [-0.0020,  0.0029, -0.0025,  ...,  0.0133, -0.0056, -0.0003],
        ...,
        [ 0.0061,  0.0016, -0.0176,  ...,  0.0005,  0.0110, -0.0037],
        [-0.0322,  0.0006, -0.0225,  ...,  0.0164,  0.0157,  0.0267],
        [-0.0281, -0.0234,  0.0054,  ...,  0.0009, -0.0107, -0.0129]])

llm.base_model.model.model.layers.12.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-8.0078e-01,  2.3535e-01, -1.5747e-02, -8.7891e-02,  3.2812e-01,
         6.5918e-02,  7.5781e-01,  1.1084e-01,  4.2725e-02,  1.8652e-01,
        -1.7090e-01,  2.9663e-02, -2.3828e-01, -1.7700e-02,  4.9316e-02,
         1.3281e-01,  3.6914e-01,  4.5898e-02, -6.2988e-02, -1.4688e+00,
        -4.2969e-02, -4.7363e-02,  1.1484e+00, -3.5889e-02, -1.6719e+00,
        -3.0078e-01, -7.1777e-02, -1.3086e-01, -2.1875e+00, -6.8848e-02,
        -8.3984e-02, -4.1504e-02, -1.2207e-02,  2.2949e-02,  4.7302e-03,
        -1.4160e-01, -1.0400e-01,  8.6426e-02, -1.4746e-01, -3.0151e-02,
        -3.9062e-01, -1.3965e-01, -9.5215e-02, -2.5513e-02, -1.2256e-01,
         1.4258e-01,  2.0215e-01,  6.7871e-02, -1.7383e-01,  4.0625e+00,
        -3.2812e-01,  3.6719e-01, -1.4648e-01, -5.7031e-01,  1.1406e+00,
         1.1016e+00,  8.8281e-01, -8.2031e-01,  1.6625e+01,  1.7812e+00,
         3.9219e+00,  3.6250e+00,  1.2188e+00,  1.2656e+00,  7.4609e-01,
        -1.8799e-02,  8.3984e-01,  1.0986e-01, -3.9844e-01, -1.3965e-01,
         4.9609e-01, -1.0156e-01,  3.7354e-02, -9.5312e-01, -6.9885e-03,
         2.2363e-01,  1.1768e-01,  1.2422e+00,  9.1309e-02,  3.4912e-02,
         9.9219e-01,  1.1768e-01, -4.1797e-01,  2.6562e-01, -9.8633e-02,
        -9.7656e-03, -3.6719e-01,  1.2398e-04,  2.1973e-01, -5.9814e-02,
         1.2085e-02, -1.6699e-01,  3.7305e-01,  4.8096e-02, -1.4844e-01,
        -1.0352e-01, -1.3672e-01, -7.5000e-01,  1.2598e-01,  6.6895e-02,
        -2.6875e+00, -3.3203e-02,  1.7090e-02, -1.3086e-01, -4.9414e-01,
         2.9297e-02,  2.6953e-01, -2.9102e-01,  6.1719e-01,  3.4424e-02,
        -7.4707e-02, -7.1777e-02, -1.2695e-01,  3.9062e+00, -2.9297e-01,
         4.7656e-01, -3.1641e-01, -2.2827e-02, -3.9453e-01,  4.2725e-02,
        -3.1836e-01, -2.0874e-02, -9.0625e+00, -3.8281e+00, -1.0547e+00,
         7.6172e-01, -1.7891e+00, -6.1250e+00,  7.0801e-02, -2.4414e-01,
         2.0508e-01, -5.0781e-01, -7.2266e-02, -6.6016e-01, -2.0508e-01,
         7.7344e-01,  8.6914e-02, -2.7344e-01,  4.0625e-01, -1.3281e-01,
         1.8066e-02,  8.3496e-02,  4.9561e-02,  5.5859e-01,  2.6172e-01,
         1.7334e-02, -9.6484e-01,  5.6885e-02,  1.4343e-02,  8.7500e-01,
        -5.7129e-02, -1.1963e-01, -1.1133e-01, -7.7148e-02,  9.8438e-01,
         4.2236e-02,  1.3672e-02, -1.3203e+00,  8.1055e-02, -1.4746e-01,
        -1.2817e-02,  5.7129e-02,  1.3672e-01, -4.1992e-02, -2.5269e-02,
        -1.7422e+00,  2.9907e-02, -2.4780e-02,  3.6621e-02, -1.1621e-01,
        -7.0312e-02,  2.2339e-02, -1.2012e-01, -1.6357e-02, -1.2891e-01,
        -4.2725e-02,  1.0791e-01, -2.7500e+00, -1.0156e-01, -2.6172e-01,
        -1.3477e-01, -1.3379e-01, -2.4414e-01, -2.8198e-02,  2.3633e-01,
        -5.1953e-01, -9.1406e-01, -1.6797e-01, -3.5742e-01,  2.5977e-01,
        -2.9297e-01, -2.6758e-01, -3.4961e-01,  5.6250e-01, -5.7422e-01,
         2.1777e-01, -7.7637e-02,  2.2266e-01, -1.4746e-01,  7.1777e-02,
        -1.8997e-03,  1.3867e-01, -6.4453e-01,  5.7812e-01,  2.9907e-02,
        -5.2734e-01, -4.4922e-01,  4.0234e-01, -2.6001e-02,  2.7344e-01,
        -1.7676e-01,  1.8359e-01,  2.8711e-01,  4.9072e-02, -9.8633e-02,
         8.9844e-01, -2.2266e-01,  1.5625e-01,  3.3936e-02, -2.4292e-02,
         5.8594e-03,  5.0781e-02,  2.0020e-02,  8.3594e-01,  1.5430e-01,
         1.2891e-01,  5.7129e-02,  1.1475e-01,  1.9629e-01, -1.5332e-01,
         4.0283e-02,  4.1748e-02, -2.5146e-02,  3.3203e-02, -7.3242e-02,
        -8.6426e-02,  1.1016e+00, -2.0386e-02, -1.4343e-02, -7.7148e-02,
         7.2754e-02,  7.9688e-01, -1.9727e-01, -2.0625e+00, -1.7578e-01,
        -1.0938e-01, -3.5547e-01, -3.8452e-03,  1.2109e-01, -3.0664e-01,
         1.9141e-01, -9.3994e-03,  2.3633e-01, -3.3008e-01, -4.8637e-04,
        -7.6562e-01, -1.4746e-01,  6.7444e-03,  5.3125e-01,  2.5977e-01,
         9.6680e-02,  8.2031e-02,  2.4023e-01, -3.4180e-02, -2.2339e-02,
         3.9062e-02,  1.8848e-01, -4.8523e-03, -2.0752e-03,  1.1914e-01,
        -8.9844e-01,  1.2390e-02,  2.1875e-01,  8.9062e-01, -3.2227e-01,
        -8.9062e-01,  2.9688e-01,  2.1191e-01,  9.8828e-01,  1.9238e-01,
        -1.4746e-01, -1.1094e+00,  2.0898e-01, -5.1562e-01, -1.4844e-01,
        -2.4536e-02, -3.2715e-02,  1.8359e-01,  9.6875e-01, -1.4954e-03,
         1.0498e-01,  1.5430e-01,  7.6172e-01,  7.3730e-02, -3.4668e-02,
         3.2422e-01,  1.6504e-01,  8.4961e-02, -6.7383e-02,  1.3672e-01,
        -2.7812e+00, -8.0566e-02, -1.2012e-01, -2.0703e-01, -4.4678e-02,
        -5.0049e-02,  4.0234e-01, -2.1729e-02,  5.8350e-02, -3.4766e-01,
         3.5781e+00,  4.7461e-01, -2.5000e+00, -1.4531e+00,  1.2734e+00,
         3.0859e-01, -1.6016e+00,  1.0703e+00,  2.3125e+00,  1.7578e-01,
         6.4844e-01, -5.7812e-01,  3.1445e-01,  1.0693e-01, -5.7422e-01,
        -2.5195e-01, -6.9531e-01, -1.1719e-01,  7.4219e-01,  7.3242e-02,
        -7.5000e-01,  3.2031e-01, -2.7344e-01,  4.2578e-01, -1.3281e-01,
        -1.3672e-01,  6.4941e-02,  5.3711e-03,  1.4062e-01,  5.0354e-03,
         8.0078e-02,  2.6758e-01, -1.5430e-01, -1.0791e-01, -1.1133e-01,
         1.1572e-01, -4.9609e-01,  1.0986e-01, -1.1406e+00,  1.3086e-01,
        -4.4922e-02,  1.6172e+00, -2.0801e-01,  4.5654e-02, -9.1797e-02,
        -1.3086e-01, -2.3340e-01, -3.6328e-01,  8.3008e-02,  1.9219e+00,
        -5.1270e-02,  1.1475e-01,  2.3193e-03,  8.7402e-02,  8.0859e-01,
        -1.6357e-02, -1.8066e-01, -4.8828e-03,  1.3086e-01,  2.7539e-01,
         1.7090e-01,  1.7480e-01,  4.6875e-01, -2.4609e-01, -6.8359e-01,
         8.1250e-01, -2.0781e+00,  5.8838e-02, -3.9062e-01, -1.8125e+00,
        -3.3125e+00, -8.2812e-01,  6.0547e-01, -6.7188e+00,  1.1377e-01,
        -2.3633e-01, -2.1875e-01, -1.9629e-01,  2.1973e-02,  9.8633e-02,
         1.2207e-01, -5.8203e-01,  1.0078e+00, -2.6953e-01, -4.7852e-02,
         1.0625e+00, -2.3047e-01, -9.6094e-01,  2.6489e-02,  4.8242e-01,
        -3.3594e-01,  2.3535e-01,  6.1279e-02, -1.3477e-01,  1.1377e-01,
        -1.5234e+00, -2.0703e-01,  8.0078e-01,  2.4121e-01, -7.4219e-02,
        -1.4258e-01, -8.1055e-02,  6.1523e-02, -1.9238e-01, -1.1328e-01,
         7.1289e-02,  2.3438e-02,  1.8066e-01,  1.6953e+00,  3.0151e-02,
         2.1387e-01,  5.4321e-03,  1.7969e-01, -2.3047e-01,  2.5195e-01,
        -2.2461e-02,  9.1309e-02,  5.8105e-02,  2.2168e-01, -2.2949e-02,
        -5.3223e-02, -1.3770e-01,  2.6001e-02, -2.9419e-02, -4.4678e-02,
         5.3125e-01,  3.7598e-02, -2.1406e+00,  1.0742e-01, -9.3994e-03,
        -5.8105e-02, -1.5320e-02, -7.5684e-02, -9.0820e-02, -1.4941e-01,
         1.9727e-01, -8.5938e-02, -1.9141e-01, -9.2188e-01,  8.5156e-01,
        -5.1172e-01,  8.8672e-01, -1.2695e-01, -1.0156e+00, -1.5234e-01,
        -7.0801e-02,  7.4219e-02,  2.8516e-01,  2.9688e-01,  1.9141e-01,
        -5.4199e-02, -1.4453e-01, -1.8945e-01, -4.9561e-02,  2.7148e-01,
        -1.3359e+00, -1.0010e-01,  1.6992e-01,  2.3535e-01, -8.9844e-02,
        -4.9072e-02,  1.7188e-01, -4.1797e-01,  5.5542e-03, -9.8145e-02,
        -1.5938e+00, -7.0496e-03,  3.4180e-02, -1.6846e-02, -1.1484e+00,
         9.7168e-02, -1.7090e-01,  1.4844e-01,  1.0205e-01, -1.5747e-02,
         1.1670e-01,  9.8145e-02, -1.1230e-01, -1.1414e-02,  1.6235e-02,
        -3.5858e-03,  1.4355e-01, -1.4219e+00, -5.3467e-02, -8.6426e-02,
        -8.8867e-02,  2.1362e-02, -1.1902e-02,  3.5400e-03, -1.3672e-01,
         2.5879e-02, -3.7031e+00,  1.0303e-01, -5.1270e-03,  3.8910e-03,
         2.1240e-02,  7.1289e-02,  6.5430e-02, -1.2891e-01,  3.6133e-01,
         7.7820e-03,  2.9492e-01])

llm.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 2.7931e-03,  1.4283e-02,  2.7078e-02,  ..., -2.0854e-02,
          4.9290e-05,  2.2947e-02],
        [-4.7954e-02,  3.2089e-02,  9.2368e-03,  ...,  1.5366e-03,
         -2.9434e-02,  8.9430e-04],
        [ 3.2785e-02,  1.4682e-02,  1.7942e-03,  ..., -3.1921e-02,
         -1.1620e-02,  2.0290e-02],
        ...,
        [ 5.6237e-03, -5.0480e-02, -2.3674e-02,  ...,  8.4705e-03,
          4.4554e-02,  2.4565e-03],
        [-1.7834e-02, -1.6147e-02, -1.5000e-03,  ..., -2.1908e-02,
          5.6229e-03, -1.2026e-02],
        [ 8.5154e-03, -2.2715e-02, -1.3620e-02,  ..., -2.3280e-02,
          1.3314e-02, -1.4427e-02]])

llm.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 1.2395e-02,  1.5312e-02, -8.1318e-03,  ..., -1.4285e-02,
          3.6766e-03, -9.2799e-05],
        [-1.1278e-02, -4.3368e-02,  5.7110e-02,  ...,  4.2990e-02,
          9.7966e-03,  3.8314e-02],
        [ 4.0186e-02, -2.4685e-03, -2.3439e-02,  ..., -1.8932e-02,
         -4.6019e-02, -2.8563e-02],
        ...,
        [ 2.2678e-03, -1.5202e-02,  9.0943e-03,  ...,  2.7139e-03,
         -4.3493e-03,  2.1307e-04],
        [ 5.2022e-02,  2.3458e-02,  3.0136e-02,  ..., -1.1007e-02,
         -4.4425e-02,  1.7233e-02],
        [-1.6226e-02, -6.0602e-03, -6.4660e-03,  ...,  2.4132e-02,
         -1.3866e-02, -2.0593e-02]])

llm.base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0019,  0.0085,  0.0084,  ..., -0.0067, -0.0162,  0.0256],
        [ 0.0249, -0.0193, -0.0447,  ...,  0.0112, -0.0034,  0.0034],
        [ 0.0005,  0.0167,  0.0310,  ..., -0.0146, -0.0035, -0.0021],
        ...,
        [ 0.0125, -0.0104, -0.0046,  ...,  0.0356, -0.0129,  0.0077],
        [-0.0047, -0.0143,  0.0004,  ..., -0.0019, -0.0128,  0.0046],
        [ 0.0087,  0.0217, -0.0070,  ...,  0.0025,  0.0051, -0.0147]])

llm.base_model.model.model.layers.12.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 7.3242e-02, -1.7334e-02,  8.9844e-02, -6.0547e-02, -5.5176e-02,
        -5.1880e-03,  6.0547e-02, -9.5703e-02, -2.6953e-01,  1.4453e-01,
         3.2031e-01, -2.2583e-02, -7.1716e-03, -1.0193e-02,  4.5654e-02,
         7.7148e-02,  1.6602e-01, -2.0386e-02, -2.5146e-02,  5.2185e-03,
        -8.6914e-02, -3.6377e-02, -2.1484e-02, -9.4727e-02, -7.2754e-02,
         1.6724e-02,  9.9609e-02, -1.7334e-02,  3.5645e-02,  2.0117e-01,
        -8.5938e-02,  1.5332e-01, -6.2012e-02,  3.8086e-02,  3.8452e-03,
        -4.2480e-02,  1.2061e-01, -3.0884e-02,  4.0894e-03, -1.0986e-01,
         1.9727e-01, -1.3867e-01, -2.1851e-02, -7.6660e-02, -4.4556e-03,
         4.7607e-02,  4.4922e-02,  1.0742e-01, -1.2402e-01, -5.4932e-02,
        -6.9824e-02,  4.0283e-02, -6.6895e-02,  5.2490e-02, -3.0396e-02,
         5.9570e-02,  9.1309e-02, -8.3984e-02,  3.1982e-02,  8.9844e-02,
         3.3789e-01, -1.1719e-01,  7.6172e-02,  1.4496e-03,  2.6733e-02,
        -9.1309e-02,  3.0664e-01,  2.8125e-01,  9.7656e-02,  1.1230e-02,
        -9.2773e-02, -5.8289e-03, -1.7969e-01, -9.4238e-02,  4.3213e-02,
         1.2451e-01, -6.5430e-02,  7.3730e-02, -3.3417e-03,  2.7344e-02,
        -2.5635e-02,  2.7771e-03, -1.1035e-01, -6.8848e-02, -9.8633e-02,
         1.0254e-01, -6.5308e-03, -3.9551e-02, -1.5015e-02,  1.0840e-01,
        -3.2959e-02,  7.7637e-02, -4.2578e-01,  1.2158e-01, -5.1758e-02,
         4.2725e-02,  7.5195e-02, -1.3123e-02, -7.2266e-02, -3.2471e-02,
        -2.9907e-02, -1.5450e-04,  6.7871e-02,  1.6357e-02, -2.9688e-01,
        -2.2070e-01, -7.3438e-01,  1.2109e-01,  3.0273e-02, -1.8359e-01,
         3.7109e-02,  5.5908e-02, -6.5430e-02, -4.5166e-02,  4.3164e-01,
        -6.4941e-02, -2.0874e-02, -5.4550e-04,  4.1992e-02,  7.4219e-02,
        -8.0566e-02, -8.9844e-02,  1.4771e-02,  1.0840e-01,  5.7373e-02,
        -4.9805e-02,  5.2490e-02, -3.8574e-02,  2.5757e-02, -2.0885e-04,
        -5.0049e-02,  1.1658e-02,  1.4844e-01,  4.1260e-02, -3.7109e-02,
         3.8330e-02, -2.5146e-02,  5.8105e-02, -5.1025e-02, -4.9072e-02,
        -4.9805e-02, -2.5757e-02, -8.5449e-02,  6.1646e-03,  8.0566e-02,
         1.0498e-01, -2.0264e-02, -1.0071e-02,  4.6387e-02, -4.7852e-02,
         3.1494e-02, -7.6599e-03, -6.4941e-02, -1.1426e-01,  3.1006e-02,
         2.8076e-02,  2.0264e-02, -5.3955e-02, -4.0771e-02, -5.4443e-02,
         5.9326e-02,  2.3804e-02,  1.4771e-02,  1.1047e-02,  9.8145e-02,
        -5.2734e-02, -3.7842e-02,  6.0425e-03,  1.6235e-02,  5.1025e-02,
        -1.3000e-02,  2.1606e-02,  1.2390e-02,  5.3467e-02,  3.6133e-02,
         7.3730e-02,  4.9744e-03, -2.5586e-01, -3.5400e-02,  5.9082e-02,
         1.0910e-03, -1.6357e-02,  8.7280e-03, -2.7588e-02, -1.8433e-02,
         9.2773e-03, -5.8838e-02, -2.1851e-02,  3.8330e-02,  2.1362e-02,
         6.7383e-02,  1.7090e-02,  5.0293e-02,  6.0547e-02,  5.3467e-02,
         1.1749e-03,  1.7773e-01,  3.6774e-03,  5.6152e-02, -4.6997e-03,
         6.6406e-02,  3.3203e-02,  3.6011e-03, -7.1411e-03,  3.4912e-02,
        -3.1494e-02,  7.7148e-02, -9.2773e-02,  5.8594e-02,  5.9082e-02,
         5.0781e-02, -2.5635e-02,  7.9590e-02, -5.1117e-04,  1.8921e-02,
        -2.1118e-02, -4.5166e-02,  4.6875e-02,  6.3477e-02,  2.8564e-02,
         5.3955e-02, -1.8066e-02,  3.9062e-02,  4.8828e-02, -7.0953e-04,
        -9.2285e-02, -1.9989e-03,  2.9053e-02,  5.2734e-02, -9.6191e-02,
        -1.8433e-02,  5.9082e-02, -1.9287e-02,  1.1963e-01, -5.1575e-03,
         3.5645e-02,  7.3242e-02, -1.1230e-02, -4.1504e-03, -1.2146e-02,
        -3.9551e-02,  4.9316e-02, -3.5645e-02,  1.8997e-03, -4.1016e-02,
        -3.8818e-02, -7.0312e-02,  7.9346e-03,  3.7689e-03,  1.8555e-02,
         3.2227e-02,  9.8419e-04,  2.5879e-02,  2.2125e-03, -6.9336e-02,
        -7.4219e-02, -1.2512e-02, -1.2451e-02,  5.4932e-03, -2.1240e-02,
        -5.5847e-03, -1.3733e-02,  2.7771e-03,  1.5869e-03, -7.0312e-02,
         1.1536e-02, -5.0659e-03, -3.1738e-02, -1.9684e-03, -2.1210e-03,
         3.2715e-02, -2.8442e-02, -1.0071e-02,  1.0742e-02,  6.3477e-02,
        -2.8320e-02,  1.2573e-02,  1.2360e-03,  1.6113e-02, -3.3203e-02,
         2.7710e-02,  1.2589e-03, -1.7944e-02, -1.8799e-02,  2.2583e-02,
        -9.8267e-03, -1.4404e-02, -2.2949e-02,  3.1738e-02,  1.1780e-02,
        -3.6926e-03,  2.5146e-02,  8.1787e-03, -2.0874e-02, -1.8433e-02,
        -4.5410e-02,  4.1992e-02,  3.5156e-02, -2.9755e-04, -8.0566e-03,
        -1.7624e-03,  2.2461e-02, -2.1484e-02, -1.2878e-02, -4.6387e-02,
        -1.1414e-02, -3.5400e-03, -6.2988e-02, -4.9133e-03,  5.0354e-03,
         2.1515e-03,  3.0823e-03, -3.9062e-03, -5.8594e-03,  3.3379e-04,
        -2.0996e-02,  1.8311e-02,  2.1820e-03, -2.3926e-02,  1.5747e-02,
        -9.5215e-03,  3.0029e-02, -1.9775e-02, -1.4221e-02,  9.8267e-03,
        -3.2715e-02,  3.7598e-02, -7.0496e-03,  2.4170e-02, -3.1494e-02,
        -1.8921e-02,  5.3101e-03,  3.2959e-02, -1.4420e-03,  1.1047e-02,
         2.7466e-02,  3.1128e-02,  1.5869e-02, -3.9368e-03, -3.0396e-02,
         1.1902e-02,  1.8234e-03, -2.0264e-02, -6.7444e-03,  1.1108e-02,
        -3.6011e-03, -4.0894e-03, -1.9409e-02,  5.7861e-02, -2.3071e-02,
         1.6937e-03,  1.4648e-02,  3.8330e-02, -1.1719e-02,  4.1992e-02,
        -1.0986e-02,  4.4861e-03, -1.1108e-02,  1.5747e-02,  2.9297e-02,
        -3.3691e-02, -1.0864e-02,  7.3242e-03,  2.4414e-02, -2.8809e-02,
        -2.0386e-02,  2.5146e-02, -9.1553e-04,  1.0803e-02, -1.8066e-02,
         2.6489e-02,  6.0425e-03, -1.8188e-02, -5.6152e-02,  3.6133e-02,
         3.2959e-02,  1.2054e-03,  4.3945e-02,  2.2461e-02,  4.0039e-02,
         4.9744e-03,  2.9755e-03,  3.0518e-02,  3.7994e-03,  1.4648e-01,
        -1.8921e-02,  1.9897e-02, -2.5635e-02, -4.4189e-02,  7.1777e-02,
        -4.1748e-02,  8.2031e-02,  1.8066e-02, -4.3701e-02, -4.6387e-02,
        -1.9165e-02, -4.5586e-04,  3.9062e-02, -2.5513e-02,  1.6479e-02,
        -4.1992e-02, -1.1780e-02, -2.7954e-02, -5.2734e-02, -3.4668e-02,
        -7.5989e-03,  5.5420e-02, -1.4941e-01,  1.1377e-01,  3.1250e-02,
        -7.3242e-02,  4.6875e-02, -6.1279e-02,  1.5430e-01,  9.2285e-02,
         7.6660e-02, -6.6406e-02,  2.8442e-02,  5.6152e-02,  6.5918e-02,
         1.0938e-01, -2.4567e-03,  4.9072e-02,  6.8359e-02,  7.3242e-02,
         5.5176e-02, -1.2512e-02,  6.5918e-02, -5.2979e-02,  1.9653e-02,
        -6.2988e-02,  7.8125e-02,  7.8613e-02, -4.8096e-02, -1.0681e-02,
        -8.5938e-02, -4.1797e-01,  2.5879e-02, -7.8613e-02, -2.3926e-02,
         6.7383e-02,  1.4709e-02,  5.8105e-02, -1.1230e-01,  1.3916e-02,
        -2.9541e-02, -7.2479e-04, -1.3306e-02, -1.6235e-02, -1.4160e-02,
         1.7944e-02,  2.1729e-02, -5.6396e-02,  5.9570e-02,  3.2806e-03,
        -1.8457e-01,  1.8188e-02,  1.4355e-01,  1.2283e-03, -2.6855e-02,
        -3.5156e-02,  4.1260e-02, -2.7100e-02,  6.3477e-02,  2.8687e-02,
        -7.0496e-03, -5.8899e-03,  9.3750e-02, -3.9062e-02, -8.9844e-02,
        -2.7344e-02,  5.0537e-02, -3.8574e-02,  3.2997e-04, -1.7212e-02,
         1.7212e-02,  6.5430e-02,  4.5166e-02,  3.4180e-02, -4.4922e-02,
         2.0020e-02,  7.2266e-02,  9.3384e-03,  3.6621e-02,  7.4219e-02,
        -1.1035e-01,  4.1504e-03,  3.0640e-02,  2.0020e-02, -1.8066e-02,
        -6.1279e-02, -2.1362e-02, -9.5825e-03,  1.0791e-01,  7.8125e-02,
         3.0029e-02, -2.0264e-02, -3.5889e-02,  3.3691e-02, -8.8867e-02,
        -3.7598e-02, -1.5747e-02,  2.6733e-02,  3.0029e-02,  8.1543e-02,
        -7.1289e-02,  2.6245e-02, -2.4170e-02,  1.4282e-02, -5.0781e-02,
         1.7334e-02, -6.9336e-02])

llm.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0034, -0.0087, -0.0129,  ..., -0.0071, -0.0063,  0.0093],
        [ 0.0153,  0.0311,  0.0223,  ...,  0.0286,  0.0387, -0.0267],
        [-0.0259, -0.0224,  0.0314,  ..., -0.0355,  0.0267,  0.0439],
        ...,
        [ 0.0237,  0.0026,  0.0106,  ...,  0.0455,  0.0167,  0.0013],
        [-0.0140, -0.0115, -0.0025,  ..., -0.0123, -0.0311, -0.0189],
        [-0.0122,  0.0018, -0.0169,  ...,  0.0035,  0.0486, -0.0232]])

llm.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0247,  0.0023,  0.0064,  ..., -0.0259, -0.0213,  0.0003],
        [-0.0196, -0.0142,  0.0138,  ...,  0.0049,  0.0133, -0.0199],
        [-0.0256, -0.0124,  0.0101,  ...,  0.0066, -0.0093,  0.0676],
        ...,
        [ 0.0020,  0.0185, -0.0323,  ...,  0.0240, -0.0120, -0.0090],
        [-0.0203, -0.0258, -0.0071,  ..., -0.0073, -0.0362, -0.0066],
        [ 0.0138,  0.0147,  0.0280,  ...,  0.0220, -0.0030, -0.0279]])

llm.base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0022, -0.0294, -0.0089,  ..., -0.0167, -0.0203, -0.0031],
        [-0.0070,  0.0082, -0.0101,  ...,  0.0220, -0.0081, -0.0005],
        [-0.0030,  0.0430, -0.0334,  ...,  0.0012, -0.0135, -0.0007],
        ...,
        [-0.0025, -0.0015,  0.0082,  ...,  0.0168, -0.0017, -0.0121],
        [ 0.0157, -0.0023,  0.0007,  ...,  0.0160, -0.0082,  0.0093],
        [-0.0233, -0.0073, -0.0056,  ...,  0.0115, -0.0051,  0.0179]])

llm.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0234,  0.0353,  0.0517,  ...,  0.0069, -0.0164,  0.0230],
        [-0.0680, -0.0147,  0.0495,  ...,  0.0332, -0.0540,  0.0482],
        [ 0.0289,  0.0007, -0.0232,  ..., -0.0079, -0.0474,  0.0363],
        ...,
        [-0.0019,  0.0045,  0.0107,  ...,  0.0226,  0.0075, -0.0310],
        [-0.0062, -0.0139,  0.0371,  ...,  0.0215, -0.0370, -0.0109],
        [ 0.0026,  0.0527,  0.0074,  ...,  0.0020,  0.0236,  0.0477]])

llm.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0068,  0.0272, -0.0200,  ..., -0.0152, -0.0217,  0.0292],
        [-0.0162, -0.0019,  0.0016,  ...,  0.0077,  0.0447, -0.0473],
        [ 0.0012, -0.0455,  0.0238,  ...,  0.0146, -0.0119,  0.0274],
        ...,
        [ 0.0023,  0.0190,  0.0152,  ..., -0.0273,  0.0266,  0.0117],
        [-0.0462, -0.0250, -0.0234,  ..., -0.0148, -0.0508, -0.0006],
        [-0.0441,  0.0234,  0.0216,  ..., -0.0094,  0.0154,  0.0388]])

llm.base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0166,  0.0154,  0.0170,  ..., -0.0016, -0.0039,  0.0090],
        [ 0.0114, -0.0042, -0.0016,  ...,  0.0219, -0.0225,  0.0044],
        [-0.0216,  0.0115,  0.0121,  ..., -0.0042, -0.0242, -0.0023],
        ...,
        [ 0.0157,  0.0237, -0.0082,  ..., -0.0126,  0.0090, -0.0381],
        [-0.0286, -0.0093,  0.0361,  ..., -0.0123,  0.0060,  0.0170],
        [ 0.0153, -0.0150,  0.0052,  ..., -0.0154, -0.0229, -0.0271]])

llm.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0762, -0.0689, -0.0175,  ..., -0.0189, -0.0065,  0.0357],
        [ 0.0286,  0.0343,  0.0306,  ..., -0.0013, -0.0153,  0.0223],
        [-0.0144,  0.0134, -0.0218,  ...,  0.0140, -0.0699,  0.0465],
        ...,
        [ 0.0163,  0.0259, -0.0027,  ..., -0.0361,  0.0317,  0.0099],
        [ 0.0403, -0.0276,  0.0077,  ...,  0.0349, -0.0035,  0.0279],
        [ 0.0086, -0.0084,  0.0075,  ..., -0.0354, -0.0157, -0.0240]])

llm.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 1.8633e-02,  1.0634e-02,  1.8434e-02,  ..., -4.2036e-03,
          1.0947e-03, -9.5095e-03],
        [ 2.1836e-02,  1.9994e-02, -3.8492e-02,  ..., -1.4168e-02,
         -3.1519e-05, -9.1515e-03],
        [ 1.7450e-02, -5.8050e-02,  1.3901e-02,  ...,  6.5606e-03,
         -1.4504e-02, -4.0719e-02],
        ...,
        [ 7.5742e-03,  2.1303e-02,  1.9321e-02,  ...,  6.2570e-02,
          2.4286e-02, -6.5322e-03],
        [ 2.0804e-02, -2.1666e-02, -9.5851e-03,  ..., -2.2197e-02,
         -6.3530e-03,  2.7208e-02],
        [-5.7909e-02, -1.3999e-02,  4.0407e-03,  ...,  2.8341e-02,
         -1.8338e-02, -1.6619e-02]])

llm.base_model.model.model.layers.12.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0147, -0.0067, -0.0015,  ..., -0.0198, -0.0044, -0.0006],
        [-0.0176, -0.0004, -0.0029,  ...,  0.0079, -0.0215,  0.0097],
        [-0.0209,  0.0216,  0.0170,  ...,  0.0090,  0.0143,  0.0255],
        ...,
        [-0.0082, -0.0201,  0.0064,  ..., -0.0008, -0.0074, -0.0104],
        [-0.0005, -0.0332,  0.0115,  ...,  0.0116,  0.0276, -0.0081],
        [-0.0479,  0.0065, -0.0092,  ..., -0.0037, -0.0070,  0.0153]])

llm.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0030, -0.0219, -0.0185,  ...,  0.0226, -0.0259, -0.0312],
        [ 0.0040,  0.0057, -0.0709,  ..., -0.1160,  0.0760,  0.0150],
        [-0.0553,  0.0132,  0.0270,  ...,  0.0655, -0.0101,  0.0213],
        ...,
        [ 0.0065, -0.0093,  0.0408,  ..., -0.0100,  0.0142, -0.0096],
        [-0.0080, -0.0321, -0.0556,  ...,  0.0184,  0.0146, -0.0174],
        [-0.0267, -0.0134,  0.0015,  ...,  0.0028,  0.0177,  0.0045]])

llm.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0312, -0.0052,  0.0067,  ..., -0.0061,  0.0391, -0.0168],
        [ 0.0070, -0.0209,  0.0074,  ..., -0.0216,  0.0060,  0.0513],
        [ 0.0590,  0.0033, -0.0034,  ...,  0.0407,  0.0369, -0.0764],
        ...,
        [ 0.0124, -0.0060, -0.0112,  ...,  0.0214,  0.0148, -0.0130],
        [ 0.0456, -0.0041,  0.0077,  ...,  0.0087,  0.0101,  0.0067],
        [ 0.0067,  0.0084, -0.0073,  ..., -0.0114,  0.0355, -0.0036]])

llm.base_model.model.model.layers.12.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 3.1006e-02, -1.1597e-02, -1.8188e-02,  ..., -2.3438e-02,
          1.3000e-02, -2.5757e-02],
        [-2.6550e-03,  3.7231e-03,  6.6528e-03,  ..., -7.4005e-04,
         -8.9111e-03, -3.6774e-03],
        [-4.5776e-03,  4.4189e-02,  1.0193e-02,  ...,  2.6245e-03,
          2.8491e-05,  8.6060e-03],
        ...,
        [-2.1362e-02,  4.8828e-03, -3.2501e-03,  ..., -1.7212e-02,
         -2.2217e-02,  1.5991e-02],
        [ 8.3160e-04, -2.0386e-02, -1.3428e-03,  ..., -1.0376e-02,
          3.6926e-03,  3.5248e-03],
        [-1.4973e-04, -2.4292e-02,  1.6724e-02,  ..., -5.6763e-03,
          5.5542e-03,  5.3101e-03]])

llm.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0214, -0.0238, -0.0203,  ...,  0.0187, -0.0109, -0.0200],
        [-0.0778, -0.0115, -0.0487,  ...,  0.0195, -0.0013,  0.0179],
        [ 0.0514,  0.0363,  0.0120,  ..., -0.0280, -0.0159, -0.0003],
        ...,
        [-0.0096,  0.0584, -0.0479,  ..., -0.0263,  0.0027,  0.0250],
        [-0.0094, -0.0157,  0.0029,  ..., -0.0252,  0.0005, -0.0372],
        [ 0.0225, -0.0033,  0.0069,  ...,  0.0261,  0.0123,  0.0167]])

llm.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0166,  0.0408,  0.0035,  ..., -0.0090, -0.0079,  0.0310],
        [ 0.0032,  0.0328, -0.0179,  ..., -0.0303,  0.0199, -0.0230],
        [ 0.0364,  0.0608,  0.0015,  ..., -0.0168, -0.0129, -0.0237],
        ...,
        [ 0.0479,  0.0166, -0.0276,  ..., -0.0002, -0.0377, -0.0209],
        [-0.0144, -0.0255, -0.0072,  ..., -0.0231,  0.0293, -0.0034],
        [-0.0208, -0.0342, -0.0254,  ...,  0.0061, -0.0457, -0.0328]])

llm.base_model.model.model.layers.12.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.5234, 0.6328, 1.8359,  ..., 0.3496, 0.8672, 1.0625])

llm.base_model.model.model.layers.12.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.1797, 0.9727, 1.5938,  ..., 0.5234, 1.0938, 1.1094])

llm.base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0183,  0.0013, -0.0183,  ...,  0.0164, -0.0038,  0.0233],
        [-0.0101, -0.0038, -0.0160,  ...,  0.0053, -0.0038, -0.0161],
        [-0.0145, -0.0050, -0.0151,  ..., -0.0032,  0.0045, -0.0051],
        ...,
        [ 0.0232, -0.0181, -0.0322,  ..., -0.0006,  0.0298,  0.0250],
        [ 0.0018, -0.0010,  0.0057,  ...,  0.0078,  0.0177, -0.0115],
        [-0.0203, -0.0110,  0.0344,  ..., -0.0125,  0.0009,  0.0066]])

llm.base_model.model.model.layers.13.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([-0.6797,  0.7344, -0.2178,  ..., -0.2432,  0.2598, -1.7656])

llm.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0246,  0.0003, -0.0189,  ..., -0.0109, -0.0536, -0.0014],
        [ 0.0310,  0.0189,  0.0624,  ..., -0.0419,  0.0136, -0.0354],
        [-0.0191,  0.0306,  0.0165,  ...,  0.0112,  0.0263,  0.0277],
        ...,
        [ 0.0526, -0.0321,  0.0039,  ..., -0.0330, -0.0124,  0.0392],
        [ 0.0197,  0.0423,  0.0175,  ..., -0.0016, -0.0009,  0.0037],
        [-0.0028, -0.0244, -0.0036,  ...,  0.0149, -0.0488, -0.0299]])

llm.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-2.8453e-02,  1.2479e-02, -9.9683e-03,  ...,  6.8439e-03,
          3.7525e-03,  1.9147e-02],
        [-2.2283e-02,  1.1680e-03,  8.9037e-05,  ..., -3.1383e-02,
          5.7135e-02,  2.3108e-02],
        [-1.1884e-02,  1.7591e-02, -2.6572e-02,  ..., -1.6000e-02,
         -2.0672e-02, -1.4942e-03],
        ...,
        [ 5.9717e-03,  4.5459e-04,  9.7539e-03,  ..., -1.5748e-02,
         -6.4177e-03, -5.2313e-03],
        [ 3.7737e-03, -1.2157e-02,  1.4304e-02,  ...,  8.5765e-03,
         -1.5625e-02, -4.8896e-03],
        [ 2.0917e-03,  4.6501e-03, -1.2000e-02,  ..., -5.1978e-04,
         -9.1058e-05, -9.7858e-03]])

llm.base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 3.2806e-03,  1.3199e-03, -1.0254e-02,  ...,  1.9287e-02,
          6.7520e-04,  1.6235e-02],
        [ 8.8501e-03,  4.0283e-03, -1.3977e-02,  ..., -1.3672e-02,
          1.9043e-02,  3.7842e-03],
        [-4.1008e-05, -2.4261e-03, -1.8555e-02,  ...,  1.5137e-02,
          1.4648e-02,  8.7891e-03],
        ...,
        [ 1.0803e-02,  2.2827e-02,  1.0376e-02,  ..., -7.8125e-03,
         -2.2461e-02, -2.8076e-02],
        [ 4.0039e-02, -8.9722e-03, -1.7776e-03,  ...,  3.1738e-02,
          1.5991e-02,  1.4771e-02],
        [-1.4404e-02,  1.4954e-02,  5.9204e-03,  ..., -3.4485e-03,
          1.2695e-02, -1.1230e-02]])

llm.base_model.model.model.layers.13.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-7.1484e-01, -6.4844e-01, -3.6523e-01,  2.7148e-01, -1.0469e+00,
         2.3071e-02,  2.1387e-01, -2.8125e-01, -1.2061e-01,  2.0117e-01,
         1.7871e-01,  7.9590e-02, -1.6016e-01,  2.7539e-01,  3.2031e-01,
        -1.6797e-01,  1.0625e+00,  2.7734e-01, -1.6406e-01,  1.2500e+00,
         7.0312e-02,  2.5195e-01, -1.7773e-01, -4.1260e-02,  1.1279e-01,
        -2.6953e-01, -1.0986e-01, -3.3789e-01,  1.4453e-01,  9.4727e-02,
        -2.1484e-01,  2.0781e+00, -3.1982e-02,  1.6992e-01, -2.0703e-01,
         1.1426e-01,  8.4473e-02, -2.3438e+00,  1.1914e-01, -1.5137e-01,
        -2.4316e-01,  2.6172e-01,  5.2979e-02,  7.8613e-02, -5.4932e-02,
        -4.4922e-01,  1.4453e-01, -4.4922e-02,  7.9688e-01, -3.6621e-02,
         1.4258e-01,  4.1406e-01, -5.5078e-01, -6.1279e-02, -1.9238e-01,
         1.1963e-01,  8.6250e+00,  2.6172e-01, -1.7383e-01, -4.8750e+00,
         2.0781e+00,  2.1289e-01, -2.6500e+01,  5.8203e-01, -8.1250e-01,
         9.0625e-01, -4.1992e-01, -4.1992e-01, -4.5898e-01, -2.1484e-01,
        -5.6396e-02,  1.0000e+00,  3.2959e-02,  6.3782e-03, -9.3359e-01,
         1.4258e-01,  4.2969e-01, -9.5703e-01,  1.0791e-01,  2.2559e-01,
        -8.7891e-03,  2.2266e-01, -7.5195e-02, -7.9102e-02,  1.5198e-02,
         2.5879e-02, -1.4531e+00,  1.9336e-01, -2.6367e-01, -1.7422e+00,
        -2.1289e-01, -3.3594e-01,  2.0020e-01,  3.8672e-01, -2.4048e-02,
        -3.0273e-01, -2.8711e-01,  5.5420e-02, -1.3379e-01, -3.4766e-01,
         1.0059e-01,  4.9316e-02, -5.4321e-03, -2.9053e-02, -2.0215e-01,
         1.9684e-03, -1.0303e-01,  6.9922e-01, -1.5918e-01,  1.0938e-01,
        -4.9805e-01, -2.7344e-01, -2.9531e+00,  4.6875e-01,  3.7598e-02,
        -3.9258e-01, -4.2188e-01,  6.0547e-01,  1.6699e-01,  7.6562e-01,
        -1.2562e+01, -7.6562e-01, -1.0254e-01, -8.8750e+00,  5.2188e+00,
         3.8477e-01,  3.9750e+01, -6.7188e+00, -8.2031e-01, -3.4766e-01,
         1.5469e+00, -2.5391e-01,  2.6367e-01,  2.0215e-01,  5.3125e-01,
        -5.3955e-02,  1.7266e+00,  1.5527e-01,  1.7578e-01, -2.4512e-01,
        -1.8047e+00,  7.4219e-02,  1.7969e-01, -1.5430e-01,  1.9922e+00,
        -1.4258e-01,  3.6719e-01,  3.5645e-02, -1.8164e-01,  2.7148e-01,
         1.9727e-01, -2.9297e-02,  8.0566e-02,  1.6211e-01, -1.7969e-01,
        -2.2168e-01,  2.3193e-02, -1.3867e-01,  2.4219e-01,  5.8350e-02,
        -1.4954e-02,  2.9102e-01,  2.3594e+00,  7.4707e-02, -1.5137e-01,
         5.6885e-02, -3.1250e-01, -1.9775e-02,  1.1621e-01, -8.3008e-02,
         3.6133e-02, -3.2422e-01,  5.2246e-02,  1.4648e-01,  4.3750e+00,
        -1.7188e-01,  5.1758e-02,  3.1836e-01,  2.5312e+00, -1.3086e-01,
         8.8379e-02,  7.1094e-01,  8.5938e-01, -1.6562e+00, -3.3203e-01,
        -8.9453e-01,  2.2500e+00,  7.1875e-01,  5.9766e-01, -2.0469e+00,
        -1.7656e+00, -1.5156e+00, -2.9688e+00, -1.1279e-01,  4.2969e-02,
        -1.5332e-01, -1.5391e+00,  1.7188e-01,  5.6250e-01,  1.4062e-01,
         2.5977e-01, -2.2363e-01, -5.0781e-02, -2.4170e-02, -2.1582e-01,
         4.5898e-01,  1.0938e-01,  2.0801e-01,  4.1748e-02,  1.8457e-01,
         6.0156e-01,  1.2988e-01, -8.5938e-02,  1.8945e-01,  2.1875e+00,
         9.6680e-02, -2.4609e-01,  2.0703e-01, -1.6992e-01, -1.8750e+00,
         1.5076e-02,  1.6797e-01,  1.0078e+00,  1.8555e-01,  2.0996e-01,
         1.4746e-01, -1.7578e-01, -1.4844e-01,  6.4453e-02,  1.1328e-01,
        -4.1992e-02,  2.9663e-02, -3.2227e-02, -1.1133e-01, -1.3184e-01,
         4.2578e-01,  1.1902e-02,  2.6611e-02,  2.3125e+00, -4.2578e-01,
         2.5977e-01,  1.0596e-01,  2.1562e+00,  2.5977e-01,  1.0107e-01,
        -4.3945e-01,  5.9766e-01, -7.2656e-01,  2.0117e-01,  2.4688e+00,
         1.2812e+00, -8.7109e-01,  3.0664e-01,  5.3516e-01, -1.1562e+00,
         1.6484e+00,  8.9844e-01, -5.6396e-02,  8.1641e-01, -6.6016e-01,
        -6.4062e-01,  4.6680e-01, -1.1914e-01, -3.9062e-01, -5.8594e-02,
         1.0938e-01, -4.9609e-01,  5.7617e-02,  4.1016e-02,  2.0630e-02,
        -3.1445e-01, -1.0791e-01,  4.1748e-02, -5.9375e-01, -3.5889e-02,
         5.5859e-01,  3.4570e-01, -6.7383e-02,  8.6328e-01, -2.0020e-01,
        -1.5723e-01,  7.7344e-01,  2.1387e-01,  8.6670e-03, -1.4688e+00,
        -1.7822e-02,  5.0781e-02,  1.7812e+00,  6.0547e-02,  1.6699e-01,
         1.7090e-01, -3.9258e-01, -1.6724e-02, -1.3086e-01, -2.0020e-01,
         1.9531e-01,  7.2754e-02, -2.2363e-01, -1.6211e-01,  1.2598e-01,
         6.9336e-02,  3.6719e+00,  1.4258e-01, -3.9258e-01, -4.7656e-01,
        -1.8359e-01, -1.1133e-01,  3.8281e-01, -3.0078e-01,  1.2891e+00,
         4.5625e+00,  1.6724e-02, -3.7656e+00, -3.1250e-01,  3.8281e-01,
         2.3125e+00, -3.3125e+00,  4.4375e+00,  1.0062e+01, -5.1953e-01,
         3.1641e-01, -4.4336e-01,  2.4902e-01, -2.0215e-01, -3.8330e-02,
         3.4570e-01, -3.0664e-01,  8.2812e-01,  3.4766e-01,  6.1523e-02,
        -4.6680e-01,  9.9609e-02,  8.9844e-01, -8.9844e-02,  6.6797e-01,
         2.2461e-01,  2.4805e-01,  6.8054e-03, -1.0498e-01,  9.2578e-01,
         6.3281e-01,  2.8906e-01, -4.5312e-01,  1.4062e-01,  3.1055e-01,
         1.4219e+00,  1.7188e-01,  1.6797e-01,  1.8359e-01, -8.9355e-02,
         1.3086e-01, -8.2812e-01,  7.6904e-03,  1.6724e-02, -9.1797e-02,
        -1.6016e+00,  1.2109e-01, -1.4160e-01, -1.7188e-01, -1.6562e+00,
         2.7954e-02,  2.9688e-01, -2.2705e-02, -2.1484e-01, -3.9307e-02,
        -2.5938e+00, -1.5625e-01, -1.0010e-01, -1.9922e-01, -4.1504e-02,
         7.7148e-02, -1.8652e-01,  4.2725e-02,  1.5625e-01,  1.1500e+01,
        -3.4961e-01,  3.9258e-01, -4.3359e-01, -6.7871e-02, -1.0391e+00,
         1.8875e+01,  3.8281e-01, -2.8281e+00,  4.0938e+00,  4.6484e-01,
         5.3516e-01, -4.5654e-02,  4.5312e-01,  1.4258e-01, -3.1445e-01,
         3.9844e-01,  1.1963e-01, -6.8359e-01,  2.1118e-02,  1.3086e-01,
         3.0859e-01, -2.4658e-02,  1.0498e-01,  9.2578e-01, -2.2070e-01,
        -3.6523e-01, -1.4453e-01, -1.1406e+00, -4.1211e-01, -1.5234e-01,
        -3.3594e-01, -4.6289e-01, -1.9531e-01,  9.0332e-02, -1.4297e+00,
        -6.2988e-02,  2.0117e-01, -2.1094e+00,  3.4766e-01,  3.4424e-02,
         7.9102e-02,  8.5156e-01,  2.2949e-02, -1.4746e-01, -1.8433e-02,
        -3.2031e-01,  1.6504e-01,  2.6367e-01, -2.1118e-02,  1.1426e-01,
        -2.2266e-01,  1.5723e-01, -1.9531e-01, -3.3984e-01, -4.6875e-02,
        -1.0938e-01, -2.5000e+00,  1.2207e-01, -4.1797e-01, -9.4238e-02,
        -2.4902e-01,  6.8359e-02,  3.3594e+00,  5.4297e-01, -1.5312e+00,
         9.2188e-01,  6.8359e-01,  4.7656e-01,  2.2812e+00,  2.3750e+00,
        -5.8203e-01, -6.9688e+00,  1.3625e+01,  1.0205e-01,  5.0781e-01,
        -3.7305e-01, -6.1719e-01, -2.8711e-01,  4.8633e-01, -2.7344e-01,
         1.5430e-01, -5.8984e-01,  1.4648e-01, -1.5411e-03, -7.2266e-01,
         1.1719e-01,  6.5308e-03,  3.3203e-01,  1.8652e-01, -6.1719e-01,
         9.6680e-02, -1.8457e-01, -4.0588e-03, -1.1133e-01, -3.2812e-01,
        -1.5391e+00,  1.6992e-01,  2.0508e-01,  1.8555e-01, -2.4023e-01,
         1.0620e-02,  5.4688e-01, -7.0801e-02,  1.6992e-01, -1.4453e-01,
        -1.0742e-02, -1.0400e-01, -1.8945e-01, -4.9072e-02, -2.8594e+00,
         1.0986e-01,  4.4922e-02,  2.5391e-01,  1.6211e-01, -1.2256e-01,
        -1.6797e-01,  1.7383e-01,  1.8359e-01,  6.8848e-02,  1.5723e-01,
        -4.4375e+00, -3.5742e-01,  7.1777e-02,  2.4609e-01,  1.8066e-01,
        -8.8867e-02,  9.7500e+00, -1.2266e+00, -8.3984e-02,  1.0193e-02,
         7.3828e-01, -1.2109e+00, -4.8438e+00, -1.5234e+00,  1.3984e+00,
         1.4219e+00,  3.4219e+00])

llm.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0215,  0.0186, -0.0047,  ..., -0.0278, -0.0190, -0.0646],
        [ 0.0173, -0.0314,  0.0005,  ...,  0.0061,  0.0097,  0.0392],
        [-0.0187,  0.0059, -0.0064,  ...,  0.0343, -0.0061, -0.0101],
        ...,
        [ 0.0204, -0.0031, -0.0233,  ..., -0.0170,  0.0279,  0.0109],
        [-0.0127,  0.0127, -0.0014,  ..., -0.0188,  0.0289,  0.0190],
        [-0.0037,  0.0008,  0.0024,  ...,  0.0003, -0.0018, -0.0221]])

llm.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.1049, -0.0760,  0.0833,  ..., -0.1176, -0.0239,  0.0944],
        [-0.0235,  0.0028, -0.0312,  ..., -0.0146,  0.0148, -0.0288],
        [ 0.0164, -0.0247,  0.0064,  ...,  0.0022, -0.0387,  0.0401],
        ...,
        [-0.0160,  0.0391,  0.0236,  ...,  0.0206, -0.0101,  0.0102],
        [-0.0022, -0.0091,  0.0058,  ..., -0.0164, -0.0178, -0.0212],
        [-0.0074, -0.0022,  0.0216,  ..., -0.0035, -0.0211,  0.0047]])

llm.base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0201, -0.0205,  0.0222,  ...,  0.0201, -0.0547, -0.0203],
        [ 0.0101,  0.0052, -0.0090,  ..., -0.0020, -0.0118, -0.0096],
        [-0.0106, -0.0096, -0.0052,  ..., -0.0208,  0.0400,  0.0149],
        ...,
        [-0.0178, -0.0193, -0.0027,  ..., -0.0342,  0.0067, -0.0039],
        [ 0.0081, -0.0125,  0.0045,  ...,  0.0003,  0.0172, -0.0155],
        [-0.0037,  0.0239, -0.0002,  ...,  0.0001, -0.0093, -0.0004]])

llm.base_model.model.model.layers.13.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-4.7363e-02,  3.6865e-02,  7.8964e-04, -3.1738e-02, -4.4434e-02,
         5.4626e-03,  2.5269e-02,  2.9663e-02,  1.3086e-01, -6.0120e-03,
        -6.5918e-02,  1.6357e-02,  1.3794e-02, -1.1597e-02, -2.6733e-02,
         1.4404e-02,  8.4877e-05, -1.8066e-02,  9.0942e-03,  1.2207e-02,
        -3.9795e-02,  9.2163e-03,  9.5215e-03,  4.5776e-03,  1.2268e-02,
        -5.4932e-03,  3.8818e-02, -4.0588e-03,  1.3184e-02, -1.3574e-01,
        -5.9814e-03,  4.9561e-02, -1.4832e-02,  2.7832e-02, -6.1035e-03,
        -2.3193e-02,  4.6631e-02,  1.1169e-02, -6.8359e-02,  4.6143e-02,
         1.9897e-02,  6.6406e-02, -7.5073e-03, -4.3640e-03,  3.5156e-02,
         1.0376e-02, -5.6396e-02,  4.1992e-02,  1.4709e-02,  1.0498e-01,
         1.7700e-02,  4.6387e-02,  5.5908e-02,  5.9204e-03,  5.4321e-03,
        -3.6621e-02,  2.3193e-03, -8.7280e-03,  7.8735e-03,  3.3447e-02,
         2.7344e-02, -3.2959e-02,  2.0218e-04, -1.2207e-02,  3.3936e-02,
        -5.5237e-03,  4.8584e-02, -2.3926e-02, -2.7710e-02, -2.4414e-02,
        -2.6367e-02, -5.4016e-03,  2.8931e-02, -2.4292e-02,  1.7578e-02,
         9.7656e-03, -5.1025e-02, -1.1658e-02, -2.1362e-03, -3.2227e-02,
         1.6708e-03,  1.7578e-02, -3.7354e-02,  1.1658e-02, -1.4648e-02,
        -2.5024e-02,  6.6895e-02, -1.0254e-02,  1.8677e-02, -3.6621e-02,
         1.7212e-02, -8.8867e-02,  6.0059e-02, -7.8735e-03, -2.2095e-02,
        -2.1118e-02, -1.6016e-01,  8.5449e-03,  7.8735e-03,  5.1880e-03,
        -8.9844e-02,  2.2583e-02, -6.2988e-02,  1.0315e-02, -3.8086e-02,
         1.4404e-02, -3.6133e-02, -1.4709e-02, -4.0527e-02,  4.0039e-02,
        -1.8082e-03, -1.4099e-02,  9.3994e-03,  3.9673e-03,  7.2937e-03,
        -1.0742e-02,  8.9722e-03,  3.0640e-02, -5.3711e-02, -2.4658e-02,
        -6.1951e-03, -2.6489e-02,  4.3945e-02,  4.3701e-02, -8.0566e-03,
        -1.1353e-02, -4.3213e-02,  1.3611e-02, -8.9355e-02, -1.1414e-02,
        -4.4861e-03, -1.0132e-02,  1.0156e-01, -1.4062e-01, -1.0376e-02,
        -2.7954e-02,  6.4697e-03,  9.5215e-03,  5.5847e-03, -1.5137e-02,
        -2.6489e-02, -6.0654e-04,  2.6489e-02, -7.0312e-02,  2.2217e-02,
        -1.4709e-02,  5.5420e-02, -4.2725e-03, -1.1475e-02, -1.9653e-02,
        -3.5667e-04,  1.0620e-02,  1.0132e-02, -3.1250e-02, -1.3306e-02,
         1.5747e-02,  9.6436e-03, -1.4038e-02, -1.1523e-01, -1.5793e-03,
        -2.1118e-02, -1.4771e-02, -2.0142e-02,  5.3406e-03,  3.1250e-02,
        -3.2654e-03, -3.8818e-02,  2.8442e-02,  1.3855e-02,  6.3477e-02,
         1.0376e-02,  2.4170e-02, -2.3926e-02,  1.7822e-02, -5.1270e-02,
         6.2988e-02,  1.2268e-02, -3.1128e-03, -8.8501e-03, -2.7771e-03,
        -1.3086e-01,  3.1128e-02,  1.4038e-02,  1.1826e-03,  4.7119e-02,
        -2.4048e-02,  2.3651e-03, -3.9795e-02,  3.0151e-02,  4.1260e-02,
        -5.0354e-03,  5.1270e-03,  1.1035e-01,  2.1118e-02, -1.5381e-02,
        -4.0527e-02, -3.6621e-02, -1.8066e-02,  1.5198e-02, -2.1362e-02,
         1.7212e-02, -3.5645e-02,  3.4424e-02, -3.4912e-02, -1.0742e-02,
         1.5747e-02, -2.1729e-02, -3.6621e-02,  9.9487e-03, -8.9844e-02,
        -1.5869e-02,  8.6914e-02, -5.6763e-03, -8.3984e-02, -2.6855e-02,
        -4.0039e-02, -1.1780e-02, -2.9175e-02,  1.9287e-02,  4.2236e-02,
        -1.8555e-02, -3.7354e-02, -9.6436e-03,  4.8584e-02,  4.3701e-02,
         5.2734e-02,  8.5938e-02, -1.9165e-02,  1.7822e-02,  3.2227e-02,
        -7.1106e-03, -3.5547e-01, -6.9336e-02,  2.6245e-02, -8.7402e-02,
        -2.8076e-02, -8.1253e-04, -9.6191e-02,  1.1780e-02,  3.7598e-02,
         4.7913e-03,  1.6235e-02, -2.7832e-02,  2.8198e-02, -2.4048e-02,
        -2.4780e-02,  4.6387e-02,  1.3916e-02, -6.0303e-02,  4.0039e-02,
        -2.5635e-03,  2.2705e-02, -1.3916e-02,  1.8555e-02,  2.1484e-02,
         4.1992e-02,  9.6436e-03, -6.4697e-03,  2.6611e-02,  1.2024e-02,
         1.9836e-03, -1.3123e-02,  3.3691e-02, -1.8555e-02,  6.8359e-02,
        -6.1798e-04,  3.7994e-03,  3.9368e-03,  4.2480e-02, -5.8899e-03,
         2.0386e-02,  1.1475e-02,  7.9956e-03,  1.0605e-03, -2.3682e-02,
         1.0437e-02,  5.2490e-03,  1.4210e-04, -1.9897e-02,  7.6675e-04,
         4.0588e-03, -1.2634e-02, -2.8687e-02, -1.4160e-02, -6.8359e-03,
        -2.5879e-02,  1.6113e-02,  3.3008e-01,  1.2756e-02,  2.8442e-02,
        -2.6245e-02, -4.6997e-03,  2.4780e-02, -9.7046e-03,  1.9531e-03,
         1.4587e-02, -1.7090e-02,  5.9509e-03,  1.5411e-03,  1.1597e-02,
        -2.5787e-03,  3.7994e-03,  7.1106e-03, -1.6968e-02,  1.3672e-02,
         5.5542e-03, -2.3193e-02, -6.7139e-03, -2.5024e-02,  2.3804e-02,
        -3.0273e-02,  2.2827e-02,  5.0354e-03,  3.3875e-03, -1.7700e-02,
         1.1292e-03, -1.5991e-02,  2.8564e-02, -1.3855e-02,  3.7994e-03,
        -2.1851e-02, -3.1738e-03,  4.8218e-03,  3.7384e-03, -1.6602e-02,
         2.8320e-02, -1.3367e-02,  1.2939e-02,  3.4668e-02, -1.9226e-03,
         6.6528e-03, -2.5879e-02, -3.5156e-02, -1.1841e-02, -1.5015e-02,
        -2.9449e-03,  1.4832e-02, -4.7913e-03, -7.2937e-03,  2.0386e-02,
        -6.1951e-03, -3.5889e-02, -2.3926e-02,  3.3112e-03,  8.3008e-03,
        -1.8555e-02, -3.8330e-02, -2.3926e-02, -5.7983e-03, -1.4099e-02,
        -2.5024e-02,  1.6556e-03, -1.2207e-02,  1.4709e-02, -8.0078e-02,
        -1.9043e-02,  1.6113e-02, -1.8555e-02, -2.8442e-02, -5.0049e-03,
         8.1787e-03, -8.6670e-03,  2.8442e-02,  1.2512e-02,  9.8267e-03,
         1.2024e-02, -1.1536e-02, -9.0942e-03, -2.3315e-02, -3.5156e-02,
        -9.2285e-02,  7.5989e-03, -1.7822e-02, -7.3242e-03, -7.8583e-04,
         5.2490e-03,  7.5073e-03,  2.9297e-03,  5.9204e-03,  2.1240e-02,
        -1.9775e-02, -1.7700e-02, -1.1414e-02,  1.2695e-02,  1.4267e-03,
        -1.5747e-02,  1.1169e-02,  1.4282e-02,  1.1902e-02,  9.8877e-03,
         1.4465e-02,  5.0049e-03, -3.1006e-02,  1.3977e-02, -1.1169e-02,
         2.2217e-02, -8.4839e-03, -9.7656e-03, -1.2207e-02,  1.5747e-02,
         2.6611e-02,  2.2461e-02,  4.3030e-03, -1.2939e-02, -1.7456e-02,
         6.1035e-03, -2.0020e-01,  1.7853e-03,  2.3804e-03,  1.7090e-02,
        -2.3926e-02, -2.5757e-02, -9.9487e-03, -9.0942e-03, -1.2390e-02,
        -2.5757e-02, -6.8054e-03,  4.5776e-03,  2.6855e-03, -4.8584e-02,
        -5.6763e-03,  2.5757e-02,  2.7588e-02, -5.4199e-02, -1.5625e-02,
         5.5847e-03,  1.8311e-02,  3.0975e-03,  1.2878e-02,  2.2339e-02,
         1.5076e-02,  1.7548e-04,  1.1414e-02, -8.3008e-03, -3.0518e-02,
         1.7700e-02,  7.3242e-03,  1.2756e-02,  7.5073e-03, -2.1606e-02,
         1.4343e-02, -3.0762e-02,  1.2024e-02, -2.4414e-03, -2.5757e-02,
         2.3315e-02, -1.1963e-02,  2.7100e-02,  3.4943e-03,  1.2878e-02,
         1.9531e-03,  2.2461e-02,  2.0996e-02, -1.5991e-02,  1.2878e-02,
        -3.9864e-04, -1.5030e-03,  1.3611e-02, -3.8757e-03, -1.8433e-02,
         5.8889e-05, -1.8555e-02,  1.6406e-01,  2.9419e-02, -9.0820e-02,
         1.1230e-02, -1.6022e-03,  3.4332e-03, -3.6865e-02,  9.5215e-03,
        -1.4404e-02,  2.1729e-02, -2.8076e-03,  2.9907e-03, -3.3569e-03,
        -1.0071e-02,  1.4893e-02,  8.0490e-04,  3.6377e-02, -8.7280e-03,
        -2.0752e-03, -9.5825e-03,  1.0303e-01,  5.5542e-03,  8.7891e-02,
         9.3994e-03, -3.4180e-02,  1.6098e-03,  1.2085e-02, -2.3193e-03,
        -1.7456e-02,  4.4922e-02,  7.9346e-03, -1.0132e-02, -1.1475e-02,
         1.4709e-02, -2.3438e-02,  9.3994e-03,  8.9722e-03, -1.2268e-02,
        -1.0986e-02,  7.2327e-03, -1.8433e-02, -7.1716e-03,  3.4912e-02,
         3.7354e-02,  2.4414e-02,  7.1716e-03,  1.5625e-02, -9.5825e-03,
        -1.5450e-04, -1.0498e-02])

llm.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0364, -0.0295, -0.0164,  ..., -0.0211,  0.0526, -0.0355],
        [ 0.0347,  0.0459, -0.0069,  ..., -0.0490,  0.0108, -0.0143],
        [-0.0151, -0.0383, -0.0349,  ..., -0.0011,  0.0178, -0.0274],
        ...,
        [-0.0219,  0.0454,  0.0002,  ..., -0.0310,  0.0265, -0.0071],
        [-0.0138, -0.0008, -0.0258,  ...,  0.0150,  0.0071, -0.0225],
        [ 0.0042, -0.0052, -0.0229,  ..., -0.0335,  0.0216,  0.0163]])

llm.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0036,  0.0252,  0.0179,  ..., -0.0012,  0.0101,  0.0061],
        [ 0.0114, -0.0140, -0.0106,  ...,  0.0277, -0.0026, -0.0233],
        [-0.0263,  0.0278,  0.0073,  ...,  0.0078, -0.0299,  0.0007],
        ...,
        [ 0.0071,  0.0167, -0.0142,  ..., -0.0150, -0.0021, -0.0330],
        [ 0.0154,  0.0016, -0.0137,  ...,  0.0444, -0.0048, -0.0061],
        [ 0.0084, -0.0214,  0.0042,  ..., -0.0010,  0.0027,  0.0027]])

llm.base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0072,  0.0107,  0.0152,  ...,  0.0022,  0.0042, -0.0187],
        [-0.0100,  0.0172, -0.0049,  ..., -0.0175,  0.0134, -0.0155],
        [ 0.0073, -0.0086,  0.0003,  ...,  0.0006,  0.0056, -0.0067],
        ...,
        [-0.0015, -0.0033, -0.0041,  ..., -0.0021, -0.0144,  0.0031],
        [-0.0262,  0.0193,  0.0115,  ..., -0.0115, -0.0070,  0.0001],
        [-0.0015, -0.0045,  0.0173,  ...,  0.0069,  0.0121,  0.0029]])

llm.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0242, -0.0002,  0.0147,  ..., -0.0027,  0.0357,  0.0086],
        [ 0.0276, -0.0079, -0.0277,  ..., -0.0121, -0.0148,  0.0123],
        [-0.0476,  0.0105, -0.0177,  ...,  0.0084,  0.0038,  0.0072],
        ...,
        [-0.0055,  0.0250,  0.0406,  ..., -0.0127, -0.0155, -0.0333],
        [-0.0370, -0.0204, -0.0508,  ...,  0.0138,  0.0238, -0.0278],
        [-0.0261,  0.0016,  0.0310,  ..., -0.0341, -0.0314, -0.0678]])

llm.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0238, -0.0316, -0.0058,  ..., -0.0095, -0.0150, -0.0156],
        [-0.0286, -0.0047, -0.0027,  ...,  0.0024, -0.0149,  0.0277],
        [-0.0464, -0.0002, -0.0371,  ..., -0.0289, -0.0083,  0.0086],
        ...,
        [-0.0393,  0.0156, -0.0123,  ..., -0.0246, -0.0241, -0.0173],
        [ 0.0488,  0.0156,  0.0369,  ...,  0.0017, -0.0146, -0.0057],
        [ 0.0007,  0.0015,  0.0262,  ...,  0.0534,  0.0190, -0.0337]])

llm.base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0050,  0.0076,  0.0029,  ...,  0.0043, -0.0049, -0.0310],
        [-0.0118,  0.0017, -0.0038,  ...,  0.0107, -0.0170, -0.0041],
        [-0.0039, -0.0072,  0.0172,  ..., -0.0145, -0.0052, -0.0015],
        ...,
        [ 0.0108, -0.0125,  0.0055,  ...,  0.0156, -0.0182, -0.0092],
        [ 0.0038,  0.0265,  0.0028,  ..., -0.0099, -0.0024,  0.0032],
        [ 0.0008,  0.0102,  0.0035,  ...,  0.0019,  0.0048, -0.0192]])

llm.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0090,  0.0342, -0.0357,  ...,  0.0431,  0.0139,  0.0209],
        [ 0.0526,  0.0365,  0.0037,  ..., -0.0625,  0.0440,  0.0398],
        [ 0.0206,  0.0213, -0.0031,  ..., -0.0546, -0.0141,  0.0081],
        ...,
        [-0.0050, -0.0230,  0.0082,  ..., -0.0554,  0.0453, -0.0485],
        [ 0.0057, -0.0449,  0.0160,  ..., -0.0004,  0.0036, -0.0008],
        [ 0.0126, -0.0079,  0.0090,  ...,  0.0016, -0.0164, -0.0400]])

llm.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 1.4759e-03, -7.9228e-03, -5.4878e-03,  ...,  2.5774e-02,
         -3.2903e-02, -3.1532e-02],
        [ 2.8511e-02, -1.6145e-02,  5.3242e-03,  ..., -3.3774e-02,
         -5.7153e-03, -1.7143e-02],
        [-2.7078e-02, -1.5567e-02,  1.9825e-02,  ...,  1.5127e-02,
         -3.0859e-02,  5.3097e-02],
        ...,
        [-2.2373e-02, -1.8568e-03,  9.5950e-03,  ...,  2.8159e-02,
         -2.7443e-02, -1.2858e-02],
        [-4.1952e-02, -5.0858e-05,  5.4217e-03,  ...,  1.3950e-02,
         -1.2839e-02,  1.0566e-02],
        [ 7.8936e-03, -2.7906e-02, -1.2967e-04,  ...,  7.8741e-03,
          1.1575e-02, -4.1169e-04]])

llm.base_model.model.model.layers.13.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0303, -0.0332, -0.0043,  ...,  0.0238, -0.0027,  0.0040],
        [-0.0125, -0.0223, -0.0032,  ..., -0.0173, -0.0273, -0.0118],
        [-0.0062,  0.0050,  0.0125,  ...,  0.0156,  0.0178,  0.0092],
        ...,
        [ 0.0254,  0.0153, -0.0020,  ..., -0.0099, -0.0049,  0.0097],
        [-0.0047, -0.0249,  0.0089,  ..., -0.0144,  0.0099,  0.0032],
        [-0.0255, -0.0015,  0.0020,  ..., -0.0243,  0.0127, -0.0010]])

llm.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0295, -0.0121,  0.0472,  ...,  0.0030, -0.0131, -0.0089],
        [-0.0310,  0.0093, -0.0421,  ...,  0.0208,  0.0164,  0.0184],
        [ 0.0109,  0.0051, -0.0461,  ...,  0.0280,  0.0089, -0.0380],
        ...,
        [-0.0174, -0.0133,  0.0298,  ..., -0.0470,  0.0249, -0.0526],
        [-0.0029,  0.0703,  0.0275,  ..., -0.0119, -0.0154, -0.0102],
        [-0.0007, -0.0112,  0.0210,  ...,  0.0052, -0.0013,  0.0179]])

llm.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-3.6370e-02, -2.5347e-02, -2.5283e-02,  ..., -2.0800e-03,
         -7.6482e-02,  7.8250e-03],
        [ 2.4104e-02,  1.6657e-02, -4.4323e-02,  ..., -1.5518e-02,
          2.4929e-02, -1.3063e-02],
        [-1.8193e-02,  3.5175e-03,  3.9094e-03,  ...,  3.5639e-02,
         -9.1538e-03, -1.3119e-02],
        ...,
        [ 1.1165e-02,  1.2293e-03, -1.6481e-02,  ..., -6.5571e-03,
         -1.9990e-02,  2.6599e-02],
        [-5.3905e-03, -3.7332e-03,  7.2376e-03,  ...,  9.1422e-03,
         -3.2823e-02,  1.5156e-02],
        [-1.1887e-02, -7.1454e-05, -2.2958e-02,  ...,  1.0437e-02,
         -3.1081e-02,  2.2562e-02]])

llm.base_model.model.model.layers.13.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 1.1963e-02,  2.8992e-04, -8.4229e-03,  ...,  7.3853e-03,
         -6.5613e-03, -1.4343e-02],
        [-1.7944e-02, -1.4160e-02,  8.2397e-03,  ..., -7.1526e-06,
         -1.9073e-03, -1.6113e-02],
        [-1.2360e-03,  5.3711e-03,  1.7822e-02,  ...,  2.6550e-03,
          1.1414e-02,  1.3046e-03],
        ...,
        [ 2.5513e-02, -6.0730e-03, -4.3945e-03,  ..., -1.5564e-02,
          2.7466e-03,  6.9275e-03],
        [ 7.0496e-03, -2.8442e-02,  8.7280e-03,  ..., -2.2461e-02,
          3.1982e-02, -2.8839e-03],
        [ 5.5237e-03, -2.1484e-02,  6.6528e-03,  ..., -2.6245e-03,
          1.0559e-02,  1.3550e-02]])

llm.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0463,  0.0140, -0.0431,  ..., -0.0009,  0.0096, -0.0149],
        [-0.0186,  0.0136,  0.0260,  ..., -0.0209,  0.0027, -0.0184],
        [ 0.0145, -0.0402, -0.0044,  ...,  0.0584,  0.0098, -0.0016],
        ...,
        [-0.0197,  0.0278,  0.0335,  ...,  0.0404,  0.0168,  0.0049],
        [-0.0174, -0.0103,  0.0265,  ..., -0.0454,  0.0232,  0.0280],
        [ 0.0097,  0.0086, -0.0211,  ..., -0.0158, -0.0080, -0.0173]])

llm.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0005, -0.0030, -0.0042,  ..., -0.0089,  0.0218, -0.0277],
        [-0.0120, -0.0051, -0.0283,  ..., -0.0408, -0.0409, -0.0178],
        [-0.0432, -0.0002,  0.0252,  ..., -0.0406,  0.0307,  0.0179],
        ...,
        [ 0.0112, -0.0040, -0.0202,  ..., -0.0410,  0.0302, -0.0063],
        [-0.0073, -0.0053,  0.0053,  ..., -0.0204, -0.0212, -0.0320],
        [ 0.0469, -0.0225, -0.0555,  ..., -0.0131,  0.0022,  0.0294]])

llm.base_model.model.model.layers.13.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.6094, 0.7500, 2.3281,  ..., 0.3477, 1.0625, 1.1875])

llm.base_model.model.model.layers.13.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.1328, 0.9961, 2.4062,  ..., 0.5117, 1.1250, 1.1328])

llm.base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0008,  0.0023,  0.0026,  ..., -0.0118, -0.0018, -0.0092],
        [ 0.0020, -0.0187, -0.0117,  ..., -0.0036, -0.0090,  0.0023],
        [-0.0022, -0.0032,  0.0059,  ...,  0.0036,  0.0194,  0.0029],
        ...,
        [ 0.0415,  0.0153, -0.0376,  ...,  0.0003,  0.0535,  0.0408],
        [ 0.0007, -0.0160, -0.0447,  ..., -0.0093, -0.0203, -0.0317],
        [-0.0084, -0.0248,  0.0087,  ..., -0.0410, -0.0114, -0.0190]])

llm.base_model.model.model.layers.14.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.0864, -0.0232, -0.0327,  ...,  0.5625,  0.0309,  0.2695])

llm.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0098, -0.0115, -0.0378,  ...,  0.0435,  0.0271, -0.0141],
        [ 0.0180, -0.0125, -0.0107,  ...,  0.0298, -0.0142,  0.0338],
        [-0.0265, -0.0096, -0.0112,  ...,  0.0252, -0.0259, -0.0022],
        ...,
        [-0.0091, -0.0194, -0.0051,  ..., -0.0293,  0.0127,  0.0359],
        [ 0.0213, -0.0048,  0.0070,  ..., -0.0378, -0.0290, -0.0154],
        [-0.0093,  0.0082, -0.0155,  ..., -0.0261, -0.0101,  0.0085]])

llm.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0239, -0.0283, -0.0032,  ..., -0.0015, -0.0133,  0.0029],
        [ 0.0026,  0.0063, -0.0037,  ...,  0.0089,  0.0119,  0.0078],
        [-0.0135,  0.0095, -0.0238,  ..., -0.0175,  0.0155,  0.0054],
        ...,
        [ 0.0303, -0.0137, -0.0057,  ...,  0.0114, -0.0070,  0.0077],
        [-0.0105, -0.0076,  0.0187,  ..., -0.0216, -0.0065,  0.0072],
        [ 0.0058, -0.0011, -0.0176,  ...,  0.0105, -0.0107,  0.0070]])

llm.base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-3.8452e-03, -7.7515e-03, -9.3384e-03,  ..., -9.8877e-03,
          6.3705e-04, -6.4697e-03],
        [ 1.5488e-03, -3.7079e-03,  2.6703e-03,  ...,  2.5177e-03,
         -3.6316e-03,  4.0283e-03],
        [-2.1210e-03, -2.7313e-03, -5.1880e-03,  ..., -2.1729e-02,
          1.2939e-02, -1.0132e-02],
        ...,
        [ 1.1292e-02,  4.0527e-02, -5.7861e-02,  ...,  3.5048e-05,
         -1.4893e-02,  3.8086e-02],
        [-9.5215e-03, -1.3855e-02,  5.5908e-02,  ..., -1.2024e-02,
          8.2397e-03, -3.8818e-02],
        [ 1.0437e-02, -2.5879e-02, -1.2024e-02,  ...,  1.8188e-02,
          1.3855e-02, -8.7280e-03]])

llm.base_model.model.model.layers.14.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-1.5527e-01,  2.1777e-01, -1.9141e-01,  2.3438e-01, -1.8164e-01,
         1.0107e-01, -1.7676e-01,  2.2852e-01,  1.4844e-01,  3.7695e-01,
        -5.7129e-02,  1.5747e-02, -3.5547e-01,  4.8096e-02,  4.8096e-02,
         2.8906e-01,  6.0938e-01, -4.4141e-01, -1.3281e-01,  1.2500e-01,
         6.4844e-01, -3.0078e-01, -3.9844e-01,  3.3984e-01, -4.5117e-01,
        -1.6992e-01,  1.5015e-02,  2.4316e-01, -4.5703e-01, -3.6914e-01,
         6.1523e-02, -5.7422e-01, -1.4160e-01, -5.3516e-01, -2.4780e-02,
         1.4941e-01, -1.3379e-01, -1.0742e-01,  8.4961e-02,  4.4531e-01,
        -2.4023e-01, -1.6211e-01, -4.5508e-01,  1.7773e-01,  1.3965e-01,
        -2.3828e-01,  1.4551e-01, -6.7871e-02, -2.5391e-01, -1.5076e-02,
        -3.3984e-01,  9.9121e-02, -2.6978e-02,  3.3594e-01,  3.2715e-02,
        -1.2573e-02,  8.3496e-02, -7.7637e-02,  3.0151e-02,  3.6523e-01,
         1.5625e-02, -4.5703e-01,  2.8711e-01, -4.0312e+00, -3.2471e-02,
         2.6855e-03,  7.7148e-02,  7.3242e-02, -8.0566e-02, -2.7734e-01,
        -2.1680e-01,  1.2402e-01, -2.5977e-01,  1.3281e-01,  2.6562e-01,
        -4.7656e-01,  7.9346e-03, -3.3203e-01, -5.5859e-01, -1.0449e-01,
        -1.1572e-01,  1.3965e-01,  2.7734e-01,  6.9531e-01,  2.3315e-02,
        -1.5820e-01,  4.0234e-01, -7.1484e-01, -1.7090e-01,  1.0234e+00,
         8.1641e-01, -8.3594e-01,  1.2512e-02,  3.4570e-01,  1.0889e-01,
         2.4219e+00, -9.0820e-02,  1.4941e-01, -5.6152e-02,  1.3359e+00,
        -3.3789e-01,  9.0820e-02, -2.7344e-02, -2.2344e+00,  4.8828e-01,
        -1.9043e-01,  4.2480e-02, -3.3750e+00,  1.4282e-02, -2.6123e-02,
         4.3555e-01, -5.4297e-01, -2.1387e-01, -5.4199e-02, -1.6406e-01,
        -1.2207e-01,  3.0762e-02,  1.7871e-01, -1.5527e-01,  2.1582e-01,
        -2.1362e-02,  2.6953e-01,  3.0273e-01,  3.6914e-01, -1.8164e-01,
        -1.9336e-01, -1.9531e-01,  9.1406e-01,  5.1953e-01, -2.1484e-01,
         3.6523e-01,  1.1353e-02,  5.0391e-01, -3.9062e-01, -5.1172e-01,
         8.6060e-03, -5.5078e-01,  1.1108e-02,  1.5381e-02,  1.5991e-02,
         6.4453e-02, -9.4238e-02,  7.8125e-02, -6.2109e-01, -2.3633e-01,
         2.9175e-02, -6.2500e-02, -8.0859e-01, -4.8828e-01,  1.0156e-01,
        -9.0332e-02,  1.4221e-02, -8.0566e-03,  1.0938e+00,  1.0352e-01,
        -8.4473e-02, -1.8457e-01, -1.2939e-02, -1.6479e-02, -3.3936e-02,
        -2.8320e-01,  7.5684e-02,  1.7188e-01, -4.2480e-02,  9.1797e-02,
         1.6895e-01, -1.6846e-02,  2.5330e-03, -5.8594e-02, -6.0791e-02,
        -1.0156e-01,  4.8438e-01,  8.5449e-03, -1.0620e-02,  3.8605e-03,
         6.2988e-02,  1.5137e-01,  1.0107e-01, -5.0537e-02,  1.3086e-01,
         2.5146e-02,  1.7676e-01,  1.2158e-01,  2.5513e-02,  7.9956e-03,
        -5.8203e-01, -8.2397e-03, -1.0312e+00, -1.4531e+00,  1.4453e-01,
        -1.1797e+00, -1.4648e-01,  1.5320e-02,  2.6489e-02, -2.0020e-02,
         4.9609e-01,  1.9897e-02, -3.3447e-02,  3.6133e-02,  5.3125e-01,
         7.8613e-02,  6.2500e-01,  6.5234e-01,  2.6758e-01,  7.3438e-01,
        -2.7539e-01,  8.0859e-01, -6.0791e-02,  1.0938e-01,  9.5703e-01,
         2.1094e-01, -4.1016e-02,  6.8359e-02,  1.6797e-01, -1.0078e+00,
         1.0078e+00,  5.7373e-02,  1.8848e-01,  2.3926e-01, -1.0254e-01,
         1.1641e+00,  1.3489e-02, -6.1719e-01, -8.3984e-01,  2.1387e-01,
        -1.7944e-02, -1.6172e+00,  5.6641e-02, -3.6133e-02, -1.7344e+00,
        -4.6387e-02,  6.7383e-02,  4.6875e-02, -7.8125e-03, -4.1260e-02,
        -1.9141e+00,  8.4229e-03, -1.5015e-02,  4.0771e-02, -1.1279e-01,
         1.6211e-01, -8.5938e-02, -2.0801e-01, -1.6211e-01, -8.4473e-02,
         9.7168e-02,  5.6152e-02,  4.3945e-02, -6.2891e-01,  3.4570e-01,
         3.2422e-01,  1.5000e+01,  7.7148e-02,  6.7578e-01,  9.7266e-01,
        -1.1328e+00,  8.8379e-02,  1.6699e-01, -2.5391e-01,  2.0020e-01,
        -1.3770e-01,  3.2227e-01, -9.4727e-02, -3.3789e-01, -4.6387e-03,
        -4.1211e-01, -1.2598e-01,  3.5352e-01, -2.0801e-01, -5.1172e-01,
        -2.2559e-01,  1.6211e-01,  8.9844e-01, -9.7168e-02,  1.0449e-01,
         1.1719e+00, -1.8799e-02,  6.0425e-03, -1.2812e+00,  1.2329e-02,
        -2.9297e-01, -3.0078e-01,  1.7773e-01, -3.9062e-01, -3.4424e-02,
        -2.2949e-01,  4.1016e-01, -4.2188e-01,  7.6562e-01,  7.5684e-02,
        -1.1768e-01,  1.0625e+00,  3.3984e-01,  1.4160e-01,  1.1279e-01,
        -3.2188e+00,  2.0020e-02, -1.2793e-01, -3.3203e-02,  5.5469e-01,
        -1.6719e+00, -2.6562e-01,  5.3125e-01,  1.3281e-01,  3.8906e+00,
        -7.5684e-02, -1.2578e+00, -2.4844e+00, -5.9326e-02,  2.2070e-01,
        -1.2793e-01,  2.4219e-01,  2.4902e-02, -1.2344e+00, -1.4922e+00,
         1.3184e-01,  7.8125e-01,  1.2578e+00,  2.3594e+00, -2.4219e+00,
        -5.3516e-01, -8.5547e-01,  6.1279e-02, -7.7734e-01,  1.1084e-01,
        -6.7578e-01, -4.3750e-01,  6.6406e-01, -7.1289e-02,  4.1602e-01,
         5.6885e-02, -8.3594e-01, -6.9336e-02,  1.1621e-01,  8.7109e-01,
        -6.5430e-02, -3.6377e-02,  8.6060e-03, -3.1055e-01, -2.2168e-01,
         2.8076e-02, -1.7969e-01, -8.1177e-03, -2.0142e-02, -1.1484e+00,
         1.6357e-02,  8.7891e-01,  2.3926e-01,  8.2520e-02, -1.3086e-01,
         1.9297e+00, -8.6914e-02,  4.5013e-04, -8.0078e-02, -1.4258e-01,
        -5.5078e-01,  2.2363e-01,  4.3701e-02, -6.5430e-02, -5.0049e-02,
        -1.8164e-01,  2.7734e-01, -9.7656e-02, -5.4688e-01,  3.9453e-01,
         8.0859e-01,  1.0312e+00, -6.0156e-01,  1.7773e-01, -4.8438e-01,
         7.9297e-01,  1.2031e+00,  4.2578e-01, -5.4688e-01, -3.5352e-01,
         6.9922e-01, -7.1777e-02,  1.3281e-01, -1.8047e+00, -8.8281e-01,
         2.0625e+00, -1.1484e+00,  5.1953e-01,  2.9375e+00, -1.5781e+00,
        -4.3555e-01,  3.4570e-01, -1.4160e-01,  1.3672e-01, -1.9336e-01,
         1.0156e+00, -5.7129e-02, -1.8262e-01,  1.1328e+00, -1.3770e-01,
         5.4932e-02, -5.1562e-01,  1.9897e-02, -1.1572e-01, -1.9238e-01,
         1.7578e-02,  4.5654e-02,  1.1475e-02,  1.4160e-01,  1.3203e+00,
        -6.2988e-02, -1.3672e-01,  2.2852e-01,  7.8125e-02,  1.9336e-01,
        -1.6094e+00,  1.1572e-01,  2.5781e-01,  7.2656e-01,  2.5586e-01,
        -4.6143e-02, -6.6895e-02,  1.8164e-01, -2.6367e-01, -7.7637e-02,
        -2.5391e-01, -6.5625e-01, -1.0791e-01,  7.0312e-02,  6.1279e-02,
        -7.8125e-02,  3.7354e-02, -1.6406e-01,  5.1953e-01,  3.1445e-01,
        -9.5703e-02, -1.2012e-01,  2.7539e-01, -1.1133e-01,  5.3223e-02,
         1.4258e-01,  1.7188e-01, -1.4941e-01, -8.1641e-01,  4.4922e-01,
        -5.5469e-01, -3.4688e+00, -9.1406e-01,  4.0039e-02,  3.6328e-01,
        -2.4531e+00, -3.5469e+00,  7.6172e-01,  5.7031e-01,  8.2422e-01,
        -6.0156e-01, -4.8047e-01, -1.2422e+00, -9.6680e-02,  1.8555e-01,
         1.7090e-01,  1.4062e-01,  5.0537e-02,  2.7344e-01,  1.3867e-01,
         7.2656e-01,  9.9121e-02, -4.0039e-02, -1.6211e-01,  1.2500e+00,
         2.2461e-01,  1.6968e-02, -2.5586e-01,  5.3223e-02, -1.0498e-01,
         2.0801e-01, -8.5938e-02, -4.3945e-01, -5.3467e-02,  8.2520e-02,
         3.2959e-02, -1.4258e-01,  2.2852e-01,  4.3701e-02, -7.4609e-01,
        -1.4688e+00, -7.3730e-02, -1.7090e-01,  4.1602e-01,  6.6406e-02,
        -3.0664e-01, -1.2354e-01, -5.6396e-02, -2.7222e-02,  2.2969e+00,
        -1.8616e-03,  1.9165e-02,  1.5430e-01, -1.3867e-01, -3.9062e-01,
        -2.7148e-01,  6.4844e-01, -4.4678e-02,  2.1582e-01,  9.3750e-02,
        -2.8125e-01,  7.1777e-02, -3.6875e+00,  2.6953e-01,  9.0625e-01,
        -7.4219e-01, -1.9375e+00,  9.6094e-01, -3.7842e-03,  4.5312e-01,
         6.4062e-01, -1.4141e+00])

llm.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-3.2243e-03,  1.9641e-02,  5.2936e-02,  ...,  3.2223e-02,
          5.5132e-03, -1.0214e-02],
        [-8.9088e-03, -2.0031e-02,  6.0202e-02,  ...,  1.8844e-02,
          9.9927e-03,  1.4129e-04],
        [ 1.1750e-02,  9.6550e-03, -3.4415e-03,  ..., -3.8611e-02,
          8.9982e-03, -1.4575e-02],
        ...,
        [ 4.6648e-02,  3.1736e-02, -1.7265e-02,  ..., -1.8066e-02,
          4.7141e-02, -5.1278e-02],
        [ 2.1107e-02, -2.4427e-02, -2.4340e-02,  ..., -2.3185e-02,
          2.0768e-02,  5.9745e-03],
        [ 2.1663e-02, -3.3096e-02,  1.9321e-02,  ..., -2.2882e-02,
         -3.5773e-02,  3.5682e-05]])

llm.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0060, -0.0210,  0.0018,  ..., -0.0112, -0.0103, -0.0199],
        [ 0.0026, -0.0079, -0.0042,  ...,  0.0246,  0.0072,  0.0127],
        [-0.0056, -0.0223, -0.0176,  ..., -0.0345, -0.0183, -0.0066],
        ...,
        [ 0.0157, -0.0026, -0.0142,  ..., -0.0078, -0.0008, -0.0234],
        [-0.0148, -0.0080,  0.0234,  ..., -0.0127,  0.0265, -0.0004],
        [-0.0034,  0.0279,  0.0042,  ..., -0.0133, -0.0313,  0.0195]])

llm.base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 2.0599e-03,  6.0730e-03, -2.3193e-02,  ...,  1.3550e-02,
          6.5231e-04,  4.6387e-03],
        [-2.0630e-02,  8.4229e-03,  2.1057e-03,  ...,  1.8921e-02,
         -1.6556e-03, -1.6357e-02],
        [ 4.4556e-03, -1.3489e-02, -3.1891e-03,  ...,  8.4229e-03,
         -6.9885e-03,  1.7624e-03],
        ...,
        [ 8.2016e-05, -3.6621e-03, -4.9744e-03,  ..., -9.9487e-03,
          1.8555e-02, -1.9043e-02],
        [-7.6294e-04,  7.3242e-04, -3.3447e-02,  ..., -1.1047e-02,
         -6.0425e-03,  1.1169e-02],
        [-6.7520e-04,  1.3580e-03, -5.0659e-03,  ..., -1.4832e-02,
          2.3438e-02,  2.0294e-03]])

llm.base_model.model.model.layers.14.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 2.9663e-02, -3.7598e-02, -4.0283e-02, -1.0156e-01, -1.4221e-02,
        -1.4282e-02,  1.7212e-02, -1.8066e-02,  3.0060e-03, -1.2817e-02,
         3.5547e-01,  1.0071e-02, -8.9111e-03, -9.8877e-03,  4.9438e-03,
         8.5449e-03, -3.3691e-02,  1.6556e-03,  2.2949e-02,  3.5889e-02,
        -2.6978e-02,  3.6163e-03,  1.8433e-02,  1.5442e-02, -1.3062e-02,
        -5.8289e-03,  4.6387e-03, -1.3984e+00,  2.5269e-02,  1.7334e-02,
        -6.1340e-03, -1.1047e-02, -5.6152e-03,  2.5391e-02, -4.5898e-02,
        -6.6833e-03, -1.3977e-02,  1.6479e-02, -9.0942e-03, -1.9653e-02,
         1.6113e-02, -3.0029e-02,  3.9978e-03,  6.0730e-03,  2.1973e-02,
        -1.4343e-03, -2.6367e-02, -2.6733e-02, -1.8433e-02,  1.1475e-02,
         7.7209e-03,  3.2959e-02,  2.9297e-02,  1.6861e-03,  1.4221e-02,
         5.4688e-02,  4.3869e-05,  6.4453e-02,  6.6528e-03,  5.5542e-03,
         1.4771e-02, -2.6855e-02, -5.3223e-02, -1.8677e-02, -6.9580e-03,
         6.5430e-02, -3.7842e-03,  3.7842e-02,  5.7373e-03,  1.3306e-02,
        -3.7109e-02,  9.0942e-03,  1.0010e-02, -1.6235e-02, -4.8584e-02,
         1.1292e-02, -4.9133e-03,  5.6885e-02,  1.0547e-01, -5.8289e-03,
         3.0029e-02,  4.9316e-02,  7.0190e-04,  4.0894e-03, -2.4048e-02,
        -8.1055e-02, -3.1738e-02, -5.6396e-02, -1.5747e-02,  4.1504e-03,
         1.9043e-02,  1.6113e-02, -3.1128e-03, -7.8125e-03, -1.8433e-02,
        -6.8665e-03,  4.5166e-03,  4.6387e-03, -6.9885e-03,  1.9775e-02,
         2.0630e-02, -1.7944e-02, -2.7466e-02, -4.2480e-02, -3.8330e-02,
         5.7617e-02,  2.8320e-02,  1.8555e-02,  5.7068e-03,  7.7820e-03,
        -5.1758e-02,  1.9336e-01,  2.8442e-02,  3.9673e-03,  7.9346e-03,
         1.9897e-02,  2.2030e-04,  4.6875e-02, -4.9561e-02, -1.4801e-03,
        -3.2715e-02,  2.2095e-02, -1.7700e-02, -1.0559e-02, -6.0425e-03,
        -3.0708e-04, -1.9165e-02, -4.7119e-02, -1.7578e-02, -5.0049e-02,
         1.0254e-02,  6.5430e-02,  3.9795e-02, -2.8839e-03,  7.1777e-02,
        -3.0029e-02,  9.7656e-04,  4.9561e-02, -3.2715e-02,  5.5908e-02,
         4.6875e-02,  2.2461e-02,  1.6846e-02, -7.1289e-02,  2.9541e-02,
        -8.1177e-03, -2.2827e-02, -1.9653e-02, -1.3000e-02,  2.0508e-02,
        -2.5757e-02,  3.0273e-02, -3.2806e-03,  4.1809e-03,  2.1484e-02,
         5.3101e-03,  2.7344e-02,  1.1536e-02, -4.3213e-02,  4.6143e-02,
         1.5234e-01,  5.7617e-02,  2.5635e-02, -6.6895e-02, -2.6367e-02,
        -2.4261e-03, -1.5991e-02,  2.5757e-02,  3.8818e-02,  5.5847e-03,
        -7.5989e-03, -3.6621e-02,  4.2969e-02,  2.7100e-02,  2.5482e-03,
         4.5898e-01, -8.2397e-03, -6.8359e-03,  2.2095e-02, -9.7656e-04,
        -8.8379e-02,  1.0742e-02, -9.2773e-02, -3.5889e-02,  3.3691e-02,
        -3.8086e-02,  5.6885e-02, -3.8330e-02, -7.7637e-02, -1.0254e-01,
        -1.4954e-02,  5.3406e-03, -1.9775e-02, -5.1758e-02, -3.8330e-02,
        -3.6865e-02,  1.1230e-02,  5.1575e-03,  1.8066e-02,  3.6377e-02,
        -5.1758e-02, -2.2339e-02,  1.0693e-01,  3.7689e-03,  4.2969e-02,
         3.2471e-02,  2.2827e-02,  2.6611e-02, -6.7383e-02,  6.2500e-02,
        -2.5879e-02, -1.0156e-01, -3.3203e-02, -9.1553e-03,  1.8848e-01,
        -1.2109e-01,  1.2695e-02,  8.4229e-03,  4.0817e-04, -6.1035e-02,
        -1.0498e-01, -7.4463e-03, -1.0437e-02, -8.3984e-02, -1.0010e-02,
         1.4465e-02, -4.6631e-02,  4.5898e-02,  1.8555e-02, -3.1494e-02,
        -3.2812e-01,  2.1118e-02, -4.5898e-02, -5.9082e-02,  1.8921e-02,
         3.6865e-02,  7.8125e-02, -1.1414e-02,  1.6724e-02,  3.3203e-02,
         4.7852e-02, -6.2012e-02,  3.4912e-02,  4.5898e-02,  3.0884e-02,
        -1.5747e-02, -2.8442e-02,  1.4282e-02,  2.2656e-01, -1.3611e-02,
         2.1973e-02,  3.5156e-02, -2.2095e-02,  5.7068e-03,  6.4941e-02,
         1.1963e-01, -6.8970e-03, -8.0566e-03,  1.7456e-02, -3.0273e-02,
         2.7832e-02,  2.0508e-02,  2.0264e-02, -6.4087e-04,  6.9427e-04,
         1.7700e-02, -7.4158e-03, -6.4087e-03,  2.1118e-02, -7.6904e-03,
         5.5542e-03, -5.6885e-02, -6.1035e-03, -2.2095e-02, -4.4434e-02,
        -2.0996e-02,  1.2451e-02,  1.3184e-02,  2.6978e-02, -1.7090e-02,
         6.0425e-03,  1.2024e-02, -1.6602e-02, -6.0120e-03,  9.3994e-03,
        -2.3804e-02,  1.6479e-03,  1.1414e-02,  1.2634e-02, -4.5166e-03,
        -9.8877e-03, -3.0273e-02,  2.3193e-02,  1.8799e-02, -2.1606e-02,
         2.1362e-02,  1.1719e-02, -2.4414e-03,  4.6692e-03,  9.2163e-03,
         5.1270e-03, -4.3457e-02,  3.6865e-02, -3.7079e-03,  1.9653e-02,
        -7.5073e-03,  1.7700e-02, -8.9722e-03,  3.6621e-03, -6.3782e-03,
        -1.2817e-02, -2.4414e-02, -2.6978e-02, -7.1716e-03, -9.5215e-03,
        -1.1902e-02,  4.9133e-03, -4.3945e-03,  1.8921e-03, -9.9945e-04,
        -1.8188e-02, -3.0884e-02, -3.6469e-03,  4.3457e-02, -5.2979e-02,
         3.0762e-02, -6.4087e-03,  4.7607e-02, -1.8188e-02,  8.4229e-03,
        -3.8300e-03,  1.1597e-02, -8.3008e-03, -1.4221e-02, -2.6001e-02,
        -3.7598e-02,  2.7954e-02,  4.8828e-03,  3.2715e-02,  4.3945e-02,
         5.6641e-02, -4.5410e-02, -1.5381e-02,  2.3682e-02, -3.0060e-03,
        -5.0964e-03, -3.9062e-02, -1.5991e-02, -3.2959e-02, -1.8799e-02,
        -5.4016e-03,  9.7656e-03,  1.9165e-02, -3.6469e-03, -1.7456e-02,
        -5.3406e-04, -2.3041e-03,  5.1880e-03,  2.5024e-02,  2.7832e-02,
        -7.4768e-03, -3.8452e-03,  5.8899e-03, -2.4780e-02,  1.2756e-02,
         9.7656e-03,  1.4038e-02, -3.8719e-04, -3.3722e-03, -1.0986e-01,
         4.5776e-03,  2.3682e-02,  3.9551e-02,  4.2480e-02,  2.6550e-03,
        -9.2773e-03, -2.5024e-03, -8.1787e-03, -6.7902e-04, -7.8125e-03,
        -8.2397e-03,  8.2397e-03,  1.7822e-02, -1.8433e-02, -7.2266e-02,
         6.9824e-02, -8.1787e-03, -3.6133e-02,  1.3281e-01,  8.1787e-03,
         1.1426e-01,  1.0010e-02,  1.0071e-02, -7.8613e-02, -4.2725e-03,
         8.3496e-02,  3.4180e-02, -1.1426e-01, -8.3618e-03, -9.0942e-03,
        -7.8613e-02, -5.9570e-02, -1.8311e-02, -1.0449e-01, -3.8574e-02,
         7.4219e-02, -4.6631e-02, -5.3955e-02, -1.2598e-01, -1.4709e-02,
         5.8838e-02, -6.7383e-02, -7.1289e-02,  6.2500e-02,  7.2266e-02,
         8.9844e-02,  1.3306e-02, -2.3804e-03,  5.3711e-02,  7.4707e-02,
        -1.3379e-01,  1.0645e-01,  1.4062e-01,  1.6992e-01, -3.0151e-02,
         6.6459e-06, -1.0156e-01, -4.7852e-02, -3.0029e-02,  1.8311e-02,
         5.0781e-02, -4.7607e-02, -2.5879e-02, -1.1292e-02,  1.4038e-02,
         1.1768e-01, -7.4219e-02,  8.5938e-02, -5.1758e-02,  1.0645e-01,
         1.2817e-02, -1.6113e-01,  6.3477e-02,  1.3672e-01,  2.4658e-02,
        -9.3262e-02,  6.8359e-02, -1.0352e-01, -8.2520e-02, -3.3936e-02,
        -7.9102e-02, -7.9590e-02,  9.1553e-04,  3.9062e-02, -4.7363e-02,
         3.7500e-01,  9.1309e-02, -2.8931e-02, -2.3560e-02,  3.0518e-02,
         6.4453e-02, -5.1758e-02, -3.0151e-02, -1.1035e-01,  5.6641e-02,
        -3.1250e-02, -1.1914e-01,  1.0449e-01,  4.1016e-02, -1.4062e-01,
        -1.6309e-01, -8.5938e-02,  4.9316e-02,  7.4387e-04,  1.7090e-02,
        -5.0659e-03,  1.1133e-01, -1.3281e-01,  8.9844e-02, -4.1260e-02,
         2.0117e-01, -1.0010e-01, -2.7222e-02, -2.1973e-02, -6.4941e-02,
         5.9814e-02, -1.2598e-01,  6.9336e-02,  2.4048e-02,  5.8594e-02,
        -3.1006e-02, -1.1865e-01, -5.6763e-03, -1.6406e-01, -1.2207e-01,
        -9.5749e-04,  1.0010e-02,  1.5625e-01,  3.5645e-02,  2.2827e-02,
         7.9590e-02, -5.2246e-02, -2.4805e-01, -1.0547e-01,  7.6660e-02,
        -1.0156e-01,  1.5137e-01, -1.6602e-02, -1.3855e-02,  1.5723e-01,
         7.2754e-02,  3.4424e-02])

llm.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0302, -0.0182,  0.0232,  ...,  0.0427, -0.0512, -0.0030],
        [ 0.0265, -0.0297, -0.0055,  ..., -0.0063, -0.0043, -0.0137],
        [ 0.0122, -0.0618,  0.0029,  ..., -0.0006, -0.0218, -0.0560],
        ...,
        [-0.0019,  0.0143, -0.0074,  ...,  0.0501,  0.0144, -0.0080],
        [-0.0144, -0.0107, -0.0157,  ..., -0.0156, -0.0033, -0.0312],
        [ 0.0113, -0.0383, -0.0013,  ...,  0.0513,  0.0371,  0.0273]])

llm.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0104, -0.0037, -0.0166,  ..., -0.0146,  0.0009, -0.0395],
        [-0.0598, -0.0232, -0.0195,  ...,  0.0004,  0.0365, -0.0171],
        [-0.0111,  0.0115, -0.0285,  ..., -0.0057, -0.0236, -0.0124],
        ...,
        [-0.0109,  0.0002,  0.0130,  ..., -0.0352,  0.0262,  0.0020],
        [ 0.0113,  0.0005,  0.0060,  ...,  0.0017, -0.0183,  0.0192],
        [-0.0173, -0.0197,  0.0112,  ..., -0.0030,  0.0181,  0.0053]])

llm.base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0173, -0.0037, -0.0101,  ...,  0.0027, -0.0013,  0.0013],
        [ 0.0107, -0.0105, -0.0072,  ..., -0.0166,  0.0072, -0.0170],
        [ 0.0029, -0.0004,  0.0109,  ..., -0.0015,  0.0003, -0.0024],
        ...,
        [ 0.0065, -0.0090, -0.0029,  ..., -0.0006,  0.0027, -0.0160],
        [-0.0168,  0.0070, -0.0093,  ...,  0.0126, -0.0138,  0.0133],
        [-0.0014, -0.0046, -0.0031,  ..., -0.0049, -0.0067,  0.0237]])

llm.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.1094,  0.0320,  0.0413,  ..., -0.0145, -0.0244, -0.0020],
        [ 0.1098, -0.0188,  0.0844,  ..., -0.0010,  0.0106, -0.0322],
        [-0.0087,  0.0257,  0.0037,  ..., -0.0139,  0.0125, -0.0003],
        ...,
        [-0.0038,  0.0074, -0.0351,  ..., -0.0172,  0.0143, -0.0238],
        [-0.0608, -0.0015,  0.0013,  ..., -0.0100, -0.0225, -0.0100],
        [-0.0051,  0.0015, -0.0790,  ...,  0.0456, -0.0477,  0.0070]])

llm.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-3.8384e-03, -9.8867e-03,  3.2732e-02,  ..., -2.8920e-02,
         -2.9912e-02, -1.0766e-02],
        [-6.3292e-04, -1.1077e-02,  1.1588e-02,  ...,  8.4098e-05,
         -2.4232e-03, -1.5511e-02],
        [ 1.0602e-02,  3.3310e-02, -2.1071e-02,  ...,  8.9069e-04,
         -3.6257e-02, -1.8702e-02],
        ...,
        [-2.1999e-02, -2.4276e-02, -1.8204e-02,  ..., -2.2768e-02,
         -3.6731e-03,  5.1283e-03],
        [-4.9952e-02, -1.4787e-02, -1.1310e-02,  ...,  1.6391e-03,
         -4.3208e-02, -2.0130e-02],
        [-1.4992e-02,  9.2326e-03, -1.9830e-02,  ..., -4.6938e-03,
         -2.5271e-03,  3.7235e-02]])

llm.base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 1.4954e-02, -1.8677e-02,  3.9062e-03,  ..., -3.5645e-02,
          1.0315e-02, -6.6528e-03],
        [ 2.3682e-02,  2.3804e-02, -9.5367e-05,  ...,  1.9653e-02,
         -6.0425e-03, -1.4465e-02],
        [ 1.0559e-02, -1.7090e-02,  1.1719e-02,  ..., -9.1553e-03,
          8.6670e-03,  1.0620e-02],
        ...,
        [ 3.2196e-03, -1.2756e-02,  1.2634e-02,  ...,  1.4771e-02,
          1.1520e-03,  1.7944e-02],
        [-2.4109e-03,  9.7656e-03,  2.2705e-02,  ...,  1.4771e-02,
         -2.3560e-02, -7.9956e-03],
        [-1.1963e-02, -7.5684e-03,  1.1902e-02,  ...,  9.8877e-03,
         -2.5391e-02,  6.2256e-03]])

llm.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0109, -0.0311, -0.0198,  ...,  0.0002, -0.0341,  0.0034],
        [-0.0188,  0.0036,  0.0192,  ..., -0.0291, -0.0077,  0.0124],
        [-0.0644,  0.0112,  0.0629,  ...,  0.0271, -0.0195,  0.0285],
        ...,
        [ 0.0049,  0.0332,  0.0212,  ..., -0.0449,  0.0201,  0.0278],
        [-0.0796,  0.0310,  0.0564,  ..., -0.0007,  0.0706,  0.0023],
        [-0.0267, -0.0061,  0.0210,  ...,  0.0615,  0.0065,  0.0461]])

llm.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0011, -0.0032,  0.0286,  ..., -0.0286,  0.0023, -0.0039],
        [-0.0186, -0.0008,  0.0336,  ...,  0.0119, -0.0022,  0.0079],
        [ 0.0187,  0.0076,  0.0211,  ...,  0.0298,  0.0116,  0.0168],
        ...,
        [ 0.0430, -0.0023, -0.0023,  ..., -0.0031,  0.0407,  0.0438],
        [-0.0121, -0.0153,  0.0229,  ...,  0.0230, -0.0099,  0.0329],
        [ 0.0031,  0.0092,  0.0178,  ...,  0.0013, -0.0056, -0.0253]])

llm.base_model.model.model.layers.14.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 2.7466e-02,  1.6602e-02, -5.7068e-03,  ...,  6.0558e-05,
         -3.0823e-03, -1.3000e-02],
        [ 1.4343e-02,  1.1597e-02,  1.0014e-04,  ...,  1.4420e-03,
         -5.3101e-03, -1.3794e-02],
        [ 1.9897e-02,  6.8665e-03, -2.5024e-03,  ...,  2.4536e-02,
          8.8501e-03,  1.9897e-02],
        ...,
        [-1.2329e-02, -2.5391e-02,  2.2949e-02,  ...,  4.8218e-03,
          1.8799e-02, -1.0071e-02],
        [ 7.3242e-03,  3.9551e-02,  1.8921e-02,  ..., -3.5286e-04,
         -1.2695e-02, -2.3804e-02],
        [-1.1536e-02, -4.9744e-03, -2.3682e-02,  ..., -1.8768e-03,
          1.6602e-02, -9.6130e-04]])

llm.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0224, -0.0187, -0.0162,  ...,  0.0611,  0.0231, -0.0355],
        [-0.0181,  0.0193, -0.0262,  ...,  0.0228,  0.0227,  0.0127],
        [ 0.0339, -0.0076,  0.0089,  ..., -0.0366, -0.0068,  0.0222],
        ...,
        [-0.0101, -0.0208, -0.0027,  ..., -0.0197, -0.0420,  0.0635],
        [ 0.0012, -0.0033, -0.0244,  ...,  0.0608,  0.0246,  0.0007],
        [ 0.0461, -0.0723, -0.0022,  ..., -0.0003, -0.0144,  0.0292]])

llm.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0394,  0.0326,  0.0059,  ...,  0.0188,  0.0026,  0.0135],
        [-0.0103, -0.0388,  0.0243,  ...,  0.0283,  0.0137, -0.0301],
        [ 0.0100, -0.0237,  0.0081,  ...,  0.0256,  0.0074,  0.0281],
        ...,
        [ 0.0189, -0.0160, -0.0166,  ...,  0.0141, -0.0062, -0.0355],
        [-0.0371, -0.0370,  0.0524,  ..., -0.0251, -0.0483,  0.0171],
        [ 0.0297, -0.0178,  0.0102,  ..., -0.0443, -0.0316, -0.0363]])

llm.base_model.model.model.layers.14.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 2.1362e-02,  7.3547e-03,  2.3193e-02,  ..., -1.5137e-02,
          4.1992e-02,  1.0071e-03],
        [ 1.4038e-02, -6.5804e-05, -4.6997e-03,  ..., -9.2773e-03,
          2.1851e-02,  1.3977e-02],
        [ 4.4556e-03,  5.9204e-03,  2.6489e-02,  ...,  2.1484e-02,
          3.8910e-03, -8.7891e-03],
        ...,
        [ 2.4048e-02, -1.0925e-02,  5.8594e-03,  ...,  8.1787e-03,
         -1.5259e-02, -5.5237e-03],
        [-2.5482e-03, -5.8594e-03,  6.4087e-03,  ...,  7.9346e-03,
          1.1230e-02,  1.7822e-02],
        [-2.7466e-02, -2.3193e-02,  1.6556e-03,  ..., -2.0905e-03,
         -1.2695e-02, -6.1340e-03]])

llm.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[-0.0228,  0.0261,  0.0156,  ..., -0.0280, -0.0456,  0.0319],
        [-0.0433,  0.0178,  0.0613,  ...,  0.0227,  0.0076, -0.0162],
        [ 0.0095,  0.0098,  0.0395,  ..., -0.0173,  0.0014, -0.0138],
        ...,
        [-0.0117,  0.0065, -0.0095,  ...,  0.0218,  0.0091,  0.0307],
        [ 0.0066,  0.0126, -0.0688,  ..., -0.0580, -0.0548, -0.0107],
        [-0.0075, -0.0046, -0.0073,  ...,  0.0317,  0.0173, -0.0080]])

llm.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0020, -0.0366, -0.0145,  ..., -0.0423,  0.0245, -0.0288],
        [ 0.0175,  0.0248,  0.0186,  ...,  0.0129, -0.0053,  0.0064],
        [ 0.0431,  0.0302,  0.0672,  ...,  0.0372,  0.0310, -0.0022],
        ...,
        [-0.0095,  0.0411,  0.0128,  ...,  0.0164, -0.0583,  0.0315],
        [-0.0577, -0.0566,  0.0224,  ...,  0.0052,  0.0211, -0.0293],
        [-0.0438, -0.0097, -0.0646,  ..., -0.0022,  0.0276, -0.0501]])

llm.base_model.model.model.layers.14.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.7891, 0.6992, 2.2812,  ..., 0.3906, 1.0547, 1.1094])

llm.base_model.model.model.layers.14.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.1641, 1.0391, 1.8750,  ..., 0.5234, 1.1875, 1.1875])

llm.base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0088, -0.0140, -0.0094,  ...,  0.0122,  0.0022,  0.0054],
        [ 0.0069, -0.0183,  0.0112,  ...,  0.0115, -0.0021, -0.0096],
        [-0.0208, -0.0122,  0.0002,  ...,  0.0243,  0.0064,  0.0056],
        ...,
        [ 0.0071, -0.0008,  0.0247,  ...,  0.0090,  0.0028, -0.0096],
        [ 0.0222, -0.0181, -0.0026,  ..., -0.0078,  0.0090,  0.0198],
        [-0.0110,  0.0156, -0.0175,  ..., -0.0075, -0.0249, -0.0244]])

llm.base_model.model.model.layers.15.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.5078, -1.7500,  0.0359,  ...,  0.4277,  0.1875,  0.0101])

llm.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0030, -0.0213, -0.0245,  ..., -0.0247,  0.0257,  0.0338],
        [ 0.0050,  0.0038, -0.0409,  ...,  0.0221, -0.0532,  0.0156],
        [ 0.0110, -0.0129, -0.0446,  ..., -0.0070,  0.0275,  0.0005],
        ...,
        [ 0.0710, -0.0060, -0.0425,  ..., -0.0027,  0.0305,  0.0276],
        [ 0.0409,  0.0429, -0.0386,  ..., -0.0107,  0.0453, -0.0254],
        [ 0.0113,  0.0015,  0.0193,  ...,  0.0320, -0.0103,  0.0222]])

llm.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0302, -0.0015, -0.0039,  ...,  0.0135,  0.0203, -0.0340],
        [ 0.0313, -0.0005,  0.0239,  ..., -0.0117, -0.0586,  0.0504],
        [-0.0216, -0.0187, -0.0155,  ...,  0.0260,  0.0162, -0.0134],
        ...,
        [ 0.0223, -0.0179, -0.0180,  ...,  0.0361,  0.0281, -0.0368],
        [ 0.0155,  0.0163, -0.0104,  ...,  0.0178, -0.0059, -0.0212],
        [ 0.0040,  0.0164,  0.0021,  ...,  0.0075,  0.0334, -0.0080]])

llm.base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-3.5524e-05, -8.5449e-03, -2.4658e-02,  ...,  9.7656e-03,
          6.8665e-03,  1.3123e-02],
        [ 1.7471e-03, -1.1063e-03, -7.0801e-03,  ...,  3.5553e-03,
          8.9111e-03,  5.5847e-03],
        [ 6.8359e-03, -5.1270e-03, -2.0264e-02,  ...,  1.5015e-02,
          2.4170e-02, -6.1340e-03],
        ...,
        [-5.1575e-03,  1.1047e-02,  1.6117e-04,  ...,  7.9346e-03,
          2.8229e-04, -1.5747e-02],
        [-8.0872e-04, -3.5400e-03, -1.2329e-02,  ..., -2.4109e-03,
         -1.1597e-02, -2.9785e-02],
        [-4.2419e-03, -5.1880e-04,  8.1787e-03,  ..., -2.3682e-02,
          3.5706e-03, -8.3160e-04]])

llm.base_model.model.model.layers.15.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-8.0078e-01,  9.8047e-01, -4.4922e-01,  7.2266e-01, -1.6309e-01,
        -6.8750e-01,  3.8867e-01,  3.8672e-01, -2.8125e-01, -5.0391e-01,
         3.9062e-02, -8.9453e-01,  2.4609e-01,  1.3906e+00,  4.8438e-01,
        -7.6172e-01,  1.5137e-01,  4.8047e-01,  1.8555e-01,  3.7500e-01,
         5.4688e-01, -4.8438e-01, -3.8818e-02,  5.7422e-01, -7.8613e-02,
         1.9141e+00, -9.7168e-02,  1.1816e-01, -3.4424e-02,  8.3984e-02,
         9.5703e-02, -1.6484e+00,  5.8105e-02, -2.7539e-01, -2.2095e-02,
         1.6016e-01, -4.9414e-01, -1.5430e-01, -7.3242e-02, -6.8750e-01,
        -3.5156e-02,  4.3457e-02,  4.8523e-03, -6.2012e-02, -1.6641e+00,
        -1.3962e-03, -7.3730e-02, -4.2725e-02,  2.4780e-02, -9.7266e-01,
         2.2217e-02, -3.7842e-02,  9.6875e-01,  5.8594e-02, -6.6406e-02,
        -6.3965e-02, -1.3867e-01,  1.5000e+00,  8.4961e-02,  2.1094e-01,
         5.1514e-02, -2.5146e-02, -1.2354e-01,  1.0254e-01,  6.6016e-01,
        -2.1094e-01,  6.5430e-02,  6.4062e-01,  1.0078e+00,  4.0039e-01,
        -5.0000e-01, -1.1328e+00, -2.4902e-01, -1.2109e+00, -6.4844e-01,
        -1.3477e-01,  5.4932e-02,  4.5508e-01, -3.0078e-01, -4.1602e-01,
         1.5234e+00, -6.9531e-01, -9.7168e-02, -1.4297e+00,  8.7402e-02,
         1.7500e+00, -5.7129e-02,  4.5312e-01, -8.2031e-02,  9.3750e-02,
         4.4922e-01, -1.1182e-01,  1.6719e+00, -1.0254e-02, -6.9824e-02,
        -2.0410e-01, -4.0039e-01, -2.8687e-02, -2.5879e-02,  1.1084e-01,
         2.4375e+00, -3.4668e-02, -2.2070e-01,  3.5156e-01, -8.8379e-02,
         4.3945e-02, -7.3730e-02, -8.5938e-02,  3.8672e-01,  8.4961e-02,
        -1.6602e-02, -4.1260e-02,  1.0132e-02, -2.2344e+00, -8.8882e-04,
         4.0039e-02,  1.2812e+00, -2.5635e-02,  4.3750e-01, -9.5703e-02,
        -1.3428e-02,  1.0547e+00, -2.1267e-04,  1.6797e-01,  8.1641e-01,
        -2.0386e-02,  4.2383e-01, -1.2158e-01,  2.7656e+00,  6.7969e-01,
        -1.7109e+00, -2.4048e-02, -1.3516e+00,  2.5391e-01, -1.6953e+00,
         6.3281e-01,  2.6953e-01,  3.6523e-01,  6.0059e-02, -4.7070e-01,
         2.9541e-02,  3.8672e-01,  2.0630e-02, -1.9453e+00, -9.3262e-02,
         1.8457e-01,  1.7871e-01,  3.0518e-02,  1.3574e-01,  3.6133e-02,
        -2.2583e-02,  3.7231e-03,  8.1055e-02,  4.0771e-02,  6.4392e-03,
        -1.9922e+00,  1.2512e-02,  2.0386e-02, -9.7656e-02,  3.1738e-02,
         1.2158e-01, -5.1514e-02,  2.7588e-02,  1.5527e-01,  1.3611e-02,
        -7.8125e-02,  5.2643e-04,  2.0996e-01,  9.2285e-02, -2.4170e-02,
        -1.9043e-02,  5.6885e-02, -5.3467e-02, -5.2246e-02, -1.4221e-02,
         3.5858e-03, -7.2021e-03,  1.1475e-02,  2.1118e-02,  4.4678e-02,
         9.8877e-03, -2.4536e-02, -5.0938e+00, -1.7456e-02, -5.3223e-02,
        -4.3213e-02, -2.4780e-02,  1.9165e-02,  3.4424e-02,  3.2501e-03,
        -9.6680e-02,  3.0664e-01, -1.0703e+00,  2.3633e-01,  2.7148e-01,
        -3.6377e-02, -2.5586e-01,  1.9824e-01, -6.7969e-01, -1.9141e-01,
        -5.4199e-02, -1.4766e+00, -7.8125e-02,  1.8281e+00,  2.5391e-02,
        -1.3770e-01, -6.9336e-02, -3.0273e-01,  3.8574e-02, -2.4567e-03,
        -1.9219e+00,  4.6082e-03,  2.3193e-02, -1.3750e+00,  4.2236e-02,
         8.5938e-02, -2.1406e+00, -2.8564e-02, -5.9814e-03, -1.4551e-01,
        -3.4180e-03, -4.3457e-02, -2.6367e-02, -8.0566e-02, -9.2773e-03,
         1.4688e+00,  3.7842e-02,  2.6245e-03, -3.2227e-02, -1.9287e-02,
         5.5176e-02,  7.0312e-02, -3.5553e-03, -3.1128e-02,  1.4801e-03,
         6.2256e-02, -7.2937e-03, -6.5918e-02,  3.1982e-02,  3.7598e-02,
        -6.9824e-02,  2.9602e-03,  4.0283e-03,  5.9082e-02,  8.4839e-03,
        -1.9989e-03,  1.0234e+00,  3.7109e-02,  7.0801e-02, -2.6550e-03,
        -1.3367e-02,  6.7444e-03,  6.7871e-02, -1.4404e-02, -2.0117e-01,
        -2.4023e-01,  2.1875e-01,  4.2188e-01,  3.8867e-01, -4.6875e-01,
         4.0625e-01,  4.0430e-01, -5.5542e-03,  1.9336e-01,  4.5508e-01,
         1.1035e-01,  3.1641e-01, -6.7969e-01,  2.3438e-01,  1.8945e-01,
         6.4844e-01, -1.2598e-01,  1.5820e-01, -9.1797e-01,  4.5898e-01,
        -3.3398e-01,  6.1328e-01, -1.1641e+00,  2.0312e-01, -6.4844e-01,
         1.1953e+00, -1.2256e-01,  4.1797e-01, -1.6406e-01, -1.3828e+00,
        -1.6797e-01, -3.4027e-03, -2.3906e+00, -2.1094e-01,  1.3672e+00,
         1.7090e-01,  2.5000e-01,  3.3008e-01,  4.9072e-02, -2.1406e+00,
        -2.0215e-01,  6.4941e-02, -2.8750e+00, -6.6895e-02,  2.6953e-01,
        -2.8906e-01,  7.2754e-02,  4.4688e+00, -1.9824e-01,  7.1875e-01,
        -1.3672e-01, -5.4297e-01,  9.6680e-02,  2.7344e-01,  2.0508e-01,
        -1.0303e-01, -2.8198e-02,  1.3379e-01,  2.1118e-02,  2.8320e-01,
        -1.2500e-01, -3.5352e-01, -6.5625e-01, -4.1016e-01, -1.2266e+00,
        -2.8516e-01, -1.7188e-01, -2.2754e-01,  7.7637e-02,  3.4180e-01,
        -2.7148e-01, -5.5078e-01, -6.5234e-01, -3.8086e-01,  6.8750e-01,
        -4.7266e-01, -2.1667e-03, -5.7031e-01,  8.5156e-01, -2.0410e-01,
        -7.8906e-01, -5.1172e-01,  1.9727e-01,  7.6953e-01,  4.6094e-01,
        -5.9766e-01,  1.2109e-01,  6.9141e-01,  5.0000e-01,  1.7944e-02,
        -1.4062e-01, -1.4375e+00, -4.9023e-01,  1.6016e-01, -4.2188e-01,
         1.8359e-01, -1.9531e-01, -6.2500e-02,  2.5195e-01,  1.2402e-01,
        -1.1377e-01,  1.6211e-01,  2.1680e-01, -2.6953e-01, -6.5430e-02,
         3.5889e-02, -7.5684e-02,  5.7617e-02,  9.2773e-02, -1.3770e-01,
         2.2070e-01,  1.4258e-01,  1.3477e-01, -2.5195e-01, -3.0664e-01,
        -4.1748e-02, -1.6968e-02,  1.1035e-01,  4.9316e-02,  6.5430e-02,
         4.4434e-02, -1.0938e-01,  2.1289e-01, -6.8359e-02, -4.9561e-02,
        -5.3906e-01,  9.9609e-01,  2.3633e-01, -5.3906e-01, -9.2578e-01,
         8.0469e-01, -1.9434e-01, -9.2969e-01,  7.0312e-02, -4.5312e-01,
         2.3828e-01, -1.6699e-01, -9.3750e-01, -1.4941e-01, -2.0801e-01,
         6.3477e-02,  3.1250e-02,  3.1250e-01,  3.4766e-01,  6.3672e-01,
        -1.1719e+00, -6.2256e-02,  1.3672e-01, -6.1328e-01, -6.1279e-02,
         2.3926e-02,  1.9219e+00,  2.7832e-02,  2.4707e-01, -1.7212e-02,
        -1.0312e+00,  5.0293e-02,  2.7832e-02, -3.6926e-03, -2.3594e+00,
        -6.4844e-01,  1.2817e-02, -1.3516e+00, -7.2266e-02, -1.6895e-01,
         4.3555e-01, -1.0791e-01,  1.2061e-01,  2.9297e-02,  6.5430e-02,
         6.3281e-01,  4.2969e-01, -1.0840e-01, -2.2461e-01,  8.2031e-01,
        -2.6758e-01, -2.2656e+00,  1.5723e-01,  2.5586e-01, -2.3828e-01,
         3.5352e-01, -2.8320e-02,  2.2266e-01, -1.6309e-01,  4.3750e-01,
         3.9453e-01, -1.2812e+00,  1.5562e+01, -1.6016e+00, -8.6719e-01,
         1.0750e+01, -8.4375e-01,  2.3125e+00,  7.8735e-03,  1.2061e-01,
         2.8711e-01, -2.1680e-01,  2.8516e-01, -4.3945e-01, -2.8125e-01,
        -2.7148e-01, -1.6846e-02,  3.4668e-02, -2.6758e-01, -1.1250e+00,
         2.3438e-01, -1.7773e-01, -1.8262e-01, -1.6699e-01,  2.0410e-01,
         1.0498e-01,  7.8125e-03, -1.3438e+00, -7.2937e-03,  8.3008e-02,
        -5.0000e-01,  2.2852e-01, -5.6152e-02,  1.3770e-01, -1.0156e-01,
         4.6094e-01,  3.0078e-01, -1.4038e-02,  9.2969e-01, -1.0840e-01,
        -3.7994e-03,  7.8516e-01,  1.6211e-01, -9.8633e-02, -1.0840e-01,
         2.5000e-01,  1.2188e+00,  1.1963e-01,  1.0547e-01, -4.2773e-01,
         1.7188e-01,  2.4121e-01, -2.2754e-01,  1.8921e-02, -1.4355e-01,
        -3.0625e+00,  1.9531e-01, -1.0352e-01,  8.5938e-02, -5.5420e-02,
        -7.9297e-01,  2.4512e-01,  8.7500e-01,  2.9297e-01, -2.5391e-01,
        -5.1562e-01,  2.3750e+01,  3.8281e-01,  9.6875e-01,  1.8203e+00,
         3.0156e+00, -5.6250e-01])

llm.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0336, -0.0297, -0.0200,  ...,  0.0100, -0.0353, -0.0073],
        [ 0.0183, -0.0233,  0.0105,  ..., -0.0226,  0.0044, -0.0066],
        [ 0.0445, -0.0074,  0.0083,  ...,  0.0013, -0.0199,  0.0170],
        ...,
        [-0.0213, -0.0195,  0.0268,  ..., -0.0350,  0.0134, -0.0377],
        [-0.0041, -0.0018,  0.0144,  ...,  0.0109, -0.0151,  0.0018],
        [ 0.0394,  0.0033,  0.0193,  ...,  0.0026, -0.0349,  0.0040]])

llm.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-5.4655e-02, -8.5643e-03, -2.1318e-02,  ...,  4.0521e-03,
         -5.0734e-03, -8.4310e-03],
        [ 4.2874e-02,  3.9923e-03,  2.4687e-02,  ...,  1.4463e-02,
         -9.9991e-03,  5.6565e-02],
        [ 9.5447e-03,  4.9754e-03,  2.2203e-02,  ..., -1.7949e-02,
          1.1569e-02, -7.7912e-06],
        ...,
        [ 5.5494e-03, -2.1597e-02,  1.7519e-02,  ...,  7.6442e-04,
         -5.6462e-02,  4.3840e-02],
        [ 3.2526e-03,  5.9670e-03,  2.0003e-02,  ...,  1.7435e-03,
         -2.3130e-02,  2.3230e-02],
        [ 8.4517e-03, -8.3361e-03,  2.0237e-03,  ..., -1.3801e-02,
         -1.5020e-02, -1.2407e-02]])

llm.base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0029, -0.0085,  0.0013,  ..., -0.0200, -0.0049, -0.0095],
        [ 0.0182, -0.0107, -0.0089,  ..., -0.0041,  0.0087, -0.0068],
        [-0.0091, -0.0184,  0.0043,  ...,  0.0133, -0.0374, -0.0255],
        ...,
        [-0.0063,  0.0094,  0.0106,  ..., -0.0025,  0.0260,  0.0383],
        [ 0.0297, -0.0292,  0.0045,  ...,  0.0057, -0.0187, -0.0200],
        [ 0.0354, -0.0075, -0.0010,  ...,  0.0033, -0.0022,  0.0137]])

llm.base_model.model.model.layers.15.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 3.1006e-02, -4.0527e-02, -1.4221e-02, -2.9419e-02,  1.1841e-02,
         3.8574e-02, -2.4414e-02, -2.1362e-02,  7.5684e-03,  1.2207e-02,
        -3.8086e-02,  2.0996e-02,  1.0254e-02, -1.1230e-02,  4.2725e-02,
         1.9653e-02, -1.8311e-02,  1.3962e-03,  2.8198e-02, -1.4038e-02,
         2.7588e-02,  2.8687e-02,  1.4832e-02,  1.0352e-01, -1.0193e-02,
        -1.9302e-03, -2.2583e-02, -3.7598e-02, -2.6367e-02,  1.5869e-02,
        -1.8188e-02,  2.3438e-02,  1.2207e-02, -5.6763e-03, -2.1973e-02,
         6.4392e-03,  2.0142e-02, -1.9653e-02,  3.7598e-02,  8.2031e-02,
         1.2817e-02,  7.0953e-04,  1.3794e-02,  2.4170e-02,  1.9775e-02,
         1.3306e-02, -5.6763e-03,  2.4567e-03,  4.1504e-02, -1.0620e-02,
         2.7344e-02,  3.1891e-03,  1.4343e-02, -2.7588e-02,  8.3496e-02,
        -7.7148e-02, -3.7842e-02, -1.3367e-02,  2.5513e-02, -2.0142e-02,
         3.0060e-03,  9.7046e-03, -1.5381e-02,  7.0801e-03, -3.8910e-03,
         3.3203e-02,  4.2480e-02,  3.5095e-04,  6.4453e-02,  2.5482e-03,
        -1.6357e-02, -1.0315e-02, -3.7500e-01, -1.0352e-01, -1.5015e-02,
        -4.8096e-02,  2.4414e-02, -2.7954e-02, -6.6406e-02,  5.8105e-02,
         1.8188e-02, -6.9336e-02,  1.9165e-02, -4.4434e-02, -1.4221e-02,
        -4.0771e-02, -1.0803e-02, -4.1748e-02, -3.7994e-03,  1.0925e-02,
        -3.3789e-01, -2.1729e-02, -3.0273e-02, -2.8320e-02, -1.2109e-01,
         1.3184e-02,  3.3447e-02,  8.9111e-03,  2.5391e-02, -2.2827e-02,
        -1.6724e-02, -4.5410e-02,  1.1914e-01, -1.3611e-02, -4.0039e-02,
        -3.8574e-02, -6.2256e-03,  3.6377e-02,  1.1536e-02,  1.4771e-02,
         1.9653e-02,  4.8584e-02,  2.1118e-02, -4.5898e-02, -2.5879e-02,
         4.2236e-02, -7.5378e-03, -2.7344e-02, -7.9346e-03, -9.5215e-03,
         2.6978e-02,  4.5410e-02,  2.9541e-02,  2.5024e-02, -9.2163e-03,
        -2.2583e-02, -2.5757e-02, -7.6599e-03, -7.4219e-02, -3.6865e-02,
        -5.2734e-02,  1.5869e-02, -7.4768e-03,  9.5825e-03,  1.7929e-03,
         2.5513e-02,  9.3750e-02, -4.0039e-02, -1.9653e-02, -6.5430e-02,
        -3.9307e-02, -3.6621e-03,  6.3965e-02, -1.9043e-02, -4.5410e-02,
        -7.9346e-03, -1.4893e-02, -7.3242e-02, -1.3672e-02, -1.2939e-02,
         7.3730e-02,  1.6235e-02, -8.3008e-03,  5.7129e-02, -7.6660e-02,
         6.3477e-02, -3.1128e-02,  1.8188e-02,  8.5938e-02, -2.0386e-02,
         3.9673e-03,  8.1177e-03,  9.9487e-03, -5.1514e-02, -9.2773e-03,
         3.8574e-02,  4.1992e-02, -6.5430e-02,  5.2734e-02, -1.4343e-02,
        -4.1260e-02, -2.2217e-02, -6.5430e-02,  4.9561e-02, -5.1575e-03,
         9.5215e-02,  1.1963e-02, -7.4707e-02, -5.5664e-02, -1.3611e-02,
        -8.6426e-02,  6.8054e-03, -3.6523e-01, -4.9316e-02,  6.0425e-03,
        -8.1787e-03,  3.4180e-02, -3.9307e-02, -4.2480e-02, -3.0518e-03,
         1.9409e-02, -3.0396e-02, -1.9775e-02,  2.4658e-02,  2.1362e-02,
         6.4087e-04,  2.2583e-03,  2.7344e-02, -2.4872e-03, -3.1433e-03,
        -7.5684e-03, -7.3242e-02, -5.7617e-02, -1.4343e-02,  1.2500e-01,
         3.0762e-02, -2.0752e-02, -1.5625e-01, -5.2246e-02,  3.5889e-02,
        -2.5269e-02, -4.4556e-03, -1.5625e-02,  2.5635e-02, -2.1240e-02,
        -4.1748e-02, -4.0039e-02, -3.0884e-02, -1.2012e-01,  2.2278e-03,
        -1.7334e-02, -5.9204e-03, -2.0752e-02, -1.9409e-02, -3.2471e-02,
         1.3000e-02,  2.3193e-02,  4.8828e-02, -2.2217e-02,  2.6733e-02,
        -1.4404e-02,  1.6724e-02, -2.8442e-02, -3.6621e-02,  2.1362e-02,
         6.2866e-03,  1.6724e-02, -4.2236e-02,  5.2490e-02,  2.8809e-02,
         2.7588e-02, -2.7344e-02,  2.1118e-02,  1.0254e-02, -1.4648e-03,
         1.0254e-01, -5.6396e-02, -1.4648e-02,  2.3926e-02,  5.1514e-02,
         9.3384e-03,  1.1169e-02, -1.6602e-02,  4.7852e-02, -2.9785e-02,
        -2.7100e-02,  1.2634e-02, -4.2236e-02,  1.7334e-02, -2.1606e-02,
         3.3691e-02,  4.6631e-02, -4.1260e-02,  2.0996e-02,  2.8076e-02,
         1.6113e-02, -2.5269e-02, -7.8964e-04, -1.7853e-03,  8.3008e-03,
         1.6602e-02,  1.7700e-02, -3.3936e-02, -4.7363e-02,  1.9043e-02,
        -1.8921e-02, -1.0681e-02, -2.7588e-02, -4.0771e-02,  2.5146e-02,
        -4.2725e-03,  2.8687e-02,  2.0020e-02,  1.0559e-02,  1.4355e-01,
         7.9102e-02, -3.7598e-02,  1.0254e-02, -9.0820e-02, -2.5024e-02,
        -3.4637e-03,  8.1055e-02, -7.2754e-02,  2.0874e-02,  3.5248e-03,
         7.3438e-01, -1.4648e-02, -3.3447e-02,  4.1260e-02,  1.4877e-03,
         1.3062e-02,  4.0894e-03, -1.1658e-02, -2.7100e-02,  3.2227e-02,
         3.6621e-02, -1.5918e-01,  3.2715e-02,  3.9307e-02, -9.3994e-03,
         4.3457e-02,  4.8828e-03,  2.0020e-02, -6.5613e-03,  2.3438e-02,
        -2.4048e-02, -6.0730e-03, -9.8877e-03, -1.1169e-02,  2.8687e-02,
        -1.9073e-03, -1.0791e-01, -2.9297e-02,  8.7280e-03, -2.2583e-02,
         1.6724e-02, -1.1279e-01, -1.1475e-01,  3.8574e-02,  1.3794e-02,
        -1.2573e-02, -1.3550e-02, -3.6377e-02,  4.0283e-02, -2.0996e-02,
        -4.5898e-02,  1.8921e-02, -1.8692e-03, -4.4189e-02,  4.4250e-03,
        -2.3438e-02, -6.2256e-02,  1.3306e-02,  1.8921e-02, -2.4170e-02,
         2.3346e-03,  2.9541e-02, -6.8665e-03, -2.2339e-02,  2.6855e-03,
        -3.0762e-02, -1.1169e-02, -1.2207e-01,  1.7578e-02, -1.7090e-02,
        -5.2002e-02,  5.1172e-01,  1.3855e-02, -4.9744e-03, -2.5757e-02,
         1.0376e-02, -8.4229e-03, -7.5378e-03,  1.9409e-02, -1.9165e-02,
         1.3672e-02, -2.3560e-02,  9.1553e-03, -9.7046e-03,  2.0264e-02,
        -2.5635e-02,  1.1536e-02,  4.8523e-03, -7.5073e-03,  1.1475e-02,
         3.0762e-02, -1.4404e-02,  1.1963e-02, -1.3916e-02,  3.4180e-03,
        -1.2756e-02, -7.2327e-03,  1.1658e-02, -2.1729e-02,  4.3457e-02,
         8.4473e-02,  4.2725e-02, -3.0884e-02,  4.1992e-02, -4.8340e-02,
         7.7515e-03, -3.0151e-02,  6.3477e-02, -4.8523e-03,  3.9062e-03,
        -1.5503e-02, -1.5442e-02, -6.1035e-02,  2.8931e-02,  1.0449e-01,
         2.4414e-03, -3.1128e-02,  7.6660e-02,  4.5776e-03,  1.9897e-02,
        -7.7820e-03,  5.1575e-03,  3.3691e-02, -2.0386e-02,  7.3730e-02,
        -2.7710e-02, -3.7109e-02, -4.9561e-02,  1.6357e-02,  8.3008e-03,
         1.1719e-01, -3.2227e-02,  3.3936e-02, -3.0273e-02,  2.6733e-02,
        -1.8188e-02,  7.5073e-03,  6.3965e-02, -1.9932e-04,  3.1494e-02,
        -3.4180e-02,  8.7402e-02, -8.5938e-02,  7.6172e-02, -1.8188e-02,
        -1.2329e-02,  3.6621e-02, -2.0996e-02, -4.2236e-02, -3.5156e-02,
        -4.5410e-02,  2.5940e-04,  1.9165e-02,  3.7598e-02,  2.7954e-02,
         4.5654e-02, -5.6396e-02, -2.1729e-02,  3.3203e-02,  5.7861e-02,
         6.2866e-03,  1.3550e-02,  4.1016e-02, -4.0039e-02,  1.6724e-02,
         7.9102e-02, -5.8838e-02, -1.8677e-02, -7.1716e-03,  6.2012e-02,
        -3.9673e-03, -7.7820e-03,  6.8970e-03, -1.1182e-01,  1.0742e-02,
         4.7607e-02, -2.1240e-02, -1.0605e-03,  1.0315e-02, -2.4902e-02,
         1.3867e-01, -1.1047e-02,  1.9897e-02,  1.2939e-02, -2.2583e-02,
         2.6001e-02,  2.9297e-02,  7.4219e-02,  8.9111e-03,  1.1914e-01,
         1.7822e-02,  3.5553e-03, -1.3672e-01, -2.4170e-02, -3.8574e-02,
        -8.3008e-02,  4.0283e-03, -2.1118e-02,  3.3691e-02, -2.0020e-02,
        -2.4414e-02, -2.4902e-02,  3.5286e-04,  2.1118e-02,  9.6436e-03,
         8.9844e-02, -4.5654e-02, -6.0425e-03,  2.0508e-02,  5.4688e-02,
        -6.7383e-02, -4.1748e-02, -3.6621e-02,  2.5513e-02, -4.6875e-02,
         1.2634e-02, -4.8828e-02, -2.9541e-02, -3.1494e-02,  2.0874e-02,
        -7.8125e-02,  4.2236e-02, -2.9175e-02,  2.3315e-02,  1.6968e-02,
        -1.6113e-02, -3.1982e-02])

llm.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0355,  0.0131,  0.0051,  ...,  0.0551,  0.0023,  0.0160],
        [ 0.0158, -0.0414, -0.0439,  ..., -0.0524, -0.0137, -0.0565],
        [ 0.0106,  0.0613,  0.0258,  ...,  0.0330,  0.0402,  0.0096],
        ...,
        [-0.0539,  0.0412, -0.0421,  ..., -0.0485, -0.0315, -0.0069],
        [ 0.0276,  0.0144, -0.0068,  ...,  0.0211, -0.0349,  0.0743],
        [-0.0049,  0.0336,  0.0269,  ...,  0.0489, -0.0407,  0.0192]])

llm.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0172, -0.0363, -0.0068,  ...,  0.0284,  0.0260,  0.0095],
        [ 0.0001,  0.0209, -0.0108,  ..., -0.0314, -0.0135,  0.0227],
        [-0.0132,  0.0091, -0.0093,  ..., -0.0007,  0.0011, -0.0085],
        ...,
        [-0.0038,  0.0038, -0.0178,  ..., -0.0103,  0.0076, -0.0014],
        [-0.0209, -0.0282,  0.0130,  ...,  0.0091, -0.0148,  0.0248],
        [-0.0161,  0.0297, -0.0194,  ...,  0.0121, -0.0065, -0.0069]])

llm.base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0261,  0.0045,  0.0454,  ...,  0.0065,  0.0117, -0.0033],
        [ 0.0062,  0.0044, -0.0047,  ..., -0.0021, -0.0013,  0.0096],
        [-0.0261, -0.0089, -0.0413,  ...,  0.0223,  0.0030, -0.0051],
        ...,
        [-0.0024,  0.0150, -0.0145,  ...,  0.0009, -0.0071,  0.0122],
        [ 0.0067,  0.0216,  0.0171,  ...,  0.0128,  0.0087,  0.0003],
        [ 0.0125,  0.0034, -0.0127,  ...,  0.0278,  0.0095, -0.0067]])

llm.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0459, -0.0234, -0.0485,  ..., -0.0737, -0.0009, -0.0138],
        [ 0.0358,  0.0206, -0.0291,  ..., -0.0415,  0.0049,  0.0601],
        [-0.0129,  0.0034,  0.0089,  ..., -0.0008, -0.0074,  0.0257],
        ...,
        [ 0.0342,  0.0261, -0.0018,  ...,  0.0263, -0.0161,  0.0150],
        [ 0.0032,  0.0180, -0.0032,  ...,  0.0193, -0.0249,  0.0577],
        [-0.0184, -0.0176, -0.0096,  ..., -0.0122, -0.0231, -0.0217]])

llm.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 4.1048e-02,  1.7917e-02, -8.3893e-03,  ..., -8.4168e-03,
          3.0408e-02, -5.6604e-03],
        [ 1.8512e-02, -1.7764e-02,  3.2641e-02,  ...,  2.3674e-03,
         -4.6780e-03, -2.4746e-02],
        [-6.7034e-05,  4.3455e-02, -4.2523e-02,  ...,  2.3576e-02,
          4.7570e-03, -1.0503e-02],
        ...,
        [ 7.1178e-03, -5.8563e-03,  2.5497e-03,  ...,  4.2462e-04,
         -4.7602e-02, -4.3734e-03],
        [-1.3044e-02, -4.1601e-03,  2.7472e-02,  ..., -1.4731e-02,
          1.2610e-02,  2.1829e-02],
        [ 1.8336e-02, -3.3603e-02,  3.4034e-02,  ..., -2.6333e-03,
         -1.5647e-02,  3.9790e-02]])

llm.base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0131, -0.0258,  0.0245,  ..., -0.0161,  0.0022, -0.0024],
        [ 0.0094,  0.0089, -0.0131,  ...,  0.0079,  0.0062,  0.0138],
        [ 0.0157,  0.0090,  0.0016,  ...,  0.0131, -0.0147, -0.0003],
        ...,
        [-0.0023,  0.0076,  0.0132,  ..., -0.0141,  0.0175,  0.0183],
        [-0.0002, -0.0219, -0.0168,  ...,  0.0137,  0.0081, -0.0079],
        [-0.0128,  0.0141, -0.0240,  ..., -0.0138,  0.0125, -0.0029]])

llm.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0303, -0.0272, -0.0156,  ..., -0.0408,  0.0131,  0.0277],
        [ 0.0698,  0.0086,  0.0127,  ...,  0.0243, -0.0519,  0.0079],
        [ 0.0198,  0.0303, -0.0119,  ..., -0.0328,  0.0149,  0.0167],
        ...,
        [ 0.0011, -0.0243,  0.0302,  ..., -0.0192,  0.0016,  0.0594],
        [ 0.0124, -0.0240,  0.0071,  ...,  0.0671, -0.0042, -0.0448],
        [ 0.0513, -0.0191, -0.0165,  ...,  0.0304,  0.0038, -0.0268]])

llm.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0073, -0.0138,  0.0262,  ...,  0.0131, -0.0019,  0.0361],
        [-0.0113,  0.0086, -0.0053,  ...,  0.0105,  0.0027, -0.0215],
        [ 0.0308, -0.0060, -0.0125,  ...,  0.0541, -0.0271, -0.0276],
        ...,
        [ 0.0127,  0.0239, -0.0424,  ..., -0.0003,  0.0263, -0.0010],
        [-0.0095,  0.0044,  0.0048,  ...,  0.0040,  0.0292,  0.0216],
        [-0.0128, -0.0297, -0.0317,  ...,  0.0150, -0.0293, -0.0135]])

llm.base_model.model.model.layers.15.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0058, -0.0146,  0.0265,  ...,  0.0088,  0.0474, -0.0025],
        [ 0.0064,  0.0068,  0.0036,  ..., -0.0159, -0.0162,  0.0386],
        [ 0.0020, -0.0030, -0.0151,  ..., -0.0212, -0.0045,  0.0061],
        ...,
        [-0.0038,  0.0042,  0.0073,  ..., -0.0078,  0.0211,  0.0136],
        [ 0.0035,  0.0266,  0.0006,  ...,  0.0034, -0.0111,  0.0020],
        [ 0.0109,  0.0045,  0.0120,  ...,  0.0089, -0.0035, -0.0037]])

llm.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0395, -0.0508, -0.0361,  ...,  0.0487,  0.0203, -0.0421],
        [ 0.0561,  0.0334,  0.0116,  ...,  0.0307, -0.0236,  0.0527],
        [ 0.0606,  0.0203, -0.0196,  ...,  0.0435, -0.0073,  0.0024],
        ...,
        [-0.0251,  0.0307,  0.0142,  ...,  0.0094,  0.0017,  0.0156],
        [-0.0509,  0.0162, -0.0756,  ...,  0.0164,  0.0577,  0.0153],
        [ 0.0158, -0.0208, -0.0572,  ..., -0.0103, -0.0331, -0.0438]])

llm.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-1.7656e-02, -2.4598e-05,  1.4808e-02,  ...,  3.9242e-02,
         -3.4191e-02, -2.1967e-02],
        [ 6.3335e-03, -1.2267e-02, -6.3849e-02,  ...,  5.6084e-03,
         -9.5881e-03,  3.6739e-02],
        [ 3.5293e-02, -1.2875e-02,  3.8747e-03,  ...,  2.7165e-03,
          4.7986e-02,  1.0486e-02],
        ...,
        [ 5.0472e-02, -8.1809e-03,  2.3518e-02,  ...,  2.0805e-02,
         -2.1876e-02,  4.1952e-02],
        [ 2.5281e-02, -4.3734e-02,  1.0026e-03,  ..., -9.2968e-03,
          1.3349e-02,  4.1009e-02],
        [ 9.7204e-03,  3.6375e-02, -1.9107e-02,  ..., -6.3482e-03,
         -2.6423e-03, -2.6925e-02]])

llm.base_model.model.model.layers.15.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 0.0164,  0.0004,  0.0154,  ..., -0.0156,  0.0001,  0.0103],
        [-0.0108,  0.0134,  0.0165,  ...,  0.0165, -0.0179,  0.0212],
        [ 0.0104, -0.0013, -0.0061,  ..., -0.0159, -0.0017,  0.0123],
        ...,
        [-0.0056,  0.0112,  0.0179,  ..., -0.0053, -0.0031, -0.0012],
        [ 0.0339, -0.0011, -0.0034,  ..., -0.0003, -0.0008, -0.0079],
        [-0.0123,  0.0310, -0.0162,  ..., -0.0028,  0.0168, -0.0096]])

llm.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[-0.0006,  0.0050,  0.0398,  ..., -0.0207, -0.0027,  0.0194],
        [ 0.0070, -0.0098, -0.0262,  ...,  0.0444,  0.0274,  0.0152],
        [-0.0292, -0.0065,  0.0242,  ..., -0.0204,  0.0086,  0.0189],
        ...,
        [-0.0288,  0.0007,  0.0351,  ..., -0.0033,  0.0257, -0.0301],
        [ 0.0182, -0.0125, -0.0412,  ..., -0.0079, -0.0015,  0.0124],
        [ 0.0189, -0.0067,  0.0132,  ..., -0.0062, -0.0224, -0.0462]])

llm.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 1.3783e-02,  6.2329e-02, -1.3382e-03,  ..., -8.6579e-03,
          2.6876e-02, -5.5884e-03],
        [ 1.1812e-02, -2.3232e-05, -1.6718e-02,  ...,  2.2709e-02,
         -3.2111e-02,  8.5241e-03],
        [-1.6722e-02, -1.5755e-02,  3.3350e-03,  ...,  1.4046e-02,
          1.0706e-02, -3.7480e-02],
        ...,
        [-1.9596e-03, -3.1239e-02,  1.0070e-02,  ..., -3.5220e-02,
          1.5675e-03, -2.5508e-02],
        [-4.3298e-02,  5.0061e-03, -2.1909e-03,  ...,  2.5045e-02,
         -1.4963e-02, -3.1396e-02],
        [ 3.6626e-02,  3.9785e-02, -1.9946e-02,  ..., -4.8610e-03,
         -3.8957e-03, -2.4726e-02]])

llm.base_model.model.model.layers.15.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.4688, 0.5547, 2.1094,  ..., 0.3281, 1.0000, 1.0625])

llm.base_model.model.model.layers.15.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.2188, 1.0312, 2.2031,  ..., 0.5234, 1.2188, 1.2344])

llm.base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0168,  0.0075, -0.0058,  ...,  0.0225,  0.0148, -0.0027],
        [-0.0016, -0.0034, -0.0017,  ..., -0.0044, -0.0061,  0.0101],
        [ 0.0032,  0.0029,  0.0038,  ..., -0.0061, -0.0065, -0.0065],
        ...,
        [ 0.0063, -0.0359, -0.0100,  ...,  0.0177, -0.0203,  0.0021],
        [ 0.0176,  0.0244,  0.0206,  ...,  0.0513,  0.0117,  0.0304],
        [-0.0250,  0.0188, -0.0211,  ..., -0.0042,  0.0056, -0.0177]])

llm.base_model.model.model.layers.16.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.4043, -0.0996,  0.3867,  ...,  0.0947,  0.3242,  0.4395])

llm.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0395,  0.0019, -0.0195,  ...,  0.0124,  0.0078,  0.0514],
        [ 0.0493,  0.0164,  0.0460,  ...,  0.0207,  0.0137, -0.0246],
        [-0.0863, -0.0142,  0.0032,  ..., -0.0033, -0.0737, -0.0346],
        ...,
        [ 0.0405, -0.0407,  0.0445,  ...,  0.0133, -0.0121,  0.0246],
        [-0.0218,  0.0148,  0.0392,  ...,  0.0509,  0.0195,  0.0105],
        [ 0.0449, -0.0341,  0.0689,  ...,  0.0394, -0.0274,  0.0396]])

llm.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0375, -0.0146, -0.0042,  ...,  0.0307, -0.0005,  0.0063],
        [-0.0467, -0.0068, -0.0329,  ...,  0.0074,  0.0403, -0.0141],
        [ 0.0002, -0.0003, -0.0154,  ...,  0.0018,  0.0091, -0.0059],
        ...,
        [ 0.0167, -0.0304,  0.0188,  ..., -0.0046,  0.0221, -0.0064],
        [ 0.0029, -0.0214, -0.0301,  ...,  0.0186, -0.0002, -0.0057],
        [ 0.0118, -0.0179,  0.0394,  ..., -0.0286, -0.0256, -0.0048]])

llm.base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0043,  0.0070,  0.0048,  ...,  0.0037, -0.0114,  0.0101],
        [ 0.0007,  0.0006, -0.0027,  ...,  0.0144,  0.0060, -0.0091],
        [-0.0042,  0.0079,  0.0095,  ...,  0.0095, -0.0099,  0.0079],
        ...,
        [ 0.0070, -0.0063, -0.0055,  ...,  0.0103,  0.0038,  0.0036],
        [-0.0049, -0.0049,  0.0131,  ..., -0.0175, -0.0103, -0.0222],
        [ 0.0415, -0.0028, -0.0005,  ...,  0.0364, -0.0232,  0.0212]])

llm.base_model.model.model.layers.16.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-4.6484e-01,  1.4844e-01, -5.3906e-01, -4.2773e-01, -6.0547e-01,
        -1.8555e-01, -2.8125e-01,  1.1670e-01, -7.2266e-02,  2.7148e-01,
         5.8594e-01, -3.3594e-01, -1.7090e-01,  3.3203e-01,  2.2168e-01,
        -7.0312e-01, -3.5156e-01,  3.7109e-01, -7.8125e-01,  3.5553e-03,
         3.4570e-01, -4.5508e-01,  1.4453e-01,  2.2168e-01, -1.3867e-01,
         2.0215e-01,  1.3125e+00, -1.7871e-01, -2.8711e-01,  1.1250e+00,
        -2.9688e-01,  1.9531e+00,  4.1260e-02,  1.2695e-01, -1.4746e-01,
         1.3184e-01, -3.9453e-01, -8.5938e-02,  2.2500e+00, -1.0254e-02,
         5.5859e-01, -1.8848e-01,  5.5176e-02,  1.5332e-01,  1.1621e-01,
         1.4453e-01, -4.7266e-01, -2.7344e+00, -3.0664e-01,  2.9688e-01,
        -8.7500e-01,  2.5977e-01, -3.8867e-01, -1.6797e-01, -2.8906e-01,
        -1.4141e+00, -2.9297e-01,  1.6641e+00, -9.3750e-01, -2.9375e+00,
        -1.3203e+00, -1.2422e+00,  1.1406e+00, -2.5156e+00, -9.8145e-02,
        -4.7266e-01, -3.3203e-01,  1.0254e-01, -1.3672e-01,  6.1719e-01,
        -1.7773e-01, -4.9023e-01, -6.2891e-01,  7.0801e-02,  5.2734e-01,
        -2.9102e-01, -2.4707e-01, -7.6172e-01,  7.9590e-02, -3.4766e-01,
        -2.6367e-01,  1.5527e-01, -2.7734e-01,  3.8477e-01, -9.4531e-01,
        -1.0803e-02, -7.3730e-02, -1.1172e+00, -6.2256e-02, -7.6953e-01,
         2.5586e-01, -9.8145e-02,  2.2705e-02, -5.0781e-02, -2.3730e-01,
         9.4238e-02,  1.0859e+00,  7.2266e-02, -7.1094e-01, -1.6724e-02,
         5.8594e-03,  2.4902e-01, -2.2559e-01,  2.2559e-01,  2.9492e-01,
        -9.0820e-02, -2.6094e+00, -7.8613e-02,  2.4512e-01, -6.7969e-01,
        -5.8984e-01, -5.3125e-01,  5.6250e-01, -2.0117e-01,  4.7656e-01,
        -6.4062e-01,  4.2969e-01,  6.7578e-01,  3.1250e-01, -4.4531e-01,
        -2.4316e-01, -2.3438e+00,  6.3672e-01, -1.7578e+00, -7.5781e-01,
        -1.5812e+01,  1.8516e+00,  2.0469e+00,  3.7695e-01,  3.8672e-01,
        -8.2520e-02,  2.8906e-01,  4.8242e-01,  3.7500e-01, -6.3672e-01,
        -1.0938e-01, -6.6797e-01,  1.1536e-02, -7.2656e-01,  2.9492e-01,
         1.5234e-01, -4.1211e-01, -1.4355e-01,  2.2461e-01,  4.5117e-01,
        -3.4375e-01, -7.7344e-01, -1.5723e-01,  4.4434e-02, -4.8047e-01,
         3.7500e-01,  7.1094e-01, -9.4531e-01, -3.3789e-01,  1.7578e+00,
         4.9561e-02,  7.0703e-01, -3.3984e-01,  1.9629e-01, -1.6895e-01,
         2.2461e-01, -1.2158e-01,  3.2227e-01,  2.4805e-01, -3.4570e-01,
        -3.8574e-02,  1.7891e+00, -2.0117e-01, -9.5703e-02,  2.8750e+00,
        -2.6953e-01, -2.6172e-01, -1.5137e-01,  7.2656e-01,  5.3516e-01,
        -3.4375e-01, -1.0840e-01,  5.6641e-01,  2.4121e-01,  4.4250e-03,
         2.4805e-01,  3.6328e-01, -3.0273e-01, -5.1270e-02, -7.0703e-01,
        -1.4771e-02, -3.9062e-02,  9.8047e-01,  5.0293e-02, -2.1406e+00,
        -6.3477e-02,  1.3047e+00,  2.5781e-01, -1.5527e-01, -5.9766e-01,
         9.7656e-02, -4.0039e-01, -3.1128e-02, -4.2773e-01,  2.9297e-01,
        -7.1716e-03, -4.2188e-01, -3.7305e-01, -3.2617e-01,  7.2266e-01,
        -2.3926e-02,  9.9609e-01,  2.9297e-01,  6.9531e-01,  4.7070e-01,
        -1.3086e-01,  2.4023e-01,  1.1875e+00, -2.8320e-01,  7.6953e-01,
        -6.5430e-02,  2.7539e-01,  3.5352e-01, -1.4648e-01, -6.2256e-02,
        -2.1484e-01, -9.3359e-01,  1.7822e-02,  2.6094e+00,  4.1992e-02,
         9.3750e-02,  6.0938e-01,  7.7637e-02, -5.7812e-01,  2.5781e-01,
         2.9102e-01, -2.3047e-01,  1.6113e-02, -2.2363e-01,  2.5757e-02,
        -1.7773e-01,  1.2695e-01, -1.5234e-01, -3.1719e+00, -7.4219e-01,
        -2.3828e-01,  3.3203e-01, -3.9258e-01, -9.7656e-02, -8.8379e-02,
         1.2451e-01,  4.3359e-01,  2.8906e-01, -2.8516e-01,  4.2969e-01,
        -4.1211e-01, -3.0859e-01, -5.0391e-01, -2.4375e+00,  8.3984e-01,
        -2.2812e+00,  3.3594e-01,  3.8867e-01,  3.9844e-01,  4.6680e-01,
        -4.0039e-02,  6.6016e-01, -4.1016e-01, -2.0898e-01,  8.1641e-01,
         1.6602e-01, -2.1582e-01, -3.3447e-02,  5.2344e-01, -2.0020e-01,
        -2.3047e-01,  5.2979e-02,  3.8867e-01,  1.6406e-01, -8.9844e-02,
        -1.7383e-01, -1.7285e-01, -4.8438e-01,  1.0781e+00, -1.2573e-02,
        -2.3047e-01, -9.1797e-01, -2.3535e-01,  2.4609e-01,  4.1016e-02,
         9.6680e-02,  6.0547e-02, -2.0469e+00,  1.4844e-01,  6.6895e-02,
        -1.5000e+00,  9.1309e-02,  2.4902e-01, -4.0625e-01, -5.9814e-02,
        -4.9805e-01,  7.5684e-02, -4.2578e-01, -6.6757e-04,  2.5312e+00,
        -4.6875e-02,  4.4727e-01, -2.6758e-01, -1.5234e-01,  2.8125e+00,
        -1.0742e-01, -2.4023e-01,  1.3184e-01,  3.2812e-01,  2.1875e+00,
         1.1978e-03, -5.6250e-01, -3.3789e-01, -1.3672e-01,  1.4922e+00,
        -7.7734e-01, -5.3906e-01,  1.8828e+00, -1.3867e-01, -3.7500e-01,
        -8.6328e-01,  2.5391e-01, -7.0703e-01,  4.5117e-01,  3.1055e-01,
         3.2617e-01,  5.3516e-01, -1.8066e-01,  1.6699e-01, -5.6641e-02,
        -2.0898e-01,  7.0312e-01,  1.8555e-02,  3.3984e-01,  7.9688e-01,
        -4.0820e-01, -4.0820e-01,  5.2734e-01, -9.4531e-01,  2.7710e-02,
        -2.0020e-02,  1.1426e-01, -1.7969e-01,  8.8867e-02, -6.0156e-01,
         2.0117e-01, -5.7031e-01,  1.2188e+00, -1.8750e-01,  2.2266e-01,
         1.6016e-01,  3.4180e-01, -4.4434e-02,  2.8906e-01,  3.0664e-01,
        -2.6953e-01, -3.3203e-01,  1.0059e-01,  3.7109e-02, -2.3125e+00,
        -1.5723e-01,  9.5703e-02, -3.3398e-01, -6.7383e-02, -8.8672e-01,
         2.2754e-01, -2.3047e-01, -2.9688e-01, -2.0312e-01,  1.6895e-01,
         3.8574e-02, -1.7773e-01, -1.6699e-01,  2.5000e+00, -5.4297e-01,
        -2.6367e-01, -1.9531e-01,  2.4316e-01,  9.4375e+00,  5.7422e-01,
         2.3438e-01,  4.5508e-01, -4.4727e-01,  1.3750e+00, -3.1836e-01,
        -6.2561e-03,  4.4531e-01,  1.8677e-02,  1.7578e-01, -2.1484e-01,
         6.1035e-02, -1.3062e-02,  1.4922e+00, -4.5410e-02,  1.5137e-01,
         3.4570e-01, -1.5723e-01, -3.7695e-01,  6.9824e-02, -2.4902e-01,
        -1.8066e-01,  2.4219e+00,  9.3262e-02,  1.4551e-01, -2.2344e+00,
        -9.4238e-02,  1.7188e-01,  2.8750e+00, -1.9531e-01, -1.2891e-01,
        -4.6875e-02,  5.2344e-01,  1.1133e-01,  3.5938e-01,  8.3496e-02,
         3.4375e+00, -1.7090e-01,  1.1133e-01,  1.8945e-01,  9.2773e-02,
         1.2109e-01,  1.0938e-01,  5.4297e-01, -2.6367e-01,  8.7402e-02,
         2.3535e-01,  1.5234e-01,  6.3965e-02,  1.9922e-01,  4.2773e-01,
        -1.4114e-03, -1.0791e-01, -3.3281e+00, -2.1387e-01, -1.2573e-02,
        -1.8848e-01, -5.6250e-01, -1.6699e-01,  1.5750e+01,  1.6602e-01,
        -3.7598e-02, -7.7500e+00,  2.0469e+00, -3.3691e-02,  5.0781e-01,
        -9.7656e-01,  6.2891e-01,  4.7266e-01,  1.0078e+00,  1.4355e-01,
        -3.1445e-01,  2.5586e-01, -1.1562e+00,  3.4570e-01, -1.3574e-01,
        -2.2217e-02, -1.0107e-01,  4.4922e-02,  8.5449e-02,  1.8750e+00,
         9.3750e-02,  4.0527e-02,  1.9336e-01, -1.2578e+00,  1.3184e-01,
        -6.9922e-01,  8.6914e-02, -4.9316e-02,  6.1328e-01,  1.0938e-01,
         4.4141e-01, -4.2773e-01,  1.7383e-01, -2.7930e-01,  1.0312e+00,
         9.7168e-02, -1.1328e-01, -1.3672e-01, -1.1279e-01, -4.7070e-01,
         3.8330e-02, -9.7656e-02, -2.0117e-01,  1.4531e+00,  6.9336e-02,
         1.8311e-02,  1.0059e-01,  2.9492e-01,  9.1309e-02, -6.5625e-01,
         8.0859e-01,  6.6895e-02,  3.5156e-02,  3.9258e-01, -6.5918e-02,
         1.2207e-01,  3.2500e+00,  9.1309e-02,  3.8574e-02, -1.0193e-02,
         3.0664e-01, -2.8931e-02, -3.5469e+00, -1.5527e-01, -6.8359e-01,
        -3.0625e+00,  1.3047e+00, -6.8750e-01, -2.5513e-02, -1.7188e+00,
         1.0693e-01,  3.0859e-01])

llm.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 1.7612e-02,  2.1182e-02, -3.2746e-02,  ..., -4.2502e-02,
         -1.8892e-02,  3.6311e-02],
        [-2.6925e-02, -4.5093e-02,  2.7166e-02,  ...,  9.7194e-03,
          2.9806e-03, -3.9606e-02],
        [-4.5068e-02, -4.0419e-02, -1.4833e-03,  ...,  2.0900e-02,
          7.8033e-03,  8.4205e-03],
        ...,
        [ 5.2851e-02,  7.2988e-03,  1.8385e-02,  ..., -8.3238e-05,
          9.4523e-03, -1.2157e-02],
        [-5.1580e-02,  4.7163e-02, -1.1512e-02,  ..., -1.1227e-02,
         -2.7451e-02, -1.7576e-02],
        [-4.9961e-02,  1.9218e-02,  8.4917e-03,  ...,  3.5412e-02,
         -1.2158e-02, -4.6092e-03]])

llm.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0250,  0.0085, -0.0243,  ..., -0.0103, -0.0319,  0.0051],
        [ 0.0002,  0.0098,  0.0072,  ..., -0.0133,  0.0070,  0.0139],
        [ 0.0080,  0.0110, -0.0032,  ...,  0.0358, -0.0198, -0.0219],
        ...,
        [-0.0125,  0.0501,  0.0243,  ...,  0.0480, -0.0209, -0.0456],
        [ 0.0180, -0.0019,  0.0003,  ..., -0.0357,  0.0232, -0.0267],
        [-0.0009, -0.0014,  0.0053,  ...,  0.0093, -0.0357, -0.0156]])

llm.base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0159,  0.0167,  0.0204,  ..., -0.0239, -0.0037, -0.0063],
        [-0.0234, -0.0210, -0.0030,  ..., -0.0057,  0.0063,  0.0079],
        [ 0.0042, -0.0139, -0.0007,  ...,  0.0143, -0.0256,  0.0239],
        ...,
        [-0.0041,  0.0017, -0.0179,  ...,  0.0002,  0.0244,  0.0211],
        [-0.0003, -0.0112, -0.0103,  ...,  0.0022,  0.0081, -0.0134],
        [ 0.0095, -0.0144, -0.0092,  ..., -0.0143,  0.0120, -0.0009]])

llm.base_model.model.model.layers.16.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-2.7222e-02,  4.0527e-02,  1.2634e-02,  2.5269e-02, -3.0396e-02,
         2.4902e-02,  2.0996e-02,  7.8125e-02, -1.4038e-02, -1.9043e-02,
         2.7222e-02, -2.8687e-02,  4.1992e-02, -1.4160e-02,  2.3346e-03,
         6.1035e-02, -2.1118e-02, -8.7402e-02, -3.7109e-02,  4.5654e-02,
        -3.0273e-02,  9.2773e-03, -8.9722e-03,  5.2734e-02,  3.0365e-03,
        -1.1597e-02, -2.2095e-02, -6.7871e-02,  3.2471e-02,  5.6152e-02,
         2.3193e-02,  7.2266e-02, -1.4282e-02,  7.1106e-03,  3.1494e-02,
        -1.3428e-02,  2.0264e-02,  1.1169e-02, -1.9531e-02,  8.0109e-05,
         8.3618e-03,  3.3691e-02,  4.1748e-02, -2.2095e-02,  2.5146e-02,
         2.0599e-03, -5.5237e-03, -4.5471e-03,  4.2725e-02, -1.3867e-01,
        -9.2285e-02, -4.7607e-03,  4.7852e-02,  2.7954e-02,  1.6724e-02,
        -4.7852e-02,  1.6357e-02,  2.0264e-02, -2.8381e-03,  1.1963e-02,
        -2.7618e-03, -1.8311e-02, -2.5024e-03, -7.1777e-02, -2.9907e-02,
        -3.2715e-02, -1.5869e-02, -1.9775e-02,  4.9072e-02, -4.0588e-03,
        -6.8970e-03, -2.3651e-03, -2.2827e-02,  1.3062e-02,  1.6357e-02,
         6.8054e-03, -6.9336e-02,  7.3242e-02, -4.0771e-02,  8.3618e-03,
        -1.6235e-02,  1.9989e-03, -2.6703e-03, -3.6865e-02,  2.5024e-02,
         4.2236e-02,  1.6724e-02, -2.4292e-02,  2.6367e-02,  9.6436e-03,
        -8.0566e-03, -1.4893e-02,  6.9046e-04,  2.7588e-02,  3.5352e-01,
        -5.8594e-02, -6.5430e-02,  8.4473e-02, -1.0986e-02,  2.1606e-02,
        -1.0986e-02,  4.1748e-02, -2.7832e-02, -2.2461e-02, -3.8330e-02,
        -2.9297e-02,  1.9775e-02,  8.4961e-02,  1.0498e-02, -7.1716e-03,
        -4.2419e-03, -7.5195e-02, -1.3428e-03, -2.7161e-03, -2.4048e-02,
         1.2012e-01,  2.0386e-02, -5.4321e-03,  2.5391e-02, -2.3193e-02,
         1.4038e-02, -5.8838e-02, -4.0283e-02,  2.9175e-02,  8.1177e-03,
         2.3682e-02,  7.9346e-03,  2.7222e-02, -1.0132e-02,  7.0801e-03,
        -2.3560e-02,  3.0823e-03,  1.7578e-02, -4.0588e-03,  1.7700e-02,
        -4.4922e-02,  1.7944e-02, -3.6133e-02,  8.5449e-03,  5.7861e-02,
        -3.2959e-02,  5.2002e-02, -2.3804e-02,  7.9590e-02,  1.4954e-02,
         4.9316e-02,  3.4912e-02,  1.1292e-02,  3.3447e-02,  4.3457e-02,
        -1.9897e-02, -6.5002e-03, -5.0537e-02,  6.1035e-03,  2.2363e-01,
        -1.7334e-02,  2.7161e-03, -4.2114e-03,  1.4160e-02,  1.3611e-02,
        -4.1748e-02,  7.5378e-03,  1.8463e-03,  8.4473e-02, -3.2471e-02,
         1.2146e-02, -3.7598e-02,  1.1719e-02,  5.4626e-03, -3.5889e-02,
        -8.3984e-02, -1.3000e-02, -1.3367e-02,  2.2949e-02,  3.6621e-02,
        -3.2715e-02, -4.6631e-02,  4.5410e-02,  2.4109e-03,  7.1411e-03,
         1.0681e-02, -2.1973e-02,  3.0640e-02, -2.9297e-02,  9.0332e-03,
        -4.2969e-02,  1.1353e-02,  2.4986e-04, -2.2266e-01, -3.9062e-02,
        -6.3782e-03,  1.9531e-02, -1.7212e-02,  8.1787e-03, -1.1658e-02,
         2.1362e-03,  5.4169e-04,  9.7656e-03, -6.3171e-03, -1.7738e-04,
         6.6895e-02,  2.5024e-02,  5.0293e-02,  5.1025e-02,  5.2490e-03,
         1.2436e-03, -3.0518e-02, -7.2021e-03,  2.4780e-02,  1.0620e-02,
        -9.3994e-03,  6.8665e-03, -1.2878e-02, -4.9805e-02, -1.9455e-03,
         1.1841e-02,  3.1250e-02,  1.9043e-02, -1.7578e-02,  5.4626e-03,
         3.2196e-03,  7.5378e-03,  9.7656e-04, -4.9438e-03,  7.8125e-03,
         4.7302e-03,  1.3855e-02,  7.7515e-03,  2.2217e-02, -2.8931e-02,
        -2.0264e-02, -9.9487e-03, -2.9297e-01,  2.1851e-02, -2.0264e-02,
        -1.9165e-02, -3.6377e-02, -8.6060e-03,  2.2705e-02,  2.1973e-02,
         2.2217e-02,  3.6133e-02, -1.5747e-02, -2.0599e-03, -6.3965e-02,
         1.0376e-02,  1.7822e-02,  1.3306e-02, -1.7822e-02, -5.5542e-03,
         2.8320e-02,  1.3245e-02,  1.8799e-02, -1.0010e-02,  3.0640e-02,
         1.9897e-02, -4.1809e-03,  3.2471e-02,  8.5449e-03,  4.3945e-03,
        -3.0029e-02,  5.7983e-03,  4.4250e-03,  1.4496e-04, -2.3071e-02,
         6.9885e-03,  1.5564e-02,  9.0332e-03, -1.0132e-02, -1.6937e-03,
        -1.6357e-02, -2.1851e-02,  2.3804e-03,  3.3691e-02, -1.1063e-03,
        -7.1106e-03, -1.2573e-02,  1.1826e-03,  1.0376e-02,  3.0396e-02,
        -2.3315e-02, -4.1199e-03, -1.3123e-02, -6.2500e-02, -1.0437e-02,
        -1.4526e-02, -4.6387e-03, -1.0498e-02, -2.2461e-01,  2.7100e-02,
         2.4292e-02, -3.6011e-03,  1.6785e-03,  1.5442e-02, -1.2939e-02,
         2.6367e-02, -1.8799e-02,  6.9580e-03,  2.0508e-02, -3.2349e-03,
         2.4780e-02, -9.3384e-03, -1.2634e-02,  8.1787e-03,  1.4648e-02,
         2.7954e-02,  2.5513e-02,  5.9204e-03,  5.2185e-03,  9.3994e-03,
         2.6855e-02, -8.3618e-03,  2.7924e-03,  1.4954e-02, -2.2736e-03,
        -6.5002e-03,  4.7913e-03, -1.3550e-02, -3.1006e-02, -8.3008e-03,
        -2.1210e-03,  2.8564e-02,  2.7008e-03,  9.7046e-03,  5.1880e-03,
         4.6997e-03, -1.5442e-02, -2.4536e-02, -2.4536e-02, -2.0752e-02,
         2.5482e-03, -2.6978e-02, -5.7068e-03, -1.8921e-03, -3.5248e-03,
         9.1553e-04,  2.8076e-02,  6.9885e-03,  1.7578e-02, -1.3489e-02,
         1.4587e-02,  1.3245e-02, -7.2937e-03,  5.2490e-03, -2.1515e-03,
         4.2114e-03,  7.0312e-02, -1.8677e-02, -1.7212e-02, -3.4912e-02,
        -4.2480e-02,  2.4414e-03, -5.4321e-03,  2.0630e-02,  1.0132e-02,
        -3.2654e-03,  3.1128e-02, -2.0752e-03, -4.7913e-03,  1.6602e-02,
        -1.0559e-02,  3.0823e-03, -1.3611e-02,  1.7969e-01,  1.7456e-02,
         7.4158e-03, -3.4714e-04, -2.0142e-02, -2.2949e-02, -6.0730e-03,
        -1.5335e-03, -7.2754e-02,  2.3315e-02, -8.4229e-03, -3.4180e-02,
        -2.1606e-02, -2.1210e-03, -1.5076e-02, -1.1536e-02, -2.8687e-03,
         1.0559e-02,  1.8433e-02, -3.7384e-03, -1.2329e-02, -2.3193e-02,
        -1.0059e-01, -1.3245e-02,  8.9355e-02, -9.7656e-03, -2.3560e-02,
         5.5664e-02, -7.4707e-02,  5.7617e-02, -6.3477e-02, -1.4465e-02,
        -5.8594e-02,  1.3367e-02, -1.6861e-03, -6.9275e-03,  3.4668e-02,
         5.4626e-03, -4.6387e-02,  8.3984e-02, -1.0303e-01,  3.6133e-02,
         4.6997e-03,  2.7847e-04, -8.2031e-02,  7.5684e-02,  2.6245e-02,
         1.5918e-01, -4.2969e-02, -6.2012e-02,  6.2012e-02,  8.3984e-02,
        -6.1951e-03, -2.6855e-02, -2.7008e-03,  5.2002e-02, -3.6865e-02,
        -1.9836e-03,  6.9809e-04, -8.3984e-02, -4.4250e-03,  1.7334e-02,
        -2.7100e-02,  2.7618e-03, -2.1362e-02, -9.4727e-02, -2.1387e-01,
         4.3701e-02,  9.7656e-04, -7.6660e-02, -4.5410e-02, -1.9836e-03,
        -3.2471e-02, -5.4932e-03,  1.1816e-01, -4.7363e-02,  4.2969e-02,
         4.5654e-02, -3.8574e-02, -1.1780e-02, -8.8867e-02,  1.5259e-02,
         3.3203e-02, -4.4632e-04,  2.9419e-02, -4.3701e-02, -1.0059e-01,
        -6.7383e-02, -4.3457e-02, -1.2756e-02, -4.2480e-02,  1.1536e-02,
        -1.6724e-02,  3.1494e-02,  4.9805e-02, -2.3956e-03,  1.1475e-02,
         2.3193e-02,  3.7842e-02,  4.4678e-02,  4.1809e-03, -1.6846e-02,
        -1.0193e-02,  1.1426e-01, -3.5400e-02, -4.1748e-02, -1.5564e-02,
         3.0029e-02,  2.9907e-02,  2.3193e-02,  9.4604e-03, -3.3936e-02,
        -1.8997e-03, -1.6602e-02, -2.2070e-01,  1.2634e-02, -3.3691e-02,
         3.0640e-02, -7.4219e-02, -8.2031e-02, -1.4832e-02,  1.2695e-02,
        -2.0142e-02,  9.8419e-04, -1.2817e-02, -1.2158e-01, -3.7598e-02,
        -9.4604e-03, -8.9844e-02, -3.7384e-03, -3.3203e-01,  1.3123e-02,
         1.0498e-02, -1.2512e-02,  1.3428e-02, -8.4686e-04, -5.2734e-02,
        -1.4526e-02, -7.6660e-02,  8.7738e-04,  8.6914e-02, -8.2520e-02,
        -1.1658e-02,  3.9062e-02,  2.6611e-02, -6.8359e-02, -8.7402e-02,
         2.7710e-02,  4.2480e-02])

llm.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0471,  0.0013,  0.0099,  ...,  0.0324,  0.0224,  0.0011],
        [-0.0651,  0.0279, -0.0025,  ..., -0.0206,  0.0063, -0.0067],
        [ 0.0375, -0.0200, -0.0112,  ..., -0.0344, -0.0076,  0.0122],
        ...,
        [ 0.0026, -0.0276, -0.0393,  ..., -0.0116,  0.0028, -0.0136],
        [-0.0351,  0.0030, -0.0178,  ..., -0.0468,  0.0209,  0.0501],
        [ 0.0189, -0.0237, -0.0097,  ..., -0.0938, -0.0212,  0.0210]])

llm.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 1.6038e-02, -2.3233e-02,  1.2508e-03,  ...,  3.8808e-05,
         -3.1294e-03, -2.0000e-02],
        [ 1.8194e-02,  4.2803e-02, -2.6669e-02,  ...,  1.0366e-02,
          1.6455e-02, -7.2894e-03],
        [ 1.4650e-03, -2.7318e-02, -1.7397e-03,  ..., -2.9073e-03,
         -1.5053e-02, -2.1774e-02],
        ...,
        [-3.1776e-02,  7.2969e-03,  1.7961e-02,  ...,  8.7262e-03,
         -2.4813e-02,  1.7535e-02],
        [-9.7440e-03, -8.8677e-03,  7.0581e-03,  ...,  1.3676e-02,
         -5.7845e-03,  3.0496e-02],
        [ 6.5274e-03,  1.4970e-02, -1.6985e-02,  ...,  7.2143e-03,
          9.9080e-03,  1.0408e-02]])

llm.base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 5.7068e-03,  1.1658e-02, -2.3804e-02,  ..., -3.4180e-02,
          2.4414e-02,  7.5340e-05],
        [-8.1177e-03,  4.6082e-03,  5.1575e-03,  ..., -1.6113e-02,
          2.0508e-02, -9.8877e-03],
        [-2.6093e-03,  1.3657e-03, -4.4556e-03,  ..., -1.9531e-02,
          3.3936e-02,  7.6294e-03],
        ...,
        [-7.7209e-03, -7.1106e-03,  1.2390e-02,  ..., -9.5825e-03,
         -6.3782e-03,  4.3030e-03],
        [ 1.7334e-02, -2.0630e-02, -2.6703e-04,  ..., -1.7822e-02,
          7.9346e-03, -1.3733e-02],
        [ 2.1362e-02,  2.2949e-02, -2.5635e-02,  ..., -3.8528e-04,
         -5.7983e-03, -1.7471e-03]])

llm.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0004,  0.0065,  0.0125,  ...,  0.0249, -0.0187,  0.0347],
        [ 0.0082,  0.0345, -0.0181,  ...,  0.0199, -0.0130,  0.0298],
        [-0.0464,  0.0101,  0.0177,  ..., -0.0062, -0.0670,  0.0305],
        ...,
        [-0.0344, -0.0463,  0.0039,  ...,  0.0544, -0.0299,  0.0319],
        [ 0.0107,  0.0180, -0.0319,  ...,  0.0034,  0.0429, -0.0331],
        [-0.0232,  0.0164,  0.0230,  ...,  0.0154,  0.0030, -0.0727]])

llm.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0332, -0.0273,  0.0285,  ..., -0.0111, -0.0168, -0.0236],
        [-0.0203,  0.0029, -0.0011,  ..., -0.0238,  0.0318,  0.0025],
        [ 0.0185,  0.0171, -0.0227,  ...,  0.0412,  0.0235,  0.0031],
        ...,
        [-0.0158, -0.0059, -0.0198,  ...,  0.0010, -0.0093, -0.0116],
        [-0.0123,  0.0454, -0.0093,  ..., -0.0061,  0.0076, -0.0412],
        [ 0.0243,  0.0121, -0.0106,  ...,  0.0078,  0.0059, -0.0128]])

llm.base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0157, -0.0109, -0.0135,  ..., -0.0216, -0.0124,  0.0056],
        [-0.0029, -0.0247, -0.0167,  ...,  0.0153, -0.0003, -0.0027],
        [-0.0089,  0.0221, -0.0070,  ...,  0.0025,  0.0017, -0.0014],
        ...,
        [-0.0100,  0.0102, -0.0182,  ..., -0.0025, -0.0344, -0.0214],
        [-0.0090,  0.0036, -0.0317,  ..., -0.0043,  0.0043, -0.0096],
        [-0.0113, -0.0067, -0.0179,  ..., -0.0087, -0.0381, -0.0188]])

llm.base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 2.6696e-02,  2.9052e-02,  8.3233e-02,  ...,  1.9069e-02,
          1.9795e-02, -3.5116e-02],
        [ 7.4588e-03, -5.5839e-03,  5.0094e-02,  ...,  1.4257e-02,
         -2.2943e-02, -2.7338e-02],
        [-4.4613e-02,  3.1707e-02,  1.2098e-02,  ...,  3.2919e-02,
          3.0940e-02, -7.5919e-02],
        ...,
        [ 5.0450e-05, -4.1978e-02, -1.9131e-02,  ..., -6.7161e-02,
          7.1360e-04, -3.3795e-02],
        [-3.6853e-02,  9.4154e-03,  5.5932e-03,  ...,  1.1053e-02,
          3.7442e-02, -2.9401e-02],
        [ 2.0373e-04, -6.1016e-02,  1.5782e-02,  ..., -2.7177e-03,
          2.1122e-02, -1.6757e-02]])

llm.base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0553,  0.0050, -0.0105,  ...,  0.0241,  0.0013, -0.0142],
        [ 0.0051, -0.0032,  0.0099,  ...,  0.0024,  0.0053,  0.0114],
        [ 0.0037, -0.0048, -0.0027,  ...,  0.0089,  0.0188, -0.0003],
        ...,
        [ 0.0021, -0.0145,  0.0037,  ..., -0.0004, -0.0038,  0.0280],
        [-0.0063,  0.0227,  0.0258,  ...,  0.0142,  0.0059, -0.0068],
        [ 0.0019,  0.0035,  0.0052,  ...,  0.0118,  0.0230,  0.0207]])

llm.base_model.model.model.layers.16.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0029, -0.0403,  0.0087,  ...,  0.0148, -0.0275, -0.0152],
        [ 0.0167, -0.0004, -0.0171,  ...,  0.0103,  0.0015,  0.0080],
        [ 0.0096, -0.0120,  0.0044,  ...,  0.0172,  0.0152,  0.0096],
        ...,
        [-0.0374,  0.0026,  0.0209,  ..., -0.0151,  0.0093,  0.0028],
        [-0.0091, -0.0039,  0.0014,  ...,  0.0036, -0.0014, -0.0091],
        [-0.0084,  0.0104, -0.0300,  ...,  0.0043, -0.0068,  0.0078]])

llm.base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0222, -0.0078,  0.0378,  ..., -0.0123,  0.0263,  0.0139],
        [-0.0476, -0.0331, -0.0326,  ...,  0.0407,  0.0127, -0.0138],
        [-0.0166,  0.0123, -0.0292,  ...,  0.0175,  0.0223, -0.0214],
        ...,
        [-0.0925,  0.0335, -0.0250,  ...,  0.0178,  0.0044, -0.0395],
        [ 0.0040,  0.0604,  0.0486,  ..., -0.0182,  0.0041, -0.0627],
        [-0.0108, -0.0040, -0.0516,  ...,  0.0523, -0.0075,  0.0178]])

llm.base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0142,  0.0057, -0.0008,  ..., -0.0329,  0.0144, -0.0259],
        [ 0.0326, -0.0181, -0.0075,  ..., -0.0260,  0.0224, -0.0154],
        [-0.0494, -0.0063, -0.0024,  ...,  0.0013,  0.0080,  0.0157],
        ...,
        [ 0.0042,  0.0072,  0.0079,  ...,  0.0069, -0.0004,  0.0120],
        [ 0.0246,  0.0127,  0.0162,  ...,  0.0185,  0.0122,  0.0002],
        [-0.0021,  0.0314, -0.0055,  ..., -0.0459,  0.0203, -0.0089]])

llm.base_model.model.model.layers.16.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[-0.0091,  0.0085, -0.0029,  ..., -0.0002,  0.0147, -0.0209],
        [-0.0166, -0.0096, -0.0067,  ..., -0.0007, -0.0124, -0.0068],
        [ 0.0060,  0.0064,  0.0117,  ...,  0.0146,  0.0010, -0.0209],
        ...,
        [-0.0179,  0.0167, -0.0007,  ..., -0.0352, -0.0113,  0.0101],
        [-0.0198, -0.0184,  0.0010,  ..., -0.0255,  0.0023, -0.0317],
        [-0.0078, -0.0010,  0.0056,  ...,  0.0033,  0.0147, -0.0120]])

llm.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0285, -0.0284, -0.0140,  ...,  0.0018, -0.0265,  0.0595],
        [ 0.0194,  0.0156,  0.0021,  ...,  0.0450, -0.0376,  0.0047],
        [-0.0103,  0.0194,  0.0175,  ..., -0.0019,  0.0138, -0.0420],
        ...,
        [ 0.0013,  0.0304, -0.0530,  ..., -0.0549, -0.0107,  0.0335],
        [ 0.0358, -0.0503, -0.0112,  ...,  0.0039,  0.0202,  0.0169],
        [-0.0524, -0.0075, -0.0216,  ..., -0.0109,  0.0193,  0.0225]])

llm.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0198,  0.0378, -0.0021,  ...,  0.0306, -0.0287, -0.0406],
        [ 0.0118, -0.0254,  0.0269,  ...,  0.0371,  0.0109,  0.0090],
        [-0.0084,  0.0176, -0.0279,  ..., -0.0445, -0.0386,  0.0306],
        ...,
        [-0.0077,  0.0372, -0.0094,  ...,  0.0200,  0.0124, -0.0204],
        [ 0.0148, -0.0024, -0.0384,  ...,  0.0050, -0.0056, -0.0025],
        [ 0.0385,  0.0484, -0.0287,  ...,  0.0395, -0.0227,  0.0254]])

llm.base_model.model.model.layers.16.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.7656, 0.8086, 2.3750,  ..., 0.3691, 1.2500, 1.4453])

llm.base_model.model.model.layers.16.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.2422, 1.0625, 1.9609,  ..., 0.5391, 1.3438, 1.3203])

llm.base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 4.9744e-03, -3.3569e-03, -1.8539e-03,  ...,  1.3977e-02,
          6.2180e-04,  9.7046e-03],
        [-6.0120e-03, -5.1880e-03, -7.9346e-03,  ..., -7.8125e-03,
          2.8839e-03, -6.9618e-05],
        [-1.1444e-03,  1.0376e-02, -6.3705e-04,  ...,  4.5776e-03,
          9.9945e-04, -2.7618e-03],
        ...,
        [-4.0527e-02, -3.7842e-03,  8.3008e-03,  ...,  4.6387e-03,
         -5.3406e-04, -2.5635e-03],
        [ 5.1270e-03,  1.2817e-02, -1.4771e-02,  ...,  1.0681e-02,
         -5.7983e-03,  1.7700e-02],
        [-3.7842e-03,  2.3346e-03, -3.4668e-02,  ..., -1.2085e-02,
          1.6113e-02, -1.4709e-02]])

llm.base_model.model.model.layers.17.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.2773, -0.4414,  0.2393,  ..., -0.1147, -0.0299,  0.4023])

llm.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0299,  0.0379,  0.0425,  ..., -0.0106, -0.0008,  0.0310],
        [ 0.0534, -0.0012, -0.0259,  ..., -0.0446, -0.0245,  0.0240],
        [ 0.0422,  0.0066,  0.0196,  ...,  0.0371, -0.0854,  0.0103],
        ...,
        [ 0.0614,  0.0156, -0.0482,  ..., -0.0293, -0.0004,  0.0518],
        [-0.0143,  0.0570, -0.0423,  ...,  0.0408, -0.0008,  0.0145],
        [ 0.0157,  0.0108, -0.1026,  ...,  0.0072,  0.0047,  0.0138]])

llm.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0055, -0.0455, -0.0128,  ..., -0.0444,  0.0129, -0.0129],
        [ 0.0023,  0.0127, -0.0363,  ..., -0.0327, -0.0089, -0.0501],
        [-0.0283, -0.0067, -0.0212,  ..., -0.0283, -0.0222,  0.0470],
        ...,
        [ 0.0108, -0.0064,  0.0041,  ...,  0.0024, -0.0229, -0.0003],
        [-0.0056,  0.0001, -0.0238,  ..., -0.0171,  0.0246, -0.0134],
        [-0.0153, -0.0074,  0.0095,  ...,  0.0179, -0.0111, -0.0007]])

llm.base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0015,  0.0030,  0.0125,  ..., -0.0065,  0.0175,  0.0101],
        [-0.0052, -0.0095,  0.0046,  ..., -0.0003,  0.0011,  0.0049],
        [-0.0086, -0.0159, -0.0009,  ..., -0.0085,  0.0261, -0.0028],
        ...,
        [ 0.0032, -0.0184, -0.0131,  ...,  0.0138,  0.0096,  0.0192],
        [-0.0114,  0.0201,  0.0013,  ..., -0.0135, -0.0157, -0.0026],
        [-0.0206,  0.0037, -0.0137,  ..., -0.0214, -0.0076, -0.0112]])

llm.base_model.model.model.layers.17.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-1.6250e+00,  3.5938e-01, -7.3828e-01, -2.7930e-01, -1.3828e+00,
        -9.3994e-03,  2.6562e-01, -1.2500e+00,  3.3936e-02,  3.1445e-01,
        -5.8838e-02,  9.2163e-03, -2.1777e-01, -2.4048e-02, -1.6968e-02,
        -9.9121e-02,  2.2168e-01, -5.4932e-02,  1.9766e+00,  1.2500e-01,
         9.3750e-01, -2.0020e-01,  3.6621e-02,  2.7734e-01,  2.2500e+00,
        -1.8750e-01, -5.2002e-02,  2.9297e-01, -4.5654e-02, -2.2812e+00,
         1.8066e-01, -2.2266e-01, -6.5613e-03, -4.8584e-02, -1.4062e-01,
        -6.4453e-02,  6.6895e-02, -2.1875e-01, -6.2256e-02,  8.3008e-03,
         1.1902e-02,  7.3730e-02,  4.3457e-02, -7.0312e-02, -7.0312e-02,
        -6.3965e-02, -9.4238e-02,  8.6426e-02,  3.6094e+00,  1.2793e-01,
        -3.7598e-02, -1.2988e-01,  1.1768e-01,  5.2002e-02,  4.4434e-02,
        -4.0234e-01, -2.2656e-01, -2.8125e-01,  2.7148e-01, -1.8516e+00,
        -7.0703e-01,  2.2031e+00,  1.6719e+00,  1.8984e+00, -3.4180e-01,
         3.1836e-01,  8.7500e-01,  3.3594e-01, -3.2031e-01,  2.6172e-01,
         1.2158e-01,  1.9409e-02,  2.2461e-01, -1.3672e+00,  2.9688e-01,
        -8.3984e-02,  1.5234e+00,  1.7834e-04, -1.9238e-01,  1.6094e+00,
        -9.5844e-05, -2.3145e-01,  2.0312e-01,  1.0352e-01,  1.8457e-01,
         4.1992e-02, -5.0781e-02,  1.8945e-01,  6.6406e-02, -9.0820e-02,
         1.0742e-01,  3.6865e-02, -1.2793e-01, -1.8799e-02, -3.3936e-02,
        -7.9590e-02, -1.0147e-03, -1.2695e-01,  6.2988e-02, -6.7383e-02,
         2.9785e-02,  2.3125e+00, -5.0537e-02, -2.7313e-03, -5.6152e-02,
         9.9487e-03, -2.0874e-02,  1.3000e-02,  5.4199e-02, -4.9805e-02,
        -5.8838e-02, -4.6875e-02, -3.6328e-01,  1.2085e-02, -1.0193e-02,
         1.4355e-01, -2.6953e-01,  9.0332e-02,  1.9043e-01, -1.6113e-01,
        -1.2938e+01, -9.2285e-02, -2.6172e-01, -2.3145e-01, -4.5312e-01,
        -5.0354e-03, -5.8594e-01,  3.2031e-01,  9.2969e-01,  5.9326e-02,
        -7.0312e-01,  2.6001e-02, -8.9453e-01,  6.9824e-02, -6.3281e-01,
         3.7109e-01, -4.2188e-01,  1.4258e-01,  1.3516e+00, -3.0664e-01,
         1.1536e-02,  1.0781e+00,  1.6016e-01, -9.7266e-01, -1.1719e+00,
        -1.6797e-01,  1.4062e-01, -5.7031e-01, -2.5586e-01, -2.4512e-01,
        -1.3438e+00,  4.2969e-01, -2.8125e-01,  7.2266e-01,  1.8984e+00,
        -2.4414e-01,  1.9141e-01, -3.8281e-01,  6.9824e-02,  2.3594e+00,
        -1.6895e-01,  1.0547e-01, -1.4160e-01,  1.2756e-02, -9.2578e-01,
         3.3936e-02,  1.5469e+00, -1.3770e-01,  4.3457e-02,  2.1094e+00,
         2.6953e-01,  5.6641e-01,  1.1230e-01, -5.4932e-02, -1.2266e+00,
         8.8672e-01,  5.9814e-02,  6.9336e-02,  4.5625e+00,  3.4766e-01,
        -1.5820e-01,  2.1094e-01, -1.8945e-01,  3.6133e-01,  3.1445e-01,
         2.0752e-02, -9.2773e-02,  1.6708e-03,  1.6504e-01, -3.5312e+00,
         6.9922e-01, -1.9922e+00,  1.6211e-01,  2.4609e-01, -5.3125e-01,
        -6.7383e-02, -2.0117e-01,  1.5234e-01, -5.7812e-01,  1.1673e-03,
         6.2109e-01, -6.4087e-03, -8.1055e-02,  2.5513e-02,  1.6113e-01,
        -2.4219e-01, -1.0547e-01,  3.1641e-01,  2.6367e-01,  2.1875e-01,
         2.4023e-01, -1.4375e+00,  2.8125e-01, -1.0156e-01,  4.7266e-01,
        -1.7188e-01, -1.6504e-01,  4.2419e-03, -7.7344e-01, -4.4336e-01,
         5.1562e-01,  7.8125e-02, -2.9102e-01, -4.7070e-01,  2.2559e-01,
         8.3984e-02,  6.2988e-02, -5.3125e-01,  1.2891e+00, -2.3438e-01,
        -8.5449e-02,  1.2500e-01, -5.6641e-01, -3.0518e-02, -4.2383e-01,
        -2.3730e-01,  2.6953e-01, -1.3125e+00, -1.5312e+00, -4.1016e-02,
        -2.4121e-01,  2.9688e-01,  2.6367e-01, -1.2329e-02, -9.3750e-02,
        -4.0283e-02,  2.0605e-01,  4.4189e-02, -2.9102e-01, -9.7266e-01,
        -2.9102e-01, -4.0234e-01, -1.3574e-01,  2.5625e+00,  9.4141e-01,
        -2.4902e-01,  5.0781e-01, -4.3555e-01,  3.9648e-01,  3.7500e-01,
         4.4141e-01,  1.9043e-01,  1.4941e-01,  1.3281e-01, -4.1748e-02,
        -6.7969e-01,  1.8262e-01,  6.7871e-02,  3.5742e-01, -1.3281e-01,
         1.4648e-01, -1.1670e-01, -1.8555e-01, -2.3340e-01,  5.9814e-02,
        -9.1406e-01,  2.4707e-01,  3.3008e-01, -1.3379e-01, -5.7422e-01,
         2.7734e-01,  2.8125e-01, -1.4688e+00, -9.3359e-01, -1.3477e-01,
        -2.7734e-01, -2.6367e-01, -2.6094e+00,  5.1758e-02, -1.4141e+00,
         3.2715e-02,  8.6914e-02,  5.5859e-01, -2.0801e-01,  1.0254e-01,
        -1.4551e-01,  1.7383e-01, -2.6250e+00,  1.6602e-01,  5.2979e-02,
        -2.4414e-01,  3.5547e-01,  3.0312e+00, -8.6719e-01,  4.0820e-01,
         1.4941e-01,  3.7891e-01,  1.2500e-01,  2.9102e-01, -2.9883e-01,
         9.5215e-02,  3.1836e-01, -2.4023e-01, -3.2617e-01, -7.7637e-02,
        -2.5195e-01, -1.4766e+00, -9.4141e-01, -8.1055e-02,  2.3281e+00,
        -8.8379e-02, -3.0859e-01,  2.5391e-02, -6.2256e-02,  3.1055e-01,
         4.1992e-01,  2.2363e-01,  5.5469e-01,  7.6660e-02, -2.0801e-01,
         2.8906e-01, -5.5078e-01,  3.5645e-02,  4.0625e-01,  5.0000e-01,
         9.3750e-01, -7.3242e-02, -6.8750e-01,  1.4062e-01,  1.9824e-01,
        -1.2158e-01,  1.0547e+00,  7.2656e-01,  8.8379e-02, -3.8867e-01,
         1.3750e+00, -3.4668e-02, -1.0156e-01, -1.4258e-01,  4.4434e-02,
         3.0078e-01,  1.1670e-01, -1.6211e-01,  9.4727e-02,  1.5234e-01,
         6.6895e-02, -1.1670e-01, -1.1865e-01,  2.0625e+00,  1.4258e-01,
         2.4609e-01,  3.5156e-01, -4.5312e-01,  3.6328e-01, -3.3984e-01,
        -3.1641e-01,  1.0703e+00,  6.0059e-02, -2.3145e-01,  4.4727e-01,
        -3.4180e-02,  3.9062e-01, -4.3359e-01,  4.7852e-02,  7.3853e-03,
         2.5000e-01,  3.1641e-01,  3.5938e-01, -1.5918e-01,  1.4941e-01,
        -1.4844e+00, -1.2422e+00, -9.1016e-01,  7.5938e+00, -5.2734e-01,
         5.8984e-01, -1.7871e-01,  6.0547e-01,  2.5586e-01, -6.4941e-02,
         5.1562e-01,  3.8330e-02,  4.6289e-01, -5.1270e-03, -1.0742e-02,
         2.3560e-02,  3.0859e-01,  1.4648e-01,  1.2793e-01,  4.4922e-01,
        -8.2031e-01,  2.1191e-01, -1.2578e+00, -3.1055e-01,  7.5989e-03,
         2.3438e-01, -1.6406e+00,  1.7188e-01,  2.4805e-01, -1.5312e+00,
        -1.0803e-02, -1.5918e-01,  1.5859e+00, -6.1279e-02, -4.6143e-02,
        -2.4902e-01,  1.3086e-01, -5.0049e-02,  1.0071e-02, -7.6294e-03,
         8.7280e-03,  1.8359e-01,  3.0781e+00,  1.4258e-01,  3.0078e-01,
        -4.5117e-01,  4.4922e-01, -1.6992e-01,  2.7344e-01,  2.1289e-01,
         2.5469e+00, -9.0332e-02, -4.7363e-02, -3.5312e+00, -1.3574e-01,
        -7.7344e-01, -5.0391e-01,  4.0234e-01, -8.6719e-01,  5.4688e-01,
         3.9844e-01,  6.9141e-01,  6.4453e-01, -2.2500e+00, -4.9805e-01,
         1.2656e+00,  3.2500e+00, -6.3281e-01, -1.2500e-01, -9.9121e-02,
        -3.8086e-01, -2.8320e-01,  1.9141e-01, -1.1133e-01, -7.0312e-01,
        -1.2329e-02,  2.7344e-01,  3.8818e-02,  6.0156e-01,  2.0312e-01,
        -1.0156e+00,  2.9175e-02, -2.4219e-01, -1.2109e-01, -1.8555e-01,
         7.1777e-02,  5.6152e-02,  3.1836e-01,  3.0078e-01, -3.2422e-01,
         9.1797e-02, -9.1797e-02,  1.4062e-01,  2.0605e-01, -1.4709e-02,
        -2.3071e-02, -4.8438e-01,  1.5137e-01, -2.9053e-02, -2.6094e+00,
         6.1279e-02,  1.0681e-03, -1.9336e-01, -3.2227e-01,  1.6602e-01,
         2.2656e-01,  2.0801e-01, -6.4453e-02, -1.5320e-02,  1.7969e-01,
         3.1006e-02, -4.4141e-01, -4.9023e-01, -8.4473e-02,  3.7500e+00,
         2.5513e-02, -2.7344e-01,  1.2500e+00,  1.0469e+00, -4.2773e-01,
        -6.5430e-02,  6.0156e-01,  1.0469e+00, -1.2266e+00, -1.7773e-01,
         2.6953e-01, -3.3398e-01, -3.0273e-01,  5.7812e-01, -1.4609e+00,
         1.0469e+00,  1.7422e+00])

llm.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0256, -0.0124, -0.0081,  ...,  0.0623, -0.0560, -0.0345],
        [ 0.0250, -0.0130,  0.0113,  ..., -0.0125, -0.0302, -0.0274],
        [ 0.0191, -0.0291, -0.0098,  ..., -0.0208, -0.0129, -0.0446],
        ...,
        [ 0.0013,  0.0007,  0.0234,  ..., -0.0240,  0.0152,  0.0172],
        [-0.0084,  0.0059,  0.0220,  ...,  0.0432, -0.0292,  0.0037],
        [ 0.0189, -0.0491, -0.0058,  ...,  0.0600, -0.0365,  0.0197]])

llm.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0552, -0.0820,  0.0626,  ...,  0.0018, -0.0548, -0.0512],
        [ 0.0192,  0.0224, -0.0041,  ..., -0.0022,  0.0293,  0.0176],
        [ 0.0022,  0.0052,  0.0032,  ...,  0.0053, -0.0035, -0.0034],
        ...,
        [-0.0148,  0.0180, -0.0013,  ...,  0.0381,  0.0012, -0.0325],
        [-0.0038, -0.0101,  0.0024,  ..., -0.0195, -0.0167, -0.0036],
        [-0.0018, -0.0287, -0.0086,  ...,  0.0266,  0.0006, -0.0076]])

llm.base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 1.1536e-02, -7.0801e-03, -1.2634e-02,  ..., -3.6774e-03,
         -1.0986e-03,  8.4229e-03],
        [ 4.6387e-03, -8.9169e-05,  2.8687e-02,  ..., -1.2695e-02,
          2.5787e-03,  7.2327e-03],
        [ 1.6113e-02, -1.5991e-02, -1.6846e-02,  ...,  1.8921e-02,
          9.7656e-03,  4.7302e-03],
        ...,
        [-1.5198e-02,  8.4229e-03, -5.3101e-03,  ...,  6.7444e-03,
         -1.5137e-02, -1.5442e-02],
        [-2.2736e-03,  4.3030e-03,  5.7678e-03,  ...,  2.5330e-03,
         -1.6602e-02,  1.1963e-02],
        [-1.7212e-02, -1.8799e-02, -3.0518e-02,  ..., -2.1515e-03,
          7.4768e-03,  1.1292e-02]])

llm.base_model.model.model.layers.17.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 3.2959e-02,  6.5430e-02,  5.0293e-02, -6.4392e-03, -2.9541e-02,
         3.6865e-02, -1.7456e-02, -4.4189e-02,  6.6895e-02, -6.7383e-02,
         1.3379e-01, -6.0547e-02, -9.5703e-02, -2.9785e-02, -3.1128e-02,
        -1.2207e-02,  7.3242e-03,  3.0518e-02,  2.3438e-02, -1.7700e-02,
         4.2236e-02,  6.9336e-02,  6.5918e-02,  1.9165e-02, -4.4922e-02,
         4.3750e-01,  3.3691e-02, -5.8203e-01, -2.0996e-02, -4.8584e-02,
        -4.6143e-02, -6.3477e-02,  2.2705e-02, -1.3611e-02, -4.4922e-02,
         5.5420e-02,  9.2773e-02, -4.2725e-03,  5.2734e-02,  2.1240e-02,
         1.0693e-01,  1.3306e-02,  7.7148e-02, -8.0872e-04, -4.1260e-02,
         7.6599e-03,  1.7212e-02,  8.4961e-02,  3.3447e-02, -7.2861e-04,
        -6.2561e-03, -8.5449e-02,  4.5654e-02,  4.0527e-02,  1.2500e-01,
         8.0566e-02,  2.3560e-02,  8.3984e-02,  3.7354e-02, -3.0273e-02,
         3.5645e-02,  2.7100e-02,  4.7607e-03, -3.0396e-02,  4.7607e-02,
        -4.4922e-02,  5.6641e-02,  3.0884e-02,  1.1475e-02,  3.4180e-03,
        -4.3457e-02,  2.9175e-02, -6.6895e-02, -1.4267e-03, -3.2043e-04,
         5.8594e-03,  1.9775e-02, -3.0884e-02,  3.8086e-02, -1.5488e-03,
        -8.7402e-02, -8.7280e-03, -8.3008e-02, -1.0132e-02,  9.2285e-02,
         5.8838e-02,  1.6724e-02, -7.1777e-02, -1.3489e-02,  2.7222e-02,
         3.2715e-02,  1.4160e-02,  1.6113e-02,  5.0781e-02, -7.8125e-02,
        -5.0049e-03, -7.3242e-02,  5.1880e-03,  5.5664e-02,  9.0820e-02,
        -1.4258e-01, -1.5430e-01,  4.2969e-02,  1.9226e-03,  6.4453e-02,
         7.7637e-02, -3.6377e-02,  1.1719e-02, -2.5330e-03, -7.3730e-02,
         6.5918e-02, -6.6895e-02,  2.4261e-03,  7.8613e-02, -7.1289e-02,
        -6.1035e-02, -2.6245e-02,  2.9785e-02, -1.2939e-02,  2.9419e-02,
         3.8818e-02, -9.8633e-02,  7.2266e-02,  2.2827e-02,  3.0640e-02,
         2.9907e-02,  5.9326e-02, -7.7148e-02, -4.7852e-02,  1.9409e-02,
         1.8677e-02,  2.5757e-02,  3.7354e-02,  7.3242e-02, -7.6599e-03,
        -1.2268e-02, -2.7100e-02,  4.1992e-02,  3.7079e-03, -1.0620e-02,
        -9.2773e-03,  2.6733e-02,  7.9956e-03, -5.0659e-03,  8.1787e-03,
         3.5553e-03, -1.3245e-02, -2.7084e-04,  8.1177e-03, -2.8564e-02,
        -4.0527e-02, -9.5215e-03,  1.2146e-02, -2.8442e-02,  3.7354e-02,
        -5.9204e-03,  2.0752e-02, -1.3000e-02, -2.2583e-02,  4.4556e-03,
        -1.6113e-02, -1.2634e-02, -7.5073e-03,  1.4404e-02, -2.3560e-02,
        -1.0681e-02, -7.2632e-03, -3.0151e-02, -4.3945e-02, -2.1362e-02,
        -2.0142e-02,  8.1787e-03,  9.9487e-03,  1.0300e-03, -9.4986e-04,
         1.6357e-02, -6.9275e-03, -6.8848e-02, -1.3184e-02,  9.9487e-03,
         4.8096e-02,  7.6294e-04,  1.8921e-02, -1.1902e-02, -5.7129e-02,
        -1.5869e-02, -3.7598e-02,  1.9775e-02,  1.7578e-02,  1.1963e-02,
        -2.8931e-02, -2.7832e-02,  5.9082e-02, -4.3457e-02, -7.0801e-03,
        -1.0193e-02,  3.1250e-02, -4.4678e-02,  2.8076e-02, -4.5166e-02,
         5.4321e-03, -4.6082e-03,  5.5420e-02, -5.2734e-02,  4.9316e-02,
         2.7222e-02, -1.7456e-02,  1.8677e-02,  4.9133e-03,  1.0559e-02,
         4.0283e-03, -8.5449e-03, -5.1575e-03, -1.8311e-02, -3.1445e-01,
         1.6309e-01, -1.5137e-02,  1.9531e-02,  1.0559e-02,  2.4902e-02,
         6.0120e-03, -4.2725e-03,  1.6479e-02, -2.6123e-02,  1.9165e-02,
        -3.9978e-03,  1.3733e-02, -2.0142e-02, -1.7090e-02, -7.9956e-03,
        -1.8799e-02, -1.4465e-02, -8.9111e-03, -2.5757e-02, -3.3691e-02,
         1.8066e-02, -4.6875e-02,  1.9653e-02,  2.7832e-02,  1.8677e-02,
        -2.4414e-02, -1.6602e-02,  8.6060e-03,  9.5825e-03, -3.5645e-02,
         2.0508e-02,  4.2725e-03,  3.8147e-03,  7.4219e-02,  2.3315e-02,
         8.6670e-03,  1.8677e-02, -6.1035e-02, -2.6123e-02,  4.4189e-02,
        -5.2734e-02,  8.2397e-03, -1.9165e-02, -1.9531e-02,  3.8574e-02,
        -3.5400e-02, -5.0293e-02,  1.5381e-02, -2.4170e-02, -1.1368e-03,
         8.4961e-02, -1.1230e-02, -6.3965e-02, -4.9072e-02, -7.0312e-02,
        -2.6367e-02,  2.6733e-02,  2.0752e-02,  1.4038e-02, -9.6436e-03,
        -4.2725e-02,  4.4922e-02,  5.7373e-03, -1.5076e-02,  4.2480e-02,
         2.1484e-02, -2.1484e-02, -2.3682e-02, -7.5073e-03, -1.7456e-02,
         6.8359e-03, -8.7402e-02, -6.0547e-02, -3.7231e-03,  1.2878e-02,
         1.4709e-02, -1.3489e-02,  1.7334e-02, -7.5684e-03,  8.9111e-03,
         5.9509e-03,  2.0508e-02,  1.0742e-02, -8.2520e-02, -5.0781e-02,
        -1.0315e-02,  2.3926e-02, -5.3406e-04,  2.7344e-02, -1.8555e-02,
        -4.4434e-02,  1.2741e-03, -4.1504e-02, -3.6865e-02, -2.8076e-02,
        -2.3651e-03,  4.0039e-02, -2.9053e-02,  1.6785e-04,  1.2878e-02,
         1.8845e-03,  5.1117e-04,  3.6316e-03, -3.5156e-02,  3.3936e-02,
        -8.1787e-03,  1.0742e-02,  1.3916e-02,  1.4832e-02, -3.8818e-02,
        -1.5137e-02,  5.6396e-02, -2.2827e-02,  9.3994e-03,  2.1362e-02,
        -4.2480e-02, -2.9755e-03, -1.7700e-02, -3.2227e-02, -1.1719e-02,
         1.3977e-02,  1.7456e-02,  2.3956e-03,  2.8198e-02, -1.6235e-02,
         3.4668e-02,  1.0205e-01,  2.6367e-02, -2.1606e-02, -2.7466e-02,
         1.2283e-03,  4.8340e-02, -8.3496e-02,  2.8687e-03,  4.0039e-02,
        -3.1494e-02,  1.6016e-01, -1.1768e-01,  2.2363e-01,  4.7119e-02,
         1.4343e-02,  1.4221e-02,  1.2256e-01,  1.5640e-03, -5.4626e-03,
        -1.0864e-02,  4.5776e-03,  2.7344e-02, -1.2573e-02,  1.0193e-02,
        -7.2266e-02, -1.6479e-02,  1.2756e-02, -1.8311e-02,  4.6387e-03,
        -1.7944e-02,  3.9551e-02, -1.2329e-02, -1.2085e-02, -5.0537e-02,
        -1.2146e-02,  1.8311e-02,  9.2163e-03,  1.4465e-02,  3.4424e-02,
         2.9541e-02,  6.1719e-01,  4.2969e-02,  3.6133e-02,  5.3711e-03,
        -7.0801e-03, -3.9307e-02, -2.6611e-02, -1.7929e-03,  6.6406e-02,
        -3.4180e-02, -4.4060e-04, -2.3682e-02,  4.5166e-03,  4.1992e-02,
        -6.7139e-03, -2.1484e-02,  3.1738e-02, -1.2360e-03,  2.2583e-02,
         2.0874e-02, -3.2471e-02, -3.9368e-03, -4.5166e-03, -2.1667e-03,
         2.8931e-02,  6.7139e-03,  1.4465e-02,  1.6235e-02, -7.4158e-03,
        -2.3926e-02,  2.4292e-02,  1.1047e-02, -3.0396e-02, -4.3945e-03,
         4.5776e-03,  1.1169e-02,  1.1719e-02, -1.7929e-03,  3.3447e-02,
         1.1108e-02, -1.3916e-02,  1.0681e-02,  3.2959e-02, -2.6245e-03,
        -5.9204e-03,  2.4512e-01,  1.9653e-02,  2.8931e-02, -2.5024e-02,
         3.3875e-03, -2.4902e-01,  3.1494e-02, -9.0942e-03,  1.0010e-02,
        -6.3782e-03, -6.6528e-03, -2.3804e-02, -8.1055e-02,  1.5137e-02,
         2.4261e-03, -1.0315e-02,  1.6479e-02,  2.6123e-02,  3.0151e-02,
         9.1553e-04,  4.3030e-03, -1.2695e-01,  8.2397e-03, -8.4839e-03,
         2.3560e-02, -2.2217e-02,  1.2024e-02, -2.6367e-02, -3.1738e-02,
         1.6968e-02,  6.9824e-02, -3.1891e-03, -1.3733e-03,  6.5308e-03,
        -2.8320e-02,  1.8082e-03,  2.2705e-02, -3.2959e-02, -1.8555e-02,
         2.0996e-02, -1.9455e-03, -4.7363e-02,  1.8188e-02,  8.1787e-03,
        -2.0410e-01, -1.2360e-03,  1.7822e-02,  2.6855e-02, -2.1240e-02,
        -8.9844e-02, -1.3306e-02, -1.2054e-03, -1.7822e-02,  3.4180e-02,
        -2.4414e-02, -1.0376e-02, -3.4912e-02,  2.4261e-03,  1.2268e-02,
         1.0437e-02, -1.8921e-02,  3.7598e-02, -1.0840e-01, -1.6113e-02,
        -3.9368e-03,  2.0264e-02,  1.9287e-02,  9.0332e-03,  2.3071e-02,
        -1.3306e-02,  3.6377e-02, -3.2043e-04,  4.7302e-03, -2.3651e-03,
        -7.5195e-02,  4.2969e-02, -1.1047e-02,  1.2451e-02,  1.1292e-02,
         1.0986e-02, -1.0437e-02,  4.6875e-02,  1.6602e-02,  3.0396e-02,
         6.7749e-03, -5.6839e-04])

llm.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0268, -0.0305, -0.0369,  ...,  0.0148, -0.0301,  0.0371],
        [-0.0175,  0.0123, -0.0031,  ..., -0.0394,  0.0363,  0.0019],
        [ 0.0007, -0.0250, -0.0169,  ..., -0.0282, -0.0183, -0.0136],
        ...,
        [ 0.0306, -0.0471,  0.0201,  ..., -0.0563,  0.0033,  0.0529],
        [-0.0051, -0.0301, -0.0048,  ..., -0.0163,  0.0153,  0.0088],
        [ 0.0185, -0.0539,  0.0005,  ..., -0.0295,  0.0110,  0.0519]])

llm.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 4.8412e-02,  1.6572e-02,  3.9000e-02,  ...,  7.9340e-04,
          3.0467e-02, -3.3711e-03],
        [ 3.3756e-02,  2.1479e-02,  3.0357e-02,  ..., -5.4142e-02,
         -2.3260e-02, -3.5430e-02],
        [ 1.6094e-05, -4.0862e-02, -8.8273e-03,  ..., -4.7789e-02,
          2.5962e-02, -9.0856e-03],
        ...,
        [-3.6390e-02,  1.6274e-02, -5.2925e-03,  ..., -4.7737e-02,
         -1.5483e-02, -3.0879e-03],
        [ 1.8969e-02, -5.3648e-02,  2.1483e-02,  ..., -3.7687e-02,
          2.5220e-02, -9.3839e-03],
        [-2.3347e-02,  1.1525e-02, -1.3850e-02,  ..., -1.8779e-02,
          9.0456e-03,  1.7773e-02]])

llm.base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0270,  0.0052,  0.0178,  ..., -0.0149, -0.0354,  0.0138],
        [ 0.0245, -0.0105,  0.0087,  ..., -0.0022,  0.0029,  0.0096],
        [-0.0088,  0.0031,  0.0063,  ...,  0.0018,  0.0100,  0.0342],
        ...,
        [ 0.0053,  0.0089,  0.0026,  ...,  0.0137,  0.0056,  0.0071],
        [-0.0086,  0.0039, -0.0013,  ..., -0.0069,  0.0161,  0.0001],
        [ 0.0084, -0.0058,  0.0062,  ...,  0.0010, -0.0066,  0.0410]])

llm.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0118, -0.0072,  0.0511,  ...,  0.0157, -0.0149, -0.0061],
        [ 0.0054, -0.0280, -0.0067,  ..., -0.0270, -0.0210,  0.0122],
        [ 0.0417,  0.0140,  0.0204,  ...,  0.0015,  0.0091,  0.0111],
        ...,
        [-0.0076,  0.0229,  0.0236,  ..., -0.0255,  0.0066,  0.0141],
        [-0.0008, -0.0170, -0.0068,  ...,  0.0229, -0.0332,  0.0010],
        [-0.0208,  0.0405,  0.0273,  ...,  0.0423, -0.0036,  0.0214]])

llm.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0130,  0.0368,  0.0212,  ...,  0.0083, -0.0150,  0.0241],
        [ 0.0252,  0.0308, -0.0017,  ...,  0.0275,  0.0309, -0.0271],
        [ 0.0044,  0.0070, -0.0563,  ...,  0.0022,  0.0135, -0.0374],
        ...,
        [ 0.0065,  0.0150, -0.0324,  ..., -0.0141, -0.0365,  0.0098],
        [-0.0073, -0.0175,  0.0350,  ..., -0.0030, -0.0288,  0.0424],
        [-0.0197,  0.0353, -0.0109,  ..., -0.0018,  0.0147,  0.0011]])

llm.base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0021,  0.0111,  0.0080,  ...,  0.0250,  0.0099,  0.0361],
        [ 0.0073, -0.0127,  0.0028,  ..., -0.0013,  0.0030,  0.0041],
        [ 0.0099,  0.0226, -0.0029,  ..., -0.0061, -0.0189, -0.0040],
        ...,
        [-0.0151, -0.0203,  0.0117,  ...,  0.0093,  0.0143,  0.0258],
        [-0.0205, -0.0129,  0.0044,  ..., -0.0010,  0.0018, -0.0055],
        [-0.0137,  0.0056,  0.0033,  ..., -0.0215,  0.0022, -0.0060]])

llm.base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0314, -0.0488,  0.0502,  ..., -0.0091,  0.0218,  0.0446],
        [-0.0275,  0.0310,  0.0364,  ...,  0.0241,  0.0215,  0.0107],
        [ 0.0399, -0.0518,  0.0290,  ...,  0.0031, -0.0063,  0.0339],
        ...,
        [-0.0088,  0.0641, -0.0457,  ...,  0.0571, -0.0290,  0.0103],
        [ 0.0346,  0.0188, -0.0639,  ...,  0.0274, -0.0752,  0.0481],
        [ 0.0332, -0.0187, -0.0722,  ..., -0.0122,  0.0924, -0.0351]])

llm.base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0205, -0.0291,  0.0262,  ...,  0.0238, -0.0182,  0.0197],
        [-0.0355, -0.0222,  0.0316,  ...,  0.0342, -0.0161,  0.0158],
        [-0.0002,  0.0049, -0.0198,  ...,  0.0106, -0.0227,  0.0061],
        ...,
        [ 0.0222,  0.0022, -0.0055,  ..., -0.0355, -0.0038, -0.0017],
        [ 0.0033,  0.0046,  0.0143,  ..., -0.0327,  0.0097, -0.0108],
        [ 0.0533, -0.0218, -0.0465,  ...,  0.0066,  0.0308,  0.0141]])

llm.base_model.model.model.layers.17.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0183,  0.0238,  0.0150,  ...,  0.0011,  0.0157,  0.0053],
        [-0.0085,  0.0403,  0.0232,  ..., -0.0040,  0.0125, -0.0364],
        [-0.0078, -0.0061,  0.0042,  ...,  0.0242,  0.0182, -0.0090],
        ...,
        [ 0.0119,  0.0056, -0.0009,  ...,  0.0115,  0.0154, -0.0186],
        [-0.0010,  0.0031, -0.0073,  ..., -0.0135,  0.0046, -0.0011],
        [ 0.0152,  0.0076,  0.0024,  ...,  0.0264, -0.0083, -0.0094]])

llm.base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0083, -0.0048,  0.0239,  ..., -0.0261,  0.0052,  0.0441],
        [-0.0202, -0.0309, -0.0442,  ...,  0.0813, -0.0303, -0.0604],
        [ 0.0181,  0.0043,  0.0317,  ...,  0.0153, -0.0224, -0.0482],
        ...,
        [ 0.0107,  0.0006, -0.0226,  ..., -0.0284, -0.0474, -0.0105],
        [-0.0218,  0.0080,  0.0701,  ..., -0.0552,  0.0109,  0.0035],
        [ 0.0516,  0.0127,  0.0819,  ..., -0.0675,  0.0177,  0.0065]])

llm.base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0038, -0.0155,  0.0012,  ...,  0.0532,  0.0159,  0.0318],
        [ 0.0087, -0.0111, -0.0558,  ..., -0.0408,  0.0236, -0.0108],
        [ 0.0205, -0.0037, -0.0028,  ..., -0.0099,  0.0153,  0.0327],
        ...,
        [-0.0027,  0.0408,  0.0073,  ..., -0.0202,  0.0229,  0.0311],
        [-0.0009, -0.0048, -0.0107,  ..., -0.0299, -0.0048,  0.0062],
        [-0.0010, -0.0341, -0.0013,  ...,  0.0211, -0.0540,  0.0275]])

llm.base_model.model.model.layers.17.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[-0.0251,  0.0047,  0.0022,  ...,  0.0134,  0.0047,  0.0261],
        [ 0.0245,  0.0190, -0.0177,  ...,  0.0141, -0.0102,  0.0132],
        [-0.0034,  0.0226, -0.0022,  ..., -0.0120, -0.0065, -0.0058],
        ...,
        [-0.0087,  0.0073, -0.0005,  ..., -0.0052,  0.0245,  0.0125],
        [ 0.0067,  0.0175, -0.0064,  ..., -0.0074,  0.0151, -0.0132],
        [ 0.0322, -0.0282, -0.0101,  ..., -0.0154,  0.0105, -0.0101]])

llm.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0106,  0.0054, -0.0072,  ...,  0.0067,  0.0118,  0.0126],
        [ 0.0076,  0.0403,  0.0134,  ..., -0.0232,  0.0198,  0.0021],
        [-0.0117,  0.0205, -0.0285,  ...,  0.0118, -0.0327,  0.0384],
        ...,
        [ 0.0375,  0.0046, -0.0104,  ...,  0.0187,  0.0207,  0.0181],
        [ 0.0484,  0.0185,  0.0012,  ...,  0.0051, -0.0386, -0.0081],
        [ 0.0201,  0.0071,  0.0223,  ...,  0.0043, -0.0429, -0.0240]])

llm.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0222, -0.0206,  0.0340,  ..., -0.0567,  0.0121,  0.0088],
        [ 0.0356, -0.0106,  0.0322,  ..., -0.0181, -0.0102, -0.0310],
        [ 0.0224, -0.0182,  0.0028,  ...,  0.0190,  0.0043, -0.0526],
        ...,
        [-0.0320, -0.0108,  0.0370,  ...,  0.0147,  0.0162, -0.0299],
        [-0.0029, -0.0560, -0.0240,  ..., -0.0219,  0.0373,  0.0128],
        [-0.0426,  0.0084, -0.0635,  ...,  0.0231, -0.0031, -0.0044]])

llm.base_model.model.model.layers.17.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.8438, 0.8398, 2.9219,  ..., 0.4199, 1.4453, 1.4844])

llm.base_model.model.model.layers.17.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.2422, 1.0703, 2.2188,  ..., 0.6016, 1.3906, 1.3594])

llm.base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0201,  0.0008, -0.0147,  ...,  0.0070, -0.0188, -0.0115],
        [-0.0028,  0.0280, -0.0035,  ..., -0.0005,  0.0137, -0.0110],
        [-0.0236, -0.0049,  0.0092,  ..., -0.0139,  0.0096,  0.0042],
        ...,
        [ 0.0154,  0.0065,  0.0006,  ..., -0.0118,  0.0129,  0.0048],
        [ 0.0042, -0.0206, -0.0182,  ..., -0.0004, -0.0330,  0.0052],
        [ 0.0181,  0.0014,  0.0033,  ...,  0.0053, -0.0061,  0.0018]])

llm.base_model.model.model.layers.18.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 1.7734, -1.0391,  0.9688,  ..., -1.0312,  0.6914,  0.1758])

llm.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0248, -0.0114, -0.0091,  ..., -0.0252,  0.0303, -0.0398],
        [-0.0318, -0.0279,  0.0438,  ...,  0.0440,  0.0241, -0.0113],
        [-0.0210,  0.0217,  0.0406,  ...,  0.0841,  0.0427, -0.0247],
        ...,
        [ 0.0147,  0.0711, -0.0132,  ...,  0.0218, -0.0144, -0.0320],
        [-0.0020, -0.0653, -0.0237,  ...,  0.0364, -0.0139, -0.0451],
        [ 0.0742, -0.0374,  0.0045,  ...,  0.0432, -0.0222, -0.0436]])

llm.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-2.7444e-02, -3.5985e-03, -2.6763e-02,  ..., -2.2985e-02,
         -2.6341e-02, -1.2152e-02],
        [-2.0702e-04, -5.1203e-03, -4.3624e-02,  ..., -1.5799e-02,
         -3.0866e-02,  2.4291e-02],
        [ 1.1744e-02,  3.4712e-02, -1.9054e-02,  ..., -2.5117e-02,
          2.7207e-02,  4.9666e-02],
        ...,
        [ 8.7701e-03, -1.1581e-03, -4.3846e-02,  ..., -1.9505e-02,
          2.0847e-05,  3.0942e-02],
        [ 2.0954e-02, -1.8860e-02,  2.1361e-02,  ...,  4.2804e-02,
          3.3388e-02, -8.7984e-04],
        [-5.8268e-02, -4.8752e-02, -2.8758e-02,  ...,  8.3276e-03,
          3.1474e-02,  4.7277e-02]])

llm.base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0120,  0.0036, -0.0133,  ..., -0.0103, -0.0082, -0.0159],
        [ 0.0067, -0.0010,  0.0066,  ...,  0.0171,  0.0201, -0.0084],
        [ 0.0069, -0.0051,  0.0208,  ..., -0.0076,  0.0065, -0.0026],
        ...,
        [ 0.0003, -0.0078,  0.0047,  ..., -0.0076,  0.0090, -0.0256],
        [-0.0037, -0.0295, -0.0092,  ..., -0.0143,  0.0050,  0.0264],
        [ 0.0091,  0.0265,  0.0005,  ...,  0.0035,  0.0167, -0.0143]])

llm.base_model.model.model.layers.18.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 1.1250e+00, -2.4023e-01,  2.9688e-01, -5.3906e-01,  8.5449e-02,
         2.1387e-01, -2.8906e-01, -3.7305e-01, -1.5723e-01,  1.2891e-01,
        -1.4219e+00,  6.2500e-02, -2.1851e-02, -2.2852e-01, -2.0156e+00,
         1.5820e-01,  2.4219e-01,  2.7148e-01, -1.2988e-01, -4.2969e-02,
         2.7344e+00, -1.0791e-01,  1.1523e-01, -2.0874e-02,  1.0645e-01,
         2.1582e-01,  2.5146e-02,  2.7031e+00,  6.3477e-02, -9.3384e-03,
         2.9883e-01,  8.8379e-02,  9.3384e-03,  6.3171e-03,  7.2266e-02,
        -2.3828e-01, -1.0596e-01,  2.7734e-01,  5.2734e-02, -1.5918e-01,
        -2.5269e-02, -3.0664e-01,  2.0996e-01,  2.1973e-02,  5.0391e-01,
         5.7861e-02, -2.5156e+00, -4.4189e-02,  1.2589e-03,  5.9570e-02,
         2.1250e+00,  7.6172e-02, -1.0645e-01,  1.3379e-01,  4.7266e-01,
        -3.9551e-02, -5.0391e-01,  6.0547e-01, -7.8516e-01,  4.9609e-01,
         9.8047e-01, -7.8516e-01, -1.8555e-01, -1.7812e+00,  9.5703e-01,
         1.0352e-01, -2.6562e-01,  1.2578e+00, -1.1230e-01,  1.6309e-01,
        -1.8433e-02,  1.3750e+00,  5.6641e-02, -2.3315e-02,  9.7656e-02,
        -1.4844e-01,  1.8164e-01, -3.0762e-02,  1.4062e-01,  2.8198e-02,
         1.7822e-02,  1.8984e+00,  1.2598e-01,  6.0547e-02, -2.0898e-01,
        -5.5176e-02, -8.6914e-02, -2.0312e-01,  8.6426e-02, -1.3770e-01,
        -1.4258e-01, -6.4844e-01, -4.3213e-02, -7.8613e-02,  3.1250e+00,
         1.1816e-01,  5.7031e-01,  6.8848e-02,  7.8125e-02,  3.1494e-02,
        -1.0352e-01,  1.3184e-01,  4.3555e-01, -2.9663e-02, -1.0352e-01,
        -2.3750e+00, -7.8613e-02,  1.2695e-01,  1.3086e-01, -7.1484e-01,
        -2.7500e+00, -3.0859e-01,  7.5195e-02, -4.0039e-01, -2.7969e+00,
         9.9121e-02, -6.5918e-02,  1.7871e-01, -6.3477e-02,  6.8359e-03,
         6.4844e-01,  2.0000e+00, -5.0781e-01, -1.3281e-01, -2.7031e+00,
        -1.2031e+00, -1.5703e+00, -1.5469e+00, -4.7461e-01,  1.5137e-01,
         5.7031e-01, -2.6758e-01, -1.8555e-01,  1.7969e-01, -3.1250e-01,
         1.6632e-03, -7.1777e-02,  8.0859e-01,  2.1191e-01,  7.8125e-02,
        -3.7598e-02, -1.6211e-01, -5.2979e-02,  1.5625e-02,  1.0693e-01,
         6.6016e-01,  6.6016e-01, -8.3496e-02,  2.3926e-01,  2.2339e-02,
         7.9297e-01,  6.2256e-02, -8.5547e-01, -2.7148e-01,  9.7168e-02,
        -1.5332e-01, -2.6406e+00,  5.5664e-02, -9.2773e-02,  2.5195e-01,
         1.9897e-02, -4.9072e-02,  1.4062e-01, -8.8867e-02,  3.0000e+00,
        -1.6113e-01,  1.6309e-01, -4.4141e-01,  1.4746e-01,  2.5195e-01,
         7.9297e-01,  1.5039e-01,  6.4453e-02, -3.5352e-01,  4.6250e+00,
        -2.1240e-02, -2.6172e-01, -4.2500e+00,  1.4453e-01,  1.3906e+00,
         1.1279e-01,  2.2070e-01, -3.1641e-01,  7.2266e-01,  1.3965e-01,
         2.1606e-02,  1.1953e+00,  1.0938e+00, -9.6875e-01, -3.0859e-01,
         1.0437e-02,  8.2422e-01,  7.7734e-01,  2.3071e-02, -6.8750e-01,
         8.3496e-02,  1.3867e-01, -6.6895e-02,  1.2109e+00, -8.4473e-02,
        -1.5039e-01, -6.1768e-02, -1.9141e-01, -3.4424e-02, -1.7734e+00,
         1.7969e-01, -2.4316e-01,  2.9883e-01,  1.9531e-01, -7.6660e-02,
         1.9688e+00,  9.7168e-02,  1.1963e-01, -3.0859e-01,  2.0938e+00,
        -5.2979e-02,  6.1798e-04, -1.2812e+00,  1.5918e-01,  3.1982e-02,
         1.2656e+00,  3.5858e-04, -9.8145e-02,  1.8438e+00,  2.0117e-01,
         2.1680e-01,  4.4189e-02,  6.8359e-02,  1.9844e+00,  2.8442e-02,
        -7.4609e-01, -3.7500e-01, -1.0986e-01,  3.3203e-01,  1.1172e+00,
         2.0508e-01,  9.2773e-03,  2.9297e-01, -5.0000e+00,  2.1875e-01,
        -1.9043e-01,  2.6250e+00, -8.4229e-03, -5.9375e-01, -5.1172e-01,
        -4.2383e-01,  1.4062e-01,  5.6885e-02, -5.5420e-02, -4.1602e-01,
        -8.2812e-01,  3.0156e+00, -4.9805e-01, -2.0938e+00,  2.9219e+00,
         2.2812e+00, -7.9688e-01,  2.9297e-01, -5.8594e-01,  6.4062e-01,
        -4.4141e-01,  7.6953e-01,  6.3281e-01, -4.9805e-01, -3.4375e-01,
         9.7266e-01,  2.1387e-01,  5.2734e-01, -3.2422e-01,  1.3359e+00,
         6.4453e-02, -5.5078e-01, -9.4531e-01, -4.8047e-01, -4.3750e-01,
         2.1289e-01, -2.8125e-01,  4.9219e-01, -8.3496e-02,  1.2500e+00,
         1.3672e-01, -8.4961e-02, -5.3516e-01,  5.1514e-02,  7.1777e-02,
         1.6094e+00, -4.7119e-02,  1.5859e+00, -5.7129e-02, -5.2979e-02,
         5.7373e-02,  1.8945e-01,  2.4902e-01,  2.3242e-01,  1.6113e-01,
        -1.7871e-01,  3.2617e-01,  3.2959e-02,  1.2500e-01, -1.3828e+00,
         8.7402e-02,  3.4961e-01, -2.6367e-02, -3.8867e-01,  1.0645e-01,
        -3.6875e+00,  1.1719e-01,  1.8066e-01, -4.1016e-01,  4.5166e-02,
         2.9883e-01, -2.3535e-01,  2.3340e-01, -1.5625e-01,  8.8672e-01,
         6.4062e-01, -3.2344e+00,  5.9375e-01, -3.1836e-01, -6.4375e+00,
        -1.0703e+00, -8.8281e-01, -1.6406e-01, -1.0156e+00, -1.8750e-01,
         4.9316e-02,  4.1211e-01, -4.1016e-01,  2.4805e-01, -4.3457e-02,
        -2.2656e-01, -9.3750e-02, -1.8750e-01,  3.0078e-01,  2.8125e-01,
        -2.0020e-02,  7.5195e-02,  1.0352e-01,  1.0547e-01,  7.8906e-01,
        -1.5859e+00,  8.7402e-02,  3.3594e-01, -9.8633e-02, -2.6489e-02,
         1.6846e-02, -1.9766e+00, -3.6621e-02, -1.0693e-01, -3.4180e-01,
        -6.8848e-02, -9.2773e-02,  3.5352e-01,  1.4258e-01, -2.8711e-01,
         1.6016e-01,  2.3594e+00,  1.1426e-01,  1.3086e-01, -2.0599e-03,
         8.0566e-02,  3.8757e-03, -3.0078e-01, -4.6250e+00, -1.2158e-01,
         1.1475e-01, -2.6367e-01, -8.8867e-02,  5.9082e-02,  1.2344e+00,
         3.9453e-01, -2.6758e-01, -5.5420e-02,  4.3359e-01, -4.3359e-01,
         1.5820e-01,  1.5820e-01, -5.2246e-02,  2.1973e-01,  6.3281e-01,
         9.4922e-01,  1.0000e+00,  5.0000e-01, -1.7344e+00,  9.6094e-01,
        -8.9062e-01,  1.4941e-01, -2.4023e-01,  4.1992e-02,  3.1641e-01,
         2.3242e-01, -4.9609e-01,  5.0391e-01,  6.7383e-02, -3.9453e-01,
         1.1094e+00, -6.2500e-02, -1.5320e-02, -4.2773e-01, -1.3906e+00,
        -5.8594e-03, -2.0605e-01, -4.6143e-02, -1.9531e-01,  4.4141e-01,
        -2.5977e-01, -4.1504e-02, -2.0000e+00, -2.3633e-01,  5.5664e-02,
        -8.0469e-01,  4.6094e-01,  1.2793e-01,  2.9663e-02, -1.5527e-01,
        -1.5469e+00, -8.4473e-02,  2.4219e-01,  2.6245e-02,  1.5391e+00,
         1.5015e-02, -1.4648e-01,  1.3086e-01,  4.2188e-01, -1.7578e-01,
         3.2617e-01, -9.4238e-02, -3.0938e+00,  2.0312e-01,  1.6113e-01,
         3.0469e-01, -1.9287e-02, -4.0625e-01,  1.1182e-01,  7.3853e-03,
        -1.0400e-01, -3.3691e-02,  1.0547e-01,  1.4746e-01, -8.1875e+00,
        -3.9844e-01,  5.1172e-01,  2.2949e-01, -1.3281e+00,  9.2773e-03,
         9.8828e-01, -6.0938e-01,  1.5820e-01, -7.6953e-01,  1.6602e-01,
         4.6289e-01, -1.1035e-01,  3.3984e-01, -1.0000e+00, -2.4023e-01,
        -4.0527e-02, -1.8164e-01,  1.9336e-01,  1.8188e-02,  4.9561e-02,
        -6.4453e-02, -5.7861e-02, -6.2256e-02,  1.8848e-01, -1.5234e-01,
         1.5332e-01,  3.8086e-02, -3.3447e-02,  1.3672e+00, -2.1606e-02,
         5.9814e-02,  6.9141e-01, -5.4443e-02, -6.3672e-01,  2.7954e-02,
        -2.6172e-01,  4.7461e-01, -3.1641e-01, -3.0469e-01, -1.9062e+00,
         1.2256e-01, -1.8750e-01,  9.1797e-02,  1.9922e+00, -1.4465e-02,
         1.3574e-01, -4.4141e-01, -3.2031e-01,  5.8105e-02,  5.7031e-01,
         8.6975e-04,  1.4453e-01,  1.6797e-01,  1.8164e-01,  1.3574e-01,
         3.2812e-01,  2.0703e-01,  9.4727e-02,  8.8867e-02,  2.6367e-01,
         3.5938e-01, -2.4121e-01, -1.3672e-01,  3.0312e+00,  5.0781e-01,
        -5.0781e-01,  6.2012e-02, -8.9355e-02, -8.7891e-01, -1.4648e-01,
        -3.0859e-01,  1.9062e+00])

llm.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0222,  0.0094, -0.0269,  ..., -0.0083, -0.0222, -0.0339],
        [ 0.0259, -0.0237, -0.0044,  ..., -0.0048, -0.0124, -0.0307],
        [ 0.0047,  0.0114, -0.0391,  ...,  0.0035, -0.0426,  0.0193],
        ...,
        [-0.0100,  0.0191, -0.0633,  ...,  0.0243, -0.0259, -0.0296],
        [-0.0284, -0.0351,  0.0174,  ..., -0.0191, -0.0116, -0.0189],
        [ 0.0116, -0.0649, -0.0567,  ...,  0.0427, -0.0269, -0.0130]])

llm.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0407,  0.0251, -0.1099,  ...,  0.0404, -0.0178,  0.0190],
        [-0.0413, -0.0097,  0.0222,  ..., -0.0030, -0.0274,  0.0193],
        [ 0.0127, -0.0371,  0.0349,  ...,  0.0016, -0.0507,  0.0129],
        ...,
        [-0.0542, -0.0402,  0.0271,  ..., -0.0076,  0.0448, -0.0409],
        [-0.0220,  0.0025, -0.0050,  ..., -0.0498, -0.0224,  0.0422],
        [ 0.0175,  0.0113, -0.0248,  ...,  0.0249, -0.0227,  0.0069]])

llm.base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0133, -0.0211, -0.0050,  ...,  0.0153, -0.0294, -0.0233],
        [ 0.0128,  0.0217,  0.0021,  ...,  0.0188,  0.0107, -0.0193],
        [-0.0029, -0.0028,  0.0152,  ..., -0.0002,  0.0308, -0.0018],
        ...,
        [ 0.0182,  0.0322, -0.0281,  ...,  0.0156, -0.0025,  0.0248],
        [-0.0125,  0.0033,  0.0118,  ..., -0.0325,  0.0172,  0.0026],
        [ 0.0071,  0.0166, -0.0315,  ..., -0.0055, -0.0442,  0.0002]])

llm.base_model.model.model.layers.18.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 1.4160e-02, -1.1670e-01,  2.9541e-02,  1.5625e-02, -1.5198e-02,
         5.4443e-02, -2.8442e-02, -8.6975e-04, -1.0596e-01, -9.5703e-02,
        -1.2268e-02,  1.5869e-02,  2.5635e-02,  2.5977e-01,  7.4219e-02,
         3.9062e-02,  1.6504e-01,  8.1177e-03, -4.8096e-02,  8.3008e-02,
         5.4688e-02, -9.4604e-04, -1.8921e-02, -4.1016e-02,  3.3447e-02,
         3.9062e-02,  5.1758e-02,  9.2773e-02,  5.2246e-02, -3.4180e-02,
        -3.9795e-02,  2.2168e-01, -1.1658e-02,  2.3926e-02,  1.7578e-02,
        -1.8677e-02, -7.2021e-03, -2.4414e-02,  1.1230e-02, -9.3262e-02,
         9.6191e-02, -4.1748e-02, -8.1787e-03, -5.9326e-02, -3.7842e-02,
         7.4609e-01,  5.0781e-02,  1.5717e-03,  5.0293e-02, -8.3008e-02,
         1.6113e-02,  1.3306e-02, -4.9072e-02,  3.1494e-02,  2.5391e-02,
        -2.5513e-02,  1.6357e-02,  8.0566e-03,  5.7373e-03,  1.5381e-02,
         3.2471e-02, -1.1253e-04, -1.8799e-02,  5.1514e-02, -3.0762e-02,
        -1.5259e-02, -1.3245e-02,  1.3977e-02, -1.3489e-02, -2.1606e-02,
         2.6978e-02,  8.3008e-02,  2.9175e-02,  2.1362e-02, -7.5531e-04,
        -1.8799e-02, -4.8828e-03, -1.0376e-03, -1.9409e-02,  5.7129e-02,
        -6.6895e-02,  1.0376e-02,  9.0332e-02, -2.0508e-02,  6.9336e-02,
        -7.2754e-02,  2.5635e-02, -1.5503e-02, -1.2329e-02,  3.3936e-02,
        -1.5259e-03,  4.8828e-03,  5.1758e-02,  4.3945e-02,  1.7822e-02,
         5.0537e-02, -2.1606e-02, -1.6632e-03,  2.5391e-02,  3.9795e-02,
         2.1973e-02, -9.1797e-02, -2.2339e-02, -1.5625e-02, -1.0437e-02,
        -3.0273e-02,  1.0864e-02,  6.0791e-02, -4.4922e-02, -1.6968e-02,
         4.3945e-02,  3.1738e-02,  6.3171e-03,  5.2979e-02, -2.5635e-02,
         1.9531e-02,  1.4099e-02, -2.1484e-02,  4.7607e-02,  9.9609e-02,
        -7.7209e-03, -6.8665e-03,  5.5908e-02,  3.0029e-02, -1.2756e-02,
        -2.0874e-02,  3.2715e-02, -1.0010e-02,  1.2793e-01,  1.8799e-02,
        -1.7700e-02, -6.2500e-02, -5.9082e-02,  4.2969e-02, -3.5156e-02,
         8.1055e-02,  4.1504e-02,  4.6692e-03, -1.4062e-01, -3.4668e-02,
        -8.9355e-02, -5.8105e-02, -7.8735e-03, -1.4648e-02, -2.2656e-01,
         2.9755e-03, -2.9785e-02, -6.3477e-02, -9.2285e-02,  1.0681e-02,
        -3.7842e-02, -3.3691e-02, -1.2109e-01,  1.6504e-01,  4.6875e-02,
         3.6621e-02,  1.5332e-01,  7.1289e-02, -1.8555e-02, -2.7008e-03,
         2.0874e-02, -1.4062e-01, -1.7676e-01,  1.2598e-01,  8.9111e-03,
        -2.1820e-03, -3.5645e-02, -5.4443e-02, -1.2402e-01, -8.4961e-02,
         9.6893e-04, -6.7871e-02,  1.1670e-01, -2.6733e-02, -6.5918e-02,
        -1.1597e-02, -7.0801e-02,  1.4258e-01, -2.1606e-02, -6.8848e-02,
        -3.0151e-02,  5.9082e-02, -1.7334e-02,  2.0142e-02,  5.0537e-02,
         7.3242e-02, -1.9629e-01,  1.0010e-01, -4.6875e-02, -4.1748e-02,
        -1.1475e-01,  4.6387e-02, -8.8501e-03,  2.0508e-02, -4.9316e-02,
         6.4453e-02, -3.8086e-02,  6.4453e-02,  2.0020e-02, -3.5889e-02,
        -1.3477e-01, -9.0820e-02,  3.6621e-02, -1.8845e-03, -2.9785e-02,
         1.3672e-01,  4.9316e-02,  1.1572e-01,  8.4839e-03,  1.8652e-01,
         7.9102e-02, -9.0332e-02,  1.8799e-02,  7.5684e-02,  4.3213e-02,
         3.7109e-02,  1.8799e-02,  1.7090e-02, -4.9438e-03,  1.3281e-01,
        -2.4780e-02,  5.2246e-02, -9.9609e-02,  4.6143e-02, -4.5166e-02,
         7.3242e-02, -1.8066e-01,  5.4688e-02, -8.8379e-02, -1.1768e-01,
        -6.5430e-02,  1.5039e-01,  2.8320e-02,  1.6211e-01, -5.4443e-02,
        -6.8848e-02, -1.1047e-02,  7.5684e-02, -1.0986e-02, -3.6621e-02,
         6.7383e-02, -9.5825e-03, -3.5156e-02, -2.2168e-01,  3.2471e-02,
        -1.7822e-02, -4.1748e-02,  3.2715e-02, -1.1133e-01, -3.2959e-02,
        -2.8320e-02, -1.1719e-01, -1.0498e-02,  1.0156e-01,  2.6953e-01,
         3.7537e-03, -1.7212e-02, -1.4709e-02, -1.3306e-02, -1.2085e-02,
        -6.8848e-02, -8.1787e-03,  2.2095e-02,  2.0630e-02,  7.5195e-02,
        -2.0752e-02,  1.0620e-02, -6.8359e-02, -5.8594e-03,  5.6885e-02,
         1.7822e-02, -1.2329e-02,  2.6733e-02, -2.0312e-01,  3.3447e-02,
        -2.5513e-02,  3.3203e-02,  3.3417e-03, -1.5015e-02, -4.3457e-02,
        -4.0588e-03,  9.2773e-03, -3.7109e-02,  2.9053e-02, -4.0771e-02,
        -5.5664e-02, -9.9945e-04,  3.1494e-02,  1.5015e-02, -2.8809e-02,
         3.8086e-02,  2.5146e-02, -6.7383e-02,  5.6641e-02, -1.2354e-01,
         2.8198e-02,  8.9111e-03, -3.9551e-02, -2.0508e-02, -2.5635e-02,
        -1.3306e-02,  6.6895e-02,  1.9409e-02,  7.7148e-02, -3.8528e-04,
        -2.9297e-03,  7.9956e-03,  1.7090e-02,  4.3335e-03, -4.7119e-02,
        -2.0020e-02, -7.4219e-02, -1.5918e-01, -9.1309e-02, -1.5198e-02,
         4.9805e-02, -1.0559e-02,  1.0071e-02, -2.3730e-01, -4.9561e-02,
        -1.1230e-02, -1.1292e-02,  3.4180e-02, -3.5400e-02, -2.6489e-02,
        -7.3242e-02,  2.2217e-02, -3.4424e-02,  1.6602e-02, -2.0264e-02,
         2.9907e-02, -2.7344e-02,  2.6733e-02, -4.9316e-02, -7.4387e-04,
        -3.5645e-02,  3.4668e-02, -5.3955e-02, -3.7598e-02,  8.4473e-02,
        -5.9570e-02, -5.5908e-02, -3.6621e-02, -2.9175e-02,  2.0264e-02,
        -1.6846e-02,  3.8574e-02,  2.1744e-04,  4.3701e-02, -5.5420e-02,
        -1.0205e-01, -6.3477e-03, -5.6152e-02, -6.8848e-02, -1.7700e-02,
         6.8359e-02,  3.4332e-03, -4.4250e-03,  7.7637e-02,  3.1250e-02,
        -6.6406e-02, -9.6436e-03,  3.0640e-02,  9.6191e-02,  5.5420e-02,
        -6.6895e-02, -6.7871e-02,  1.6479e-02,  1.5747e-02, -5.6763e-03,
        -9.0942e-03,  8.2031e-02,  1.0864e-02,  3.4912e-02, -5.5542e-03,
        -1.8311e-02, -5.4688e-02, -4.3030e-03,  6.0059e-02,  1.6357e-02,
        -2.1606e-02, -1.0498e-02, -4.1016e-02,  6.3965e-02,  4.7302e-03,
         6.7749e-03, -2.7344e-02,  5.9082e-02, -4.7607e-02, -1.8066e-02,
        -4.6631e-02, -2.3560e-02, -2.1118e-02, -7.6294e-03,  1.1536e-02,
         3.6621e-02, -6.5918e-03, -6.3965e-02,  4.1809e-03,  2.4902e-02,
         8.9844e-02,  2.8320e-02,  2.5787e-03, -1.2741e-03,  3.8574e-02,
        -2.2705e-02,  3.9551e-02,  9.0332e-02,  1.2573e-02,  1.6113e-02,
        -2.6611e-02, -1.6113e-02, -5.2490e-02,  3.3936e-02, -9.6436e-03,
        -1.8799e-02,  2.8229e-03, -2.4658e-02, -6.6833e-03, -5.0781e-02,
         6.6895e-02,  1.0864e-02, -4.4678e-02, -5.9082e-02, -1.0986e-03,
         2.3926e-02,  2.4658e-02,  1.3000e-02, -1.4832e-02,  1.3489e-02,
        -2.9945e-04,  2.7588e-02,  3.0670e-03,  2.8320e-02,  3.8330e-02,
         9.5215e-03, -1.5381e-02, -6.1035e-02,  1.6479e-02,  4.2969e-02,
        -9.9487e-03,  2.4292e-02, -4.7607e-02,  1.6357e-02, -4.9438e-03,
         6.2500e-02, -2.2583e-03, -2.4414e-02, -2.0386e-02, -1.4343e-02,
         5.3955e-02,  1.9653e-02, -8.7891e-03, -4.2969e-02,  9.0332e-03,
         3.1006e-02,  3.2715e-02,  3.0640e-02,  7.8125e-02,  3.3447e-02,
         1.7822e-02,  4.9744e-03, -5.3101e-03,  1.3489e-02,  1.5564e-02,
         1.7090e-03,  1.6724e-02, -4.6997e-03,  3.8818e-02,  4.6875e-02,
        -4.4678e-02,  4.9316e-02,  8.7280e-03, -8.5449e-04, -6.8359e-03,
         2.6611e-02, -1.2573e-02,  3.9795e-02,  1.2512e-02, -1.5869e-02,
        -6.4453e-02, -3.3691e-02, -2.2217e-02, -1.9043e-02, -5.9570e-02,
        -4.5204e-04, -5.4688e-02, -4.5166e-03, -5.4321e-03, -1.1841e-02,
         3.1494e-02,  1.2146e-02,  2.7618e-03, -1.2512e-03,  1.8845e-03,
        -1.2817e-02, -5.7983e-03,  2.8076e-02, -1.2817e-02,  7.0496e-03,
         1.8188e-02, -1.5320e-02,  6.2561e-03, -1.1719e-02, -2.6245e-02,
         2.2461e-02,  5.0537e-02, -1.6357e-02, -1.8677e-02,  6.5613e-03,
         7.2021e-03, -1.6602e-02])

llm.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0083,  0.0587, -0.0256,  ..., -0.0466, -0.0157, -0.0218],
        [ 0.0219,  0.0210,  0.0204,  ...,  0.0097, -0.0346,  0.0153],
        [-0.0172,  0.0675, -0.0073,  ..., -0.0592,  0.0079, -0.0273],
        ...,
        [-0.0057,  0.0436, -0.0072,  ...,  0.0501,  0.0246,  0.0222],
        [ 0.0242, -0.0425, -0.0222,  ...,  0.0383, -0.0244, -0.0352],
        [-0.0100,  0.0278, -0.0213,  ...,  0.0094,  0.0023, -0.0277]])

llm.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 3.1721e-02,  5.7706e-02,  2.4099e-03,  ...,  1.6170e-02,
         -3.3891e-02, -2.2418e-02],
        [-3.0815e-03,  1.3557e-02, -4.4307e-02,  ..., -1.8040e-02,
          3.0394e-02,  1.3350e-02],
        [-2.4738e-02,  2.6431e-02, -2.7965e-03,  ..., -1.5093e-02,
          2.1637e-02,  1.6820e-02],
        ...,
        [ 3.8342e-05,  9.0964e-03,  3.8410e-02,  ...,  1.8874e-02,
         -2.9437e-02,  2.0730e-02],
        [-2.4635e-02, -9.4851e-04,  8.1863e-03,  ..., -1.7491e-02,
          1.0007e-02,  5.2733e-03],
        [ 3.8089e-03,  2.6805e-02, -1.6487e-02,  ...,  4.4235e-02,
         -4.0794e-04, -2.8657e-02]])

llm.base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0188, -0.0172, -0.0035,  ..., -0.0023, -0.0157,  0.0049],
        [ 0.0195, -0.0148,  0.0032,  ...,  0.0229,  0.0059, -0.0092],
        [ 0.0048, -0.0113, -0.0240,  ..., -0.0010,  0.0099,  0.0036],
        ...,
        [-0.0021, -0.0092,  0.0018,  ...,  0.0159, -0.0159, -0.0126],
        [ 0.0332,  0.0015, -0.0303,  ..., -0.0071, -0.0063, -0.0143],
        [ 0.0125,  0.0215,  0.0023,  ..., -0.0192,  0.0055,  0.0107]])

llm.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 2.4220e-02,  9.3335e-03,  2.5742e-02,  ...,  7.8013e-02,
         -7.9002e-03, -1.7462e-02],
        [ 6.4723e-02, -1.2149e-02,  8.9207e-05,  ..., -3.8639e-02,
         -6.7381e-02,  1.3917e-02],
        [-4.2288e-02, -1.7576e-03, -4.0609e-04,  ..., -3.7472e-02,
         -1.0320e-02, -3.2131e-02],
        ...,
        [ 1.0664e-02,  2.4876e-02,  3.3775e-02,  ..., -4.8740e-02,
         -4.0531e-02, -2.6224e-03],
        [-4.6965e-02,  9.6353e-03, -1.9990e-02,  ...,  6.1087e-04,
          1.0784e-02,  1.0173e-02],
        [-1.7459e-02,  1.2702e-02, -7.7310e-03,  ..., -4.4801e-02,
         -3.4589e-02,  1.1807e-02]])

llm.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 3.7125e-02,  4.0476e-02,  3.2793e-02,  ..., -1.1889e-02,
         -8.1831e-03, -1.7547e-02],
        [ 3.5172e-02,  6.1860e-03,  8.6459e-03,  ...,  2.7090e-02,
         -9.0197e-03,  1.0943e-02],
        [ 2.9580e-03,  5.0395e-02, -2.2335e-02,  ...,  7.4486e-05,
         -5.6590e-03, -2.3028e-02],
        ...,
        [-1.6555e-02,  1.9140e-03,  4.5636e-02,  ...,  3.5127e-02,
         -2.5999e-02,  8.9275e-03],
        [ 3.1862e-02,  2.0534e-02, -1.6866e-02,  ...,  3.5939e-02,
         -1.6602e-02,  7.8368e-03],
        [ 7.3302e-03, -2.8591e-02,  1.1495e-03,  ..., -4.2722e-03,
         -3.6124e-02,  1.5244e-02]])

llm.base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0045,  0.0079,  0.0193,  ...,  0.0082, -0.0075,  0.0011],
        [-0.0417,  0.0179, -0.0112,  ..., -0.0250, -0.0583,  0.0209],
        [ 0.0075,  0.0181, -0.0026,  ..., -0.0176,  0.0061,  0.0012],
        ...,
        [-0.0222, -0.0281, -0.0075,  ...,  0.0342,  0.0204, -0.0006],
        [-0.0011,  0.0042,  0.0069,  ...,  0.0142,  0.0075, -0.0317],
        [ 0.0161, -0.0050,  0.0010,  ..., -0.0041,  0.0082, -0.0195]])

llm.base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0181, -0.0421,  0.0408,  ..., -0.0111, -0.0166,  0.0158],
        [-0.0485,  0.0557,  0.0154,  ..., -0.0169,  0.0125, -0.0831],
        [ 0.0265,  0.0296, -0.0102,  ...,  0.0134,  0.0260,  0.1007],
        ...,
        [ 0.0556, -0.0426,  0.0688,  ...,  0.0223, -0.0067,  0.0043],
        [-0.0495,  0.0441,  0.0034,  ..., -0.0466,  0.0268, -0.0059],
        [ 0.0281,  0.0471, -0.1118,  ..., -0.0259,  0.0505,  0.0069]])

llm.base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0289, -0.0243, -0.0178,  ..., -0.0279,  0.0356,  0.0129],
        [ 0.0096,  0.0080,  0.0434,  ...,  0.0044,  0.0144, -0.0376],
        [-0.0472,  0.0040,  0.0525,  ...,  0.0143,  0.0321,  0.0700],
        ...,
        [ 0.0394,  0.0340, -0.0107,  ..., -0.0117,  0.0320,  0.0045],
        [-0.0471, -0.0054,  0.0023,  ..., -0.0229, -0.0454,  0.0213],
        [ 0.0432, -0.0386, -0.0294,  ...,  0.0080,  0.0043, -0.0462]])

llm.base_model.model.model.layers.18.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0051, -0.0026,  0.0007,  ...,  0.0014,  0.0221,  0.0105],
        [ 0.0142,  0.0084,  0.0038,  ...,  0.0022,  0.0050,  0.0025],
        [-0.0128,  0.0054,  0.0070,  ...,  0.0168,  0.0093,  0.0050],
        ...,
        [-0.0317,  0.0283,  0.0055,  ...,  0.0193, -0.0065, -0.0309],
        [-0.0044,  0.0053,  0.0029,  ...,  0.0045, -0.0095, -0.0007],
        [ 0.0106, -0.0183,  0.0069,  ..., -0.0052,  0.0258,  0.0044]])

llm.base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0261,  0.0135,  0.0096,  ...,  0.0355,  0.0461, -0.0399],
        [-0.0258,  0.0196, -0.0692,  ..., -0.0070, -0.0180, -0.0781],
        [-0.0454, -0.0176, -0.0202,  ..., -0.0094, -0.0304, -0.0482],
        ...,
        [-0.0286,  0.0038,  0.0174,  ...,  0.0030,  0.0265,  0.0268],
        [-0.0197, -0.0028, -0.0257,  ..., -0.0990, -0.0110,  0.0178],
        [ 0.0892, -0.0296, -0.0413,  ..., -0.0514,  0.0493,  0.0740]])

llm.base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0153,  0.0173,  0.0452,  ...,  0.0063,  0.0145, -0.0370],
        [-0.0162,  0.0139,  0.0388,  ...,  0.0090, -0.0076, -0.0004],
        [-0.0136, -0.0125,  0.0134,  ..., -0.0004, -0.0187,  0.0278],
        ...,
        [-0.0394, -0.0228, -0.0283,  ...,  0.0338,  0.0319,  0.0273],
        [ 0.0047, -0.0394, -0.0041,  ..., -0.0158, -0.0466, -0.0056],
        [ 0.0388,  0.0201, -0.0042,  ...,  0.0032,  0.0028, -0.0115]])

llm.base_model.model.model.layers.18.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 0.0227,  0.0014,  0.0208,  ...,  0.0084,  0.0050,  0.0010],
        [ 0.0194,  0.0135, -0.0006,  ...,  0.0273,  0.0077, -0.0057],
        [ 0.0016, -0.0046,  0.0055,  ..., -0.0228, -0.0168,  0.0107],
        ...,
        [-0.0168,  0.0015,  0.0236,  ..., -0.0079, -0.0153,  0.0056],
        [-0.0322, -0.0083,  0.0205,  ...,  0.0071, -0.0091,  0.0022],
        [-0.0129,  0.0249,  0.0129,  ..., -0.0248, -0.0036, -0.0223]])

llm.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0069, -0.0348, -0.0215,  ..., -0.0445,  0.0034,  0.0397],
        [ 0.0408,  0.0075,  0.0432,  ..., -0.0087, -0.0424, -0.0109],
        [ 0.0186, -0.0183, -0.0015,  ..., -0.0254, -0.0744, -0.0134],
        ...,
        [ 0.0084,  0.0061, -0.0047,  ...,  0.0316,  0.0374,  0.0039],
        [ 0.0046, -0.0038,  0.0182,  ..., -0.0058,  0.0114, -0.0593],
        [ 0.0369, -0.0011,  0.0196,  ...,  0.0182, -0.0015, -0.0475]])

llm.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0386,  0.0450,  0.0193,  ...,  0.0006,  0.0303, -0.0518],
        [ 0.0333, -0.0300, -0.0601,  ...,  0.0308,  0.0127,  0.0245],
        [-0.0252, -0.0421,  0.0127,  ...,  0.0114,  0.0229, -0.0373],
        ...,
        [-0.0051, -0.0069, -0.0399,  ..., -0.0513, -0.0172, -0.0096],
        [ 0.0137, -0.0326,  0.0091,  ...,  0.0407,  0.0324,  0.0111],
        [ 0.0016,  0.0270,  0.0018,  ...,  0.0337,  0.0253,  0.0253]])

llm.base_model.model.model.layers.18.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.6094, 0.9688, 2.5156,  ..., 0.3770, 1.6484, 1.5547])

llm.base_model.model.model.layers.18.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.2109, 1.0703, 2.0156,  ..., 0.6641, 1.3750, 1.3359])

llm.base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0131,  0.0090, -0.0060,  ...,  0.0098, -0.0042, -0.0179],
        [-0.0017, -0.0120, -0.0206,  ...,  0.0015,  0.0057, -0.0083],
        [-0.0099, -0.0018, -0.0092,  ...,  0.0067, -0.0004, -0.0054],
        ...,
        [ 0.0361, -0.0109, -0.0015,  ...,  0.0025,  0.0027,  0.0469],
        [ 0.0435, -0.0115, -0.0320,  ..., -0.0243,  0.0277,  0.0172],
        [ 0.0053,  0.0044, -0.0522,  ..., -0.0018, -0.0076,  0.0021]])

llm.base_model.model.model.layers.19.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([-5.5750e+01, -9.3125e+00, -3.2750e+01,  ..., -2.8750e+00,
         3.3398e-01,  1.1414e-02])

llm.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-1.7441e-03, -4.8451e-02, -7.2532e-02,  ...,  5.7114e-03,
         -6.6654e-02, -1.1051e-02],
        [-3.2130e-02,  7.5753e-03,  3.8706e-02,  ..., -2.6943e-02,
          1.9682e-02,  2.2972e-03],
        [-1.2988e-02, -2.1372e-02, -3.3758e-02,  ..., -2.8975e-02,
          3.7218e-03, -2.9870e-02],
        ...,
        [ 4.1776e-02,  6.5049e-02,  4.3437e-02,  ...,  1.7372e-02,
         -5.1324e-02, -1.0691e-02],
        [ 6.6613e-02,  4.6092e-03,  2.3163e-02,  ...,  2.7058e-03,
         -1.1760e-02,  8.1906e-04],
        [ 1.7436e-02, -6.2591e-02,  7.3139e-05,  ..., -3.8431e-02,
          3.8590e-02, -1.6063e-02]])

llm.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0330, -0.0090,  0.0432,  ...,  0.0031,  0.0163,  0.0221],
        [-0.0102,  0.0138, -0.0326,  ...,  0.0219,  0.0420, -0.0233],
        [ 0.0537,  0.0313, -0.0031,  ...,  0.0509,  0.0392,  0.0334],
        ...,
        [-0.0225,  0.0005,  0.0115,  ...,  0.0034,  0.0019, -0.0119],
        [ 0.0041,  0.0158, -0.0237,  ...,  0.0122,  0.0369,  0.0164],
        [-0.0281,  0.0124, -0.0246,  ...,  0.0164, -0.0042,  0.0096]])

llm.base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0081,  0.0015, -0.0184,  ..., -0.0016,  0.0044, -0.0063],
        [ 0.0036, -0.0064, -0.0103,  ...,  0.0060,  0.0027, -0.0019],
        [ 0.0137, -0.0114,  0.0066,  ...,  0.0018,  0.0043,  0.0044],
        ...,
        [-0.0076,  0.0114,  0.0032,  ..., -0.0004, -0.0162, -0.0148],
        [-0.0188, -0.0225, -0.0094,  ...,  0.0010,  0.0140, -0.0125],
        [ 0.0087,  0.0283, -0.0120,  ...,  0.0032,  0.0139, -0.0179]])

llm.base_model.model.model.layers.19.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-5.3125e-01, -5.7422e-01, -4.6875e-01, -5.8594e-01, -1.8433e-02,
        -6.4844e-01,  1.3672e-01,  7.5684e-03, -5.7031e-01, -1.3184e-01,
         5.9814e-03, -1.9824e-01,  9.5825e-03,  1.9141e-01,  7.8906e-01,
        -4.7119e-02,  1.3855e-02,  1.8555e-01,  3.0078e-01, -1.4941e-01,
         1.8262e-01, -1.0938e-01, -2.3926e-01, -6.4062e-01,  9.3262e-02,
         1.0742e-01, -1.6602e-01, -2.4902e-02, -1.0938e+00, -6.5430e-02,
         2.1606e-02,  1.3516e+00, -4.3457e-02, -8.2520e-02,  5.1758e-02,
        -7.9297e-01, -6.0059e-02,  2.3145e-01, -2.1484e-01,  7.6172e-01,
        -6.0547e-02, -9.5215e-02, -2.6406e+00,  1.0400e-01,  3.9648e-01,
        -1.1250e+00,  1.9989e-03,  4.2969e-01,  1.3672e-01, -1.5430e-01,
        -2.6172e-01,  1.9434e-01, -6.4062e-01,  4.9414e-01, -2.4805e-01,
         4.1562e+00,  5.4688e-01,  7.2656e-01,  3.8906e+00, -4.7461e-01,
         3.6523e-01, -2.9531e+00,  3.2656e+00,  7.8750e+00,  5.1562e-01,
         1.9629e-01,  2.4512e-01, -7.9590e-02, -2.2754e-01,  4.4922e-01,
        -1.8845e-03, -2.2070e-01,  5.7031e-01,  1.9434e-01,  5.3101e-03,
         7.8125e-01, -1.2500e-01, -1.1621e-01,  5.4688e-02, -7.7148e-02,
        -2.5977e-01,  1.0781e+00,  1.2695e-01,  7.9102e-02, -5.0391e-01,
         1.0352e-01, -4.9219e-01, -1.2812e+00,  3.3789e-01,  2.1680e-01,
        -7.1094e-01,  1.1377e-01, -1.1953e+00,  6.6895e-02, -8.8867e-02,
         2.0312e-01,  1.2695e-01, -1.7969e-01,  3.0078e-01,  2.2031e+00,
         2.0312e-01, -6.4844e-01, -1.8848e-01, -1.8066e-01, -1.6235e-02,
         2.2754e-01,  1.8677e-02,  1.5918e-01, -3.4180e-01,  5.2344e-01,
         4.4141e-01, -2.6953e-01, -1.1768e-01, -5.8594e-01, -2.9297e-01,
         8.4229e-03,  1.6309e-01,  7.5781e-01, -3.6914e-01, -6.5000e+00,
        -1.1719e+00, -1.5625e-01, -1.5688e+01, -1.8516e+00,  1.2062e+01,
        -1.2250e+01, -3.3438e+00, -4.5500e+01,  1.6875e+00, -1.3477e-01,
        -1.9922e-01,  2.2070e-01,  2.4121e-01, -7.5195e-02,  5.0391e-01,
        -2.6562e-01, -7.6953e-01,  1.9062e+00, -2.9688e-01,  3.3936e-02,
        -2.5391e-01,  2.4062e+00, -1.8799e-02,  2.6367e-01,  2.4170e-02,
         3.2227e-01, -3.5938e-01, -1.6992e-01,  2.4609e-01,  2.6406e+00,
         2.1973e-02, -7.2266e-02, -3.7695e-01,  3.0823e-03,  2.8906e-01,
        -2.6562e+00, -1.0449e-01, -9.9609e-02,  1.1377e-01,  2.6758e-01,
         1.3086e-01,  3.3984e-01,  3.2715e-02,  3.1982e-02,  9.4238e-02,
        -4.5410e-02,  9.8145e-02,  2.5781e-01, -6.2012e-02,  7.4219e-02,
        -2.8320e-02,  6.2109e-01, -6.3477e-02,  1.1963e-01,  3.1445e-01,
        -2.1289e-01,  4.4922e-01,  3.1562e+00,  1.0156e-01, -2.2852e-01,
         8.2031e-01, -4.1406e-01,  3.0273e-02, -5.5542e-03, -6.1328e-01,
        -6.5918e-02, -6.6406e-01,  4.9023e-01,  5.4688e-01, -5.0000e-01,
        -5.5469e-01,  2.5000e+00, -2.3828e-01, -3.9062e-01, -6.4453e-02,
        -1.6797e+00, -3.9795e-02,  1.4062e-01,  8.9062e-01, -2.1851e-02,
         2.3242e-01, -2.2070e-01, -7.7820e-03, -5.6396e-02,  3.7354e-02,
        -3.2031e-01,  3.3984e-01,  2.6172e-01, -1.3477e-01,  2.3438e+00,
        -1.9141e-01,  2.0117e-01, -1.9727e-01, -2.3242e-01, -3.3203e-02,
        -2.2070e-01, -9.8633e-02,  6.1279e-02, -6.1035e-03,  3.0469e-01,
         1.5564e-02,  2.7344e-01,  6.3672e-01,  8.9355e-02, -2.8516e-01,
         1.1523e-01,  3.3447e-02,  8.4839e-03, -4.8633e-01,  1.5991e-02,
        -8.9844e-02,  3.4531e+00,  1.7578e-01,  2.5586e-01,  8.6426e-02,
         4.1992e-01, -6.4453e-02,  3.2471e-02,  1.2256e-01,  1.9629e-01,
        -1.6992e-01, -1.2969e+00,  5.1025e-02, -1.5391e+00,  8.2812e-01,
         2.5195e-01, -2.2949e-01,  6.0156e-01,  3.3203e-01, -3.4180e-01,
         1.0781e+00,  6.9922e-01,  4.0039e-01, -7.7637e-02, -1.0781e+00,
         2.6719e+00,  1.6113e-01,  2.0020e-01,  1.8945e-01,  3.2031e-01,
        -8.6914e-02,  1.5820e-01, -7.4707e-02, -3.9453e-01,  5.6641e-02,
        -3.3008e-01, -1.5430e-01, -6.0547e-01,  3.5547e-01, -1.2158e-01,
         3.9648e-01, -2.3145e-01,  1.9165e-02, -4.5166e-02, -5.4688e-01,
        -6.9922e-01,  4.7070e-01, -1.0596e-01, -9.2969e-01,  8.7402e-02,
        -9.2773e-03,  1.9989e-03, -1.0156e+00,  1.1816e-01, -1.5234e-01,
        -1.8262e-01,  8.7891e-02, -1.8594e+00,  3.2812e-01,  6.3672e-01,
        -1.2891e-01,  4.5654e-02,  2.3047e-01,  1.8203e+00,  7.7148e-02,
        -3.5889e-02, -9.1797e-01, -2.7313e-03,  1.3574e-01,  5.8105e-02,
         9.9609e-02, -3.9258e-01, -8.7109e-01, -1.9409e-02,  8.0859e-01,
         2.6367e-02, -2.9883e-01, -2.1191e-01, -7.8613e-02,  1.6797e-01,
         4.0039e-01,  5.2979e-02, -2.5586e-01, -1.1670e-01,  4.8242e-01,
        -7.3047e-01,  1.1094e+00,  1.4465e-02, -5.0391e-01, -9.0234e-01,
        -2.6367e-01, -2.8320e-01,  3.4766e-01, -7.4707e-02, -2.9883e-01,
        -4.5312e-01, -5.9814e-02, -2.4609e-01, -4.1797e-01,  2.2168e-01,
         1.3062e-02, -2.1973e-01,  1.8262e-01,  2.2266e-01, -4.4141e-01,
        -2.5195e-01, -7.7734e-01, -6.3965e-02,  1.2598e-01, -3.1836e-01,
        -1.2695e-01,  2.5391e-02,  1.4551e-01, -3.9551e-02,  1.0312e+00,
        -2.1094e-01,  5.2246e-02, -8.0078e-02, -1.1094e+00,  9.0332e-02,
         8.7402e-02,  1.0059e-01, -1.8945e-01, -4.1797e-01,  1.7944e-02,
         1.3281e-01,  1.8848e-01,  5.6885e-02,  1.8750e-01, -3.3203e-01,
        -7.9688e-01, -6.7383e-02,  1.8750e+00,  9.6680e-02, -1.0889e-01,
         4.9609e-01, -4.9561e-02,  1.9336e-01, -2.2969e+00,  6.2988e-02,
        -1.6699e-01, -1.8262e-01,  2.5781e-01, -7.3438e-01,  4.2383e-01,
         2.2754e-01, -1.8457e-01, -2.7148e-01, -7.3047e-01, -2.1484e-01,
         9.0000e+00, -1.2891e+00, -4.8047e-01, -5.6250e-01, -7.1875e-01,
        -5.3906e-01,  6.1719e-01,  2.4707e-01, -6.5234e-01,  1.9824e-01,
         1.9922e-01, -2.6953e-01,  1.1536e-02, -5.0781e-01, -2.7930e-01,
         1.3281e-01, -3.5156e-01,  3.7500e-01, -3.4375e-01,  8.4766e-01,
         4.0430e-01, -3.3203e-01, -2.5586e-01, -1.6699e-01,  2.6172e-01,
         1.2146e-02, -1.5430e-01, -4.4141e-01, -2.9492e-01, -5.1562e-01,
         1.3672e-01,  9.5215e-02, -6.7188e-01, -2.3633e-01,  1.0791e-01,
         5.1172e-01,  1.8652e-01,  5.9082e-02, -6.2891e-01, -1.2695e-01,
        -1.5039e-01,  6.1035e-02,  1.6895e-01,  7.4219e-02,  9.0625e-01,
        -2.8320e-02, -2.8906e-01, -3.2422e-01, -7.8906e-01,  6.3281e-01,
        -1.4062e+00,  1.8359e-01, -3.3789e-01, -2.0312e-01, -8.1250e-01,
         2.3633e-01,  5.7031e-01,  2.1250e+00,  6.9531e-01, -2.6367e-01,
        -1.7676e-01,  2.9102e-01, -1.8066e-01, -1.3750e+00, -1.4141e+00,
         5.2979e-02, -1.8500e+01, -1.6699e-01, -3.7695e-01,  2.3730e-01,
         3.8086e-01,  1.9336e-01, -2.4902e-01,  3.3594e-01, -6.8359e-01,
         7.3730e-02, -3.4766e-01,  5.2344e-01,  2.0020e-01, -1.7090e-02,
         6.1719e-01, -2.1875e-01,  1.9043e-02,  2.3926e-01,  2.2852e-01,
        -5.1562e-01,  2.7734e-01,  3.4766e-01,  4.1992e-02, -1.2812e+00,
        -1.1377e-01,  4.3701e-02, -2.8320e-01, -1.4688e+00, -2.4023e-01,
         7.1875e-01,  1.2305e-01, -5.2734e-01,  2.9883e-01,  2.0312e+00,
         1.1133e-01, -1.2402e-01, -1.5391e+00,  1.2268e-02,  1.2451e-02,
        -8.6719e-01,  2.4414e-01,  1.2695e-01,  2.5781e+00,  8.7402e-02,
         1.1328e-01,  6.1719e-01,  1.5332e-01,  1.5625e-01,  3.4961e-01,
         9.6191e-02,  2.4375e+00,  4.8047e-01,  2.5000e-01, -2.0020e-02,
        -2.8516e-01, -4.6143e-02, -2.2461e-01, -6.1719e-01, -1.7188e+00,
         6.2500e-01,  1.1084e-01, -7.6562e-01, -6.4844e-01,  1.4219e+00,
         7.7188e+00,  5.1953e-01])

llm.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0137,  0.0155,  0.0286,  ..., -0.0352, -0.0139,  0.0312],
        [-0.0182, -0.0485,  0.0082,  ..., -0.0036,  0.0425, -0.0135],
        [ 0.0076,  0.0137,  0.0281,  ..., -0.0134,  0.0392,  0.0312],
        ...,
        [-0.0094,  0.0059,  0.0460,  ..., -0.0081, -0.0239,  0.0120],
        [ 0.0478,  0.0297, -0.0115,  ..., -0.0152, -0.0403,  0.0101],
        [ 0.0315, -0.0229, -0.0341,  ...,  0.0260,  0.0212, -0.0139]])

llm.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 4.3435e-03,  2.3791e-02,  3.2702e-02,  ...,  2.0084e-02,
         -6.1463e-04,  1.5774e-03],
        [-1.3679e-02,  1.7136e-02,  7.6362e-03,  ..., -1.1177e-02,
         -2.8569e-05,  5.5865e-03],
        [ 2.9161e-02,  5.7714e-02, -8.8310e-03,  ...,  1.7668e-02,
         -4.5427e-03,  1.0961e-02],
        ...,
        [-3.3421e-04, -2.8651e-02,  5.6029e-03,  ...,  3.1058e-02,
          2.0307e-02,  1.5692e-02],
        [ 1.4238e-02,  5.4710e-02,  6.7800e-03,  ...,  3.2107e-02,
         -3.7247e-02, -1.9049e-02],
        [ 3.0440e-02, -2.4046e-02, -1.2742e-02,  ..., -2.2594e-02,
          4.0839e-02,  2.0237e-02]])

llm.base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0035, -0.0067, -0.0143,  ...,  0.0325, -0.0086,  0.0104],
        [ 0.0183,  0.0072,  0.0361,  ..., -0.0020,  0.0156, -0.0053],
        [-0.0065, -0.0075,  0.0140,  ..., -0.0045, -0.0041, -0.0132],
        ...,
        [-0.0193, -0.0044, -0.0077,  ...,  0.0126, -0.0137,  0.0026],
        [-0.0194, -0.0041,  0.0061,  ..., -0.0137, -0.0014, -0.0159],
        [ 0.0074, -0.0076,  0.0009,  ...,  0.0062, -0.0112,  0.0136]])

llm.base_model.model.model.layers.19.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-6.9824e-02,  1.4941e-01, -1.4160e-01,  1.5332e-01, -2.7930e-01,
        -4.0820e-01, -2.1680e-01,  1.6797e-01, -5.9082e-02,  1.0449e-01,
        -9.7168e-02, -1.5625e-01,  8.1250e-01, -3.7695e-01,  8.7891e-02,
         4.1602e-01, -3.4961e-01,  1.4648e-01,  3.5889e-02, -1.1670e-01,
         2.6367e-01,  3.2227e-02, -7.4707e-02,  1.9043e-02, -9.4727e-02,
        -5.6885e-02,  1.1768e-01, -1.4844e-01,  3.2812e-01,  2.0117e-01,
        -1.4973e-04,  1.0010e-01,  1.8921e-02,  5.1880e-03, -3.7891e-01,
         1.4844e-01, -9.7656e-01, -6.0791e-02, -4.1016e-01,  1.0596e-01,
        -5.1270e-02, -8.2520e-02, -1.9141e-01,  3.3691e-02, -3.1055e-01,
         1.9238e-01, -2.4902e-01, -3.6523e-01, -3.6523e-01, -1.6992e-01,
         6.7383e-02, -1.5234e-01, -1.7676e-01, -3.0664e-01,  2.7344e-01,
        -1.4453e-01, -1.9336e-01,  3.4180e-02, -1.5430e-01,  1.3086e-01,
        -3.0469e-01, -2.6367e-01,  3.4570e-01,  2.7539e-01, -8.3984e-02,
        -1.9336e-01, -3.1836e-01, -2.7832e-02,  1.2793e-01, -6.2988e-02,
        -4.0625e-01, -4.2969e-01, -3.1641e-01,  1.0547e-01,  2.5586e-01,
         1.0938e-01,  1.3086e-01, -1.1768e-01,  1.9629e-01,  6.3965e-02,
         3.8086e-02, -3.6719e-01,  6.1328e-01,  1.5918e-01, -2.2070e-01,
         3.9307e-02, -1.4844e-01,  1.8555e-02,  2.8125e-01, -2.1191e-01,
         3.0469e-01,  3.1641e-01, -4.4531e-01, -3.2617e-01,  1.8066e-01,
         1.5381e-02, -1.5820e-01, -3.8672e-01,  7.7637e-02, -1.0498e-01,
        -1.1865e-01,  5.6396e-02, -2.2168e-01, -5.7812e-01,  1.6504e-01,
        -2.9883e-01,  1.2012e-01, -2.8906e-01,  2.7832e-02,  3.6719e-01,
        -2.1387e-01,  1.1523e-01, -1.0681e-02,  4.5117e-01, -2.5781e-01,
         2.3242e-01, -1.5793e-03,  2.8906e-01, -3.8672e-01,  6.5430e-02,
         1.1133e-01, -2.2656e-01,  2.5977e-01,  2.2852e-01,  5.9509e-03,
        -3.5156e-01, -1.1475e-01,  2.0605e-01, -1.9043e-02,  4.2725e-02,
        -4.6631e-02, -5.0735e-04, -2.7588e-02, -2.9297e-02,  2.5757e-02,
        -1.0437e-02, -1.8921e-02,  4.4922e-02,  3.8086e-02, -5.3223e-02,
        -2.9907e-02,  2.7710e-02, -4.6875e-02, -6.2500e-02,  5.1270e-02,
         1.5503e-02,  4.7363e-02,  3.8818e-02, -5.6641e-02, -1.0132e-02,
        -3.9648e-01, -1.9531e-02, -4.3213e-02,  1.1780e-02,  1.1816e-01,
         5.5908e-02, -8.2031e-02,  5.7617e-02,  2.9907e-02,  3.6133e-02,
        -2.2949e-02, -1.2512e-02,  5.6885e-02,  2.1362e-02,  4.9805e-02,
        -2.6978e-02,  3.0029e-02,  5.5078e-01,  3.9062e-02,  1.0156e-01,
         5.1025e-02,  5.5908e-02,  5.6152e-02,  4.3701e-02, -2.0142e-02,
        -5.9814e-02,  4.9561e-02, -9.3750e-02, -9.7168e-02, -7.0801e-02,
         1.0596e-01, -4.7852e-02,  7.5684e-02,  2.5635e-02,  9.8145e-02,
        -1.1047e-02, -6.5430e-02,  1.4709e-02,  3.9062e-02, -8.4961e-02,
         9.7168e-02, -5.3223e-02,  4.0771e-02,  9.8267e-03, -2.3926e-01,
         2.4536e-02, -1.7822e-02,  8.1543e-02,  4.7852e-02, -7.5195e-02,
        -5.0049e-03,  5.9204e-03, -7.6172e-02,  5.0781e-02,  6.5430e-02,
        -3.5156e-02, -2.1118e-02,  1.3428e-02,  2.4536e-02, -5.7678e-03,
        -1.3123e-02,  6.8750e-01,  3.3936e-02, -3.6621e-02,  3.3691e-02,
        -1.3379e-01,  1.8188e-02,  6.4941e-02, -6.1768e-02, -3.3379e-04,
         7.5195e-02,  1.3062e-02,  9.0332e-02, -1.1768e-01, -2.6733e-02,
         1.8555e-02, -3.9551e-02,  1.7822e-02, -2.2339e-02,  3.9673e-03,
        -1.3489e-02,  1.1426e-01, -4.5898e-02,  7.6660e-02,  1.3000e-02,
         4.5654e-02,  8.4961e-02,  7.6660e-02, -7.5195e-02, -5.2734e-02,
         4.1504e-02,  1.7456e-02, -3.5400e-02,  2.3193e-02,  5.9326e-02,
        -4.6387e-02,  5.3223e-02,  1.1169e-02,  1.4648e-01,  2.7954e-02,
        -5.0354e-04, -5.3223e-02,  2.0117e-01, -4.4922e-02, -1.0864e-02,
        -8.0078e-02, -3.6621e-02, -6.5430e-02, -2.9419e-02, -5.7068e-03,
        -9.7656e-03, -2.9907e-02,  7.2937e-03,  1.2451e-02, -2.3651e-03,
         3.5889e-02, -1.7456e-02,  1.0925e-02, -1.3123e-02,  3.3203e-02,
        -5.3787e-04, -7.3547e-03, -8.1177e-03, -1.2360e-03, -1.4343e-02,
         3.2715e-02,  3.2715e-02,  1.4587e-02,  4.4434e-02, -1.6602e-02,
        -1.2402e-01,  2.2095e-02,  1.9897e-02, -5.4199e-02,  1.4526e-02,
         5.8899e-03, -1.2634e-02, -6.2561e-03,  1.4038e-02,  1.6846e-02,
         1.3477e-01,  1.5820e-01, -9.7656e-02,  3.1250e-02,  4.8065e-04,
        -3.1982e-02, -4.5166e-03, -1.0376e-02,  2.7588e-02,  2.0142e-02,
        -1.6846e-02, -3.1738e-03,  3.1250e-02,  5.2185e-03, -2.3071e-02,
        -3.4912e-02, -2.4780e-02,  1.5234e-01, -1.3770e-01, -1.8555e-02,
         7.9956e-03,  3.6377e-02, -4.3701e-02, -3.2959e-02,  1.8433e-02,
        -3.4424e-02, -3.9368e-03, -5.3711e-02, -9.6893e-04, -5.6396e-02,
        -2.3682e-02, -2.7466e-02, -1.8677e-02,  6.8848e-02, -7.5195e-02,
        -2.0874e-02,  7.7820e-03,  9.1797e-02,  2.7466e-02,  1.1047e-02,
         4.5395e-04, -5.2002e-02,  4.1260e-02,  3.5400e-02,  4.2236e-02,
         2.2339e-02, -1.2939e-02, -7.8125e-03, -4.0283e-02,  4.3335e-03,
         3.5645e-02, -1.3428e-02, -8.4229e-03,  1.7578e-02,  5.5664e-02,
         1.0010e-02,  7.6904e-03, -8.8501e-03,  1.7822e-02, -2.5513e-02,
        -1.0498e-01, -5.8350e-02,  2.7222e-02,  6.9336e-02, -6.4697e-03,
        -1.6602e-02,  3.3112e-03,  2.8809e-02,  3.0029e-02,  2.0020e-02,
        -9.7656e-03, -1.1523e-01, -7.2656e-01, -3.8086e-02,  1.0498e-02,
         1.5991e-02, -1.0864e-02,  1.7700e-02, -8.3008e-02, -1.5030e-03,
        -7.0312e-02,  1.3504e-03, -2.5482e-03,  5.8105e-02,  3.1006e-02,
         2.1362e-02, -7.7209e-03,  6.3782e-03,  2.4658e-02, -5.6641e-02,
        -1.2024e-02,  4.3457e-02, -1.1816e-01, -2.8809e-02,  1.3733e-02,
        -1.5015e-02, -1.5503e-02,  3.5889e-02, -3.9062e-03,  4.3945e-03,
         9.2163e-03, -3.5156e-02,  1.2695e-02, -4.1260e-02, -1.1658e-02,
        -2.3560e-02,  4.0771e-02, -3.4912e-02, -8.6670e-03,  2.3193e-03,
        -1.0254e-02, -3.8605e-03, -2.1210e-03,  3.7354e-02, -1.6479e-02,
         8.9722e-03,  6.7444e-03, -1.9653e-02, -7.4463e-03, -5.6763e-03,
        -2.9907e-02,  5.9509e-03, -4.0588e-03, -1.6602e-02, -1.7773e-01,
         8.2397e-03,  2.7222e-02, -1.3733e-02,  2.5391e-02,  2.1362e-02,
         3.1494e-02, -1.0596e-01, -2.4536e-02, -2.5940e-03,  2.2949e-02,
        -2.3560e-02, -3.3691e-02,  2.5269e-02, -3.4424e-02,  5.4932e-04,
        -1.5869e-02, -1.0010e-02,  1.1169e-02,  4.6997e-03, -1.4526e-02,
         1.9653e-02, -1.6251e-03, -8.8501e-03, -2.6398e-03, -5.4626e-03,
         3.5645e-02,  7.5684e-02,  6.3171e-03,  1.5137e-02,  7.9956e-03,
         5.3101e-03, -2.6093e-03, -2.8564e-02,  1.2451e-02,  1.9043e-02,
         4.9438e-03, -1.0742e-02, -1.4771e-02, -2.7100e-02,  3.4668e-02,
        -5.6763e-03,  2.7924e-03, -1.0376e-02,  3.3203e-02,  1.0864e-02,
         2.2339e-02, -3.0823e-03,  4.1260e-02, -1.7212e-02, -1.5564e-02,
        -3.2715e-02,  4.4861e-03, -8.6426e-02,  2.9373e-04, -1.3123e-02,
        -3.6469e-03, -4.3213e-02, -9.2773e-03,  1.3046e-03, -1.4404e-02,
         4.4556e-03, -8.4686e-04,  2.1484e-02, -1.6479e-02, -1.8555e-02,
         4.7607e-03,  2.2278e-03, -2.6367e-02,  9.8877e-03,  1.2085e-02,
        -1.5793e-03, -3.8574e-02,  1.0620e-02,  1.0986e-03, -1.6357e-02,
        -1.9165e-02, -2.0874e-02,  1.6724e-02, -1.0254e-02,  9.0942e-03,
        -1.3123e-02,  9.4604e-03,  2.5635e-02, -1.5137e-02,  1.9897e-02,
         2.4536e-02,  1.6479e-02, -3.2715e-02,  3.9307e-02,  5.6763e-03,
        -3.0212e-03,  3.5706e-03, -2.3193e-02,  6.5002e-03,  9.2163e-03,
        -2.4902e-02,  1.1902e-03])

llm.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 1.1850e-02,  1.5368e-02,  1.5313e-02,  ...,  4.0293e-02,
         -3.2436e-02, -3.2947e-02],
        [ 3.8150e-02, -1.3607e-02,  2.6330e-02,  ...,  3.8608e-02,
         -3.6579e-03, -4.0772e-02],
        [ 1.0421e-05,  2.8781e-02, -1.5279e-02,  ..., -1.3234e-02,
         -1.2084e-02, -5.4013e-02],
        ...,
        [-2.0955e-02,  1.4872e-02, -2.6882e-02,  ..., -1.5437e-02,
          1.6712e-02, -2.7467e-02],
        [ 3.4665e-02,  4.9183e-03,  1.7983e-02,  ...,  3.2732e-02,
         -2.2884e-02,  3.5202e-02],
        [ 3.0148e-02, -3.3473e-02,  3.8319e-03,  ...,  1.1695e-02,
          3.8064e-02, -4.3906e-02]])

llm.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0289,  0.0207,  0.0047,  ..., -0.0007, -0.0057, -0.0658],
        [-0.0224, -0.0241,  0.0160,  ...,  0.0043,  0.0127,  0.0139],
        [ 0.0066,  0.0563, -0.0194,  ..., -0.0271,  0.0341,  0.0103],
        ...,
        [-0.0260,  0.0243,  0.0268,  ...,  0.0056,  0.0243,  0.0144],
        [-0.0085,  0.0253, -0.0175,  ..., -0.0238,  0.0059,  0.0443],
        [ 0.0125, -0.0512, -0.0104,  ...,  0.0175, -0.0119, -0.0345]])

llm.base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0074, -0.0192,  0.0208,  ...,  0.0122,  0.0143,  0.0366],
        [ 0.0003, -0.0162,  0.0153,  ..., -0.0104,  0.0247, -0.0081],
        [ 0.0160, -0.0245, -0.0244,  ...,  0.0142, -0.0168, -0.0098],
        ...,
        [-0.0214,  0.0128,  0.0121,  ..., -0.0098,  0.0047, -0.0090],
        [ 0.0113, -0.0211,  0.0125,  ..., -0.0020,  0.0099,  0.0094],
        [-0.0177,  0.0164,  0.0300,  ...,  0.0177,  0.0036, -0.0161]])

llm.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0325, -0.0772,  0.0302,  ..., -0.0039,  0.0023, -0.0092],
        [ 0.0055,  0.0177, -0.0278,  ..., -0.0050, -0.0039,  0.0371],
        [ 0.0091,  0.0013,  0.0303,  ..., -0.0013,  0.0284,  0.0154],
        ...,
        [ 0.0011, -0.0090,  0.0016,  ..., -0.0636,  0.0215, -0.0535],
        [ 0.0308, -0.0014, -0.0011,  ..., -0.0155,  0.0119, -0.0020],
        [-0.0659,  0.0167,  0.0179,  ..., -0.0208,  0.0426, -0.0298]])

llm.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0486,  0.0096, -0.0366,  ..., -0.0701,  0.0351,  0.0200],
        [-0.0225, -0.0186,  0.0090,  ..., -0.0315,  0.0202,  0.0209],
        [ 0.0188,  0.0574, -0.0127,  ...,  0.0249, -0.0207, -0.0080],
        ...,
        [ 0.0136,  0.0258,  0.0125,  ..., -0.0608,  0.0294,  0.0263],
        [ 0.0265,  0.0481, -0.0223,  ..., -0.0229,  0.0119, -0.0188],
        [ 0.0366,  0.0376,  0.0125,  ...,  0.0022, -0.0038, -0.0001]])

llm.base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0024, -0.0015, -0.0167,  ..., -0.0075, -0.0069, -0.0103],
        [-0.0051,  0.0021,  0.0057,  ..., -0.0007,  0.0160,  0.0027],
        [-0.0104,  0.0250, -0.0053,  ...,  0.0165, -0.0061, -0.0118],
        ...,
        [ 0.0104,  0.0066, -0.0067,  ...,  0.0217, -0.0210,  0.0129],
        [ 0.0048, -0.0065, -0.0053,  ..., -0.0143, -0.0097,  0.0272],
        [ 0.0151, -0.0026,  0.0007,  ...,  0.0010, -0.0028,  0.0043]])

llm.base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0933,  0.0117,  0.0626,  ..., -0.0057,  0.0431, -0.0227],
        [ 0.0311,  0.0476,  0.1068,  ..., -0.0522, -0.0024,  0.0354],
        [ 0.0548, -0.0083, -0.0102,  ...,  0.0003, -0.0338,  0.0129],
        ...,
        [-0.0185,  0.0558,  0.0452,  ...,  0.0429, -0.0651, -0.0909],
        [-0.1021,  0.0428, -0.0785,  ..., -0.0466, -0.0400,  0.0318],
        [-0.0716,  0.0114, -0.0068,  ...,  0.0896, -0.0790, -0.1129]])

llm.base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0332,  0.0003,  0.0033,  ..., -0.0043,  0.0062,  0.0169],
        [ 0.0504, -0.0427,  0.0081,  ...,  0.0043, -0.0255,  0.0387],
        [ 0.0222,  0.0029,  0.0026,  ...,  0.0141, -0.0097, -0.0018],
        ...,
        [-0.0350,  0.0133,  0.0107,  ..., -0.0285,  0.0055,  0.0372],
        [-0.0070,  0.0034, -0.0041,  ..., -0.0039, -0.0435,  0.0205],
        [-0.0265,  0.0200, -0.0574,  ..., -0.0188,  0.0525,  0.0223]])

llm.base_model.model.model.layers.19.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0195,  0.0227,  0.0160,  ..., -0.0115, -0.0027, -0.0043],
        [ 0.0087, -0.0371,  0.0220,  ...,  0.0160,  0.0006, -0.0139],
        [ 0.0238, -0.0059,  0.0201,  ...,  0.0259,  0.0025, -0.0160],
        ...,
        [ 0.0200,  0.0259, -0.0166,  ...,  0.0249, -0.0047, -0.0188],
        [ 0.0108,  0.0033,  0.0016,  ..., -0.0245,  0.0109, -0.0085],
        [ 0.0199,  0.0069,  0.0043,  ..., -0.0082, -0.0051,  0.0089]])

llm.base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0090,  0.0369, -0.0329,  ...,  0.0138, -0.0306, -0.0276],
        [ 0.0090, -0.0298,  0.0578,  ...,  0.0406,  0.0442, -0.0823],
        [-0.0086,  0.0046,  0.0372,  ...,  0.0477, -0.0009,  0.0307],
        ...,
        [ 0.0973,  0.0635,  0.0012,  ...,  0.0251, -0.0267,  0.0506],
        [-0.0102, -0.0135, -0.0401,  ..., -0.0025,  0.0118,  0.0368],
        [-0.0218, -0.0191,  0.0398,  ...,  0.0347,  0.0332,  0.0517]])

llm.base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0011,  0.0354, -0.0273,  ..., -0.0119, -0.0114, -0.0291],
        [ 0.0380,  0.0009,  0.0146,  ..., -0.0056, -0.0393,  0.0105],
        [ 0.0277, -0.0023,  0.0461,  ..., -0.0261, -0.0523, -0.0445],
        ...,
        [-0.0227,  0.0216, -0.0294,  ..., -0.0382,  0.0037, -0.0024],
        [-0.0260, -0.0378, -0.0048,  ...,  0.0086,  0.0418,  0.0164],
        [-0.0206,  0.0101, -0.0360,  ...,  0.0394,  0.0468,  0.0157]])

llm.base_model.model.model.layers.19.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[-0.0039, -0.0098, -0.0065,  ..., -0.0278,  0.0189,  0.0114],
        [ 0.0131, -0.0320,  0.0034,  ...,  0.0124, -0.0048,  0.0250],
        [ 0.0006,  0.0104, -0.0437,  ...,  0.0029,  0.0061,  0.0132],
        ...,
        [ 0.0047,  0.0011,  0.0009,  ..., -0.0320,  0.0233, -0.0106],
        [-0.0154, -0.0209,  0.0157,  ...,  0.0127,  0.0044,  0.0137],
        [ 0.0011, -0.0103, -0.0160,  ...,  0.0086,  0.0095,  0.0056]])

llm.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[-0.0037, -0.0052,  0.0160,  ...,  0.0257,  0.0384,  0.0230],
        [-0.0054, -0.0261,  0.0286,  ..., -0.0122, -0.0364, -0.0553],
        [-0.0024,  0.0033,  0.0268,  ..., -0.0360,  0.0120, -0.0135],
        ...,
        [ 0.0208, -0.0159, -0.0154,  ..., -0.0127,  0.0028, -0.0009],
        [ 0.0289, -0.0430, -0.0450,  ..., -0.0030, -0.0254,  0.0223],
        [ 0.0267,  0.0356,  0.0180,  ...,  0.0137, -0.0293, -0.0325]])

llm.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0144, -0.0305, -0.0201,  ..., -0.0341, -0.0013,  0.0182],
        [-0.0276,  0.0059, -0.0012,  ...,  0.0163,  0.0330, -0.0145],
        [-0.0256, -0.0462, -0.0229,  ..., -0.0007,  0.0110, -0.0175],
        ...,
        [-0.0147,  0.0261,  0.0063,  ...,  0.0582,  0.0375, -0.0139],
        [ 0.0008, -0.0081, -0.0498,  ..., -0.0092, -0.0199,  0.0092],
        [ 0.0316, -0.0182, -0.0361,  ..., -0.0149, -0.0336,  0.0112]])

llm.base_model.model.model.layers.19.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.6172, 0.8867, 2.0312,  ..., 0.5859, 1.2734, 1.2969])

llm.base_model.model.model.layers.19.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.2266, 1.1484, 1.8047,  ..., 0.7617, 1.3594, 1.3516])

llm.base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0030,  0.0011,  0.0108,  ..., -0.0082,  0.0146,  0.0105],
        [-0.0004,  0.0056,  0.0057,  ..., -0.0330, -0.0007,  0.0014],
        [-0.0160, -0.0035, -0.0071,  ..., -0.0017, -0.0043, -0.0204],
        ...,
        [-0.0193,  0.0068, -0.0048,  ...,  0.0131, -0.0221, -0.0442],
        [-0.0073, -0.0378, -0.0061,  ..., -0.0219, -0.0059,  0.0049],
        [-0.0057,  0.0078, -0.0172,  ...,  0.0126,  0.0234,  0.0205]])

llm.base_model.model.model.layers.20.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.0747, -0.2695, -0.8477,  ...,  0.1504,  0.3848,  0.5664])

llm.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0166,  0.0449,  0.0155,  ...,  0.0337, -0.0289, -0.0341],
        [ 0.0071,  0.0023,  0.0341,  ..., -0.0135, -0.0159, -0.0134],
        [ 0.0750, -0.0056, -0.0317,  ...,  0.0717, -0.0202, -0.0214],
        ...,
        [-0.0059, -0.0348,  0.0316,  ..., -0.0353,  0.0648, -0.0075],
        [ 0.0618, -0.0151,  0.0448,  ...,  0.0267,  0.0153,  0.0325],
        [ 0.0218,  0.0465, -0.0288,  ..., -0.0137,  0.0042,  0.0472]])

llm.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0380,  0.0242, -0.0094,  ...,  0.0570, -0.0207, -0.0246],
        [ 0.0269, -0.0112,  0.0098,  ..., -0.0178,  0.0240, -0.0310],
        [ 0.0080, -0.0019, -0.0163,  ..., -0.0189,  0.0081, -0.0249],
        ...,
        [ 0.0265, -0.0010, -0.0143,  ..., -0.0142,  0.0424,  0.0108],
        [-0.0009,  0.0117, -0.0243,  ...,  0.0019, -0.0093, -0.0240],
        [-0.0169, -0.0176, -0.0309,  ..., -0.0081, -0.0359,  0.0008]])

llm.base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0055, -0.0100, -0.0165,  ...,  0.0054,  0.0030, -0.0052],
        [-0.0058,  0.0068,  0.0167,  ...,  0.0033,  0.0129,  0.0223],
        [ 0.0153, -0.0104,  0.0044,  ...,  0.0099, -0.0054, -0.0140],
        ...,
        [-0.0068, -0.0006,  0.0046,  ...,  0.0081,  0.0031, -0.0047],
        [-0.0249, -0.0107, -0.0093,  ..., -0.0275, -0.0295, -0.0040],
        [-0.0065,  0.0153,  0.0051,  ..., -0.0050, -0.0039, -0.0140]])

llm.base_model.model.model.layers.20.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-1.1094e+00, -7.8125e-02, -7.0703e-01,  3.3594e-01, -4.8438e-01,
         5.0391e-01, -6.2988e-02,  1.7383e-01,  1.6797e+00,  1.3867e-01,
         2.3633e-01, -1.5137e-01, -7.8125e-02, -2.8906e-01, -1.7578e-01,
        -1.0645e-01,  3.6523e-01,  2.2949e-01, -2.0898e-01, -1.1084e-01,
         1.2793e-01, -3.6719e-01,  4.9805e-01,  3.7500e-01, -1.8848e-01,
         2.2500e+00,  6.7749e-03,  4.8340e-02,  2.4844e+00, -1.2390e-02,
        -1.8677e-02,  6.7383e-02,  1.9775e-02, -2.9785e-02, -1.5234e-01,
        -5.6641e-02, -3.1445e-01, -1.7773e-01,  8.8867e-02,  1.7188e-01,
         5.6885e-02, -5.8899e-03, -4.0820e-01,  2.0938e+00, -2.2583e-03,
         1.3574e-01, -1.7969e-01,  1.1816e-01,  3.4375e-01, -1.8359e-01,
         2.1250e+00, -2.9297e-01,  1.8945e-01, -4.7070e-01,  3.5352e-01,
         5.2246e-02, -9.2969e-01, -5.0293e-02, -4.1992e-01,  1.1953e+00,
        -2.8125e-01,  1.5078e+00, -1.9297e+00, -2.9297e-01,  1.7188e+00,
        -5.5078e-01, -5.5078e-01,  1.5527e-01,  1.4766e+00,  2.8906e-01,
        -2.5269e-02, -1.5918e-01,  8.5449e-02,  1.4844e-01,  6.5918e-02,
        -8.2031e-02, -1.6797e+00, -1.8164e-01, -2.0142e-02, -1.5547e+00,
         6.0547e-02,  1.0498e-02, -1.9375e+00,  3.5156e-01,  1.3281e-01,
         8.1543e-02,  1.3867e-01,  1.7383e-01, -4.0039e-02, -4.4922e-01,
        -1.6406e-01, -1.7334e-02, -3.2031e-01, -4.4556e-03, -8.1055e-02,
         1.1133e-01, -8.7402e-02,  2.1582e-01, -9.9121e-02,  5.1270e-02,
        -2.9541e-02, -2.5781e+00, -4.1992e-02, -1.4465e-02, -3.6865e-02,
         1.1475e-01, -2.9883e-01, -3.6328e-01, -1.1133e-01,  3.5889e-02,
        -4.4531e-01,  2.8125e-01, -9.2773e-02, -1.0757e-03,  1.9219e+00,
        -7.5391e-01,  6.0303e-02,  1.3367e-02, -5.9326e-02, -3.6719e-01,
        -5.6250e-01,  3.3008e-01,  4.6484e-01,  5.3125e-01, -1.7578e-01,
         1.1279e-01,  4.3125e+00,  4.5312e-01, -6.9141e-01, -4.8584e-02,
         4.6094e-01, -2.6855e-03,  6.7969e-01, -2.6758e-01,  3.0859e-01,
        -4.1748e-02, -1.4099e-02,  6.4844e-01, -5.2002e-02, -6.8970e-03,
        -4.1211e-01,  1.2891e-01, -9.4531e-01,  4.6484e-01, -7.6172e-02,
         3.7109e-02, -4.4531e-01,  3.6377e-02, -3.4570e-01, -1.7578e+00,
        -2.1729e-02, -7.9297e-01,  1.5625e+00,  6.8359e-02, -1.4648e-01,
        -4.0430e-01, -2.6367e-01, -3.6914e-01, -2.3633e-01, -6.6016e-01,
         1.6328e+00,  5.2979e-02, -2.7344e-01, -1.4844e-01, -1.6641e+00,
         1.2390e-02,  1.4453e-01,  3.1250e-01, -8.8379e-02,  4.5410e-02,
        -1.3489e-02, -4.3945e-01, -3.6133e-01,  1.5039e-01,  6.4453e-02,
        -1.2354e-01,  1.9727e-01, -1.0254e-01,  2.5391e-01, -2.0508e-01,
        -5.4199e-02, -1.8457e-01, -4.5898e-02, -3.2471e-02, -3.3203e-02,
         6.2109e-01,  2.1680e-01, -5.0781e-01,  1.8281e+00, -1.5938e+00,
         7.4609e-01, -3.9062e+00, -4.3359e-01,  4.1602e-01,  5.3516e-01,
         4.9072e-02, -5.7031e-01,  5.1514e-02, -1.2207e-01,  1.1484e+00,
        -1.2158e-01,  2.2363e-01, -3.6621e-02, -1.3125e+00, -5.0537e-02,
        -2.6758e-01,  2.7930e-01,  1.2109e+00, -3.6719e-01,  2.8906e-01,
        -1.3125e+00, -2.1484e-01, -6.3171e-03,  5.3516e-01,  1.4453e-01,
         1.1816e-01, -5.0781e-01, -1.9653e-02,  1.8066e-01, -1.9609e+00,
         1.5039e-01, -3.3789e-01, -3.7354e-02, -9.3262e-02, -4.5508e-01,
         4.6631e-02, -1.6797e-01,  1.0400e-01,  1.1328e+00,  4.6997e-03,
         4.2578e-01,  1.2512e-02, -1.6797e-01, -1.4062e-01,  1.7656e+00,
        -1.7188e-01, -1.0205e-01,  1.1108e-02, -2.0703e-01,  1.8457e-01,
         3.1836e-01,  3.6719e+00,  1.5820e-01, -2.1289e-01,  3.5645e-02,
        -2.5977e-01, -1.6895e-01,  2.8125e-01,  3.5938e-01, -6.2500e-01,
         8.1641e-01, -3.5547e-01, -5.0391e-01,  4.4922e-01,  1.0156e+00,
         3.4531e+00,  6.9141e-01,  8.8672e-01, -1.8262e-01,  3.7891e-01,
        -3.0273e-01,  2.1777e-01, -2.5513e-02,  6.7188e-01, -8.7891e-02,
         7.4219e-02,  1.2344e+00, -1.1084e-01, -5.2344e-01, -1.0791e-01,
        -1.8750e-01,  1.1523e-01,  1.4160e-01, -1.2656e+00,  3.7891e-01,
        -1.5747e-02,  4.7266e-01, -6.1035e-02,  5.6396e-02, -3.3398e-01,
        -5.8350e-02,  5.0391e-01, -2.8687e-02, -7.4609e-01,  1.2354e-01,
         6.6406e-02, -8.5938e-02,  2.4531e+00, -2.9602e-03,  7.0312e-02,
         9.0820e-02,  3.5645e-02,  3.2031e-01, -1.3379e-01, -1.4551e-01,
        -8.9355e-02,  1.9922e-01, -1.3965e-01, -6.6406e-02,  4.3555e-01,
        -5.0781e-02, -1.8828e+00, -2.4121e-01, -2.9883e-01, -2.6758e-01,
         1.6406e-01, -2.5156e+00, -2.6172e-01, -3.3594e-01, -1.0469e+00,
        -1.1016e+00, -8.8281e-01, -6.0156e-01, -3.1250e-01,  3.5547e-01,
         5.5859e-01, -2.2168e-01,  9.5703e-02,  1.4219e+00,  4.4141e-01,
        -1.6406e-01,  1.3867e-01, -1.5039e-01,  7.9688e-01, -2.4805e-01,
         6.5625e-01,  1.4648e-01, -5.3125e-01, -1.6992e-01, -1.5820e-01,
         2.0898e-01,  3.9062e-02, -2.1582e-01,  9.1309e-02, -1.2578e+00,
        -1.8457e-01, -2.8809e-02,  4.0820e-01,  9.5703e-02,  6.9336e-02,
         1.7344e+00, -1.1523e-01, -3.6328e-01, -1.0625e+00,  4.2236e-02,
         3.4180e-01, -6.6406e-02, -1.9375e+00,  8.3496e-02, -4.1016e-02,
         2.3438e-02, -4.6094e-01,  2.6562e-01, -3.4180e-02, -5.2979e-02,
        -1.0791e-01, -9.3262e-02, -3.8818e-02, -3.0518e-02, -3.1719e+00,
         8.1543e-02,  7.3730e-02, -2.7734e-01,  1.0742e-01, -2.0410e-01,
         6.6895e-02, -3.0664e-01, -3.5156e-02, -2.3340e-01, -4.0430e-01,
         2.5000e+00,  8.7891e-02, -1.5820e-01,  8.7891e-01,  8.6328e-01,
         3.4375e-01,  3.2812e-01, -1.0596e-01,  3.5400e-02, -7.5781e-01,
         2.4805e-01,  2.4658e-02,  7.2266e-01,  7.6172e-01, -7.4219e-01,
         2.2461e-01, -6.1719e-01,  5.1270e-02,  3.8281e-01,  1.3574e-01,
         5.5078e-01, -1.5625e-01,  1.6406e-01,  4.3945e-02,  5.5420e-02,
        -2.7734e-01,  8.9844e-01,  2.5977e-01, -2.5781e-01, -5.6152e-03,
         1.6016e-01, -1.1084e-01, -1.6968e-02,  4.9023e-01,  3.8867e-01,
        -2.2754e-01, -4.0039e-01,  1.3750e+00, -1.8359e-01, -3.7305e-01,
         1.8750e-01, -1.6016e+00,  1.6113e-01, -9.1553e-03, -3.0859e-01,
         2.1406e+00, -1.1328e-01,  6.7871e-02, -5.0000e-01, -9.5703e-02,
        -9.5215e-02,  3.4961e-01,  5.1953e-01,  1.1279e-01,  5.7129e-02,
         5.5469e-01,  1.5442e-02, -1.8262e-01,  9.9609e-02,  3.9648e-01,
        -5.2344e-01, -3.4766e-01, -3.0078e-01, -2.6367e-01, -3.3906e+00,
         6.9141e-01, -7.8613e-02, -1.5312e+00, -1.3281e-01, -2.8516e-01,
         1.1328e-01, -7.0312e-01, -8.7500e-01, -1.8555e-01,  3.6328e-01,
         2.3730e-01,  8.4766e-01, -1.2344e+00, -4.3750e-01, -1.5820e-01,
        -5.7812e-01, -3.4375e-01,  2.6953e-01, -1.8164e-01, -1.0889e-01,
        -2.2217e-02, -8.8672e-01, -3.6914e-01, -1.3672e-01, -3.7305e-01,
         8.7402e-02,  2.9883e-01, -5.7422e-01,  2.3438e-02,  1.3086e-01,
        -1.0859e+00, -1.3086e-01,  9.1016e-01, -7.6660e-02,  1.4844e-01,
         2.6367e-01, -4.8828e-01,  7.3242e-02, -1.0391e+00, -1.6016e-01,
         2.5195e-01, -1.9434e-01, -1.3281e-01,  2.1582e-01, -7.2656e-01,
         1.5918e-01, -2.4316e-01,  1.0938e+00,  1.0645e-01,  7.5000e-01,
         4.8438e-01, -2.1094e-01, -1.9434e-01, -5.4688e-02,  3.2812e+00,
        -1.0547e-01, -1.8799e-02,  1.4453e-01,  3.1250e-01,  2.5977e-01,
         1.6113e-02, -1.6724e-02,  2.1680e-01,  1.7188e+00, -2.9688e-01,
        -1.8066e-02,  1.0469e+00,  1.3550e-02, -1.1875e+00,  6.8848e-02,
         8.6328e-01,  6.4062e-01, -4.3555e-01,  2.1289e-01, -2.9102e-01,
        -7.1777e-02,  3.5547e-01])

llm.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0219, -0.0280, -0.0195,  ...,  0.0127, -0.0394,  0.0093],
        [-0.0067,  0.0740, -0.0645,  ..., -0.0147,  0.0214, -0.0305],
        [-0.0146, -0.0373,  0.0164,  ...,  0.0550,  0.0051, -0.0180],
        ...,
        [-0.0226,  0.0339, -0.0142,  ..., -0.0524,  0.0307,  0.0363],
        [-0.0139, -0.0267,  0.0262,  ...,  0.0018, -0.0407, -0.0040],
        [ 0.0071, -0.0108,  0.0013,  ..., -0.0179,  0.0295,  0.0552]])

llm.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0140,  0.0486,  0.0042,  ..., -0.0337,  0.0249,  0.0112],
        [ 0.0113,  0.0095,  0.0143,  ...,  0.0222,  0.0221, -0.0213],
        [-0.0275, -0.0668,  0.0345,  ..., -0.0173, -0.0053,  0.0033],
        ...,
        [-0.0100, -0.0276, -0.0082,  ...,  0.0065, -0.0022, -0.0352],
        [ 0.0346, -0.0104,  0.0214,  ..., -0.0093, -0.0072, -0.0120],
        [-0.0120,  0.0077, -0.0031,  ..., -0.0259,  0.0018, -0.0214]])

llm.base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0153, -0.0016,  0.0214,  ..., -0.0068,  0.0106,  0.0232],
        [ 0.0117, -0.0135,  0.0115,  ..., -0.0134,  0.0025,  0.0011],
        [ 0.0049, -0.0091, -0.0109,  ..., -0.0004, -0.0085,  0.0396],
        ...,
        [ 0.0176,  0.0201, -0.0054,  ..., -0.0005,  0.0055, -0.0014],
        [ 0.0226,  0.0042,  0.0044,  ..., -0.0053,  0.0011,  0.0036],
        [ 0.0206, -0.0104,  0.0064,  ...,  0.0108, -0.0110,  0.0166]])

llm.base_model.model.model.layers.20.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-1.3965e-01,  1.9409e-02,  1.9653e-02, -2.5586e-01,  5.0293e-02,
        -2.2217e-02,  5.7617e-02, -4.7363e-02, -9.1797e-02,  1.8457e-01,
         1.8262e-01, -1.4941e-01, -1.5039e-01,  1.6016e-01, -1.0693e-01,
        -2.1875e-01, -1.2061e-01, -6.8359e-02, -4.2236e-02,  1.4551e-01,
         1.9824e-01,  1.4221e-02, -8.7500e-01,  2.8125e-01, -1.5918e-01,
         1.5234e-01, -2.4780e-02,  2.4609e-01,  1.7188e-01, -1.0840e-01,
         2.9419e-02,  7.4463e-03, -5.7983e-03,  4.4922e-02, -7.6660e-02,
        -1.0840e-01,  1.0742e-01,  1.2085e-02, -1.3867e-01,  4.8340e-02,
         3.0664e-01,  1.3733e-02,  1.6895e-01,  4.1504e-02,  8.4473e-02,
         2.9883e-01,  6.0059e-02,  7.0190e-03,  2.0142e-02,  9.8145e-02,
         8.8867e-02, -1.2695e-02, -2.1289e-01,  2.2168e-01, -5.4199e-02,
        -1.4551e-01,  4.6875e-02,  1.0693e-01, -5.3467e-02, -6.9336e-02,
         2.0508e-02,  8.9844e-02,  1.4160e-02, -1.0107e-01, -2.1680e-01,
        -7.1716e-03, -8.7402e-02, -2.7930e-01, -3.0664e-01, -2.2461e-01,
         1.5039e-01, -2.6758e-01, -1.2012e-01,  3.8086e-02,  9.9609e-02,
        -1.2598e-01,  5.5908e-02, -2.0020e-01, -1.2061e-01,  1.8066e-01,
         5.4688e-02,  9.1797e-02,  5.5664e-02, -4.5898e-02, -6.2988e-02,
        -3.1641e-01,  1.5527e-01,  6.6016e-01, -1.0449e-01, -1.2451e-01,
         2.2949e-01,  5.9204e-03, -1.7188e-01,  1.6895e-01,  7.6172e-02,
         6.4453e-02,  6.4453e-02, -1.1621e-01,  2.6562e-01, -1.2695e-01,
        -2.6758e-01,  1.5039e-01,  1.4648e-01, -2.6562e-01, -4.7363e-02,
        -1.5820e-01, -7.2754e-02,  8.3496e-02, -9.7656e-02,  1.7480e-01,
         4.0527e-02, -2.5781e-01, -1.7480e-01,  1.1572e-01, -9.9609e-02,
        -1.7188e-01,  1.0254e-01, -2.6172e-01,  7.4463e-03,  5.7861e-02,
         1.8188e-02,  2.2168e-01, -3.4766e-01, -1.4258e-01,  1.6992e-01,
        -1.4746e-01,  2.3242e-01,  1.0449e-01, -2.0294e-03,  7.3242e-02,
        -5.2246e-02,  6.2500e-02, -5.9082e-02, -2.8687e-02, -1.2024e-02,
        -1.8433e-02, -1.3000e-02, -5.2246e-02,  1.2695e-02, -2.3438e-02,
         1.4465e-02,  2.6107e-05,  1.0452e-03,  9.0332e-02, -3.8574e-02,
        -8.4839e-03, -1.4587e-02,  4.5410e-02, -2.6855e-02, -2.7100e-02,
        -1.2817e-02,  9.8145e-02,  1.4343e-02,  6.1279e-02, -4.5410e-02,
         2.9907e-02, -4.9133e-03, -1.9531e-02,  8.4961e-02, -1.2131e-03,
        -2.0996e-02, -2.1210e-03,  3.6133e-02,  8.9355e-02,  9.7168e-02,
         5.3955e-02,  2.5146e-02,  1.4709e-02, -1.0803e-02,  2.3535e-01,
         2.6489e-02,  6.8359e-02,  4.6997e-03,  6.0791e-02,  4.5166e-02,
         8.4473e-02,  2.7954e-02, -1.8311e-02,  6.1768e-02, -9.0332e-03,
         3.1494e-02,  7.6172e-02, -1.0620e-02, -3.9795e-02, -1.0010e-02,
        -2.4170e-02,  1.0498e-01, -1.7822e-02, -8.4473e-02, -4.1016e-02,
         1.9531e-02, -6.3965e-02,  2.4414e-02, -6.3477e-02, -4.0820e-01,
         1.2878e-02, -4.5898e-02,  6.9824e-02, -6.5430e-02,  2.0752e-02,
         7.6599e-03,  2.3926e-02, -8.9355e-02,  6.6833e-03, -2.4170e-02,
         7.7209e-03,  6.3171e-03, -6.8359e-02, -3.5645e-02, -2.1729e-02,
        -2.7222e-02,  1.7090e-02, -4.0820e-01, -2.1362e-02, -5.0537e-02,
         3.4424e-02, -4.8584e-02, -3.1738e-02,  3.7354e-02, -5.5664e-02,
        -7.1411e-03,  8.9355e-02,  1.2024e-02,  1.8921e-02,  3.7109e-02,
        -4.0283e-02,  3.5889e-02, -1.0803e-02, -6.0303e-02,  7.4219e-02,
        -7.0312e-02, -1.5411e-03,  1.2878e-02, -3.3691e-02, -2.6611e-02,
         6.8848e-02, -2.1729e-02, -1.7944e-02,  3.0518e-02, -3.9062e-02,
        -1.9287e-02,  1.3184e-02,  3.8818e-02, -6.0059e-02,  5.9814e-02,
         4.1504e-02,  6.5430e-02, -3.1006e-02, -3.6865e-02, -2.1118e-02,
         1.7853e-03,  4.7363e-02, -4.0588e-03,  7.4707e-02,  3.3984e-01,
        -1.7944e-02, -2.0142e-02, -2.4536e-02,  1.4282e-02, -3.1738e-02,
        -1.3855e-02,  3.8452e-03, -5.8350e-02, -2.1973e-02, -3.2715e-02,
        -2.5146e-02, -3.7842e-02, -1.5625e-02,  1.4832e-02, -5.4121e-05,
         1.4648e-02,  9.0942e-03, -7.9727e-04,  3.1250e-02,  4.6692e-03,
         1.3245e-02,  4.3945e-02,  2.1118e-02,  4.0283e-02,  5.7861e-02,
        -4.6082e-03,  6.7383e-02, -9.0942e-03,  1.9409e-02, -1.2085e-02,
        -9.2773e-03,  2.1118e-02,  3.4424e-02,  5.0659e-03,  2.5269e-02,
         8.7891e-03,  7.5073e-03, -4.8218e-03, -1.4404e-02, -1.4587e-02,
        -3.5889e-02, -9.3994e-03, -2.3560e-02, -1.7456e-02, -2.0386e-02,
         4.1016e-02,  1.4587e-02, -2.6001e-02,  6.6376e-04,  2.3682e-02,
         4.1748e-02, -2.6001e-02, -1.8799e-02,  3.2471e-02,  1.3672e-02,
         4.5471e-03, -6.4392e-03,  3.1738e-02, -2.1118e-02,  2.5757e-02,
        -6.8665e-03, -3.3691e-02,  3.6133e-02,  4.5654e-02, -3.8574e-02,
         2.4658e-02,  2.9883e-01, -1.5869e-02,  1.7090e-01,  5.4626e-03,
         4.8340e-02,  1.9043e-02,  4.5654e-02, -7.3242e-03,  5.1025e-02,
         2.4292e-02,  2.1973e-02, -2.6611e-02,  1.9653e-02, -1.2451e-02,
         9.6191e-02, -1.7090e-02,  8.4229e-03, -4.0588e-03,  4.5166e-02,
        -2.7832e-02,  2.4567e-03, -4.9438e-03,  2.4292e-02,  1.9287e-02,
        -3.4912e-02, -3.1055e-01,  4.2236e-02, -2.6245e-02,  3.8330e-02,
         2.1118e-02,  1.8799e-02,  8.3008e-03,  3.0365e-03,  2.7222e-02,
         1.0376e-02,  3.8086e-02, -2.1484e-02,  4.4531e-01, -4.9561e-02,
        -2.5024e-02, -2.8381e-03,  6.3171e-03,  1.2695e-02, -1.0193e-02,
         6.7444e-03,  2.2125e-03, -3.8300e-03,  6.4697e-03, -1.3086e-01,
        -4.6997e-03, -1.5503e-02, -1.3672e-02, -2.3315e-02,  1.2390e-02,
         1.0681e-02,  2.8198e-02,  2.0996e-02, -1.0864e-02, -3.7598e-02,
        -2.6001e-02, -2.4414e-02,  3.0273e-02,  1.3000e-02, -7.0801e-02,
        -1.6113e-02, -5.8105e-02,  2.2705e-02,  1.8387e-03, -2.0264e-02,
        -4.1504e-02, -2.7344e-02, -1.4210e-04,  6.2500e-02, -8.7891e-03,
        -5.7373e-03,  5.2490e-02,  6.4392e-03,  1.7456e-02, -5.5664e-02,
         2.4902e-02, -3.3789e-01, -1.0254e-02, -2.2095e-02, -1.6602e-02,
        -6.9336e-02, -4.5776e-03,  1.0742e-02,  4.2725e-02,  1.1328e-01,
        -3.7598e-02, -3.1982e-02,  3.4668e-02,  1.0925e-02,  1.2817e-02,
        -9.3384e-03,  2.0508e-02,  5.9814e-02,  2.0386e-02, -1.3916e-02,
         5.2734e-02, -1.2207e-01, -3.1128e-02, -2.2461e-02, -9.4604e-03,
        -4.8828e-02, -2.5635e-02,  3.3264e-03, -3.0273e-02,  5.6641e-02,
         8.0078e-02,  1.3367e-02, -4.1992e-02,  5.7861e-02, -5.5176e-02,
         1.6602e-02,  6.9275e-03,  6.3477e-02,  2.1484e-01,  1.8799e-02,
         2.7100e-02, -1.3855e-02, -2.0142e-02, -4.6143e-02, -6.5308e-03,
         3.1738e-02, -1.5991e-02,  7.8125e-02, -1.6724e-02,  5.0659e-03,
        -5.4688e-02,  2.9785e-02, -5.1270e-02, -7.6172e-02, -4.1504e-02,
         4.1504e-03, -6.8848e-02, -2.1606e-02,  2.4605e-04,  5.3711e-02,
         2.7710e-02,  1.5793e-03, -8.3008e-03, -4.8828e-03, -2.3560e-02,
         1.9775e-02, -2.6093e-03, -1.2024e-02,  4.8340e-02, -3.9307e-02,
        -1.5076e-02, -1.0059e-01,  2.3071e-02,  9.9487e-03, -1.3855e-02,
         1.1279e-01, -3.2715e-02,  7.4219e-02,  1.8652e-01, -7.7148e-02,
         6.0059e-02, -4.8340e-02, -7.3624e-04,  1.5991e-02, -3.1128e-02,
        -3.3203e-02, -8.0566e-03, -1.0376e-02, -2.6245e-02,  2.9419e-02,
         3.2043e-03,  3.8574e-02, -1.3977e-02,  1.2390e-02,  2.7222e-02,
         7.9590e-02, -6.8359e-02, -5.8105e-02,  2.7710e-02,  2.5757e-02,
         6.9885e-03, -3.3447e-02,  4.6875e-02, -1.2878e-02,  2.9419e-02,
         2.6855e-02, -3.4668e-02, -2.2949e-02, -1.8677e-02, -1.7578e-02,
         2.3438e-02,  1.0986e-02])

llm.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0221, -0.0293, -0.0433,  ..., -0.0357, -0.0064, -0.0241],
        [-0.1059, -0.0310,  0.0032,  ...,  0.0030, -0.0006, -0.0158],
        [ 0.0201,  0.0074,  0.0044,  ...,  0.0327, -0.0370,  0.0188],
        ...,
        [-0.0041,  0.0289,  0.0152,  ...,  0.0355, -0.0119,  0.0388],
        [ 0.0287,  0.0240,  0.0136,  ..., -0.0035, -0.0317,  0.0724],
        [ 0.0399, -0.0365, -0.0012,  ...,  0.0240, -0.0160,  0.0375]])

llm.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0608, -0.0143,  0.0091,  ..., -0.0190, -0.0171,  0.0092],
        [-0.0089, -0.0175,  0.0223,  ...,  0.0336, -0.0072, -0.0517],
        [-0.0073,  0.0213,  0.0679,  ...,  0.0309,  0.0265,  0.0002],
        ...,
        [ 0.0265,  0.0286, -0.0342,  ...,  0.0068, -0.0078, -0.0152],
        [ 0.0006, -0.0164, -0.0217,  ..., -0.0152, -0.0049, -0.0124],
        [-0.0311, -0.0125, -0.0073,  ...,  0.0109, -0.0035,  0.0228]])

llm.base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0156,  0.0120,  0.0129,  ..., -0.0144, -0.0260, -0.0048],
        [-0.0017,  0.0118, -0.0034,  ...,  0.0065,  0.0026, -0.0145],
        [-0.0166,  0.0110,  0.0142,  ..., -0.0159, -0.0098, -0.0049],
        ...,
        [-0.0125,  0.0046,  0.0025,  ...,  0.0184,  0.0117, -0.0023],
        [-0.0089, -0.0061,  0.0087,  ...,  0.0282,  0.0337,  0.0172],
        [ 0.0152, -0.0281, -0.0142,  ..., -0.0135,  0.0030,  0.0033]])

llm.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0215,  0.0606, -0.0013,  ...,  0.0171, -0.0258, -0.0077],
        [ 0.0180,  0.0481, -0.0326,  ..., -0.0376, -0.0436,  0.0210],
        [ 0.0165, -0.0007,  0.0249,  ..., -0.0661, -0.0072,  0.0025],
        ...,
        [-0.0345,  0.0010, -0.0076,  ...,  0.0102, -0.0163,  0.0251],
        [ 0.0028, -0.0079, -0.0332,  ...,  0.0267,  0.0441,  0.0029],
        [-0.0052,  0.0018, -0.0085,  ...,  0.0267,  0.0067, -0.0126]])

llm.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0195,  0.0276, -0.0085,  ..., -0.0405,  0.0035,  0.0270],
        [-0.0093,  0.0350, -0.0520,  ..., -0.0553, -0.0282,  0.0232],
        [-0.0061, -0.0237,  0.0100,  ...,  0.0204,  0.0247, -0.0057],
        ...,
        [ 0.0212, -0.0209,  0.0032,  ...,  0.0083,  0.0356, -0.0542],
        [ 0.0028, -0.0023, -0.0579,  ..., -0.0248, -0.0072, -0.0129],
        [-0.0048,  0.0205, -0.0100,  ...,  0.0193, -0.0310, -0.0125]])

llm.base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0221,  0.0051,  0.0149,  ..., -0.0050,  0.0036, -0.0009],
        [ 0.0066,  0.0072,  0.0304,  ...,  0.0193, -0.0002, -0.0124],
        [ 0.0127, -0.0356, -0.0072,  ...,  0.0114, -0.0049,  0.0115],
        ...,
        [ 0.0125, -0.0008, -0.0029,  ..., -0.0073, -0.0035, -0.0168],
        [ 0.0090, -0.0136,  0.0302,  ...,  0.0029, -0.0085, -0.0250],
        [-0.0062,  0.0153,  0.0118,  ..., -0.0016,  0.0025,  0.0015]])

llm.base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0437,  0.0364,  0.0152,  ..., -0.0112,  0.0022, -0.0129],
        [ 0.0206, -0.0193, -0.0477,  ..., -0.0125,  0.0718, -0.0167],
        [-0.0202, -0.0033, -0.0268,  ...,  0.0224,  0.0764,  0.0063],
        ...,
        [ 0.0600, -0.0324,  0.0299,  ..., -0.0228,  0.0288,  0.0293],
        [-0.0467,  0.0352,  0.0468,  ..., -0.0449,  0.0660,  0.0437],
        [ 0.0646,  0.0105, -0.0329,  ...,  0.0165,  0.0121,  0.0443]])

llm.base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0088,  0.0322,  0.0148,  ..., -0.0040,  0.0220,  0.0290],
        [-0.0195, -0.0052,  0.0082,  ..., -0.0065,  0.0095,  0.0366],
        [-0.0011, -0.0159, -0.0329,  ..., -0.0301, -0.0410, -0.0293],
        ...,
        [ 0.0043,  0.0318, -0.0089,  ..., -0.0189,  0.0219, -0.0034],
        [-0.0028,  0.0502, -0.0466,  ..., -0.0258, -0.0099, -0.0252],
        [ 0.0598,  0.0072,  0.0100,  ..., -0.0254,  0.0193, -0.0230]])

llm.base_model.model.model.layers.20.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0048,  0.0029, -0.0153,  ...,  0.0025, -0.0205,  0.0176],
        [-0.0121, -0.0034,  0.0066,  ...,  0.0277,  0.0128,  0.0027],
        [-0.0105,  0.0044, -0.0019,  ..., -0.0192, -0.0352, -0.0132],
        ...,
        [ 0.0035,  0.0184, -0.0178,  ..., -0.0001,  0.0027,  0.0094],
        [-0.0019,  0.0067,  0.0095,  ...,  0.0043, -0.0208, -0.0187],
        [ 0.0037,  0.0133, -0.0143,  ..., -0.0118,  0.0014,  0.0080]])

llm.base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0695,  0.1116,  0.0200,  ..., -0.0776,  0.0651,  0.0350],
        [ 0.0219,  0.0643, -0.0252,  ..., -0.0051,  0.0304, -0.0676],
        [ 0.0332, -0.0278, -0.0280,  ...,  0.0411,  0.0007,  0.0086],
        ...,
        [ 0.0550, -0.0024, -0.0201,  ..., -0.0536, -0.0253, -0.0217],
        [ 0.0193,  0.0481, -0.0573,  ...,  0.0498,  0.0138, -0.0349],
        [-0.0712,  0.0405,  0.0467,  ..., -0.1107, -0.0476,  0.0665]])

llm.base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0121, -0.0488,  0.0060,  ..., -0.0431, -0.0218,  0.0095],
        [-0.0106,  0.0278,  0.0207,  ...,  0.0749,  0.0111, -0.0060],
        [ 0.0154,  0.0009,  0.0432,  ...,  0.0057, -0.0264, -0.0113],
        ...,
        [-0.0202,  0.0011, -0.0083,  ..., -0.0195,  0.0123, -0.0062],
        [-0.0132, -0.0149,  0.0364,  ..., -0.0231, -0.0122, -0.0186],
        [-0.0154,  0.0083, -0.0578,  ...,  0.0220,  0.0081,  0.0123]])

llm.base_model.model.model.layers.20.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 0.0104, -0.0219,  0.0079,  ...,  0.0127, -0.0121, -0.0200],
        [ 0.0067,  0.0166, -0.0076,  ...,  0.0248, -0.0269, -0.0132],
        [-0.0064,  0.0022, -0.0078,  ..., -0.0095, -0.0052,  0.0114],
        ...,
        [-0.0051,  0.0276,  0.0104,  ...,  0.0082,  0.0080,  0.0184],
        [-0.0442,  0.0261, -0.0115,  ...,  0.0029, -0.0256,  0.0069],
        [-0.0237, -0.0247, -0.0090,  ...,  0.0045,  0.0063,  0.0015]])

llm.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0059, -0.0687,  0.0138,  ..., -0.0303, -0.0223, -0.0186],
        [ 0.0155, -0.0362, -0.0055,  ..., -0.0343,  0.0002,  0.0215],
        [ 0.0191, -0.0331, -0.0605,  ...,  0.0108,  0.0508, -0.0479],
        ...,
        [ 0.0181,  0.0118,  0.0676,  ..., -0.0042, -0.0095,  0.0043],
        [-0.0329,  0.0864, -0.0062,  ...,  0.0557,  0.0124,  0.0016],
        [ 0.0488, -0.0238, -0.0508,  ...,  0.0030, -0.0157,  0.0237]])

llm.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0169,  0.0400,  0.0318,  ...,  0.0262, -0.0166, -0.0365],
        [ 0.0376,  0.0055, -0.0168,  ..., -0.0674, -0.0173, -0.0211],
        [-0.0230, -0.0014, -0.0002,  ...,  0.0165,  0.0718, -0.0825],
        ...,
        [ 0.0049, -0.0495, -0.0489,  ...,  0.0221, -0.0242,  0.0125],
        [ 0.0076,  0.0145,  0.0034,  ..., -0.0076, -0.0177,  0.0187],
        [-0.0386,  0.0009, -0.0065,  ..., -0.0348,  0.0340, -0.0176]])

llm.base_model.model.model.layers.20.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.4922, 0.9336, 1.8828,  ..., 0.6602, 1.3047, 1.2578])

llm.base_model.model.model.layers.20.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.2500, 1.1953, 1.6875,  ..., 0.8633, 1.3516, 1.3203])

llm.base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0033,  0.0013, -0.0048,  ...,  0.0006, -0.0038,  0.0081],
        [ 0.0140, -0.0073, -0.0378,  ..., -0.0032,  0.0036, -0.0022],
        [ 0.0005, -0.0161, -0.0083,  ...,  0.0054,  0.0008,  0.0011],
        ...,
        [ 0.0085,  0.0077,  0.0270,  ..., -0.0167, -0.0069,  0.0065],
        [-0.0075,  0.0109,  0.0496,  ...,  0.0024,  0.0286, -0.0189],
        [ 0.0272,  0.0094,  0.0160,  ...,  0.0036,  0.0124, -0.0243]])

llm.base_model.model.model.layers.21.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.0369,  0.5000, -0.6914,  ...,  0.0181,  0.4102,  1.5469])

llm.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0022, -0.0129, -0.1220,  ...,  0.0210, -0.0043, -0.0206],
        [-0.0450, -0.0985, -0.0637,  ...,  0.0165, -0.0950,  0.0205],
        [-0.0113, -0.0657,  0.0336,  ...,  0.0114, -0.0160, -0.0167],
        ...,
        [ 0.0293, -0.0118,  0.0852,  ...,  0.0270,  0.0230,  0.0296],
        [ 0.0139,  0.0583, -0.0481,  ...,  0.0476,  0.0953, -0.0450],
        [ 0.0159,  0.0667,  0.0577,  ..., -0.0257,  0.0062, -0.0248]])

llm.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-2.3903e-02,  5.3335e-03, -1.6615e-02,  ..., -8.1428e-03,
          9.7593e-04, -2.7250e-02],
        [ 2.5552e-03, -2.6085e-02,  2.4613e-02,  ...,  1.7541e-02,
          9.5596e-03,  2.6180e-02],
        [ 1.1374e-02,  1.6022e-02, -4.5550e-02,  ..., -3.4534e-02,
         -2.7153e-05,  4.3604e-03],
        ...,
        [ 4.0661e-02,  2.1927e-02, -4.1058e-02,  ..., -2.8509e-02,
          1.0763e-02, -7.8262e-03],
        [-1.8854e-03,  4.0524e-03,  3.7007e-02,  ..., -1.6814e-02,
          3.2188e-02, -1.0336e-02],
        [ 1.7860e-02, -6.0353e-03, -1.4414e-02,  ..., -2.9317e-02,
         -3.0254e-02,  1.9729e-03]])

llm.base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0047,  0.0033,  0.0131,  ..., -0.0042,  0.0085, -0.0059],
        [ 0.0057,  0.0014, -0.0049,  ...,  0.0039,  0.0082,  0.0053],
        [ 0.0116,  0.0081, -0.0078,  ...,  0.0018,  0.0020,  0.0156],
        ...,
        [-0.0154, -0.0171, -0.0175,  ...,  0.0001, -0.0221, -0.0111],
        [-0.0076,  0.0017,  0.0123,  ..., -0.0162, -0.0315,  0.0156],
        [ 0.0123, -0.0087,  0.0059,  ..., -0.0080, -0.0142, -0.0099]])

llm.base_model.model.model.layers.21.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 8.1250e-01, -5.8984e-01,  1.4746e-01,  2.3926e-01,  1.1536e-02,
         3.0469e-01,  1.9434e-01, -1.7676e-01,  6.9922e-01,  4.8828e-02,
        -1.7188e-01, -2.0703e-01, -3.1250e-01,  6.7969e-01, -4.4727e-01,
        -1.7773e-01,  2.0605e-01, -1.9434e-01,  3.6719e-01, -1.4221e-02,
         5.3711e-02,  3.5742e-01, -2.2266e-01,  1.4062e-01, -1.5625e+00,
         1.8262e-01, -3.5742e-01,  2.8687e-02, -4.8242e-01, -2.6562e-01,
         3.5352e-01,  2.0469e+00,  3.3447e-02, -6.0547e-02,  1.0376e-02,
         9.5312e-01, -4.4922e-02, -2.7100e-02, -1.1016e+00,  2.1289e-01,
         2.6758e-01, -2.3125e+00, -2.5635e-02, -2.6953e-01, -1.8066e-01,
        -7.4219e-01,  4.6094e-01, -2.4375e+00,  1.7090e-02,  1.5918e-01,
         9.3750e-02, -3.1250e-01, -2.6367e-01, -1.0205e-01,  1.0547e-01,
         7.7637e-02, -1.0132e-02,  7.8613e-02, -3.1494e-02, -7.1777e-02,
        -5.1562e+00,  3.0469e-01,  3.6621e-02, -1.1377e-01,  4.1797e-01,
         6.5625e-01, -4.7070e-01,  2.2070e-01, -1.0156e+00,  5.8984e-01,
         3.9844e-01,  5.1172e-01, -1.8750e-01,  3.1055e-01,  1.8359e-01,
         1.0781e+00, -6.8750e-01,  3.2471e-02,  4.5703e-01, -1.2695e-01,
        -3.9648e-01,  2.5024e-02,  1.5312e+00, -1.3574e-01, -4.3457e-02,
         8.6719e-01,  1.4453e-01,  3.6328e-01,  5.4297e-01, -3.3008e-01,
        -9.9609e-01,  4.1016e-01, -1.2578e+00,  2.9297e-01,  8.3984e-02,
        -3.1836e-01, -2.4023e-01, -3.2227e-01, -1.1914e-01,  1.9609e+00,
        -9.0332e-02,  2.7148e-01,  1.9922e-01, -1.1279e-01, -1.4453e-01,
         1.1641e+00, -8.4961e-02,  2.0996e-01,  9.6191e-02, -3.5742e-01,
        -4.8828e-01,  2.9688e-01,  1.0107e-01, -4.1992e-01,  2.7148e-01,
        -3.1445e-01,  7.3047e-01,  1.0254e-01,  1.9824e-01,  2.6172e-01,
         4.2725e-02,  5.9326e-02, -2.7539e-01, -4.8828e-01,  3.3594e+00,
         5.1562e-01,  5.6250e-01, -9.6680e-02, -4.4336e-01,  2.9297e-01,
        -5.0781e-02, -1.5332e-01,  1.8555e-01,  4.0234e-01,  5.3955e-02,
        -1.3580e-03, -2.9688e-01, -8.3008e-02,  8.3496e-02, -1.7700e-02,
        -2.3633e-01, -1.3125e+00, -1.9727e-01,  1.3574e-01,  2.5977e-01,
        -1.8047e+00,  1.5259e-02, -2.6001e-02,  5.9766e-01,  6.7871e-02,
        -8.2520e-02, -7.9297e-01, -3.7598e-02, -3.5400e-02,  9.8438e-01,
         2.3145e-01, -1.7969e+00,  1.2598e-01,  1.3086e-01, -1.3574e-01,
         3.9453e-01, -1.9336e-01, -2.2852e-01,  7.2266e-02, -1.6953e+00,
        -1.4038e-02,  9.5703e-02, -2.4609e-01, -5.2734e-01,  2.5195e-01,
         3.9258e-01,  3.2715e-02, -4.2773e-01,  2.4512e-01, -1.5039e-01,
        -6.2500e-02,  6.7871e-02, -5.6885e-02, -1.3184e-01, -7.8583e-04,
         5.0625e+00,  4.3457e-02, -1.8945e-01, -5.3955e-02, -3.8086e-02,
         2.5024e-02, -4.7852e-02,  4.1016e-02,  6.0303e-02, -1.8750e-01,
         9.4141e-01,  4.6631e-02, -9.4727e-02, -3.5938e-01, -4.2969e-02,
        -4.3555e-01, -4.7852e-02,  4.3359e-01,  2.2461e-01,  5.9082e-02,
        -1.0234e+00, -6.5918e-02,  2.5195e-01,  6.0547e-01, -9.7656e-02,
         5.3516e-01, -1.3672e-01,  2.3828e-01,  2.4121e-01,  9.6094e-01,
         1.5820e-01, -9.3359e-01, -9.4238e-02,  9.8633e-02, -8.8867e-02,
        -2.3438e+00,  2.1191e-01, -7.8906e-01, -4.3945e-02, -2.1484e-02,
         5.6641e-01,  1.2451e-01,  4.7266e-01, -1.3125e+00, -8.8501e-03,
        -7.2266e-02, -1.2598e-01, -7.7637e-02,  1.4609e+00, -4.0527e-02,
         2.2168e-01, -2.6172e-01, -6.3965e-02,  1.1875e+00,  2.2583e-02,
         7.9102e-02, -9.4604e-03,  5.4932e-02, -1.9824e-01,  1.3770e-01,
        -7.5378e-03, -9.2285e-02,  1.9922e-01, -9.2285e-02, -1.6484e+00,
         1.3855e-02,  1.6016e-01,  7.2754e-02,  7.9590e-02, -5.7617e-02,
        -7.8125e-03,  3.2422e-01, -2.0703e-01,  3.6621e-02, -1.9219e+00,
         2.0508e-01,  2.6406e+00,  8.1250e-01, -3.6719e-01,  1.3359e+00,
        -5.3125e-01, -4.1797e-01,  1.4844e+00, -1.0352e-01, -2.5586e-01,
         2.6953e-01,  7.4707e-02,  1.7090e-01, -2.0020e-02, -7.6562e-01,
         2.8931e-02,  2.6367e-01, -2.0703e-01, -2.0874e-02, -5.3711e-03,
        -1.1562e+00, -1.1084e-01, -1.3965e-01,  3.1250e-01, -1.1816e-01,
         3.0664e-01, -1.3477e-01, -1.4551e-01, -2.2095e-02, -4.1992e-01,
         2.2339e-02, -6.4392e-03, -1.5156e+00, -5.5469e-01, -3.4180e-02,
         1.6309e-01, -1.5234e-01,  1.4648e-03,  3.3447e-02, -8.4375e-01,
        -9.0820e-02, -6.1768e-02,  1.0254e-01,  1.8555e-01,  1.2891e-01,
        -2.6953e-01, -5.9570e-02, -9.6191e-02,  7.9590e-02,  9.1309e-02,
         9.6191e-02,  2.1240e-02,  1.2207e-01,  2.8809e-02,  2.4062e+00,
        -9.2773e-02,  1.2598e-01, -6.3477e-02,  5.0781e-01,  2.1191e-01,
         4.3945e-02, -1.8555e-02,  1.6797e-01, -2.4512e-01,  7.4707e-02,
        -1.7700e-02, -1.8066e-01,  2.3535e-01,  6.7749e-03,  2.5195e-01,
         2.7734e-01, -1.0010e-01,  1.9434e-01,  5.9570e-02, -9.8145e-02,
         1.6094e+00,  1.1621e-01, -1.5039e-01,  8.5938e-02,  1.0400e-01,
        -3.0273e-01, -1.5391e+00,  2.6367e-01, -5.6152e-02,  2.6562e-01,
        -1.3086e-01,  7.7637e-02,  1.5000e+00, -1.3672e-01,  4.0283e-02,
        -6.6016e-01, -2.5391e-01,  9.2285e-02, -1.4688e+00, -9.0332e-02,
        -4.9316e-02,  2.0117e-01, -5.0781e-02, -4.4727e-01,  2.2070e-01,
        -1.3574e-01,  1.7285e-01,  2.3633e-01, -8.5449e-02, -4.4336e-01,
         6.0938e-01, -4.0234e-01, -9.1309e-02, -1.2305e-01, -1.5312e+00,
        -3.5645e-02,  1.0645e-01,  1.3672e-01,  1.5820e-01,  1.2695e-01,
        -3.9062e-02, -8.7109e-01,  2.5391e-01,  5.1250e+00,  1.5625e-02,
         1.1914e-01,  9.8145e-02, -1.6211e-01, -1.3477e-01,  1.9336e-01,
        -1.6211e-01, -4.5312e-01,  4.3164e-01, -5.7031e-01, -1.2344e+00,
        -3.7305e-01, -6.8359e-01, -9.0234e-01, -3.3398e-01, -6.7969e-01,
        -9.5215e-02, -3.0078e-01,  7.3730e-02, -3.9062e-01,  5.7812e-01,
        -3.2422e-01,  3.2031e-01,  6.5918e-02,  6.8665e-03,  8.3496e-02,
         3.2031e-01,  1.7578e-01,  2.2949e-02,  5.0781e-01,  2.6733e-02,
        -4.1199e-04,  1.1484e+00, -3.8867e-01,  3.5156e-02, -1.2188e+00,
         9.9609e-02, -2.4609e-01,  1.3359e+00, -1.3672e-01, -2.1191e-01,
        -3.7109e-01, -2.1094e-01,  8.4961e-02,  3.5400e-02, -2.7344e-01,
         1.2812e+00,  1.5625e-01, -2.4023e-01,  1.1250e+00,  5.4688e-02,
        -1.4941e-01, -1.7266e+00,  8.2520e-02,  5.6885e-02, -8.0469e-01,
         2.9688e-01,  1.1914e-01, -2.1777e-01, -3.6328e-01, -1.3281e-01,
        -3.3594e-01, -4.8096e-02,  3.8086e-02,  2.6562e-01, -5.7188e+00,
        -7.6660e-02,  8.6670e-03,  2.0410e-01, -7.6660e-02,  1.1865e-01,
         5.2490e-02, -5.6250e-01,  3.4766e-01, -6.7188e-01, -8.1250e-01,
        -5.2002e-02, -4.8242e-01, -3.5352e-01,  7.5781e-01, -4.3750e-01,
         2.4414e-01,  7.2656e-01,  1.3184e-01, -2.9297e-01,  1.0156e+00,
         1.3281e-01, -3.3008e-01,  2.7930e-01,  1.1406e+00, -5.2734e-01,
        -3.8086e-01,  2.6489e-02, -1.1475e-01,  1.0703e+00,  3.0762e-02,
        -5.0391e-01,  8.3496e-02,  7.9688e-01,  4.7266e-01,  3.3984e-01,
        -5.6763e-03, -4.3555e-01, -5.3467e-02,  1.2500e-01, -1.7266e+00,
        -4.6143e-02,  2.5586e-01,  7.1094e-01, -3.0469e-01,  1.9824e-01,
         1.6992e-01,  1.3965e-01, -7.3547e-03, -3.2812e-01,  2.0996e-01,
        -5.6885e-02,  4.8438e-01,  2.4261e-03,  3.8672e-01,  2.9102e-01,
         4.9438e-03,  5.7129e-02,  9.8145e-02, -1.2354e-01, -1.3574e-01,
         1.9922e-01,  4.5508e-01, -6.0303e-02,  3.3750e+00,  7.3730e-02,
        -2.2754e-01,  4.9805e-01,  5.9766e-01, -1.3611e-02,  2.9102e-01,
        -2.2656e-01, -2.0117e-01])

llm.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0893, -0.0397, -0.0121,  ...,  0.0101,  0.0093, -0.0429],
        [ 0.0466, -0.0517, -0.0337,  ...,  0.0047, -0.0362, -0.0357],
        [-0.0416, -0.0330, -0.0204,  ...,  0.0424, -0.0102,  0.0298],
        ...,
        [ 0.0261,  0.0210,  0.0198,  ...,  0.0161,  0.0479, -0.0244],
        [ 0.0470,  0.0196, -0.0159,  ..., -0.0077,  0.0054,  0.0042],
        [ 0.0208,  0.0296, -0.0636,  ..., -0.0133, -0.0139, -0.0223]])

llm.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0661,  0.0416, -0.0151,  ..., -0.0008,  0.0292, -0.0330],
        [-0.0552, -0.0239,  0.0359,  ..., -0.0225,  0.0082,  0.0176],
        [-0.0182, -0.0132, -0.0410,  ..., -0.0099, -0.0213,  0.0282],
        ...,
        [-0.0023,  0.0177,  0.0081,  ..., -0.0030,  0.0234, -0.0028],
        [ 0.0353, -0.0115, -0.0246,  ...,  0.0353,  0.0280,  0.0258],
        [-0.0260,  0.0029,  0.0193,  ...,  0.0044, -0.0126, -0.0216]])

llm.base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0035,  0.0151,  0.0034,  ...,  0.0093, -0.0057, -0.0159],
        [-0.0057, -0.0123, -0.0039,  ...,  0.0099, -0.0129,  0.0079],
        [-0.0189, -0.0007,  0.0054,  ..., -0.0075, -0.0059, -0.0006],
        ...,
        [ 0.0159, -0.0139,  0.0049,  ..., -0.0535,  0.0126, -0.0088],
        [-0.0181,  0.0165,  0.0089,  ...,  0.0082,  0.0099,  0.0019],
        [-0.0054, -0.0107, -0.0055,  ...,  0.0138,  0.0114,  0.0052]])

llm.base_model.model.model.layers.21.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-6.6895e-02, -3.5156e-02, -3.1738e-02,  2.6978e-02, -2.4902e-02,
         8.2031e-02, -5.6152e-03, -7.5989e-03, -2.4536e-02, -4.0283e-02,
         9.6436e-03,  2.8320e-02,  5.5908e-02, -4.0039e-02,  2.3438e-02,
        -5.6396e-02, -2.8687e-02, -1.0376e-02,  4.6143e-02, -7.1106e-03,
         1.4062e-01, -1.4404e-02, -1.3855e-02, -3.8086e-02, -2.0020e-02,
         2.0996e-02, -4.0283e-03, -1.6724e-02, -4.3457e-02,  2.6733e-02,
        -1.6724e-02, -2.4872e-03,  7.9956e-03, -2.1362e-02,  8.9722e-03,
        -1.6357e-02, -2.7466e-02, -5.5176e-02, -5.1270e-03, -4.8828e-02,
        -2.1118e-02,  3.9795e-02, -3.9307e-02,  1.8921e-02,  3.1128e-02,
         6.3965e-02,  1.1597e-02,  9.7656e-03, -1.2817e-03,  4.3213e-02,
         2.0752e-02,  2.0020e-02,  2.9449e-03, -4.2969e-02,  3.7842e-03,
         3.4912e-02, -1.7578e-02,  3.2959e-02, -1.3428e-02,  3.0273e-02,
         5.9082e-02, -7.3853e-03,  1.3245e-02,  1.6022e-03,  3.9307e-02,
        -1.3245e-02,  4.7852e-02, -6.8848e-02,  2.8687e-02, -2.7222e-02,
        -2.4780e-02, -3.5645e-02, -2.2705e-02, -5.1270e-03,  2.4048e-02,
        -6.8665e-03, -8.1055e-02,  8.5068e-04,  2.9175e-02, -5.4688e-02,
         6.2256e-03, -6.9885e-03, -5.1758e-02,  2.1210e-03,  4.1748e-02,
         1.1536e-02, -4.7607e-02, -1.8799e-02,  3.3447e-02,  5.5664e-02,
        -9.6893e-04, -3.1494e-02, -2.1118e-02,  1.7944e-02,  4.5898e-02,
         9.4604e-03, -3.7384e-03,  5.0964e-03,  7.1777e-02, -1.6235e-02,
         1.3611e-02, -9.8267e-03, -2.4048e-02,  7.5531e-04,  8.7280e-03,
         4.4434e-02, -2.4658e-02, -3.2783e-06, -1.5076e-02,  5.7373e-03,
         7.5912e-04,  4.9561e-02,  2.8687e-03,  2.3804e-02, -3.8757e-03,
        -6.4087e-03, -4.0283e-02,  2.7539e-01,  4.5898e-02, -4.3335e-03,
        -5.7983e-03,  2.5330e-03,  3.2654e-03, -1.2024e-02, -4.9561e-02,
        -2.6489e-02,  5.0964e-03, -3.6377e-02,  7.1289e-02, -8.2397e-03,
        -1.0437e-02,  6.5430e-02,  5.0964e-03,  1.0840e-01, -9.8267e-03,
        -4.8828e-02,  5.5908e-02, -1.1841e-02, -1.0315e-02,  6.2988e-02,
         4.7607e-02, -4.1260e-02, -1.2024e-02,  3.2654e-03,  7.3730e-02,
         2.6978e-02,  4.6387e-03,  1.4221e-02,  3.1250e-02,  3.3417e-03,
         2.0020e-02,  1.8066e-02, -2.8931e-02, -4.8584e-02,  2.9663e-02,
         7.0190e-04, -5.2734e-02,  6.2012e-02, -8.1543e-02,  4.2480e-02,
        -1.2695e-02, -3.4180e-02, -1.7578e-02,  6.4392e-03,  9.5825e-03,
         1.4282e-02,  1.2061e-01,  5.9326e-02, -2.1362e-02, -5.3101e-03,
         3.6011e-03, -1.3123e-02,  1.3281e-01,  4.6875e-02,  4.9805e-02,
        -2.4109e-03, -5.4199e-02, -3.5889e-02, -3.9307e-02, -3.5889e-02,
        -6.1951e-03,  6.0791e-02,  4.2419e-03, -9.5703e-02, -4.3701e-02,
        -2.2095e-02,  1.0193e-02,  1.2146e-02,  2.1973e-02,  3.9307e-02,
         3.0640e-02,  3.2227e-02,  4.9072e-02, -2.7771e-03, -7.4707e-02,
        -6.9580e-03,  1.8692e-03, -3.3112e-03,  3.1738e-02, -5.5176e-02,
        -6.6833e-03,  3.3203e-02,  3.3203e-02, -4.4861e-03,  4.2969e-02,
        -4.9561e-02, -2.0386e-02,  1.5259e-03,  2.9602e-03,  7.1106e-03,
         4.5898e-02, -7.6562e-01,  2.8076e-02, -5.7129e-02,  2.0264e-02,
        -4.4434e-02,  2.3633e-01,  7.3242e-02, -2.2827e-02, -4.4434e-02,
         3.4912e-02,  3.7109e-02, -1.1368e-03, -3.9062e-02,  9.1553e-03,
        -1.6846e-02,  9.6680e-02, -4.5410e-02,  8.9722e-03,  1.1292e-03,
         2.1729e-02,  1.2061e-01,  3.8338e-04,  2.6245e-02,  4.1016e-02,
         7.6660e-02,  3.5156e-02,  1.7578e-02, -9.3750e-02,  2.3682e-02,
        -2.1362e-02, -7.1289e-02,  1.3809e-03,  3.2959e-02, -4.0283e-02,
        -6.4941e-02, -1.8845e-03, -3.6621e-02,  1.9165e-02,  4.8447e-04,
        -3.4668e-02, -2.7344e-02, -6.7749e-03, -2.6733e-02,  3.3936e-02,
         3.7109e-02, -1.8677e-02, -3.0762e-02,  1.9336e-01,  1.7773e-01,
        -1.4062e-01,  6.2256e-02, -1.4160e-01, -2.5635e-02,  1.2354e-01,
        -7.1777e-02, -1.5723e-01,  8.3984e-02,  9.4238e-02, -6.3477e-02,
         2.9541e-02,  1.5234e-01,  7.0496e-03, -3.1250e-01, -1.6724e-02,
         1.0352e-01,  3.5156e-02, -4.0039e-02,  1.0803e-02,  9.1553e-03,
        -4.6692e-03,  5.1025e-02,  1.0803e-02,  1.4062e-01,  5.2795e-03,
         1.0059e-01, -1.7969e-01, -5.7129e-02, -1.2988e-01, -9.0942e-03,
         8.2397e-03, -1.2665e-03, -5.9814e-02, -9.4604e-03, -1.4551e-01,
        -2.3926e-01,  9.1797e-02,  9.4604e-03,  6.9336e-02, -2.7344e-02,
        -5.2490e-02, -1.5747e-02, -4.3701e-02,  1.1523e-01,  4.7852e-02,
        -1.2634e-02,  1.5234e-01,  9.1309e-02,  4.5410e-02,  7.9102e-02,
         1.3281e-01,  9.5215e-02, -8.6426e-02, -7.9102e-02, -2.7148e-01,
         9.3750e-02,  4.2725e-02, -7.5391e-01,  3.3789e-01, -9.0332e-02,
        -2.0410e-01, -1.3965e-01, -8.0566e-02, -3.7109e-02, -9.0820e-02,
         6.2500e-02,  1.2500e-01, -2.9419e-02,  4.4189e-02, -9.4727e-02,
        -6.0791e-02,  2.9907e-02,  2.3041e-03,  8.0566e-02, -5.3516e-01,
        -9.3750e-02,  3.1445e-01,  2.5781e-01,  5.2734e-02,  7.9346e-03,
         2.9785e-02, -2.7832e-02, -3.7994e-03, -1.5869e-02,  2.3242e-01,
         3.7384e-03,  2.5391e-02,  2.5977e-01, -1.7285e-01, -1.0840e-01,
        -6.7871e-02, -1.6992e-01,  9.3262e-02, -1.2891e-01,  1.0352e-01,
        -6.1279e-02, -1.0938e-01,  3.7842e-02, -5.5176e-02,  2.1606e-02,
        -1.1377e-01, -5.8105e-02, -1.6406e-01, -2.6489e-02,  1.9141e-01,
         7.9102e-02, -1.0303e-01, -1.2891e-01,  9.2285e-02, -5.4443e-02,
         2.5391e-02, -1.5625e-01,  2.9785e-02, -9.4604e-03, -1.5488e-03,
         4.1504e-02, -1.5991e-02,  6.4453e-02, -7.0312e-02,  1.0352e-01,
         6.0303e-02,  8.8379e-02, -1.6724e-02,  2.6001e-02,  1.3672e-01,
         4.1008e-04,  5.3711e-02, -5.7373e-02,  5.4932e-02,  4.3945e-02,
         1.9043e-02, -1.2207e-01, -9.5703e-02, -9.5215e-03, -4.2236e-02,
        -1.9043e-02,  6.6895e-02, -5.8838e-02,  1.1658e-02,  6.6406e-02,
         3.0640e-02, -4.4922e-02,  6.7383e-02, -2.6001e-02, -8.9355e-02,
         4.4434e-02, -2.2461e-02,  2.6245e-02,  4.8340e-02,  4.1992e-02,
         2.9541e-02, -2.9297e-02, -8.7402e-02, -6.7383e-02, -9.2773e-03,
         2.2266e-01,  6.4453e-02, -5.3711e-02, -1.0352e-01,  1.6602e-02,
         2.5024e-02,  7.3242e-02,  1.6602e-02,  9.8267e-03, -6.0547e-02,
         3.0762e-02,  1.6406e-01, -5.3223e-02,  9.1309e-02, -1.2988e-01,
         5.2979e-02,  6.5918e-02,  1.4844e-01, -2.1851e-02,  4.3640e-03,
        -1.5076e-02,  6.8848e-02,  1.9684e-03,  7.7820e-03,  9.0820e-02,
         5.9570e-02, -2.1729e-02, -3.9307e-02,  2.5146e-02, -6.6406e-02,
         2.0508e-02,  7.0801e-03,  4.5776e-03,  4.7363e-02, -7.4707e-02,
        -7.7148e-02,  2.4023e-01, -7.2266e-02,  7.4707e-02, -5.5078e-01,
         2.2949e-02,  9.7656e-02, -1.3367e-02,  3.8818e-02, -1.3428e-02,
         5.6885e-02, -2.5513e-02, -9.3079e-04,  7.4219e-02, -4.8340e-02,
         4.7607e-02, -8.7891e-03, -7.0312e-02,  9.5703e-02,  5.7861e-02,
        -1.1139e-03, -1.8387e-03,  2.6367e-02,  8.3008e-03, -8.0566e-02,
         5.8105e-02,  2.9907e-02,  2.6733e-02, -2.0996e-02,  1.0010e-01,
         3.7079e-03, -1.0547e-01,  1.0315e-02, -1.6602e-02, -1.1084e-01,
         1.0010e-01,  8.2520e-02,  3.6469e-03, -1.5430e-01,  6.1768e-02,
         2.2705e-02, -1.3062e-02,  7.2266e-02, -3.0762e-02,  6.0059e-02,
         1.5625e-02, -2.4872e-03,  3.9551e-02,  7.1777e-02, -1.2988e-01,
         2.0996e-02,  9.7046e-03, -8.5449e-02,  6.5918e-02, -7.9102e-02,
         1.8799e-02,  1.1035e-01,  1.4526e-02, -8.0078e-02,  5.9326e-02,
         1.0376e-02,  4.2480e-02])

llm.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0012,  0.0382, -0.0087,  ...,  0.0137,  0.0185, -0.0035],
        [ 0.0463,  0.0353,  0.0126,  ...,  0.0236, -0.0212, -0.0240],
        [ 0.0408,  0.0207, -0.0033,  ...,  0.0188, -0.0448,  0.0654],
        ...,
        [-0.0318,  0.0125,  0.0588,  ..., -0.0302,  0.0205, -0.0266],
        [ 0.0091,  0.0108, -0.0218,  ...,  0.0070, -0.0519,  0.0341],
        [-0.0209, -0.0071,  0.0315,  ...,  0.0502,  0.0156, -0.0442]])

llm.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0050, -0.0237, -0.0102,  ...,  0.0053, -0.0158,  0.0014],
        [-0.0185, -0.0138, -0.0052,  ...,  0.0173,  0.0033, -0.0064],
        [-0.0080,  0.0004, -0.0040,  ..., -0.0168, -0.0252, -0.0064],
        ...,
        [ 0.0028, -0.0062,  0.0192,  ...,  0.0076, -0.0163, -0.0332],
        [-0.0045,  0.0210,  0.0166,  ...,  0.0073, -0.0071, -0.0109],
        [-0.0240, -0.0158, -0.0142,  ...,  0.0372,  0.0268, -0.0184]])

llm.base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0334,  0.0249, -0.0094,  ...,  0.0071,  0.0192, -0.0303],
        [-0.0253,  0.0028,  0.0017,  ..., -0.0085, -0.0070,  0.0378],
        [ 0.0076,  0.0286,  0.0110,  ...,  0.0204, -0.0040, -0.0162],
        ...,
        [ 0.0073,  0.0173,  0.0106,  ..., -0.0154, -0.0069, -0.0049],
        [-0.0111, -0.0060,  0.0337,  ..., -0.0013, -0.0110, -0.0352],
        [-0.0079, -0.0036,  0.0231,  ...,  0.0113, -0.0103,  0.0189]])

llm.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0074,  0.0461, -0.0531,  ..., -0.0288,  0.0011,  0.0201],
        [ 0.0379, -0.0103, -0.0025,  ...,  0.0123, -0.0316, -0.0663],
        [-0.0095, -0.0009, -0.0606,  ...,  0.0750,  0.0279,  0.0187],
        ...,
        [ 0.0102,  0.0559, -0.0682,  ...,  0.0233,  0.0019, -0.0457],
        [-0.0178, -0.0087,  0.0510,  ..., -0.0043,  0.0120, -0.0332],
        [ 0.0579,  0.0659,  0.0334,  ..., -0.0446,  0.0411, -0.0116]])

llm.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0064,  0.0150, -0.0062,  ..., -0.0373, -0.0234,  0.0045],
        [-0.0154, -0.0051,  0.0373,  ..., -0.0825,  0.0515,  0.0294],
        [-0.0217,  0.0088, -0.0142,  ...,  0.0148, -0.0033, -0.0216],
        ...,
        [-0.0152, -0.0267,  0.0066,  ..., -0.0492, -0.0691, -0.0570],
        [-0.0185,  0.0079, -0.0103,  ...,  0.0406,  0.0356,  0.0546],
        [-0.0289, -0.0092,  0.0049,  ..., -0.0258, -0.0048,  0.0180]])

llm.base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 2.6245e-02, -2.7222e-02, -1.7212e-02,  ...,  8.4305e-04,
          3.2227e-02, -1.9775e-02],
        [-9.2163e-03, -1.4771e-02,  9.7656e-03,  ...,  7.0190e-03,
         -7.6599e-03,  2.7222e-02],
        [-1.9287e-02, -7.8735e-03, -2.8687e-02,  ..., -1.9531e-03,
         -2.7924e-03, -1.0254e-02],
        ...,
        [ 1.1963e-02, -7.2632e-03,  2.7466e-03,  ..., -1.0437e-02,
          2.4170e-02,  4.8399e-05],
        [ 1.5259e-02,  5.4321e-03,  2.6894e-04,  ..., -1.6968e-02,
          8.6308e-05,  1.0803e-02],
        [-1.8433e-02, -1.6251e-03, -2.3682e-02,  ..., -1.1108e-02,
          3.7956e-04, -5.0354e-03]])

llm.base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0557, -0.0806,  0.0190,  ..., -0.0207, -0.0044,  0.0375],
        [ 0.0423,  0.0256,  0.0127,  ..., -0.0067,  0.0156, -0.0300],
        [-0.0384, -0.0537,  0.0627,  ..., -0.0203,  0.0156,  0.0036],
        ...,
        [-0.0339,  0.0156,  0.0669,  ...,  0.0327,  0.0080, -0.0362],
        [-0.0035, -0.0214, -0.0196,  ..., -0.0764,  0.0201,  0.0257],
        [ 0.0070, -0.0304,  0.0191,  ..., -0.0701, -0.0347, -0.0429]])

llm.base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0095,  0.0243, -0.0249,  ...,  0.0103, -0.0629, -0.0244],
        [ 0.0547,  0.0204,  0.0397,  ..., -0.0074,  0.0144, -0.0126],
        [ 0.0092,  0.0926, -0.0109,  ...,  0.0799,  0.0040,  0.0653],
        ...,
        [-0.0072,  0.0427, -0.0300,  ...,  0.0341, -0.0016,  0.0134],
        [ 0.0049, -0.0178, -0.0101,  ..., -0.0271,  0.0098, -0.0045],
        [-0.0025, -0.0394, -0.0264,  ...,  0.0165, -0.0150, -0.0254]])

llm.base_model.model.model.layers.21.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-2.9297e-03,  1.1902e-02,  8.5449e-03,  ...,  1.7944e-02,
          1.0376e-02, -5.2734e-02],
        [-3.6621e-03,  5.0659e-03,  7.2937e-03,  ..., -3.0823e-03,
          1.2875e-04,  1.2146e-02],
        [-5.7678e-03,  7.9346e-03,  1.5991e-02,  ...,  7.6904e-03,
          2.5146e-02,  1.3428e-02],
        ...,
        [ 3.8147e-03, -2.0752e-02,  2.1851e-02,  ..., -1.8799e-02,
          1.2634e-02,  3.5858e-03],
        [ 3.0327e-04, -4.5776e-03,  2.2949e-02,  ...,  2.5635e-02,
         -8.8501e-03, -3.6469e-03],
        [ 1.1353e-02,  3.2196e-03,  5.4016e-03,  ..., -1.5320e-02,
         -5.1260e-05, -5.7678e-03]])

llm.base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0043, -0.0053,  0.0399,  ...,  0.0209, -0.0102,  0.0631],
        [-0.0123, -0.0365, -0.0352,  ..., -0.0077,  0.0270,  0.0366],
        [-0.0365, -0.0889, -0.0188,  ...,  0.0070, -0.0164, -0.0185],
        ...,
        [ 0.0697,  0.0027, -0.0097,  ..., -0.0453,  0.0115,  0.0141],
        [-0.0321, -0.0428, -0.0007,  ...,  0.0587,  0.0328, -0.0273],
        [ 0.0316,  0.0222,  0.0182,  ...,  0.0143,  0.0732,  0.0513]])

llm.base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0293,  0.0488, -0.0260,  ..., -0.0354,  0.0205, -0.0334],
        [ 0.0070, -0.0257, -0.0085,  ...,  0.0034,  0.0010,  0.0536],
        [-0.0319, -0.0115,  0.0187,  ..., -0.0073,  0.0284, -0.0432],
        ...,
        [ 0.0367, -0.0088,  0.0359,  ..., -0.0133,  0.0055,  0.0241],
        [ 0.0373, -0.0073,  0.0142,  ...,  0.0350,  0.0174,  0.0013],
        [ 0.0139, -0.0058, -0.0386,  ...,  0.0260, -0.0109, -0.0115]])

llm.base_model.model.model.layers.21.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 0.0245, -0.0148, -0.0036,  ...,  0.0041,  0.0040,  0.0025],
        [ 0.0142,  0.0117,  0.0011,  ..., -0.0047,  0.0006,  0.0243],
        [ 0.0074, -0.0032,  0.0089,  ..., -0.0173,  0.0079, -0.0153],
        ...,
        [ 0.0146, -0.0177, -0.0003,  ..., -0.0120, -0.0184,  0.0110],
        [-0.0012,  0.0073,  0.0172,  ..., -0.0107, -0.0002,  0.0009],
        [-0.0393,  0.0184, -0.0031,  ..., -0.0087, -0.0052, -0.0148]])

llm.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0376, -0.0049,  0.0162,  ...,  0.0287,  0.0197,  0.0169],
        [ 0.0333, -0.0012, -0.0147,  ..., -0.0073,  0.0506,  0.0395],
        [-0.0598, -0.0481,  0.0157,  ..., -0.0452, -0.0308, -0.0291],
        ...,
        [-0.0119, -0.0246,  0.0161,  ..., -0.0221, -0.0239, -0.0308],
        [ 0.0100,  0.0197,  0.0055,  ...,  0.0372, -0.0202, -0.0046],
        [ 0.0031, -0.0165,  0.0160,  ..., -0.0285, -0.0208, -0.0220]])

llm.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0239,  0.0032,  0.0174,  ..., -0.0517,  0.0696,  0.0093],
        [-0.0294, -0.0100,  0.0091,  ...,  0.0031,  0.0251,  0.0269],
        [ 0.0087,  0.0235, -0.0676,  ..., -0.0222, -0.0033,  0.0043],
        ...,
        [ 0.0468, -0.0018, -0.0090,  ...,  0.0056,  0.0191,  0.0029],
        [-0.0183,  0.0233,  0.0096,  ..., -0.0430, -0.0371, -0.0558],
        [ 0.0835,  0.0304, -0.0632,  ...,  0.0193,  0.0142,  0.0838]])

llm.base_model.model.model.layers.21.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.2656, 0.8164, 1.3203,  ..., 0.9766, 0.9922, 1.0156])

llm.base_model.model.model.layers.21.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.3516, 1.3125, 1.5703,  ..., 1.0234, 1.4141, 1.3750])

llm.base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0127,  0.0025,  0.0317,  ...,  0.0074,  0.0245, -0.0100],
        [ 0.0056, -0.0151,  0.0216,  ..., -0.0034,  0.0288, -0.0476],
        [ 0.0096, -0.0036, -0.0259,  ...,  0.0066, -0.0244,  0.0275],
        ...,
        [ 0.0105, -0.0068,  0.0284,  ..., -0.0082,  0.0104, -0.0123],
        [ 0.0569, -0.0087,  0.0048,  ...,  0.0008,  0.0166, -0.0065],
        [ 0.0171,  0.0188,  0.0043,  ...,  0.0103,  0.0137, -0.0012]])

llm.base_model.model.model.layers.22.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([-0.0376,  0.8281, -0.1641,  ..., -0.6445, -0.2695, -0.5352])

llm.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.1202, -0.0491,  0.0238,  ...,  0.0073,  0.0312,  0.0294],
        [ 0.0216,  0.0159, -0.0593,  ...,  0.0213, -0.0303, -0.0118],
        [-0.0227,  0.0371, -0.0395,  ..., -0.0463, -0.0291, -0.0718],
        ...,
        [-0.0174, -0.0093, -0.0075,  ..., -0.0617, -0.0144,  0.0085],
        [-0.0673,  0.0097,  0.0137,  ..., -0.0330,  0.0088, -0.0087],
        [ 0.0275, -0.0113, -0.0164,  ...,  0.0463,  0.0267,  0.0114]])

llm.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0043,  0.0303,  0.0222,  ...,  0.0376,  0.0257,  0.0029],
        [ 0.0024,  0.0349, -0.0195,  ...,  0.0149, -0.0288,  0.0121],
        [ 0.0601,  0.0298, -0.0049,  ..., -0.0049,  0.0273, -0.0015],
        ...,
        [-0.0340, -0.0082,  0.0416,  ...,  0.0056, -0.0130, -0.0637],
        [-0.0002, -0.0125,  0.0061,  ..., -0.0033,  0.0655,  0.0151],
        [-0.0251, -0.0330, -0.0300,  ...,  0.0268, -0.0072, -0.0317]])

llm.base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0134, -0.0020, -0.0137,  ..., -0.0146, -0.0007, -0.0030],
        [ 0.0020, -0.0101,  0.0012,  ..., -0.0075,  0.0135,  0.0034],
        [-0.0012,  0.0024,  0.0130,  ...,  0.0160,  0.0036, -0.0013],
        ...,
        [ 0.0006, -0.0201, -0.0150,  ...,  0.0041,  0.0010,  0.0025],
        [ 0.0153, -0.0176,  0.0161,  ...,  0.0138,  0.0035,  0.0119],
        [ 0.0006, -0.0013, -0.0010,  ...,  0.0232,  0.0069, -0.0081]])

llm.base_model.model.model.layers.22.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 3.5156e-01,  4.2725e-02, -1.3281e-01,  4.9023e-01,  1.4258e-01,
         5.4688e-01, -1.3770e-01,  8.3008e-02, -4.0234e-01, -2.7344e-01,
         7.6953e-01,  1.6113e-01,  3.8477e-01, -4.5703e-01, -3.0469e-01,
         9.8047e-01,  5.2734e-01, -1.4160e-01, -9.8828e-01,  8.0859e-01,
        -6.0156e-01,  1.0703e+00, -3.1445e-01,  1.2500e+00,  7.4609e-01,
         9.4531e-01, -2.9297e-01,  3.2959e-02,  1.1963e-01,  1.9434e-01,
         5.8203e-01, -1.5527e-01,  6.7383e-02,  3.2715e-02,  4.8047e-01,
        -2.9883e-01, -2.4023e-01, -1.6094e+00,  8.1543e-02, -7.2754e-02,
         3.4375e+00, -2.7954e-02, -3.0469e-01,  6.1328e-01,  2.9297e-01,
         1.6309e-01,  8.6719e-01, -1.3438e+00,  7.7637e-02,  5.3906e-01,
         5.3906e-01, -3.7109e-01, -4.5898e-02,  3.0664e-01,  2.0117e-01,
         1.0645e-01, -9.5215e-03,  1.7871e-01, -9.7046e-03,  6.2500e-01,
         2.0410e-01, -2.7344e-01, -1.5000e+00,  1.7969e-01,  4.1016e-01,
         1.2793e-01, -1.0645e-01,  3.7891e-01, -6.7383e-02,  1.1426e-01,
        -9.3262e-02,  6.2109e-01,  7.1289e-02, -4.3701e-02, -2.0508e-01,
        -1.2012e-01,  8.7109e-01, -1.1768e-01, -8.4473e-02, -2.1094e-01,
        -2.4609e-01, -8.9844e-02,  3.2812e-01, -3.8281e-01,  3.8867e-01,
        -1.3733e-02,  5.3125e-01, -3.3594e-01,  1.0889e-01, -3.5742e-01,
        -2.1875e+00,  6.0156e-01,  4.2578e-01, -7.9688e-01,  6.1768e-02,
        -2.8750e+00,  3.6328e-01,  5.3125e-01,  1.3906e+00,  1.5918e-01,
        -2.9492e-01,  4.5898e-01, -4.4727e-01, -1.1230e-01, -9.3750e-02,
        -3.2031e-01,  5.2002e-02, -6.9531e-01, -1.7812e+00, -1.2812e+00,
         5.0000e-01, -5.3750e+00, -3.9844e-01, -6.9922e-01,  3.8281e-01,
         5.2246e-02,  5.0293e-02, -4.5117e-01, -2.6562e-01,  1.0000e+00,
         1.9824e-01,  1.1182e-01, -1.8555e-01, -8.1250e-01, -2.1362e-02,
         7.0312e-01,  6.7969e-01,  1.9238e-01,  2.7148e-01, -6.8359e-01,
         2.9102e-01,  3.8477e-01,  3.7695e-01,  3.2227e-01,  5.2344e-01,
         3.7109e-02,  2.5000e-01, -9.6680e-02,  3.9062e-01, -3.1641e-01,
        -4.6289e-01,  5.3125e-01,  2.6172e-01, -2.8125e-01,  1.1094e+00,
         1.6504e-01,  6.7969e-01, -6.0938e-01,  5.1270e-02, -2.7734e-01,
        -1.9727e-01,  3.9551e-02, -2.1680e-01, -5.6396e-02, -1.2969e+00,
         1.0234e+00, -6.2012e-02,  5.2979e-02,  1.6602e-01, -2.1191e-01,
        -2.3438e-01, -1.5625e-01, -5.3125e-01,  1.6602e-01,  3.6914e-01,
         6.8750e-01,  2.0117e-01,  2.3594e+00, -4.6387e-02,  3.0273e-01,
         1.7109e+00, -2.2095e-02, -3.7305e-01, -3.8281e-01, -4.7266e-01,
        -3.1250e-01, -2.6758e-01, -2.8906e-01, -1.4062e-01,  4.4922e-01,
        -1.0889e-01, -8.5938e-02, -4.2915e-05,  5.2490e-02, -3.5547e-01,
         2.2095e-02,  1.4355e-01,  6.6016e-01, -2.0469e+00,  1.7090e-01,
         2.1582e-01, -2.3633e-01, -6.4062e-01,  5.8105e-02, -4.3359e-01,
        -3.6523e-01,  4.1211e-01,  3.6133e-01, -8.8867e-02,  6.4453e-01,
         5.6885e-02, -7.7734e-01,  1.1865e-01, -3.0136e-04, -2.5391e-01,
        -4.2773e-01,  1.7090e-01, -3.8086e-01, -1.4941e-01,  5.3467e-02,
        -2.2656e-01, -2.9907e-02,  9.9121e-02, -7.8613e-02, -1.3516e+00,
        -1.9043e-01, -1.1094e+00,  2.0312e-01,  1.4648e-01, -3.5156e-01,
        -9.2773e-02, -1.0596e-01,  3.4766e-01, -2.2188e+00, -1.5918e-01,
         4.7119e-02, -2.3242e-01, -8.0078e-02, -7.1289e-02,  4.6143e-02,
        -3.2812e-01, -8.3008e-02,  2.1973e-01,  8.2520e-02, -3.5938e-01,
        -4.6484e-01, -2.6953e-01,  2.1387e-01, -1.0889e-01, -2.5781e-01,
        -1.7285e-01, -6.9824e-02,  3.9307e-02,  3.7500e+00, -4.6484e-01,
        -2.1387e-01, -1.0303e-01, -3.2031e-01, -2.7734e-01,  9.4727e-02,
         2.4414e-01, -2.7344e-01,  3.3906e+00,  2.3438e-01, -2.2461e-01,
        -1.1133e-01, -7.5781e-01,  5.0391e-01,  8.8379e-02, -7.3242e-02,
        -5.3516e-01,  2.4292e-02, -5.3467e-02, -5.1953e-01, -2.6172e-01,
         5.6250e-01, -1.9897e-02, -1.0193e-02, -9.9219e-01, -1.4844e-01,
         9.4604e-03,  9.0332e-02,  8.1250e-01,  1.2402e-01, -6.1523e-02,
        -1.0391e+00, -1.6797e-01, -8.5547e-01, -1.3379e-01,  1.1475e-01,
        -4.4922e-02, -3.3984e-01, -1.1182e-01,  1.2422e+00, -6.3965e-02,
         1.5503e-02, -1.4453e-01,  6.6406e-01,  1.1328e-01,  2.7100e-02,
         1.8359e-01,  1.2500e-01,  1.3770e-01, -1.9629e-01, -1.7383e-01,
        -6.0791e-02,  2.9053e-02,  8.7402e-02, -2.0625e+00, -7.0801e-02,
        -2.0996e-01, -2.1851e-02,  1.4160e-01, -8.4473e-02, -3.0078e-01,
         1.3867e-01,  7.7148e-02,  1.0303e-01, -2.1094e-01,  1.3916e-02,
         1.0059e-01, -1.6016e-01,  1.1902e-02,  5.1758e-02,  5.6562e+00,
        -3.2227e-01, -1.5625e-01, -8.6914e-02, -3.6914e-01, -4.1016e-01,
         4.4922e-02,  5.4297e-01, -5.8203e-01, -4.7070e-01, -5.7422e-01,
        -1.3867e-01, -1.1963e-01,  7.6172e-01,  4.9316e-02,  3.4766e-01,
        -2.0508e-01,  4.0527e-02, -1.4551e-01,  1.5430e-01,  5.1514e-02,
         1.9336e-01, -1.0254e-01, -6.0547e-01, -6.8848e-02,  2.8711e-01,
        -1.1475e-01,  5.7422e-01, -3.2422e-01, -2.5391e-02,  1.4844e-01,
        -5.3906e-01, -1.8359e-01, -9.6875e-01,  5.1025e-02, -8.9844e-02,
         1.6211e-01,  1.6172e+00,  4.1016e-02,  4.6631e-02, -1.7676e-01,
         4.7363e-02, -1.3965e-01,  1.2500e+00, -1.3489e-02,  3.0078e-01,
         2.1191e-01,  2.1289e-01, -1.0156e-01, -1.4648e-01, -1.7773e-01,
         7.1777e-02, -1.7090e-01,  8.2031e-01,  2.0898e-01,  6.2012e-02,
        -1.6357e-02,  1.0449e-01,  1.9824e-01,  2.0996e-01, -2.5391e-01,
         3.9062e-01, -1.5234e-01, -2.5586e-01, -3.2344e+00, -2.0508e-02,
         1.7969e-01,  1.3672e-01,  4.7266e-01, -2.4316e-01,  1.7188e-01,
        -4.4531e-01, -3.1836e-01,  7.0312e-02, -4.1602e-01, -3.5547e-01,
         2.9419e-02, -3.4912e-02, -2.1777e-01,  7.5684e-02, -2.9175e-02,
        -3.8281e-01, -3.7891e-01, -5.0781e-01, -2.5586e-01, -2.7539e-01,
        -6.7188e-01,  1.4355e-01, -4.3359e-01, -3.6377e-02,  7.1289e-02,
        -4.8438e-01, -4.6484e-01,  6.8750e-01,  8.0566e-02,  8.2520e-02,
        -7.7734e-01, -1.0547e+00,  2.8906e-01, -3.3203e-01, -2.9883e-01,
         2.6562e-01,  4.4922e-01,  1.0449e-01,  1.1084e-01, -7.8516e-01,
         2.4902e-01, -1.1797e+00, -5.1172e-01,  2.6172e-01, -1.8359e-01,
         2.5469e+00, -6.7383e-02,  2.0874e-02, -1.3379e-01,  2.4609e-01,
         5.1172e-01, -1.9844e+00,  8.2812e-01,  5.1514e-02,  1.3770e-01,
        -3.2715e-02, -6.7188e-01, -2.1094e-01,  2.6562e-01, -1.2878e-02,
         5.1172e-01, -1.8652e-01,  1.5723e-01, -5.9814e-02,  3.9648e-01,
         3.0781e+00, -9.6094e-01, -2.5391e-01, -1.9727e-01, -2.0874e-02,
        -3.0078e-01,  1.3184e-01,  7.8613e-02, -2.3633e-01, -1.7871e-01,
         1.2402e-01,  3.5352e-01, -5.1562e-01, -2.0508e-01,  2.1289e-01,
         1.7090e-03,  1.8164e-01, -2.0703e-01, -5.9766e-01, -2.0508e-01,
        -9.5703e-02,  3.4766e-01, -8.2397e-04,  1.8555e-01, -1.0625e+00,
        -1.2207e-01, -4.3750e-01,  1.3086e-01,  6.7188e-01, -7.8125e-02,
         8.8379e-02,  1.2891e-01,  4.0625e-01,  2.2656e-01,  2.0312e+00,
        -1.9531e-01, -2.9541e-02,  9.4238e-02, -6.7188e-01, -5.2002e-02,
         7.0801e-02,  8.0078e-02, -1.1279e-01, -1.9043e-01, -3.5742e-01,
         5.5664e-02,  3.4912e-02, -3.0859e-01, -1.1250e+00,  1.0010e-01,
         2.1094e-01,  4.5703e-01, -2.6245e-02, -5.4688e-02, -3.6865e-02,
        -7.6660e-02,  3.9551e-02,  1.8921e-02,  9.2773e-02,  2.7930e-01,
         2.0312e-01, -6.0303e-02, -1.7188e-01,  1.8555e-01,  2.4531e+00,
         3.5156e-01, -5.6641e-01])

llm.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0065, -0.0028,  0.0221,  ..., -0.0033,  0.0160,  0.0187],
        [ 0.0554,  0.0037,  0.0581,  ...,  0.0115, -0.0227,  0.0076],
        [-0.0461, -0.0408,  0.0117,  ...,  0.0250, -0.0409,  0.0664],
        ...,
        [ 0.0079,  0.0408, -0.0270,  ..., -0.0371,  0.0374, -0.0159],
        [ 0.0232, -0.0250,  0.0466,  ..., -0.0127, -0.0249,  0.0694],
        [ 0.0127, -0.0074, -0.0157,  ..., -0.0183,  0.0042,  0.0113]])

llm.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0359, -0.0149,  0.0059,  ..., -0.0069, -0.0407, -0.0286],
        [ 0.0258, -0.0346, -0.0136,  ..., -0.0217, -0.0102, -0.0073],
        [-0.0312,  0.0087,  0.0038,  ...,  0.0228,  0.0093,  0.0029],
        ...,
        [ 0.0247,  0.0095, -0.0357,  ...,  0.0253, -0.0012, -0.0141],
        [-0.0099, -0.0152,  0.0087,  ...,  0.0445, -0.0131,  0.0072],
        [-0.0293, -0.0081,  0.0105,  ...,  0.0135,  0.0291,  0.0277]])

llm.base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-1.4954e-02,  1.1536e-02, -5.1880e-04,  ..., -6.1035e-03,
         -6.0654e-04,  3.5248e-03],
        [-7.5150e-04,  6.5002e-03,  9.4604e-04,  ..., -1.4404e-02,
          4.5166e-03,  4.2114e-03],
        [-7.3242e-03, -1.2878e-02, -1.8539e-03,  ..., -3.4332e-03,
         -1.4221e-02,  9.2773e-03],
        ...,
        [ 2.3926e-02,  1.7090e-02,  1.0864e-02,  ...,  3.2959e-02,
          2.5146e-02, -1.2207e-03],
        [-2.0386e-02, -2.4414e-02, -7.3910e-05,  ...,  1.0498e-02,
          1.2695e-02, -1.7090e-02],
        [ 2.9144e-03, -9.7046e-03,  2.8442e-02,  ..., -2.0508e-02,
         -3.3875e-03, -8.5831e-04]])

llm.base_model.model.model.layers.22.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 8.1787e-03, -7.8613e-02,  1.3855e-02,  9.0332e-02, -3.9307e-02,
        -2.4780e-02,  4.7607e-02,  5.7678e-03,  4.2725e-02, -1.1292e-02,
         7.3730e-02, -1.1597e-02, -5.4932e-02,  5.4199e-02, -2.8711e-01,
        -1.5198e-02,  8.4839e-03,  2.2827e-02,  1.3199e-03, -4.9561e-02,
        -3.6865e-02,  5.4688e-02,  4.0039e-02, -1.0693e-01,  8.8379e-02,
         1.2451e-01,  2.0874e-02, -1.7090e-02,  9.7046e-03, -2.6489e-02,
        -8.7891e-02,  3.2501e-03, -1.3657e-03, -4.6387e-02,  2.2583e-02,
        -1.2109e-01,  1.4771e-02,  9.7046e-03, -5.0781e-02, -1.6098e-03,
        -3.9795e-02, -1.0107e-01, -2.0215e-01, -7.5684e-02,  1.5503e-02,
        -1.7822e-02, -1.3062e-02,  1.2354e-01, -5.8289e-03,  8.0078e-02,
        -1.9043e-02, -1.0059e-01, -7.8125e-02, -1.8188e-02, -1.3574e-01,
        -3.9795e-02, -6.6406e-02, -8.6426e-02,  1.4038e-03,  1.0205e-01,
        -4.0039e-02,  9.7046e-03,  8.7402e-02,  2.8687e-02, -6.9824e-02,
         8.3496e-02,  8.1055e-02,  6.9275e-03, -3.0823e-03,  4.6875e-02,
        -3.4180e-02, -1.4099e-02,  2.1696e-05,  5.6152e-02,  6.9336e-02,
        -6.1035e-02,  5.4932e-03, -1.3477e-01, -1.7212e-02, -4.8633e-01,
        -8.7891e-02,  3.9062e-02, -5.4016e-03, -2.2095e-02,  1.1914e-01,
        -8.4961e-02, -2.5024e-02,  2.2949e-02, -4.6387e-02, -8.9111e-03,
         2.7275e-04, -1.1047e-02,  1.1963e-02,  2.3071e-02, -3.4912e-02,
         3.8086e-02,  1.2939e-02,  6.3477e-02, -1.2085e-02,  3.8086e-02,
         7.9346e-03, -1.2402e-01, -1.8799e-02,  9.2285e-02, -3.7384e-03,
        -1.1963e-02,  7.0312e-02,  7.8613e-02, -3.5645e-02, -7.7637e-02,
        -5.7068e-03, -1.7822e-02, -6.5002e-03, -9.1797e-02, -2.9785e-02,
         4.4922e-02,  7.9956e-03,  6.4392e-03,  8.4961e-02,  3.8330e-02,
        -2.7344e-02,  2.2705e-02,  2.7344e-02, -1.9897e-02,  9.5215e-02,
         5.8105e-02,  6.6895e-02,  6.1279e-02,  6.1279e-02, -1.5918e-01,
         3.5156e-02, -3.0518e-02, -3.2471e-02,  1.6708e-03, -4.3457e-02,
        -8.3008e-02, -1.5320e-02,  3.9062e-02,  7.6904e-03, -5.9204e-03,
        -4.4434e-02, -7.3242e-02, -7.8125e-03,  3.8330e-02, -9.3384e-03,
        -3.3936e-02,  2.1729e-02, -1.3550e-02, -4.7874e-04, -1.6235e-02,
        -1.9653e-02, -5.3711e-03,  1.2817e-02, -6.2500e-02,  1.8997e-03,
        -5.4199e-02,  9.6436e-03,  2.6611e-02, -4.1504e-02, -1.7285e-01,
         2.0752e-02,  2.6733e-02,  2.8320e-02, -6.6833e-03,  5.6152e-03,
        -4.3335e-03,  6.0791e-02,  7.1106e-03,  2.1484e-02, -2.1484e-02,
         4.0527e-02,  7.1289e-02, -4.0039e-02, -3.4912e-02,  6.4453e-02,
         9.0942e-03,  8.6914e-02,  5.0049e-03,  3.4668e-02,  9.3994e-03,
         1.0864e-02, -4.4922e-02, -7.7148e-02, -1.8750e-01, -5.8289e-03,
         3.9795e-02, -4.8828e-03, -1.3672e-02, -1.2146e-02,  1.4832e-02,
         1.5564e-02, -8.0566e-02, -2.8076e-02,  2.2583e-02, -9.1797e-02,
         7.8735e-03, -3.4424e-02,  2.9907e-02,  1.5163e-04, -1.3428e-02,
         3.2227e-02,  3.8330e-02, -1.7212e-02, -4.5654e-02,  2.8076e-02,
         9.8705e-05,  3.8086e-02, -8.6670e-03, -2.4567e-03, -3.9551e-02,
         1.0156e-01,  1.5869e-02,  2.3804e-02, -5.3223e-02,  1.6479e-02,
        -5.5542e-03, -1.6602e-02,  5.3223e-02,  3.9551e-02,  2.6398e-03,
         1.2207e-02, -3.2349e-03, -7.8735e-03,  2.4780e-02, -6.1523e-02,
         2.9053e-02, -5.8350e-02,  6.7139e-03, -3.4180e-03, -5.1880e-03,
         1.4267e-03,  1.4771e-02,  2.1484e-02,  2.5024e-02, -4.6692e-03,
        -8.5938e-02, -1.3245e-02,  1.8082e-03, -3.4790e-03, -5.4932e-02,
        -2.1606e-02,  9.7168e-02,  1.5747e-02,  6.8359e-03,  2.2827e-02,
         3.8574e-02,  3.0518e-02, -3.1471e-04, -6.2500e-01,  4.9744e-03,
        -3.6865e-02, -1.0300e-03, -1.6113e-02,  3.3691e-02,  7.9102e-02,
        -2.6093e-03,  5.0049e-02, -2.5146e-02, -2.4414e-03,  4.4434e-02,
         5.8838e-02, -2.1973e-03, -3.5400e-02,  2.0630e-02,  1.2329e-02,
        -2.8198e-02, -3.1006e-02, -2.0020e-02, -1.0312e+00,  8.5449e-02,
        -4.9072e-02, -5.9570e-02, -7.4768e-03, -3.4424e-02, -8.2397e-03,
         4.8096e-02,  4.8340e-02, -6.7383e-02, -2.8198e-02,  1.6724e-02,
        -2.1606e-02, -6.9824e-02,  3.4180e-02, -2.5024e-02,  2.5482e-03,
        -2.4780e-02,  1.9653e-02, -7.8613e-02, -4.1992e-02,  1.8677e-02,
         4.1504e-02, -6.4941e-02,  1.6479e-02,  1.0889e-01,  6.9824e-02,
         1.6602e-02, -4.1504e-02, -8.3984e-02, -2.6245e-02, -3.7384e-03,
        -3.6865e-02,  6.4087e-03, -5.6396e-02, -3.6621e-02, -4.0588e-03,
         3.6133e-02,  2.6123e-02,  4.3457e-02,  3.4943e-03,  2.8931e-02,
        -2.9419e-02, -5.7068e-03, -5.2246e-02,  5.6152e-03,  4.8828e-03,
        -1.3733e-02,  1.1536e-02, -4.2969e-02,  6.7383e-02,  3.3936e-02,
         6.7871e-02,  4.4678e-02,  2.0996e-02,  9.6191e-02,  3.9368e-03,
        -5.6885e-02, -2.5391e-02,  3.9307e-02, -6.0303e-02, -9.0332e-02,
        -6.8054e-03, -4.0527e-02, -1.1084e-01,  5.9570e-02, -4.8340e-02,
         3.0151e-02,  7.5989e-03, -5.5908e-02, -1.1816e-01,  2.0020e-01,
         1.1963e-01, -1.3733e-02, -4.9744e-03, -2.9297e-02,  1.9653e-02,
        -2.2217e-02,  2.3242e-01, -2.6703e-03, -3.3447e-02, -3.5645e-02,
         2.0874e-02,  8.8379e-02, -4.4189e-02,  5.1025e-02, -3.0273e-02,
        -1.7822e-02,  4.4922e-02, -4.1260e-02, -6.0654e-04,  7.8125e-02,
         3.8330e-02,  2.7466e-02,  1.9531e-02,  2.7618e-03, -4.5654e-02,
         4.1809e-03, -4.1748e-02, -8.4839e-03, -2.6001e-02,  2.7832e-02,
         1.8164e-01,  1.0254e-02,  1.4526e-02, -5.4443e-02,  4.6875e-02,
        -3.5400e-02,  1.9165e-02,  6.8848e-02, -3.8086e-02, -5.2490e-02,
         1.0376e-02,  5.6152e-03, -2.9419e-02,  3.5156e-02, -7.0312e-02,
         1.2256e-01,  6.2988e-02,  1.4648e-01, -1.7334e-02, -9.0820e-02,
        -2.4121e-01, -8.3008e-02, -3.1494e-02,  1.0449e-01,  1.7188e-01,
        -8.5156e-01, -5.8350e-02, -1.4258e-01,  4.5654e-02,  2.2461e-02,
        -4.6631e-02,  1.0498e-01,  1.8262e-01,  1.8750e-01,  5.4199e-02,
        -8.0566e-02, -3.3691e-02, -3.7842e-02,  8.9355e-02, -1.0986e-02,
         1.0986e-01, -1.8433e-02, -1.0742e-01,  3.6377e-02,  9.1309e-02,
        -3.2043e-03, -5.2979e-02, -4.3945e-01,  5.0781e-02,  4.6387e-02,
        -6.6895e-02, -8.1055e-02,  8.4961e-02, -2.7954e-02,  6.2500e-02,
         1.0071e-03,  9.0820e-02,  2.0752e-02,  1.5918e-01, -4.2969e-02,
         1.1768e-01, -2.0508e-01,  1.3184e-01, -9.0942e-03, -1.4258e-01,
        -6.5430e-02,  1.1475e-02, -3.0078e-01, -5.4688e-02, -2.6489e-02,
        -1.0205e-01, -2.0605e-01, -2.8992e-03,  8.9844e-02,  7.0801e-02,
         1.4526e-02, -7.1289e-02,  1.0498e-01,  7.0801e-02, -6.0938e-01,
         3.3398e-01,  1.4551e-01,  9.6191e-02,  5.7373e-02, -5.8838e-02,
        -2.4414e-02,  1.0010e-01,  5.9082e-02, -4.6387e-02,  1.2500e-01,
        -1.6846e-02, -9.6680e-02, -1.5991e-02, -8.2520e-02, -4.7607e-03,
        -6.1768e-02, -2.6489e-02, -6.2988e-02,  4.2480e-02,  1.8768e-03,
        -7.7637e-02,  7.2266e-02,  1.9629e-01,  1.2256e-01, -1.2012e-01,
         3.0396e-02, -1.0059e-01, -3.5352e-01,  7.6172e-02, -4.7363e-02,
         8.2031e-02,  9.9609e-02,  6.6406e-02, -2.7100e-02,  5.8289e-03,
         3.1250e-02,  1.9775e-02, -1.0791e-01, -9.2773e-02, -3.7956e-04,
        -1.5039e-01, -5.9509e-03,  9.3994e-03,  3.3594e-01, -6.8848e-02,
        -8.6914e-02,  9.2773e-02, -5.1025e-02, -3.6133e-02,  7.5684e-02,
        -5.8350e-02, -5.6152e-02,  2.7161e-03, -4.2114e-03, -3.1494e-02,
         1.9775e-02,  4.5898e-02,  7.9346e-03, -3.7598e-02, -3.4485e-03,
        -2.4170e-02,  8.5449e-02])

llm.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0447, -0.0174,  0.0192,  ...,  0.0479,  0.0222, -0.0197],
        [ 0.0054, -0.0523,  0.0025,  ...,  0.0021, -0.0027, -0.0053],
        [ 0.0079, -0.0045, -0.0107,  ..., -0.0193, -0.0086, -0.0380],
        ...,
        [ 0.0036, -0.0144, -0.0087,  ..., -0.0172, -0.0206,  0.0005],
        [ 0.0286,  0.0018, -0.0111,  ...,  0.0474, -0.0016, -0.0336],
        [-0.0037, -0.0383, -0.0293,  ..., -0.0054,  0.0198,  0.0226]])

llm.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0161, -0.0235,  0.0118,  ...,  0.0078, -0.0405, -0.0241],
        [ 0.0041,  0.0180,  0.0410,  ...,  0.0349,  0.0381,  0.0053],
        [-0.0145, -0.0012,  0.0193,  ..., -0.0584, -0.0420,  0.0230],
        ...,
        [ 0.0455, -0.0450, -0.0048,  ..., -0.1082, -0.0424,  0.0261],
        [ 0.0859, -0.0057,  0.0059,  ..., -0.0830, -0.0969, -0.0109],
        [ 0.0216,  0.0077, -0.0341,  ...,  0.0595,  0.1161, -0.0287]])

llm.base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 1.3184e-02,  1.8066e-02, -3.6621e-02,  ..., -3.9551e-02,
         -2.3346e-03, -2.6611e-02],
        [ 4.6387e-03,  2.1362e-03, -3.2043e-04,  ...,  9.0942e-03,
          1.7090e-02, -2.1729e-02],
        [ 7.8125e-03, -3.4180e-02,  1.3580e-03,  ..., -1.3245e-02,
          8.9722e-03, -2.6001e-02],
        ...,
        [ 5.8289e-03, -2.1820e-03, -6.2561e-03,  ...,  2.1118e-02,
          1.8311e-03,  1.3199e-03],
        [-5.7697e-05,  3.0212e-03,  1.3062e-02,  ..., -1.3123e-02,
          1.1169e-02,  2.0905e-03],
        [-6.5308e-03,  5.7678e-03, -3.5400e-02,  ...,  1.5320e-02,
         -6.4087e-03, -1.6724e-02]])

llm.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0237,  0.0119, -0.0107,  ..., -0.0583, -0.0543, -0.0017],
        [-0.0530,  0.0376,  0.0021,  ..., -0.0231,  0.0485, -0.0566],
        [-0.0028,  0.0102, -0.0393,  ..., -0.0100,  0.0055,  0.0514],
        ...,
        [-0.0047,  0.0390, -0.0030,  ..., -0.0326,  0.0195,  0.0271],
        [ 0.0303,  0.0050, -0.0503,  ...,  0.0291,  0.0015, -0.0615],
        [ 0.0441,  0.0108, -0.0032,  ...,  0.0340,  0.0697, -0.0259]])

llm.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0193, -0.0031,  0.0188,  ..., -0.0308, -0.0345,  0.0131],
        [-0.0288,  0.0496, -0.0132,  ...,  0.0085,  0.0525,  0.0079],
        [-0.0237, -0.0056, -0.0222,  ..., -0.0089,  0.0236,  0.0341],
        ...,
        [-0.0040,  0.0100,  0.0300,  ..., -0.0016, -0.0059, -0.0110],
        [ 0.0438, -0.0192, -0.0259,  ..., -0.0201, -0.0147, -0.0168],
        [-0.0392, -0.0030,  0.0086,  ...,  0.0225,  0.0396,  0.0015]])

llm.base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0123, -0.0063,  0.0125,  ..., -0.0037, -0.0253,  0.0048],
        [-0.0234, -0.0021,  0.0076,  ..., -0.0053, -0.0195, -0.0065],
        [ 0.0242,  0.0038, -0.0059,  ...,  0.0030, -0.0038,  0.0214],
        ...,
        [ 0.0084, -0.0056,  0.0042,  ..., -0.0096, -0.0129,  0.0005],
        [ 0.0040,  0.0032,  0.0354,  ...,  0.0150,  0.0065, -0.0055],
        [-0.0148,  0.0239,  0.0206,  ..., -0.0122,  0.0057, -0.0123]])

llm.base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0450,  0.0288,  0.0963,  ...,  0.0407, -0.0129,  0.0073],
        [ 0.0394, -0.1063, -0.0206,  ...,  0.0192, -0.0274,  0.0587],
        [-0.0107, -0.1006, -0.0428,  ...,  0.0002,  0.0928, -0.0261],
        ...,
        [-0.0422, -0.0077, -0.0448,  ..., -0.0111, -0.0380, -0.0266],
        [-0.0264,  0.0003, -0.0436,  ..., -0.0007,  0.0455, -0.0258],
        [ 0.0692, -0.0086,  0.0354,  ..., -0.0046, -0.0716,  0.0111]])

llm.base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0029,  0.0102,  0.0200,  ...,  0.0072, -0.0064,  0.0565],
        [-0.0294, -0.0487,  0.0383,  ...,  0.0266,  0.0045, -0.0080],
        [-0.0133,  0.0074, -0.0286,  ...,  0.0001, -0.0294,  0.0262],
        ...,
        [-0.0253, -0.0167,  0.0112,  ..., -0.0258, -0.0112,  0.0408],
        [-0.0436, -0.0216, -0.0133,  ..., -0.0344, -0.0243, -0.0148],
        [ 0.0052,  0.0005,  0.0225,  ..., -0.0514,  0.0286, -0.0074]])

llm.base_model.model.model.layers.22.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0186, -0.0148,  0.0121,  ..., -0.0106, -0.0072,  0.0245],
        [ 0.0041, -0.0023, -0.0057,  ..., -0.0055,  0.0089,  0.0278],
        [-0.0071,  0.0004, -0.0160,  ..., -0.0203, -0.0056,  0.0033],
        ...,
        [ 0.0056, -0.0299,  0.0055,  ...,  0.0162, -0.0096,  0.0273],
        [ 0.0258,  0.0188,  0.0212,  ...,  0.0256,  0.0031,  0.0096],
        [ 0.0206,  0.0190,  0.0143,  ..., -0.0003,  0.0021,  0.0003]])

llm.base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0127, -0.0189,  0.0691,  ...,  0.0703,  0.0289,  0.0365],
        [-0.0501,  0.0072, -0.0517,  ...,  0.0206, -0.0408,  0.0320],
        [-0.0413,  0.0307, -0.0031,  ...,  0.0535,  0.0666,  0.0703],
        ...,
        [ 0.0259, -0.0206,  0.0027,  ..., -0.0264, -0.0069, -0.0199],
        [-0.0754,  0.0084,  0.0352,  ..., -0.0271, -0.0051,  0.0866],
        [ 0.0566, -0.0364,  0.0658,  ..., -0.0092, -0.0408,  0.0531]])

llm.base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0168,  0.0253, -0.0071,  ..., -0.0227,  0.0508,  0.0050],
        [ 0.0105, -0.0304,  0.0365,  ..., -0.0055, -0.0151,  0.0281],
        [-0.0206, -0.0199,  0.0242,  ...,  0.0131,  0.0293,  0.0024],
        ...,
        [-0.0109, -0.0119, -0.0331,  ..., -0.0075,  0.0319, -0.0128],
        [-0.0214, -0.0120,  0.0151,  ..., -0.0262, -0.0229, -0.0141],
        [-0.0553, -0.0217,  0.0097,  ...,  0.0249,  0.0183,  0.0020]])

llm.base_model.model.model.layers.22.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 0.0181,  0.0093, -0.0087,  ...,  0.0200,  0.0036,  0.0239],
        [ 0.0317, -0.0145, -0.0145,  ...,  0.0001, -0.0074,  0.0222],
        [ 0.0117,  0.0105,  0.0094,  ...,  0.0060, -0.0075, -0.0101],
        ...,
        [-0.0206,  0.0200,  0.0042,  ..., -0.0045,  0.0031, -0.0082],
        [-0.0182, -0.0073, -0.0228,  ...,  0.0151,  0.0195,  0.0082],
        [-0.0055, -0.0282,  0.0056,  ...,  0.0151,  0.0117,  0.0003]])

llm.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[-0.0239,  0.0002, -0.0227,  ..., -0.0075, -0.0271,  0.0262],
        [-0.0202,  0.0351,  0.0045,  ...,  0.0101,  0.0444, -0.0190],
        [-0.0204, -0.0304,  0.0043,  ..., -0.0029, -0.0411,  0.0119],
        ...,
        [-0.0106, -0.0181, -0.0491,  ..., -0.0274,  0.0225, -0.0076],
        [-0.0262, -0.0197,  0.0098,  ...,  0.0129,  0.0479, -0.0276],
        [ 0.0245,  0.0046,  0.0206,  ...,  0.0271,  0.0114, -0.0113]])

llm.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0142, -0.0161, -0.0452,  ...,  0.0168, -0.0192, -0.0183],
        [-0.0260, -0.0081,  0.0108,  ..., -0.0067,  0.0757,  0.0346],
        [ 0.0045, -0.0247, -0.0084,  ..., -0.0196, -0.0282,  0.0160],
        ...,
        [ 0.0432,  0.1019,  0.0094,  ...,  0.0591,  0.0221, -0.0230],
        [-0.0351,  0.0648, -0.0206,  ..., -0.0977, -0.0240,  0.0359],
        [ 0.0115, -0.0059, -0.0231,  ..., -0.0456,  0.0672, -0.0049]])

llm.base_model.model.model.layers.22.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.2891, 0.8438, 1.2422,  ..., 0.9531, 1.0234, 1.0625])

llm.base_model.model.model.layers.22.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.4375, 1.4219, 1.5781,  ..., 1.1406, 1.5000, 1.4609])

llm.base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0131,  0.0133, -0.0051,  ...,  0.0179, -0.0056, -0.0046],
        [-0.0198, -0.0304,  0.0113,  ...,  0.0014,  0.0330, -0.0143],
        [-0.0095, -0.0027,  0.0014,  ..., -0.0192,  0.0018, -0.0214],
        ...,
        [-0.0210, -0.0162, -0.0020,  ...,  0.0081,  0.0121,  0.0159],
        [ 0.0181, -0.0162, -0.0203,  ..., -0.0055, -0.0208,  0.0024],
        [ 0.0200, -0.0121, -0.0015,  ..., -0.0106, -0.0039,  0.0187]])

llm.base_model.model.model.layers.23.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.1758, -0.5117, -0.0031,  ..., -0.8320,  0.3379,  0.0718])

llm.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0321,  0.0193,  0.0871,  ..., -0.0093,  0.0358, -0.0294],
        [ 0.0528, -0.0145, -0.0105,  ...,  0.0411, -0.0128,  0.0236],
        [-0.0087,  0.0196,  0.0091,  ...,  0.0601, -0.0867, -0.0481],
        ...,
        [ 0.0177, -0.0024,  0.0282,  ..., -0.0021,  0.0433,  0.0011],
        [-0.0208, -0.0099,  0.0260,  ..., -0.0110, -0.0956,  0.0254],
        [ 0.0403,  0.0388, -0.0702,  ..., -0.0053,  0.0052, -0.0055]])

llm.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0247, -0.0110, -0.0186,  ..., -0.0063, -0.0124,  0.0263],
        [-0.0083,  0.0239,  0.0168,  ...,  0.0138,  0.0124,  0.0005],
        [ 0.0148, -0.0094,  0.0594,  ...,  0.0022,  0.0183,  0.0242],
        ...,
        [ 0.0093, -0.0024,  0.0150,  ..., -0.0060, -0.0329, -0.0024],
        [-0.0490, -0.0225, -0.0041,  ..., -0.0232,  0.0192,  0.0195],
        [ 0.0159, -0.0230, -0.0396,  ..., -0.0297, -0.0024, -0.0082]])

llm.base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0036,  0.0012,  0.0018,  ...,  0.0127,  0.0068, -0.0059],
        [ 0.0247,  0.0139,  0.0046,  ...,  0.0009, -0.0035, -0.0032],
        [ 0.0135, -0.0164,  0.0073,  ...,  0.0062, -0.0116,  0.0111],
        ...,
        [-0.0031, -0.0056, -0.0181,  ...,  0.0034, -0.0261,  0.0161],
        [ 0.0245, -0.0175, -0.0087,  ...,  0.0184, -0.0027, -0.0264],
        [ 0.0091,  0.0085,  0.0091,  ..., -0.0253, -0.0056, -0.0015]])

llm.base_model.model.model.layers.23.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-8.8281e-01,  1.0205e-01,  1.0693e-01,  5.7422e-01,  2.4805e-01,
        -5.6250e-01, -3.2422e-01, -2.2827e-02,  5.7812e-01,  1.6992e-01,
        -1.1377e-01,  2.6123e-02, -3.2471e-02, -1.8945e-01, -3.5547e-01,
        -5.0049e-02,  1.0254e-01,  6.9141e-01,  1.9141e-01,  7.6660e-02,
        -9.7656e-01, -1.6992e-01,  2.2461e-02,  3.4766e-01, -8.2031e-01,
        -3.4375e-01,  1.4114e-03, -1.3125e+00,  2.1777e-01, -2.8809e-02,
        -7.2266e-01, -6.1523e-02, -1.5234e-01,  4.5117e-01, -2.7344e-02,
        -1.5723e-01, -1.4219e+00,  9.0332e-02, -3.2422e-01, -1.8848e-01,
        -7.9590e-02, -3.6621e-02, -5.3467e-02, -2.7344e-01,  2.0801e-01,
        -2.2461e-01, -9.9609e-02, -2.3633e-01,  7.4219e-02, -4.5410e-02,
         1.2793e-01,  1.7383e-01, -2.0312e-01,  1.0400e-01, -1.3672e-01,
         1.6992e-01,  3.7842e-02, -1.0078e+00,  1.0938e-01, -1.3184e-02,
        -3.3203e-01,  7.8906e-01, -3.3750e+00, -1.5332e-01, -5.3955e-02,
        -4.6680e-01, -2.7344e-01,  7.5391e-01,  1.6602e-01,  4.6875e-01,
         1.8652e-01, -2.8516e-01,  6.9922e-01, -8.6426e-02,  7.3828e-01,
        -2.1289e-01, -8.4229e-03,  1.9434e-01,  1.1562e+00, -1.7285e-01,
        -1.6504e-01, -3.1445e-01, -4.1748e-02,  4.8584e-02,  3.4961e-01,
        -1.2695e-01, -5.9814e-02, -4.3359e-01, -9.1406e-01,  2.8320e-01,
         1.6602e-01, -1.6699e-01,  4.3213e-02,  1.6211e-01, -1.4375e+00,
        -8.7402e-02, -5.4443e-02, -2.3926e-01, -2.6245e-03, -2.4023e-01,
         4.8438e-01, -3.7354e-02, -1.7090e-01, -3.2959e-02, -6.2988e-02,
        -7.3853e-03,  1.3516e+00, -7.0312e-02, -3.0469e-01, -2.2461e-01,
         2.1582e-01, -3.9978e-03,  3.1562e+00,  9.5215e-02,  3.3936e-02,
         4.3457e-02, -1.2891e-01,  1.5234e-01, -3.1250e-01,  3.0469e-01,
        -1.2012e-01, -1.0059e-01, -3.5742e-01, -5.8984e-01,  2.7148e-01,
        -9.8267e-03, -3.8125e+00,  4.3750e-01,  1.6992e-01,  6.0938e-01,
         5.7812e-01,  1.3086e-01, -8.7891e-02, -2.4707e-01, -1.6602e-01,
         5.8594e-02,  1.0400e-01, -2.7734e-01, -2.1484e-02, -6.0547e-01,
         1.5234e-01,  1.0107e-01, -3.1445e-01, -4.0234e-01, -1.6406e-01,
        -4.2969e-01,  9.7168e-02, -4.9805e-01,  1.1172e+00,  3.8477e-01,
        -2.5781e-01, -1.3306e-02, -9.5312e-01,  1.5234e-01,  1.6484e+00,
         1.5137e-01,  4.9609e-01, -5.9570e-02, -2.8125e-01,  2.6719e+00,
        -9.9609e-02,  1.0547e-01,  2.5781e-01,  1.0254e-01, -1.3770e-01,
        -4.2725e-02, -3.2959e-02,  5.1270e-02, -9.4727e-02,  4.6094e-01,
         1.6403e-03,  4.2969e-01, -8.8672e-01, -1.6016e-01,  4.0000e+00,
        -3.3594e-01, -3.4668e-02,  2.2754e-01,  3.0664e-01, -1.0059e-01,
        -2.7148e-01,  3.2806e-04, -1.6504e-01, -1.7773e-01,  1.7773e-01,
        -5.0391e-01,  6.4844e-01,  5.7422e-01, -3.9062e-02, -2.5000e+00,
        -8.7891e-02, -4.2188e-01,  7.1875e-01,  9.3262e-02, -2.0410e-01,
        -2.6562e-01,  2.7344e-01, -7.7344e-01,  4.4922e-01, -3.6719e-01,
         2.4805e-01, -8.6328e-01, -3.3008e-01, -4.0039e-01,  1.6504e-01,
         4.1992e-01, -1.0469e+00,  6.6895e-02, -3.3984e-01, -1.0391e+00,
         2.0312e-01, -1.8457e-01, -2.0020e-01, -3.1250e-02, -8.7500e-01,
         5.5469e-01, -3.6621e-02,  1.0703e+00, -3.6328e-01,  1.7773e-01,
         9.9609e-01, -4.6875e-02,  1.7188e-01, -2.4609e-01, -1.2988e-01,
         1.5723e-01,  5.0391e-01,  2.6953e-01, -1.8652e-01, -9.4238e-02,
         2.3125e+00, -1.4648e-01,  3.3594e-01,  2.7188e+00, -2.1875e-01,
         1.3281e-01,  1.8750e-01, -7.0801e-02, -1.0469e+00, -7.1875e-01,
        -1.6016e-01, -1.6406e-01,  2.5391e-01,  2.2754e-01,  6.8750e-01,
        -4.1809e-03, -4.5312e-01, -1.4941e-01,  4.9219e-01, -3.2031e-01,
        -2.4902e-01, -4.2725e-02,  3.0664e-01,  3.3594e+00, -5.3906e-01,
         8.5938e-01,  7.4219e-01,  1.8848e-01, -2.3535e-01, -5.3906e-01,
         6.6016e-01, -3.3203e-01, -2.7539e-01, -7.8516e-01, -1.6895e-01,
        -3.0664e-01, -1.8750e-01,  5.0735e-04,  1.7480e-01,  1.7969e-01,
        -1.7578e-01,  8.8501e-03,  1.0938e+00, -2.3730e-01,  2.3828e-01,
        -1.0703e+00, -2.2583e-02, -9.4727e-02,  8.5938e-02,  6.3965e-02,
         1.7480e-01,  1.1768e-01,  7.1875e-01, -2.2656e-01,  1.2451e-01,
         9.0625e-01, -1.7578e-01,  2.4414e-01,  9.8145e-02, -1.1797e+00,
         1.2988e-01, -1.4062e-01,  4.5898e-02, -1.2500e-01,  1.7500e+00,
        -4.5703e-01,  2.2852e-01, -1.2109e-01, -3.0078e-01,  3.5938e-01,
        -1.8457e-01,  9.6094e-01, -2.3750e+00,  4.6484e-01, -9.8145e-02,
        -1.6016e-01, -6.7383e-02,  6.7383e-02, -2.9102e-01, -5.9570e-02,
        -1.9141e-01,  8.7891e-03,  2.2656e-01, -5.7422e-01,  7.7637e-02,
         2.9492e-01, -3.3906e+00,  1.9922e-01, -1.2695e-01,  4.6094e-01,
        -2.7539e-01, -4.0234e-01,  4.2383e-01,  9.4727e-02, -1.6797e-01,
         1.9727e-01,  8.7500e-01,  7.7148e-02,  8.1055e-02, -3.2031e-01,
        -9.0625e-01, -1.9141e-01,  5.8203e-01, -3.4961e-01,  4.2969e-01,
         4.0430e-01, -1.7578e-01, -2.0508e-01,  1.9434e-01,  3.6328e-01,
         3.2812e-01, -1.0078e+00,  5.8594e-01,  1.6211e-01, -3.7109e-01,
         1.2891e+00, -2.6562e-01, -5.0000e-01,  7.0312e-02, -4.6094e-01,
        -3.7500e-01,  1.9062e+00, -1.8359e-01,  2.6758e-01, -4.4922e-01,
         1.0437e-02,  2.3438e-01,  3.2031e-01, -6.2866e-03,  2.2754e-01,
        -2.6367e-01, -3.1250e-01,  1.9609e+00,  3.3594e-01,  2.0312e-01,
         2.6172e-01, -4.6875e-01,  2.2754e-01, -6.1768e-02, -1.2451e-01,
        -2.0996e-02, -2.3682e-02, -1.0400e-01, -1.0254e-01,  7.6599e-03,
        -1.4062e-01,  5.9766e-01,  1.0703e+00,  1.6016e-01,  1.4404e-02,
         3.0312e+00, -3.7891e-01,  8.2520e-02, -7.5391e-01,  3.2812e-01,
        -8.2422e-01,  4.9609e-01, -1.1406e+00,  4.8047e-01, -3.2031e-01,
         2.7344e-01,  1.3906e+00,  2.7734e-01,  2.9541e-02,  4.5410e-02,
        -2.0508e-01, -2.0215e-01,  1.5859e+00, -3.4375e-01, -2.4023e-01,
        -4.1797e-01,  1.6992e-01, -3.4180e-01,  1.1133e-01, -2.2266e-01,
         7.8613e-02,  3.2227e-01,  2.9297e-01, -4.8340e-02, -2.3145e-01,
         3.2031e-01,  3.3984e-01, -1.2360e-03,  2.8125e-01,  7.3730e-02,
        -3.3398e-01,  2.1680e-01,  1.8906e+00, -2.7734e-01, -1.0205e-01,
        -2.4316e-01,  5.7678e-03,  1.0469e+00, -9.0332e-02,  5.2734e-01,
        -3.5889e-02, -1.3086e-01, -1.6016e-01, -2.4902e-02,  4.5312e-01,
         1.7676e-01, -1.3379e-01, -2.8320e-01,  3.7956e-04, -4.2383e-01,
        -5.3750e+00, -1.3123e-02, -2.3071e-02, -1.9824e-01,  1.1536e-02,
         2.4023e-01, -1.5918e-01, -6.9922e-01,  1.0625e+00, -9.0332e-02,
         7.8125e-02,  1.0645e-01,  1.3086e-01, -1.3594e+00,  9.4238e-02,
         5.1562e-01, -1.7188e-01,  2.7539e-01, -4.1199e-03,  6.8848e-02,
         1.4453e-01,  6.9141e-01,  1.9434e-01, -3.7305e-01, -7.4219e-01,
         3.7500e-01, -1.1963e-01,  2.1973e-01, -4.3555e-01, -1.4551e-01,
         1.7031e+00, -1.9629e-01, -4.4727e-01,  1.9434e-01, -1.4844e-01,
         7.3242e-02,  1.7266e+00,  6.9141e-01, -1.3867e-01,  1.0859e+00,
        -1.6992e-01,  4.1406e-01,  7.2656e-01,  3.4570e-01, -3.8281e-01,
         1.3672e-01,  1.0781e+00,  2.9688e-01, -9.9609e-02, -2.9297e-01,
        -1.9922e-01,  3.5938e-01, -3.7109e-02, -3.2031e-01,  2.7930e-01,
         1.6895e-01,  2.3047e-01,  4.3164e-01,  1.7969e-01,  3.5156e-01,
        -6.0303e-02,  1.9531e-01, -2.8320e-01,  1.2402e-01,  1.2031e+00,
         2.2656e-01, -4.3457e-02, -1.2878e-02,  6.5918e-03, -4.0234e-01,
        -2.6562e-01, -4.0430e-01,  7.8906e-01,  3.8867e-01, -2.5513e-02,
         3.7500e-01,  8.8281e-01])

llm.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 1.8160e-02, -1.6043e-02,  3.8838e-04,  ..., -4.0014e-02,
          5.2168e-02,  4.7308e-02],
        [-1.6940e-02, -2.3648e-02,  1.0754e-02,  ...,  4.2330e-03,
         -3.3891e-02, -8.3709e-02],
        [-6.1857e-02, -5.3398e-02, -8.9353e-02,  ..., -9.5375e-03,
         -1.3470e-02, -3.7369e-02],
        ...,
        [ 4.4366e-03,  3.2031e-05, -2.0867e-02,  ..., -2.7084e-02,
          9.9213e-03, -2.3402e-02],
        [-1.4477e-02, -5.5258e-02, -1.2706e-02,  ..., -3.2916e-02,
          3.9280e-02, -1.2168e-04],
        [-1.2832e-03, -4.7850e-02,  1.3671e-02,  ..., -1.5221e-02,
          3.4364e-02,  3.5542e-03]])

llm.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-1.6122e-02, -1.6930e-04, -4.2489e-03,  ...,  3.8511e-03,
          7.6037e-05, -1.8914e-02],
        [-2.5619e-02, -5.7109e-03, -5.7362e-03,  ...,  4.0514e-02,
         -1.3819e-02, -8.0943e-03],
        [ 6.9344e-03, -2.1380e-02,  7.9579e-03,  ...,  2.5496e-02,
          3.8187e-03,  1.8015e-03],
        ...,
        [ 2.2179e-02, -4.3627e-02,  4.2622e-02,  ...,  2.3977e-02,
          2.9545e-02,  1.8199e-02],
        [-2.9395e-02,  8.0697e-04,  1.6198e-03,  ..., -3.4982e-03,
         -4.1926e-02, -1.6831e-02],
        [-7.3837e-02,  3.8871e-02,  5.0665e-02,  ..., -2.5810e-02,
          4.1127e-03,  2.3641e-02]])

llm.base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-9.4414e-05, -1.5991e-02, -1.1826e-03,  ...,  1.2939e-02,
          5.2185e-03, -2.0447e-03],
        [ 2.4261e-03,  6.8665e-03,  1.3916e-02,  ..., -2.0905e-03,
         -1.4832e-02, -6.1340e-03],
        [-2.1484e-02, -1.9409e-02, -2.4109e-03,  ..., -3.7354e-02,
          5.6458e-04, -7.7515e-03],
        ...,
        [ 1.0681e-02,  1.6602e-02,  8.6784e-05,  ...,  2.2217e-02,
         -9.7656e-04, -1.7578e-02],
        [ 6.1035e-03,  3.1738e-02, -9.2773e-03,  ...,  1.0681e-02,
         -1.2024e-02, -2.2430e-03],
        [ 1.0681e-03, -1.2146e-02, -2.6550e-03,  ...,  4.6631e-02,
         -3.5400e-02,  1.0986e-02]])

llm.base_model.model.model.layers.23.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 2.8320e-02, -4.7607e-02, -3.3203e-02, -6.8359e-03,  1.7944e-02,
        -2.2339e-02, -1.0449e-01, -9.0820e-02, -6.9824e-02,  6.5918e-02,
        -1.4771e-02, -5.1270e-02,  5.5469e-01, -1.1475e-02, -4.7363e-02,
        -4.1016e-02,  8.3984e-02,  1.1768e-01,  4.7119e-02, -7.8613e-02,
        -5.3223e-02,  3.3447e-02,  1.5991e-02, -6.7383e-02,  3.1982e-02,
         7.0312e-02,  9.9121e-02, -7.7148e-02,  2.4658e-02,  1.2207e-02,
         9.2773e-02, -6.0791e-02,  3.5156e-02, -1.4954e-02,  1.9287e-02,
         4.1748e-02,  1.3184e-02, -2.5024e-02, -6.4392e-03,  5.2490e-02,
         1.2817e-03,  1.2817e-02,  9.1309e-02,  5.5420e-02,  3.0060e-03,
         3.4180e-02,  3.9307e-02,  5.9814e-02,  3.4142e-04,  1.0107e-01,
         5.9814e-02, -1.1749e-03,  8.1787e-03, -2.0996e-02,  1.4709e-02,
         1.1426e-01, -3.1982e-02, -2.9175e-02, -5.9814e-02, -4.1992e-02,
         1.1475e-02,  1.1230e-02,  5.4443e-02, -7.3730e-02,  4.8584e-02,
        -6.1035e-02, -1.3550e-02, -5.8105e-02,  5.2734e-02,  4.4678e-02,
        -1.3367e-02,  1.1353e-02,  1.6479e-02, -1.1621e-01, -1.3611e-02,
         6.9824e-02,  3.4912e-02,  9.8633e-02, -2.5269e-02,  2.3682e-02,
        -4.2969e-02,  1.6968e-02, -4.2969e-02,  6.1035e-03, -1.2207e-02,
         2.1729e-02, -2.4414e-02, -6.3477e-03,  1.0193e-02, -5.3955e-02,
        -1.4954e-02,  6.3965e-02, -1.4526e-02,  2.6398e-03, -5.4199e-02,
         4.4922e-02,  1.1621e-01, -3.9551e-02, -1.9653e-02,  5.1270e-02,
         1.8921e-02,  2.7832e-02,  3.3691e-02,  1.5564e-02,  1.6556e-03,
        -7.2754e-02, -7.7148e-02, -3.2471e-02, -4.8584e-02,  2.2583e-02,
        -1.4844e-01, -4.2725e-03, -3.7354e-02,  2.2827e-02,  7.4707e-02,
         2.9419e-02, -3.2471e-02,  1.2741e-03,  3.2227e-02,  3.3203e-02,
        -7.4219e-02, -1.7944e-02, -3.3417e-03, -1.0315e-02,  2.1118e-02,
         2.0142e-02, -4.3457e-02,  3.4180e-02, -7.9590e-02,  6.6895e-02,
        -1.5747e-02, -3.8574e-02, -2.5757e-02, -5.9814e-02, -8.6426e-02,
        -2.2705e-02,  5.7678e-03, -5.8594e-02, -4.6875e-02, -9.2773e-02,
         5.2734e-02, -2.6367e-02, -3.8574e-02,  1.1047e-02,  3.4912e-02,
         5.5420e-02,  8.0566e-02,  9.0332e-02, -4.0283e-02, -6.2500e-02,
        -4.5654e-02,  1.0742e-02,  1.9165e-02, -8.3496e-02,  2.1606e-02,
        -9.9121e-02,  3.1006e-02, -8.8867e-02,  2.0142e-02,  6.4850e-04,
         7.9102e-02, -1.4453e-01,  3.5645e-02,  4.8218e-03,  3.6621e-02,
         4.1260e-02, -6.8970e-03, -3.9795e-02,  2.0215e-01,  7.7209e-03,
        -1.0791e-01,  6.9824e-02,  1.1406e+00,  2.4902e-02, -2.2705e-02,
         2.5391e-02,  3.1738e-02,  1.1780e-02, -5.2795e-03, -7.2937e-03,
         2.4536e-02, -5.7983e-03,  4.2725e-02,  1.3086e-01,  2.0752e-02,
         3.6621e-02, -5.3223e-02, -1.8433e-02,  3.1982e-02,  9.4238e-02,
        -6.4087e-04, -1.1536e-02,  5.2002e-02, -9.5215e-02, -1.9409e-02,
         1.1523e-01, -2.2339e-02,  5.3223e-02, -9.9487e-03,  3.9368e-03,
        -1.9531e-02, -3.4668e-02, -2.5024e-02, -3.2959e-02,  1.8555e-02,
         5.9082e-02, -7.5195e-02, -1.5381e-02, -5.5908e-02, -8.4839e-03,
        -4.3213e-02,  4.7363e-02, -1.0059e-01, -2.2217e-02, -1.6113e-02,
        -3.9307e-02, -9.7656e-02,  1.0498e-01,  3.0640e-02,  2.1973e-02,
        -7.6294e-03,  3.5400e-02, -2.9419e-02,  3.6377e-02, -4.4922e-02,
        -4.0527e-02,  8.7402e-02,  4.5410e-02,  9.8267e-03, -2.7222e-02,
        -6.0059e-02,  1.0620e-02,  3.4912e-02,  4.1748e-02,  2.2827e-02,
        -2.5195e-01, -8.2031e-02, -4.8096e-02, -5.3467e-02,  5.9814e-02,
         1.5820e-01, -1.1902e-03,  6.0547e-02, -2.9907e-02, -1.0059e-01,
        -6.1768e-02,  7.9346e-03, -7.5684e-02,  2.0386e-02,  4.1504e-03,
        -1.0205e-01, -7.5195e-02, -4.0283e-02, -3.0029e-02,  6.2988e-02,
         1.8311e-02,  3.6377e-02, -1.2988e-01,  1.4954e-02,  3.0518e-02,
        -1.6992e-01,  5.5664e-02, -1.0645e-01, -1.2402e-01,  3.2715e-02,
         6.2561e-03,  1.1865e-01,  1.9897e-02, -1.6211e-01, -4.5166e-02,
        -1.3086e-01, -5.0049e-02,  1.6479e-02,  8.6914e-02, -8.8867e-02,
        -6.9824e-02, -1.2085e-02, -1.0986e-01, -1.8799e-02, -4.8828e-02,
        -9.7168e-02,  1.5503e-02, -1.2402e-01,  1.8799e-02,  7.0801e-02,
        -8.1787e-03, -4.4922e-02,  6.2012e-02,  4.6875e-02, -1.3855e-02,
        -8.9111e-03, -6.4697e-03,  2.2583e-02, -1.9741e-04, -2.8610e-04,
         2.3438e-02, -2.4805e-01,  7.2754e-02, -1.1816e-01,  1.2402e-01,
         1.2354e-01,  2.2827e-02,  1.1035e-01, -9.2773e-03,  9.7656e-03,
         6.3477e-02,  3.4668e-02,  8.3984e-02,  7.0312e-02,  6.9824e-02,
         5.3467e-02,  1.3611e-02, -6.6895e-02, -3.8574e-02, -9.4604e-03,
         4.8340e-02, -3.3936e-02,  5.4688e-02,  7.3730e-02, -7.5684e-03,
         7.1289e-02, -5.1270e-02,  8.8379e-02, -5.4199e-02,  6.4087e-03,
         3.3203e-02,  1.0938e-01, -1.5137e-01, -2.1484e-02,  6.9336e-02,
         1.4160e-01,  3.1433e-03, -5.1514e-02,  1.6797e-01, -7.6172e-02,
        -4.1504e-02, -9.5703e-02,  5.8594e-02, -2.6001e-02, -1.0803e-02,
         8.4839e-03,  4.9744e-03,  1.7285e-01, -7.3730e-02, -1.8799e-02,
        -1.6846e-02, -6.0547e-02,  1.6309e-01, -9.7656e-02, -8.2520e-02,
        -4.4922e-02, -5.6152e-02,  7.5378e-03,  8.7891e-02,  8.0078e-02,
        -3.2715e-02, -3.4668e-02,  8.4961e-02, -3.9062e-02,  6.8359e-02,
        -4.6143e-02, -2.4170e-02, -7.3730e-02, -1.6406e-01,  2.5146e-02,
        -6.8665e-03, -6.4453e-02,  1.6785e-03,  6.1768e-02, -5.2490e-02,
         1.0742e-02,  1.4551e-01, -8.1055e-02,  3.0273e-02, -7.9102e-02,
         1.6022e-03,  2.7832e-02, -8.4229e-03, -1.0645e-01, -4.9561e-02,
         1.4355e-01, -3.9795e-02, -2.5879e-02, -1.0840e-01,  1.5723e-01,
        -2.6562e-01, -1.3770e-01, -2.5000e-01,  1.3867e-01,  1.8848e-01,
         2.7344e-01,  1.4844e-01, -6.2256e-02, -1.8359e-01, -1.2891e-01,
         1.3867e-01,  3.3008e-01,  6.4844e-01,  2.3633e-01,  1.3672e-01,
         1.8066e-01,  2.5781e-01, -1.6504e-01, -1.9336e-01,  2.2656e-01,
         1.4844e-01,  1.7676e-01,  1.6211e-01, -6.0547e-02, -9.8633e-02,
         3.4180e-02,  8.6426e-02, -1.3379e-01, -9.2773e-02,  1.0693e-01,
        -6.8359e-02,  1.7871e-01, -2.3047e-01,  1.4258e-01,  1.5430e-01,
         2.7539e-01,  1.7188e-01, -1.2500e-01,  1.0352e-01, -1.3379e-01,
        -2.7734e-01,  1.6016e-01,  3.3984e-01,  1.1279e-01,  4.9316e-02,
         2.5000e-01,  3.1250e-01,  1.1523e-01, -3.2031e-01, -2.2949e-01,
         1.2109e-01,  1.5137e-01,  4.1748e-02, -1.4160e-01, -1.4355e-01,
         1.6602e-01,  3.2471e-02, -2.3242e-01,  8.3496e-02,  2.1582e-01,
         2.0117e-01,  3.1055e-01,  9.8633e-02, -1.6846e-02, -3.9307e-02,
         6.9885e-03, -3.6719e-01,  4.0283e-03, -4.1016e-02,  2.5586e-01,
        -9.3750e-02,  7.8125e-03, -9.7656e-02, -1.9824e-01,  2.1191e-01,
        -6.2988e-02,  1.2305e-01,  2.5977e-01,  2.3633e-01, -2.8711e-01,
         1.5234e-01,  1.4746e-01, -1.0352e-01, -1.7871e-01, -1.2988e-01,
         2.4805e-01, -8.7891e-02, -2.6172e-01,  1.7676e-01, -2.1667e-03,
         2.0117e-01, -5.5078e-01, -4.8584e-02,  1.0645e-01,  9.1797e-02,
         6.6875e+00,  1.1377e-01, -2.5391e-02, -9.3750e-02,  1.9727e-01,
         2.0703e-01, -1.2109e-01,  7.7148e-02,  1.0693e-01,  2.2559e-01,
        -1.5332e-01, -2.2852e-01, -4.0527e-02, -1.5039e-01,  1.0791e-01,
         4.2969e-02,  7.7637e-02,  2.5781e-01,  2.9102e-01, -3.5400e-02,
         2.0117e-01,  3.1250e-01,  1.6895e-01, -1.2207e-01, -3.0078e-01,
        -3.7695e-01,  1.3770e-01, -2.2266e-01, -2.2363e-01,  5.5908e-02,
        -9.9121e-02,  6.0791e-02])

llm.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0103,  0.0351, -0.0088,  ...,  0.0489,  0.0480, -0.0241],
        [ 0.0194, -0.0447,  0.0742,  ...,  0.0309,  0.0164,  0.0162],
        [-0.0083, -0.0130,  0.0074,  ...,  0.0310, -0.0039,  0.0950],
        ...,
        [-0.0046,  0.0659, -0.0051,  ..., -0.0402,  0.0115, -0.0268],
        [ 0.0060,  0.0375, -0.0307,  ...,  0.0436, -0.0464,  0.0184],
        [ 0.0037, -0.0111, -0.0561,  ..., -0.0032,  0.0436, -0.0035]])

llm.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0408,  0.0012,  0.0052,  ..., -0.0172,  0.0196,  0.0074],
        [-0.0213, -0.0153,  0.0111,  ..., -0.0135, -0.0325, -0.0187],
        [-0.0264,  0.0185, -0.0094,  ...,  0.0286, -0.0113,  0.0302],
        ...,
        [ 0.0209, -0.0060, -0.0048,  ...,  0.0051,  0.0462, -0.0087],
        [-0.0242,  0.0100,  0.0085,  ...,  0.0646,  0.0126,  0.0101],
        [ 0.0632,  0.0104,  0.0177,  ...,  0.0232, -0.0198, -0.0194]])

llm.base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0019, -0.0103, -0.0187,  ...,  0.0044,  0.0171, -0.0029],
        [ 0.0052, -0.0098,  0.0045,  ...,  0.0142,  0.0238,  0.0085],
        [ 0.0225,  0.0039, -0.0110,  ...,  0.0082,  0.0096, -0.0082],
        ...,
        [-0.0249,  0.0103, -0.0139,  ...,  0.0179, -0.0068,  0.0217],
        [ 0.0070,  0.0118,  0.0063,  ..., -0.0036, -0.0215,  0.0129],
        [ 0.0052, -0.0096,  0.0276,  ..., -0.0093,  0.0068, -0.0060]])

llm.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0282, -0.0250, -0.0483,  ...,  0.0340, -0.0059, -0.0325],
        [-0.0349, -0.0001,  0.0446,  ...,  0.0407,  0.0045, -0.0215],
        [-0.0114, -0.0374, -0.0038,  ...,  0.0638,  0.0289, -0.0001],
        ...,
        [-0.0099,  0.0543, -0.0397,  ..., -0.0116,  0.0011, -0.0007],
        [ 0.0301, -0.0008,  0.0002,  ..., -0.0020,  0.0252, -0.0170],
        [ 0.0764, -0.0004,  0.0183,  ...,  0.0051, -0.0024, -0.0552]])

llm.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0029, -0.0302, -0.0073,  ...,  0.0057,  0.0419, -0.0091],
        [ 0.0485,  0.0117,  0.0316,  ...,  0.0304, -0.0310,  0.0346],
        [-0.0140,  0.0313, -0.0181,  ..., -0.0157,  0.0200,  0.0257],
        ...,
        [-0.0105, -0.0033, -0.0111,  ..., -0.0118,  0.0324, -0.0343],
        [-0.0266,  0.0205,  0.0118,  ..., -0.0156, -0.0542,  0.0209],
        [ 0.0083,  0.0204, -0.0151,  ..., -0.0764,  0.0155, -0.0259]])

llm.base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0408,  0.0156,  0.0011,  ...,  0.0038, -0.0186, -0.0004],
        [-0.0110,  0.0006,  0.0052,  ..., -0.0005,  0.0023, -0.0117],
        [ 0.0304,  0.0250, -0.0006,  ..., -0.0205, -0.0037, -0.0095],
        ...,
        [-0.0013,  0.0109,  0.0270,  ..., -0.0044,  0.0300,  0.0057],
        [ 0.0147,  0.0009, -0.0128,  ..., -0.0011, -0.0070, -0.0164],
        [-0.0001, -0.0078,  0.0020,  ...,  0.0425, -0.0117, -0.0029]])

llm.base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0774,  0.0103,  0.0008,  ...,  0.0714, -0.0586, -0.0055],
        [ 0.0467,  0.0361, -0.0640,  ...,  0.0031,  0.1194,  0.0829],
        [ 0.0844,  0.0281, -0.0215,  ..., -0.0286, -0.0580, -0.0683],
        ...,
        [-0.0489, -0.0507, -0.0263,  ..., -0.0138, -0.0373,  0.0352],
        [ 0.0669,  0.0469, -0.0103,  ..., -0.0640,  0.0144, -0.0201],
        [-0.0812, -0.0516,  0.0014,  ..., -0.0073,  0.0386, -0.0010]])

llm.base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0399, -0.0603, -0.0168,  ..., -0.0156,  0.0180, -0.0002],
        [-0.0282,  0.0396, -0.0385,  ...,  0.0268,  0.0396, -0.0104],
        [-0.0065,  0.0784, -0.0470,  ..., -0.0212, -0.0130,  0.0380],
        ...,
        [-0.0117, -0.0193, -0.0235,  ..., -0.0148,  0.0262,  0.0099],
        [-0.0209, -0.0123,  0.0032,  ..., -0.0135, -0.0128,  0.0169],
        [-0.0138, -0.0371, -0.0561,  ..., -0.0211,  0.0229,  0.0193]])

llm.base_model.model.model.layers.23.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0074, -0.0111, -0.0242,  ...,  0.0150,  0.0076,  0.0182],
        [-0.0056,  0.0145,  0.0236,  ..., -0.0021, -0.0070, -0.0171],
        [-0.0025,  0.0008, -0.0076,  ...,  0.0074,  0.0167,  0.0036],
        ...,
        [ 0.0026, -0.0183, -0.0022,  ..., -0.0332,  0.0028, -0.0135],
        [ 0.0151,  0.0203,  0.0051,  ..., -0.0298, -0.0143, -0.0067],
        [-0.0186,  0.0003, -0.0012,  ...,  0.0258,  0.0044, -0.0231]])

llm.base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0175,  0.0117,  0.0935,  ...,  0.0873,  0.0089, -0.0639],
        [ 0.0183, -0.0191, -0.0486,  ..., -0.0711,  0.0333, -0.1175],
        [ 0.0114,  0.0159,  0.0122,  ..., -0.0032, -0.0562, -0.0243],
        ...,
        [ 0.0016,  0.0510, -0.0547,  ..., -0.0132, -0.0570,  0.0107],
        [ 0.0479, -0.0044,  0.0397,  ..., -0.0382,  0.0471, -0.0106],
        [ 0.0278,  0.0497,  0.0506,  ...,  0.0218, -0.0985, -0.1151]])

llm.base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0171,  0.0099,  0.0165,  ...,  0.0228, -0.0230, -0.0278],
        [ 0.0052,  0.0394, -0.0276,  ...,  0.0114, -0.0439,  0.0016],
        [ 0.0229,  0.0307,  0.0205,  ...,  0.0003,  0.0101,  0.0815],
        ...,
        [ 0.0154, -0.0452,  0.0069,  ..., -0.0228,  0.0348, -0.0075],
        [-0.0244, -0.0201, -0.0108,  ...,  0.0048, -0.0033, -0.0423],
        [-0.0076,  0.0190,  0.0498,  ..., -0.0242,  0.0191, -0.0132]])

llm.base_model.model.model.layers.23.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[-0.0076, -0.0013, -0.0001,  ..., -0.0016,  0.0008, -0.0118],
        [ 0.0496, -0.0026,  0.0221,  ...,  0.0024,  0.0261, -0.0083],
        [-0.0187, -0.0078,  0.0052,  ...,  0.0190, -0.0201,  0.0028],
        ...,
        [ 0.0288,  0.0101,  0.0121,  ...,  0.0076, -0.0151, -0.0178],
        [-0.0227, -0.0126,  0.0281,  ...,  0.0085, -0.0087,  0.0176],
        [ 0.0094,  0.0049, -0.0133,  ..., -0.0118, -0.0132, -0.0042]])

llm.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[-0.0406,  0.0157,  0.0063,  ..., -0.0041,  0.0237, -0.0778],
        [-0.0316,  0.0225, -0.0358,  ...,  0.0602,  0.0084, -0.0017],
        [-0.0367,  0.0053,  0.0350,  ...,  0.0071,  0.0110, -0.0252],
        ...,
        [ 0.0309, -0.0883, -0.0004,  ...,  0.0325, -0.0190,  0.0043],
        [-0.0148,  0.0840,  0.0081,  ..., -0.0540,  0.0320, -0.0220],
        [ 0.0005,  0.0445,  0.0019,  ...,  0.0487,  0.0416,  0.0180]])

llm.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0225, -0.0144, -0.0396,  ..., -0.0294, -0.0052, -0.0101],
        [-0.0702, -0.0039, -0.0317,  ...,  0.0251,  0.0804, -0.0413],
        [-0.0241, -0.0154,  0.0059,  ..., -0.0248,  0.0295,  0.0393],
        ...,
        [-0.0415,  0.0134,  0.0237,  ...,  0.0336,  0.0180,  0.0325],
        [-0.0195, -0.0207, -0.0099,  ...,  0.0253, -0.0152, -0.0359],
        [-0.0460, -0.0604, -0.0433,  ...,  0.0716,  0.0147, -0.0037]])

llm.base_model.model.model.layers.23.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.0156, 0.7695, 0.9922,  ..., 0.8203, 0.8711, 0.8672])

llm.base_model.model.model.layers.23.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.6250, 1.6172, 1.7656,  ..., 1.3828, 1.6953, 1.6719])

llm.base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0167,  0.0049,  0.0161,  ..., -0.0033, -0.0026,  0.0089],
        [ 0.0102, -0.0177, -0.0016,  ..., -0.0038,  0.0292,  0.0214],
        [-0.0006,  0.0261, -0.0222,  ...,  0.0099,  0.0042, -0.0142],
        ...,
        [ 0.0074, -0.0166,  0.0021,  ..., -0.0036,  0.0004, -0.0253],
        [-0.0493, -0.0102, -0.0302,  ...,  0.0232, -0.0222, -0.0182],
        [-0.0106,  0.0119,  0.0332,  ..., -0.0156, -0.0410, -0.0061]])

llm.base_model.model.model.layers.24.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([-1.8984,  0.6289,  0.5234,  ...,  1.3047,  1.9688,  0.1147])

llm.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0173, -0.0012, -0.0315,  ...,  0.0064,  0.0353,  0.0119],
        [ 0.0288, -0.0422, -0.0379,  ...,  0.0308,  0.0553, -0.0562],
        [ 0.0401, -0.0043,  0.0554,  ...,  0.0120,  0.0362,  0.0301],
        ...,
        [ 0.0582,  0.0529, -0.0207,  ...,  0.0272, -0.0262,  0.0015],
        [-0.0594,  0.0021, -0.0506,  ...,  0.0627,  0.0012,  0.0117],
        [ 0.0452,  0.0031, -0.0036,  ..., -0.0148,  0.0414,  0.0251]])

llm.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0124,  0.0109,  0.0230,  ...,  0.0034, -0.0199,  0.0035],
        [ 0.0257, -0.0042,  0.0338,  ..., -0.0172, -0.0285,  0.0022],
        [ 0.0354, -0.0193, -0.0048,  ...,  0.0180,  0.0037,  0.0092],
        ...,
        [-0.0014,  0.0111, -0.0005,  ...,  0.0661,  0.0183,  0.0115],
        [ 0.0133, -0.0500,  0.0231,  ...,  0.0016, -0.0291,  0.0651],
        [ 0.0108, -0.0025,  0.0630,  ..., -0.0337,  0.0229, -0.0085]])

llm.base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 1.1597e-02,  9.0332e-03,  8.3008e-03,  ...,  2.3193e-03,
         -8.0490e-04,  3.6469e-03],
        [ 1.8433e-02, -4.0283e-03, -9.7656e-03,  ...,  3.4180e-03,
         -9.8877e-03, -1.3367e-02],
        [-7.8735e-03, -7.1716e-03, -4.1504e-03,  ...,  1.5076e-02,
          1.5640e-03, -1.7014e-03],
        ...,
        [-4.2419e-03,  1.1063e-03, -1.6724e-02,  ...,  5.6763e-03,
          1.1563e-05, -6.2866e-03],
        [-7.7515e-03,  1.0864e-02, -1.7212e-02,  ..., -4.9210e-04,
          2.2507e-04, -8.6670e-03],
        [ 2.3346e-03,  2.0264e-02,  1.5198e-02,  ..., -5.7373e-03,
         -1.3611e-02,  9.8267e-03]])

llm.base_model.model.model.layers.24.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 2.0938e+00,  2.7100e-02, -7.8516e-01,  6.1035e-02, -1.3125e+00,
         1.3965e-01,  2.8711e-01, -8.2520e-02,  2.1094e-01,  3.9453e-01,
        -5.8289e-03, -8.0078e-02, -1.5625e-02, -3.8574e-02, -7.2266e-02,
         1.9629e-01,  3.2812e-01,  1.6602e-01,  1.8652e-01, -3.0273e-02,
         8.9844e-02,  1.1621e-01,  2.3535e-01, -8.1543e-02,  1.6719e+00,
        -8.4961e-02, -2.4605e-04,  6.2500e-02, -1.6895e-01,  5.8594e-01,
         2.4609e-01,  6.7578e-01, -6.4453e-02, -2.3340e-01, -3.2227e-01,
         4.0430e-01, -1.6211e-01,  1.4746e-01,  3.8867e-01, -4.5898e-01,
         2.2559e-01, -3.3203e-02,  1.6699e-01, -4.0625e-01, -1.4551e-01,
         1.7383e-01,  4.2725e-02, -6.4941e-02, -1.0889e-01,  2.0020e-01,
        -7.9102e-02, -3.9453e-01,  4.4062e+00,  1.2500e-01,  2.8906e-01,
         1.2402e-01,  7.8125e-02,  1.2158e-01,  7.1289e-02,  8.5449e-02,
         1.5137e-01,  8.7109e-01,  1.4844e-01, -9.2773e-02, -6.8359e-01,
         8.4961e-02,  7.7344e-01, -1.5039e-01, -2.8711e-01, -3.5742e-01,
         1.2158e-01, -2.3047e-01, -1.3516e+00,  1.3184e-01,  1.3611e-02,
         1.5312e+00,  6.3782e-03,  2.1777e-01, -1.4297e+00,  2.0312e-01,
         2.7148e-01, -2.9883e-01, -1.2988e-01, -2.6953e-01,  1.5703e+00,
        -1.0938e-01, -3.3398e-01,  1.4844e-01, -3.0078e-01,  2.0117e-01,
         2.5781e-01, -7.7637e-02,  1.7676e-01,  2.2827e-02, -2.7344e-01,
         4.0625e-01,  1.3672e-01,  2.2266e-01, -3.6523e-01, -1.3965e-01,
        -1.5430e-01,  1.0376e-02,  1.1377e-01, -8.0566e-02,  2.9688e-01,
        -1.5820e-01, -4.0039e-01, -1.4453e-01,  7.4219e-01,  1.6406e-01,
        -8.9355e-02,  7.5195e-02,  1.4941e-01,  2.9297e-01, -1.2402e-01,
         6.5430e-02, -1.8750e+00,  9.5703e-02, -3.6914e-01, -1.1572e-01,
        -1.2656e+00, -3.7354e-02, -1.4258e-01, -4.9805e-01,  2.9688e-01,
         1.7383e-01, -2.0215e-01, -3.5156e-01,  5.5664e-02,  6.2988e-02,
         4.9561e-02,  3.4424e-02,  2.6953e-01, -5.8838e-02,  2.3926e-01,
         5.0293e-02, -5.1514e-02, -9.4727e-02, -3.4375e-01,  7.5195e-02,
        -3.3691e-02,  3.2422e-01, -5.4297e-01, -4.7266e-01,  7.4219e-02,
        -3.1445e-01, -6.6406e-01, -1.0303e-01,  2.3242e-01, -2.8320e-01,
         1.5625e-01,  1.5430e-01, -3.2812e-01, -1.7822e-02,  3.0029e-02,
        -8.0859e-01,  1.5430e-01, -1.3477e-01, -6.7969e-01, -3.1445e-01,
        -3.4375e-01, -5.8594e-01,  3.6914e-01,  6.2500e-02,  8.7109e-01,
         2.7539e-01,  2.5000e-01,  2.3633e-01,  4.5312e-01,  5.4688e-01,
        -6.6895e-02, -5.7861e-02, -4.4531e-01, -1.1133e-01,  3.6621e-02,
        -1.1035e-01,  3.4766e-01, -2.1484e-01,  2.6953e-01,  2.7148e-01,
        -7.6172e-02, -3.4180e-01, -2.0703e-01, -5.3906e-01,  2.8711e-01,
         2.7344e-01,  2.6172e-01,  3.3984e-01, -3.6523e-01,  2.1875e+00,
         5.3125e-01, -2.2266e-01,  9.4727e-02,  1.9141e-01,  1.9238e-01,
         2.3730e-01,  1.1279e-01, -2.8711e-01,  7.1289e-02, -3.2715e-02,
         2.6758e-01,  4.5703e-01, -1.7871e-01,  1.8359e-01,  4.5312e-01,
        -2.0703e-01,  4.3213e-02, -2.9297e-01,  5.3906e-01, -1.1182e-01,
         3.5156e-02,  4.1992e-01,  3.0273e-01, -6.7969e-01,  7.1777e-02,
        -1.6309e-01, -9.3359e-01,  1.4551e-01, -1.7871e-01, -3.9844e-01,
         1.7456e-02, -1.7578e-01, -1.0078e+00, -1.5234e-01,  8.9844e-02,
        -4.1406e-01,  1.2354e-01, -5.9082e-02, -9.8047e-01, -3.2196e-03,
        -3.7500e-01, -2.3682e-02,  2.7344e-02, -1.5703e+00, -4.1016e-02,
         3.1445e-01,  3.4570e-01,  4.5312e-01, -4.6094e-01, -6.7188e-01,
        -2.2500e+00,  1.8555e-01, -6.2988e-02, -1.8848e-01, -1.2500e-01,
         4.6484e-01, -5.7812e-01, -1.8359e-01, -1.3428e-02,  4.1797e-01,
        -1.5000e+00,  1.2061e-01,  1.8457e-01,  3.4375e+00,  3.4570e-01,
        -1.4609e+00,  2.6562e-01, -4.4531e-01,  4.8828e-01, -5.3125e-01,
        -5.7422e-01, -3.8867e-01,  1.0781e+00,  7.0703e-01,  5.5908e-02,
         3.4180e-01,  1.8164e-01, -1.5625e-01, -8.5547e-01, -5.4199e-02,
        -6.3477e-02, -2.1094e-01,  3.3984e-01,  5.8594e-02,  5.8594e-02,
        -4.2773e-01,  1.7676e-01,  2.8320e-01, -1.9824e-01,  5.7422e-01,
        -1.1523e-01,  4.7852e-01, -9.6094e-01,  1.1719e-01,  3.1641e-01,
         1.1719e+00, -2.2461e-01, -1.6016e-01,  3.1250e-01,  1.8066e-02,
        -1.7188e-01,  2.1191e-01,  8.7891e-02, -1.7031e+00,  1.6602e-01,
         1.1475e-01,  7.7148e-02,  3.9795e-02,  1.0352e-01,  2.2363e-01,
        -2.2852e-01, -6.8359e-01, -1.3428e-02,  2.6562e-01,  4.7461e-01,
         3.4570e-01,  4.6094e-01,  3.7695e-01, -4.1992e-01, -6.3672e-01,
         2.8125e+00,  1.3477e-01,  2.9102e-01, -2.3438e-01, -1.6406e+00,
        -1.1670e-01, -3.0078e-01,  4.8242e-01, -3.1836e-01, -2.3926e-01,
        -1.2734e+00,  5.3906e-01, -1.0547e+00, -4.0234e-01, -2.0215e-01,
        -5.6250e-01,  2.6953e-01,  3.2031e-01, -1.5918e-01, -8.5547e-01,
         4.9561e-02,  6.7969e-01,  3.6328e-01, -1.1328e+00,  4.5703e-01,
        -5.1880e-03,  4.6484e-01,  1.8262e-01,  1.0078e+00, -1.2158e-01,
         1.6504e-01, -2.1484e-01, -1.2266e+00, -2.4707e-01,  1.6235e-02,
         7.7148e-02, -4.7363e-02, -5.1953e-01, -3.3984e-01, -3.2812e-01,
         2.3633e-01, -1.4941e-01, -2.1875e-01, -5.7031e-01,  4.8828e-01,
         1.0449e-01, -6.0425e-03,  3.1836e-01,  2.2339e-02,  2.5586e-01,
        -1.9824e-01, -3.3398e-01, -3.3594e-01, -2.1582e-01, -8.2520e-02,
         2.7734e-01,  2.7734e-01, -2.6172e-01,  4.1992e-01,  3.3984e-01,
        -4.4727e-01,  6.0156e-01, -1.5039e-01,  4.4727e-01,  4.6250e+00,
         7.1094e-01, -2.8320e-01,  6.8359e-01,  2.5195e-01, -1.3477e-01,
         3.5938e-01, -8.2812e-01, -1.5234e+00, -1.0156e+00,  2.4609e-01,
         3.3008e-01,  4.8242e-01,  2.5000e-01, -5.0781e-01, -1.1230e-01,
        -4.0820e-01, -2.4536e-02, -3.0469e-01, -5.2246e-02,  3.2471e-02,
        -8.2031e-01, -1.0645e-01, -5.3516e-01,  5.8838e-02,  3.4375e-01,
         2.3956e-03,  1.0059e-01,  1.5625e-01,  3.5547e-01, -5.7861e-02,
         2.6562e-01,  4.4727e-01,  4.2419e-03, -3.0664e-01,  6.5625e-01,
         1.9653e-02,  7.6660e-02, -3.4375e-01, -3.3789e-01, -1.0400e-01,
         1.8047e+00,  4.6094e-01, -5.8984e-01, -4.4531e-01,  2.4902e-01,
         1.6113e-01, -1.5703e+00,  3.8281e-01,  1.4832e-02, -1.9766e+00,
         1.2500e-01, -4.7656e-01,  1.6992e-01, -5.4688e-02,  5.1270e-03,
         2.9375e+00, -4.0430e-01,  2.4707e-01,  1.0742e-01,  3.5156e-01,
        -5.1562e-01, -6.5430e-02, -7.6562e-01, -2.2363e-01,  2.7344e-01,
         4.3945e-01, -4.4922e-01, -6.4453e-01,  1.2451e-01, -6.9531e-01,
         2.0312e+00, -8.2031e-01, -1.2451e-02,  5.3516e-01, -2.1680e-01,
        -2.4707e-01,  3.0664e-01,  2.0703e-01,  1.0303e-01,  3.7500e-01,
         1.4844e-01, -6.8359e-01,  1.1523e-01,  2.7734e-01,  1.9824e-01,
         1.3867e-01,  1.7188e-01,  3.1055e-01,  5.4297e-01, -9.2578e-01,
         4.3457e-02, -1.2891e-01,  1.0469e+00, -1.5039e-01,  4.1797e-01,
         1.1172e+00, -2.2070e-01, -5.4297e-01,  1.2500e+00,  1.1279e-01,
        -3.7500e-01, -6.3281e-01, -1.0469e+00,  6.8848e-02, -2.5977e-01,
         1.0312e+00, -8.1250e-01,  1.5918e-01, -9.5703e-02, -2.6367e-02,
         9.2773e-02, -7.6660e-02, -1.1035e-01,  1.1406e+00,  3.7354e-02,
        -9.9121e-02, -9.7656e-03,  7.2937e-03, -2.1094e-01, -5.7422e-01,
         7.6172e-02,  3.4180e-01, -4.8584e-02,  4.9219e-01, -6.6406e-02,
        -5.3125e-01, -3.8867e-01,  5.4688e-01,  4.4922e-01,  3.9844e-01,
         8.1543e-02, -2.0410e-01, -2.8125e-01, -1.1406e+00,  4.0312e+00,
        -1.9609e+00, -4.8633e-01])

llm.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0389, -0.0150,  0.0428,  ...,  0.0707, -0.0567,  0.0056],
        [ 0.0433,  0.0246,  0.0340,  ...,  0.0135, -0.0262,  0.0337],
        [-0.0183,  0.0030, -0.0459,  ..., -0.0734, -0.0113,  0.0089],
        ...,
        [-0.0167,  0.0084, -0.0261,  ...,  0.0196, -0.0081,  0.0046],
        [-0.0032, -0.0248, -0.0617,  ...,  0.0290,  0.0236,  0.0107],
        [ 0.0239,  0.0152,  0.0528,  ..., -0.0334,  0.0417,  0.0245]])

llm.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0017,  0.0103, -0.0070,  ...,  0.0063, -0.0156, -0.0169],
        [-0.0441,  0.0230, -0.0399,  ..., -0.0139,  0.0389,  0.0052],
        [-0.0477, -0.0119,  0.0104,  ..., -0.0089, -0.0086, -0.0152],
        ...,
        [ 0.0276,  0.0125,  0.0293,  ...,  0.0367,  0.0029, -0.0054],
        [ 0.0201, -0.0424, -0.0118,  ..., -0.0012, -0.0545, -0.0039],
        [-0.0034, -0.0057, -0.0186,  ..., -0.0508, -0.0088, -0.0269]])

llm.base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0181,  0.0053,  0.0127,  ..., -0.0312,  0.0261, -0.0479],
        [ 0.0408,  0.0255, -0.0217,  ..., -0.0011, -0.0150,  0.0266],
        [-0.0127, -0.0061, -0.0015,  ...,  0.0115,  0.0193, -0.0342],
        ...,
        [ 0.0205, -0.0227, -0.0182,  ...,  0.0204, -0.0101,  0.0061],
        [ 0.0148,  0.0004,  0.0081,  ...,  0.0061, -0.0126,  0.0070],
        [-0.0005,  0.0061, -0.0261,  ..., -0.0359,  0.0121, -0.0015]])

llm.base_model.model.model.layers.24.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 2.5977e-01, -2.0386e-02,  1.3477e-01, -5.7031e-01, -7.8613e-02,
         1.3672e-01, -1.0645e-01,  2.0508e-01, -2.1680e-01,  4.1016e-02,
        -1.4941e-01, -1.0205e-01, -1.5234e-01,  2.0874e-02,  1.8164e-01,
         2.3315e-02, -6.6406e-02,  8.1543e-02,  3.1055e-01,  1.4746e-01,
         3.3789e-01,  1.3477e-01,  4.0820e-01,  1.5430e-01,  4.5703e-01,
        -9.1309e-02,  2.6758e-01, -7.2754e-02,  6.7871e-02, -5.1514e-02,
         3.0518e-02,  9.8633e-02,  6.9336e-02,  9.9487e-03, -1.1169e-02,
         6.6016e-01,  1.1279e-01,  9.7168e-02, -7.6172e-02, -2.8320e-01,
        -4.1260e-02, -1.7334e-02, -1.3379e-01, -7.6172e-02,  9.1309e-02,
         4.5898e-02, -3.6133e-01,  3.2471e-02,  2.0605e-01,  2.3633e-01,
        -7.5684e-02, -4.1016e-01,  2.4707e-01, -2.2266e-01,  3.6719e-01,
        -1.2598e-01, -2.7930e-01,  1.8203e+00,  1.8848e-01,  3.7891e-01,
         3.6523e-01, -2.1484e-01, -2.8906e-01,  2.7930e-01,  3.2422e-01,
        -8.1543e-02,  9.3262e-02, -1.2817e-02, -7.8613e-02,  1.1963e-01,
         2.0264e-02,  2.7344e-01, -3.4961e-01, -1.3672e-01,  4.1992e-02,
         1.7578e-01, -1.0010e-02, -4.6484e-01, -1.1328e-01, -2.9297e-01,
        -5.4297e-01,  1.9824e-01, -2.3633e-01,  4.4336e-01,  1.5747e-02,
        -2.9492e-01,  3.4180e-01,  1.8555e-01, -2.1973e-01, -2.6172e-01,
        -1.4453e-01,  9.9609e-02, -1.4746e-01, -7.8906e-01,  1.9824e-01,
         2.0312e-01, -2.7930e-01, -1.4648e-01, -2.1851e-02,  2.1777e-01,
        -6.3477e-02,  7.3047e-01,  1.2158e-01, -1.9238e-01, -3.0884e-02,
         1.9043e-01,  1.4941e-01,  1.9336e-01, -1.4062e-01, -1.2012e-01,
        -1.9336e-01,  2.3956e-03, -4.7070e-01,  5.9570e-02,  2.4780e-02,
        -3.4375e-01,  3.4027e-03, -1.8164e-01, -1.6211e-01, -2.7930e-01,
         2.6758e-01,  3.1250e-01,  1.4355e-01,  4.1748e-02, -4.0820e-01,
         3.0273e-01,  3.8867e-01, -3.8086e-02,  1.4355e-01,  1.4160e-01,
         8.6426e-02,  6.2988e-02,  6.8665e-04,  9.8633e-02, -4.3945e-02,
        -1.9531e-01,  1.3965e-01,  6.3477e-02,  2.1680e-01, -3.5400e-02,
         2.6758e-01, -1.4355e-01,  1.4453e-01, -2.6367e-02,  1.7871e-01,
         4.3213e-02, -1.4832e-02, -8.3008e-02, -4.5654e-02, -2.6562e-01,
         3.3203e-02,  5.8594e-02, -1.4832e-02,  1.4465e-02, -7.4707e-02,
         8.9844e-02, -6.9580e-03,  1.7480e-01, -7.7637e-02,  5.7129e-02,
        -1.3477e-01, -5.3101e-03,  8.5938e-02, -2.0752e-02, -7.9590e-02,
         2.1729e-02,  8.9355e-02, -1.2695e-01,  5.7617e-02,  2.5000e-01,
        -2.8320e-02,  8.5938e-02,  7.0312e-02, -3.4668e-02, -5.6885e-02,
        -2.1680e-01, -7.5195e-02, -7.7637e-02,  9.7656e-03,  1.3672e-01,
        -1.7969e-01,  1.1230e-01,  6.7871e-02,  1.2354e-01, -1.2695e-01,
         6.4941e-02,  9.6680e-02, -2.4414e-02,  3.4424e-02,  4.6143e-02,
        -1.3477e-01,  1.2305e-01, -5.8838e-02, -1.0693e-01, -4.0039e-02,
        -5.2734e-02, -1.3672e-01,  3.9795e-02,  1.1914e-01,  3.6865e-02,
        -1.1035e-01,  7.5195e-02, -4.3213e-02,  5.1514e-02,  1.2402e-01,
        -3.3691e-02,  2.4567e-03,  7.3242e-02, -1.9897e-02,  2.4902e-02,
         2.3193e-02, -1.5820e-01,  2.0874e-02,  2.4414e-02, -2.9053e-02,
        -3.0640e-02,  8.1543e-02, -1.0107e-01, -1.1035e-01,  7.0312e-02,
         8.3984e-02,  1.3086e-01, -2.2168e-01, -1.0596e-01,  1.6602e-01,
         1.1841e-02,  1.6724e-02, -1.8359e-01,  1.2207e-02, -3.6865e-02,
        -4.2969e-02, -1.2573e-02, -2.9541e-02, -6.9336e-02,  2.4536e-02,
        -1.4648e-01, -9.6191e-02, -1.5332e-01,  1.2891e-01,  1.0010e-02,
        -5.8125e+00,  2.1289e-01,  5.5176e-02, -5.3955e-02,  1.8359e-01,
         1.2305e-01,  9.4727e-02,  5.1025e-02, -7.6172e-02, -2.5879e-02,
        -2.7222e-02, -1.5625e-02,  4.4556e-03, -2.2583e-02, -6.8359e-02,
         2.1680e-01, -2.9907e-03,  1.3000e-02, -8.6670e-03, -1.2012e-01,
        -4.8340e-02, -1.8799e-02,  2.5879e-02, -5.1025e-02, -1.2695e-01,
         8.0078e-02, -9.5825e-03,  3.3936e-02, -6.6406e-02,  3.4912e-02,
         2.9541e-02, -9.1797e-02, -1.3281e-01,  2.7344e-02, -5.5847e-03,
         1.2207e-02, -3.4668e-02, -7.4158e-03,  1.0376e-02, -5.9814e-02,
         9.1309e-02,  6.0120e-03,  4.1748e-02,  3.4912e-02,  9.9487e-03,
         8.3008e-02,  2.1729e-02,  2.4414e-02, -1.8066e-02, -6.8848e-02,
         2.7466e-02, -2.8442e-02, -4.3457e-02,  1.5320e-02,  4.1992e-02,
        -1.5869e-02, -4.4189e-02,  1.1279e-01,  6.1035e-02,  1.8188e-02,
         5.2490e-02, -5.0781e-02, -4.2725e-03, -5.5420e-02, -5.7617e-02,
        -4.8584e-02,  2.0020e-02,  4.3457e-02,  2.8076e-02, -2.9419e-02,
         8.5938e-02, -4.8828e-02, -2.4048e-02,  2.4170e-02,  7.6660e-02,
        -8.3008e-02,  1.0742e-01, -8.6914e-02, -1.4221e-02, -2.7734e-01,
         4.5471e-03, -7.4219e-02,  2.2461e-02, -7.5684e-02,  3.8330e-02,
        -2.3438e-02,  5.6250e-01,  3.3447e-02, -5.6885e-02, -3.9551e-02,
         1.0889e-01,  3.7354e-02, -1.3672e-01, -4.0283e-02,  2.0874e-02,
         7.1289e-02, -5.8838e-02, -1.2891e-01,  2.6001e-02,  1.0986e-01,
        -2.0874e-02,  3.7384e-03,  7.1777e-02,  6.0547e-02, -3.4637e-03,
         2.0294e-03, -7.0312e-02, -1.7548e-04, -6.2943e-04, -3.9551e-02,
         1.2695e-01,  1.0547e+00, -1.1902e-02,  3.1982e-02,  6.0059e-02,
        -5.6152e-03,  1.1719e-02,  8.0078e-02,  2.1240e-02, -5.1514e-02,
        -9.9121e-02,  4.9805e-02, -6.5430e-02,  1.3611e-02, -6.3965e-02,
         1.1780e-02,  1.9897e-02, -7.4463e-03,  3.6133e-02, -8.9844e-02,
        -3.5400e-02,  3.9795e-02, -1.5527e-01, -3.3203e-02,  2.8076e-02,
        -1.2329e-02,  2.6245e-02,  1.6357e-02, -3.0396e-02, -5.9326e-02,
        -9.3750e-02,  7.4219e-02, -1.5015e-02,  2.8320e-02,  1.2695e-01,
         9.8633e-02,  8.4473e-02,  1.0254e-02,  5.5664e-02,  8.1543e-02,
         6.0303e-02,  8.3008e-02,  8.3496e-02, -7.2754e-02, -1.1035e-01,
         3.6865e-02, -4.4922e-02,  2.2095e-02,  4.5898e-02, -6.8359e-02,
        -2.2583e-02,  5.1758e-02, -1.4062e-01,  6.7871e-02,  3.4424e-02,
        -8.9844e-02,  4.9316e-02,  9.2773e-02, -1.6113e-01, -1.8262e-01,
        -4.7119e-02, -1.1475e-01, -1.0938e-01, -1.0132e-02, -2.8198e-02,
         2.1973e-02, -1.4648e-01, -1.1328e-01,  6.3965e-02,  1.8921e-02,
         1.6968e-02,  1.3281e-01,  2.8687e-02,  1.9629e-01,  4.1504e-03,
        -1.2451e-02,  4.6692e-03,  1.6406e-01,  1.0010e-01,  8.1055e-02,
         8.6914e-02, -5.7129e-02, -1.1084e-01, -7.1289e-02,  4.6387e-02,
        -1.8311e-02,  4.1992e-02, -2.7222e-02,  1.4954e-02, -6.7383e-02,
        -4.7852e-02, -3.8818e-02, -1.6846e-02,  4.9316e-02,  1.1475e-02,
         5.6885e-02, -3.6133e-02,  5.0049e-02,  3.1738e-02, -1.2012e-01,
        -1.0742e-01,  3.4180e-02,  1.3123e-02,  1.2109e-01, -5.6885e-02,
         1.4572e-03, -1.8921e-02, -2.7832e-02, -1.6113e-01,  4.4922e-02,
        -3.6377e-02, -5.7129e-02,  1.3477e-01,  1.1719e-01, -3.3936e-02,
        -1.0254e-01, -1.7090e-02,  5.5420e-02,  3.8330e-02,  8.0566e-02,
         9.7656e-02, -6.5430e-02,  5.8105e-02, -3.8086e-02,  9.3262e-02,
         8.0078e-02,  3.2715e-02, -1.0315e-02,  9.5703e-02,  1.4160e-01,
         4.7607e-02, -8.2031e-02, -1.2793e-01, -1.0693e-01,  1.9043e-02,
        -4.5898e-02, -8.5938e-02, -6.1951e-03,  8.6426e-02,  2.0752e-02,
        -5.1758e-02, -1.5820e-01, -6.8848e-02, -1.7578e-01, -6.8359e-02,
        -3.9673e-03,  1.2012e-01,  9.2773e-03, -4.6387e-03,  1.3574e-01,
         3.4668e-02, -5.3955e-02, -4.5410e-02,  7.5195e-02,  1.6479e-02,
        -1.1035e-01,  7.1777e-02, -1.2451e-01,  1.0840e-01, -4.2236e-02,
        -7.6172e-02, -1.8164e-01])

llm.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 5.0250e-02,  6.8528e-02,  1.0022e-02,  ...,  2.1547e-02,
         -1.8780e-02,  3.4446e-02],
        [-1.8572e-03, -3.1772e-02,  2.5685e-02,  ..., -9.4077e-05,
         -3.6910e-02, -6.5130e-02],
        [-9.6767e-03, -2.0969e-02,  2.8150e-02,  ...,  3.6757e-02,
         -3.5261e-02, -5.3688e-02],
        ...,
        [ 3.0847e-02,  2.9048e-02,  1.3670e-02,  ..., -2.7185e-02,
         -7.7029e-03,  5.9685e-03],
        [ 2.3142e-02, -2.8532e-02, -1.7656e-03,  ...,  1.1790e-02,
         -1.0716e-02, -1.0175e-02],
        [ 3.0877e-02, -3.7098e-02, -1.3048e-02,  ..., -4.0956e-02,
          8.3117e-02,  8.3283e-03]])

llm.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0286, -0.0164, -0.0066,  ...,  0.0048, -0.0305, -0.0333],
        [-0.0126, -0.0080, -0.0285,  ..., -0.0467,  0.0176,  0.0548],
        [-0.0084,  0.0286,  0.0415,  ..., -0.0562,  0.0033, -0.0245],
        ...,
        [-0.0805,  0.0660,  0.0191,  ..., -0.0347, -0.0155, -0.0309],
        [ 0.0097,  0.0335, -0.0439,  ..., -0.0321,  0.0514, -0.0102],
        [ 0.0370,  0.0244, -0.0726,  ..., -0.0117, -0.0108,  0.0007]])

llm.base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-1.1719e-02,  9.6893e-04,  3.1982e-02,  ..., -2.0508e-02,
         -6.4087e-03,  8.6670e-03],
        [ 1.0315e-02, -1.7090e-02, -1.4526e-02,  ...,  1.1475e-02,
         -2.1667e-03, -1.4282e-02],
        [ 2.2583e-03, -7.6599e-03, -1.0742e-02,  ...,  1.7212e-02,
         -1.1206e-04,  8.7280e-03],
        ...,
        [ 1.5198e-02,  1.4465e-02,  2.4536e-02,  ...,  6.8665e-03,
          8.0566e-03,  2.1729e-02],
        [-1.2268e-02, -2.5482e-03, -1.0742e-02,  ...,  8.3008e-03,
          3.0670e-03, -1.1658e-02],
        [ 1.4465e-02, -8.3542e-04,  5.8889e-05,  ...,  1.1169e-02,
         -2.2125e-03, -3.0060e-03]])

llm.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0011,  0.0208,  0.0354,  ..., -0.0496, -0.0618,  0.1420],
        [-0.0113,  0.0144, -0.0494,  ...,  0.0802,  0.1444,  0.0486],
        [ 0.0035, -0.0080, -0.0181,  ...,  0.0271,  0.0214,  0.0430],
        ...,
        [ 0.0082, -0.0081, -0.0394,  ..., -0.0167, -0.0723, -0.0081],
        [-0.0210,  0.0140, -0.0185,  ..., -0.0735, -0.0979, -0.1157],
        [ 0.0090, -0.0004,  0.0163,  ...,  0.0562,  0.1783, -0.0018]])

llm.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0151, -0.0256,  0.0265,  ...,  0.0301,  0.0148, -0.0108],
        [-0.0111,  0.0051, -0.0320,  ..., -0.0219,  0.0217,  0.0077],
        [-0.0691,  0.0339,  0.0296,  ..., -0.0369, -0.0304,  0.0239],
        ...,
        [ 0.0408,  0.0518, -0.0037,  ...,  0.0164,  0.0419, -0.0081],
        [-0.0330, -0.0142, -0.0232,  ...,  0.0222,  0.0062, -0.0026],
        [ 0.0240, -0.0255, -0.0234,  ..., -0.0083,  0.0393, -0.0019]])

llm.base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0040,  0.0481,  0.0231,  ...,  0.0381,  0.0334,  0.0277],
        [-0.0229, -0.0082,  0.0192,  ..., -0.0052,  0.0247,  0.0051],
        [ 0.0240,  0.0016, -0.0245,  ..., -0.0075, -0.0093,  0.0138],
        ...,
        [-0.0074,  0.0134, -0.0012,  ..., -0.0164, -0.0190,  0.0262],
        [-0.0154,  0.0112, -0.0128,  ...,  0.0156,  0.0154,  0.0283],
        [ 0.0098,  0.0043, -0.0007,  ...,  0.0058, -0.0305,  0.0179]])

llm.base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0403, -0.0118, -0.0521,  ..., -0.0737, -0.0663,  0.0750],
        [-0.0130, -0.0534, -0.0613,  ...,  0.0208, -0.0063,  0.0348],
        [-0.0032,  0.0758,  0.0533,  ...,  0.0377,  0.0088, -0.0037],
        ...,
        [ 0.0773,  0.0565,  0.0144,  ...,  0.0218,  0.0520, -0.0270],
        [-0.0363, -0.1065,  0.0880,  ..., -0.0215, -0.0276,  0.0119],
        [-0.0606, -0.0603,  0.0736,  ...,  0.0536, -0.0650,  0.0636]])

llm.base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 3.4001e-02,  2.8677e-02,  2.1271e-02,  ...,  2.1548e-02,
         -3.1961e-04, -1.3555e-02],
        [ 3.1542e-02,  3.7098e-02, -4.0266e-02,  ..., -5.6654e-02,
          2.5210e-02,  6.1730e-04],
        [ 1.3727e-02,  2.0184e-03,  9.4171e-03,  ..., -1.3081e-02,
         -2.4686e-02, -3.9488e-02],
        ...,
        [-6.6535e-03, -1.7234e-02,  3.5313e-02,  ..., -1.8812e-02,
         -9.3023e-03, -2.5279e-02],
        [ 2.6990e-02, -2.0039e-02, -1.8800e-05,  ...,  9.7858e-03,
         -4.9707e-02, -1.0429e-02],
        [-3.1635e-02,  1.0694e-02, -4.7506e-02,  ...,  3.0291e-02,
          1.6181e-02,  1.4047e-02]])

llm.base_model.model.model.layers.24.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 9.1553e-03, -1.7212e-02, -3.7842e-02,  ...,  5.7983e-03,
          4.4556e-03,  1.9775e-02],
        [ 5.1270e-03,  1.4832e-02,  1.2085e-02,  ..., -2.4048e-02,
          4.1016e-02,  7.9956e-03],
        [-3.8147e-03, -3.6621e-04, -2.4567e-03,  ..., -1.0254e-02,
          3.9673e-03,  2.3071e-02],
        ...,
        [-4.5471e-03,  3.7689e-03, -3.8605e-03,  ...,  4.6692e-03,
          8.3618e-03, -7.4463e-03],
        [-9.5215e-03, -1.1719e-02,  1.4648e-02,  ...,  1.7578e-02,
          1.3550e-02, -3.1128e-02],
        [ 1.4832e-02, -7.9155e-05, -2.3438e-02,  ...,  2.1484e-02,
          2.0294e-03, -3.1982e-02]])

llm.base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0615, -0.0601, -0.0270,  ...,  0.0058,  0.0968,  0.0165],
        [-0.0333,  0.0614,  0.0345,  ...,  0.0727, -0.0583, -0.0096],
        [ 0.0104, -0.0154, -0.0463,  ..., -0.0015, -0.0049,  0.0077],
        ...,
        [ 0.0368, -0.0287,  0.0443,  ..., -0.0384,  0.1077,  0.0414],
        [ 0.0347, -0.0820, -0.0271,  ...,  0.0168,  0.0210,  0.0226],
        [-0.0469, -0.0374,  0.0627,  ...,  0.0310, -0.0842,  0.0821]])

llm.base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0098,  0.0253,  0.0616,  ..., -0.0114, -0.0100, -0.0083],
        [ 0.0009, -0.0023, -0.0035,  ..., -0.0102,  0.0005, -0.0315],
        [ 0.0230,  0.0143, -0.0054,  ...,  0.0337, -0.0224,  0.0041],
        ...,
        [ 0.0040, -0.0144, -0.0087,  ...,  0.0118,  0.0311, -0.0339],
        [-0.0367, -0.0477,  0.0299,  ..., -0.0072,  0.0068, -0.0512],
        [-0.0101,  0.0172,  0.0015,  ..., -0.0102, -0.0374, -0.0298]])

llm.base_model.model.model.layers.24.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[-1.1826e-03, -9.2773e-03, -3.0249e-06,  ..., -1.3000e-02,
          2.4292e-02,  1.5793e-03],
        [-3.0670e-03,  3.9978e-03,  7.0496e-03,  ...,  2.7588e-02,
         -2.8564e-02, -4.0283e-03],
        [-2.6245e-02,  2.9449e-03,  7.0190e-03,  ...,  3.1128e-02,
         -6.2866e-03,  1.6846e-02],
        ...,
        [ 1.4648e-02, -1.8555e-02,  8.4839e-03,  ...,  1.0834e-03,
          1.2054e-03,  2.0020e-02],
        [ 7.0496e-03,  2.0020e-02, -1.4420e-03,  ..., -3.7689e-03,
          2.0630e-02,  3.8086e-02],
        [ 2.5879e-02,  5.7068e-03, -5.9891e-04,  ..., -6.4087e-03,
         -2.3804e-02,  6.4087e-03]])

llm.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0126,  0.0202, -0.0483,  ...,  0.0299, -0.0005,  0.0743],
        [-0.0298,  0.0042, -0.0034,  ..., -0.0243,  0.0121,  0.0056],
        [-0.0222,  0.0015, -0.0078,  ...,  0.0140, -0.0528, -0.0219],
        ...,
        [-0.0445,  0.0063, -0.0048,  ..., -0.0062,  0.0290, -0.0248],
        [ 0.0096, -0.0255,  0.0114,  ..., -0.0403, -0.0307, -0.0085],
        [-0.0297,  0.0062, -0.0108,  ..., -0.0422,  0.0004, -0.0184]])

llm.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0404, -0.0019,  0.0581,  ...,  0.0680,  0.0220,  0.0487],
        [ 0.0228, -0.0206,  0.0166,  ..., -0.0015, -0.0164, -0.0320],
        [-0.0038, -0.0097, -0.0093,  ..., -0.0051,  0.0069, -0.0513],
        ...,
        [-0.0319, -0.0277, -0.0120,  ...,  0.0094,  0.0330,  0.0011],
        [-0.0357, -0.0145, -0.0386,  ...,  0.0590,  0.0312, -0.0480],
        [-0.0148,  0.0009, -0.0079,  ...,  0.0535, -0.0043,  0.0392]])

llm.base_model.model.model.layers.24.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.0781, 0.8750, 1.0703,  ..., 0.8750, 0.9688, 0.9805])

llm.base_model.model.model.layers.24.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.6797, 1.6797, 1.7812,  ..., 1.4688, 1.7109, 1.7031])

llm.base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-1.4267e-03,  2.2125e-03,  7.9956e-03,  ...,  9.9487e-03,
          7.7057e-04,  1.3916e-02],
        [-3.0212e-03, -3.4422e-06, -1.0681e-02,  ...,  8.0566e-03,
          1.1658e-02,  1.1658e-02],
        [-1.3580e-03,  8.1787e-03,  1.2512e-02,  ..., -8.3618e-03,
         -1.4404e-02, -1.1597e-03],
        ...,
        [ 1.3245e-02, -2.2095e-02, -3.4332e-03,  ..., -2.3560e-02,
         -7.2021e-03, -1.3367e-02],
        [ 7.5989e-03,  1.6846e-02,  5.0659e-03,  ...,  1.2085e-02,
          4.8828e-03,  8.9722e-03],
        [-7.9346e-03,  8.7891e-03,  2.8931e-02,  ...,  1.4160e-02,
          3.3264e-03, -2.7924e-03]])

llm.base_model.model.model.layers.25.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.1455,  0.7500, -0.0933,  ...,  0.0806, -4.8438, -2.0312])

llm.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0202, -0.0575,  0.0238,  ...,  0.0259,  0.0277,  0.0179],
        [ 0.0276,  0.0563, -0.0046,  ..., -0.0217, -0.0039,  0.0077],
        [-0.0215, -0.0232, -0.0282,  ...,  0.0175, -0.0031, -0.0090],
        ...,
        [ 0.0056,  0.0029,  0.0328,  ..., -0.0277,  0.0353, -0.0294],
        [-0.0031, -0.0264,  0.0145,  ...,  0.0647,  0.0731, -0.0479],
        [-0.0112, -0.0014, -0.0187,  ...,  0.0487,  0.0145, -0.0432]])

llm.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0264,  0.0060, -0.0158,  ..., -0.0188, -0.0137, -0.0386],
        [-0.0296,  0.0026,  0.0173,  ..., -0.0565,  0.0016, -0.0258],
        [ 0.0236,  0.0031,  0.0204,  ...,  0.0341, -0.0427,  0.0456],
        ...,
        [ 0.0435,  0.0076,  0.0022,  ...,  0.0254,  0.0526, -0.0239],
        [-0.0052,  0.0268, -0.0233,  ..., -0.0019,  0.0343, -0.0117],
        [ 0.0119,  0.0376, -0.0105,  ...,  0.0377,  0.0068,  0.0093]])

llm.base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0067,  0.0022, -0.0093,  ..., -0.0063, -0.0054,  0.0074],
        [-0.0046, -0.0020,  0.0193,  ..., -0.0009, -0.0126, -0.0067],
        [ 0.0001, -0.0055,  0.0068,  ...,  0.0035, -0.0083, -0.0069],
        ...,
        [-0.0064, -0.0016,  0.0049,  ...,  0.0042,  0.0053, -0.0038],
        [-0.0096,  0.0060,  0.0009,  ...,  0.0042,  0.0081,  0.0020],
        [ 0.0099, -0.0027, -0.0118,  ...,  0.0101, -0.0040, -0.0001]])

llm.base_model.model.model.layers.25.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 1.7812e+00,  4.7070e-01, -2.7734e-01, -1.9629e-01,  1.6309e-01,
         3.0469e-01, -4.8047e-01,  1.2500e+00,  2.9297e-01, -2.7930e-01,
         1.0449e-01,  3.8672e-01, -6.7383e-02, -6.6406e-02, -8.5449e-02,
        -1.7969e-01, -1.5156e+00, -3.9673e-03, -1.3367e-02, -2.2363e-01,
         1.3428e-02,  1.0840e-01,  2.1289e-01, -2.2070e-01,  1.8945e-01,
        -2.3535e-01,  4.2969e-02,  3.9453e-01,  6.6895e-02,  3.0823e-03,
         2.7930e-01,  1.1084e-01,  1.8125e+00, -1.9531e-01, -1.2158e-01,
         2.4375e+00, -2.7710e-02,  4.8584e-02,  6.5918e-02, -5.2002e-02,
        -1.1719e-01, -1.6211e-01,  3.9453e-01,  3.2422e-01,  2.5513e-02,
         4.7607e-02, -2.4048e-02,  6.6797e-01,  1.1035e-01, -6.7871e-02,
         7.4768e-03, -1.7285e-01, -2.7344e-01,  1.1035e-01, -5.1953e-01,
        -1.3770e-01, -5.1758e-02, -2.6367e-01, -1.5137e-01,  6.8750e+00,
         2.1289e-01,  1.8848e-01, -1.4771e-02,  1.4746e-01,  2.3315e-02,
         3.4180e-01,  8.9844e-02, -4.2969e-01, -1.2734e+00,  9.2969e-01,
         4.1016e-01,  2.5781e-01,  1.4258e-01,  1.1377e-01,  1.3594e+00,
        -5.4688e-02,  5.2734e-01, -1.5000e+00,  2.1875e-01,  2.7344e-01,
         2.8516e-01, -1.6797e-01, -1.4404e-02, -1.9062e+00,  7.2266e-02,
         1.5938e+00,  7.1289e-02,  3.7109e-01,  8.1543e-02,  2.1240e-02,
         4.7119e-02,  2.4375e+00,  7.3242e-02, -7.4158e-03, -8.6426e-02,
        -1.2793e-01, -3.6523e-01,  5.4932e-02, -8.5449e-02, -3.2227e-01,
         1.4941e-01, -8.2520e-02, -9.7656e-02, -1.6699e-01, -1.8164e-01,
        -1.7871e-01, -2.9102e-01,  1.3086e-01,  1.0596e-01,  4.7852e-01,
        -1.7773e-01, -1.0437e-02,  1.8066e-02,  5.5908e-02,  2.1875e-01,
         3.9795e-02,  2.6758e-01,  1.0303e-01,  2.3828e-01,  4.6875e-01,
        -6.7188e-01, -5.5078e-01,  4.1016e-01,  8.9375e+00,  2.8320e-01,
        -1.7969e-01,  1.6250e+00, -1.6016e-01, -1.8066e-01,  3.2617e-01,
         2.4902e-01, -1.2793e-01,  2.9883e-01, -3.2031e-01,  2.6953e-01,
         9.8145e-02, -1.1768e-01, -4.5654e-02, -1.8945e-01,  9.5703e-02,
        -2.1484e-01,  2.0801e-01, -1.7969e-01, -5.1172e-01,  8.8867e-02,
         4.6875e-01,  3.0078e-01,  1.3379e-01, -4.8242e-01, -5.5859e-01,
         2.2070e-01, -2.6367e-01, -5.0391e-01,  4.5898e-02, -8.5938e-01,
        -8.2031e-01, -3.9795e-02,  4.4531e-01,  3.0273e-02,  1.5625e+00,
         1.2305e-01,  1.6309e-01, -1.2402e-01, -2.9297e-01,  1.2422e+00,
         4.3335e-03,  1.1484e+00,  7.3828e-01,  2.0020e-02,  9.8877e-03,
        -1.8750e+00,  4.9133e-03,  1.3184e-01,  2.6172e-01, -1.2344e+00,
         5.2734e-01,  4.8047e-01,  2.3535e-01,  1.6309e-01, -5.1562e-01,
        -4.9072e-02, -1.0469e+00,  2.5391e-01,  1.3867e-01,  3.3398e-01,
        -9.1309e-02, -2.2888e-03,  2.8125e-01,  3.4180e-01, -9.9219e-01,
        -9.3750e-02, -3.3594e-01,  2.7734e-01, -1.3770e-01,  1.8359e-01,
         3.2812e-01, -1.0010e-01, -1.3770e-01, -2.2461e-01,  3.7305e-01,
        -1.2354e-01,  3.7695e-01,  1.5625e-01,  2.5000e-01,  3.6133e-01,
         1.8164e-01,  9.9121e-02, -2.6398e-03, -1.2695e-01,  1.0449e-01,
         2.1289e-01, -2.2583e-02,  2.5586e-01,  3.4375e-01, -1.4453e-01,
         5.9375e-01,  2.6953e-01,  1.0645e-01, -2.0508e-01,  3.8281e-01,
         1.8848e-01, -4.0039e-01,  2.8516e-01,  3.6328e-01, -5.6250e-01,
         6.1768e-02, -3.4180e-01,  1.2207e-01,  6.7188e-01,  5.0049e-02,
        -2.1875e-01, -7.2754e-02,  1.4355e-01, -2.7344e-01,  1.6699e-01,
         5.5420e-02, -2.0020e-01, -2.3438e-02, -1.3359e+00,  6.9336e-02,
         3.4375e-01, -3.9795e-02,  2.8198e-02, -1.7480e-01,  6.3281e-01,
        -2.8906e+00,  2.3438e-01,  2.1484e-02, -4.7656e-01,  3.3398e-01,
        -3.8281e-01,  3.7842e-02, -8.3203e-01, -1.3750e+00, -2.9883e-01,
         6.4844e-01, -1.7422e+00, -1.1426e-01,  1.7700e-03, -2.4048e-02,
         1.5820e-01,  1.9238e-01, -4.8584e-02,  2.5977e-01,  5.3711e-02,
        -2.3535e-01,  5.0293e-02, -1.5469e+00,  1.7871e-01, -1.1572e-01,
        -5.2490e-02, -4.9805e-02, -2.5000e-01, -9.8877e-03,  1.4587e-02,
        -2.1250e+00, -1.3306e-02,  1.1182e-01,  5.2490e-02, -2.3828e-01,
         1.3867e-01, -5.7129e-02, -1.6504e-01, -9.0820e-02, -3.0469e-01,
        -3.1055e-01,  7.2266e-02,  5.0537e-02,  7.9102e-02,  5.0781e-02,
        -9.0332e-02, -4.4189e-02,  6.6406e-01,  3.2959e-02, -2.6172e-01,
         1.4941e-01, -1.0498e-01, -4.6875e-02, -2.1289e-01,  9.4238e-02,
        -2.0801e-01,  1.7944e-02,  1.5137e-02,  2.4512e-01, -8.6914e-02,
        -3.5938e+00, -5.6152e-02,  2.1387e-01, -1.0156e-01, -2.0752e-02,
        -2.6953e-01,  4.4531e-01,  4.9072e-02,  1.8848e-01,  1.4648e-01,
        -1.5938e+00,  5.5469e-01,  9.1797e-02,  3.5889e-02, -1.3438e+00,
         2.1875e-01,  2.9688e-01, -1.3281e+00, -3.7891e-01,  1.7871e-01,
        -1.3984e+00, -9.7656e-02, -1.2578e+00, -4.2578e-01, -7.5195e-02,
        -1.8768e-03,  9.8145e-02,  2.5586e-01, -1.4609e+00,  1.5723e-01,
        -1.1641e+00,  1.8164e-01, -1.0352e-01, -2.7954e-02,  2.3145e-01,
         2.6172e-01, -1.0938e-01, -1.0559e-02, -2.0000e+00,  1.0596e-01,
        -1.7285e-01, -1.7383e-01, -5.6152e-02, -1.9531e+00, -2.6855e-02,
         4.7363e-02, -1.8921e-02, -3.0273e-01,  3.5938e-01, -2.4121e-01,
        -1.2451e-02,  2.1406e+00,  3.3447e-02, -1.7578e-01, -1.2598e-01,
        -2.7930e-01,  2.3828e-01, -1.8457e-01,  9.2773e-02,  8.7891e-02,
        -1.2109e-01, -1.1353e-02,  8.0566e-02, -3.6865e-02,  3.5547e-01,
        -1.1230e-01, -1.6699e-01,  2.6562e-01,  1.4062e-01, -1.6016e-01,
         3.5742e-01,  4.0234e-01,  4.7607e-03, -2.3438e-01,  2.7930e-01,
        -1.6113e-01, -8.2031e-01, -7.3438e-01,  2.9375e+00,  8.7891e-01,
         1.7676e-01, -2.9297e-01,  8.3984e-02,  8.6426e-02,  5.2002e-02,
        -4.1797e-01, -1.4648e-01, -9.8633e-02, -1.0400e-01, -9.3359e-01,
         4.1016e-01, -1.6113e-01,  1.1094e+00,  2.5586e-01,  8.9355e-02,
         3.0078e-01, -7.1484e-01,  2.9688e-01, -1.5039e-01, -8.8281e-01,
        -1.9434e-01, -4.4336e-01, -4.0820e-01, -4.9609e-01,  1.3574e-01,
         4.4141e-01,  5.1025e-02,  2.1484e-01,  1.7944e-02, -1.5547e+00,
         2.6953e-01, -1.8457e-01,  3.3417e-03,  1.0781e+00,  1.7090e-02,
        -3.7500e-01, -2.6562e-01,  3.1250e-01, -7.6172e-01,  1.6016e-01,
        -8.8867e-02, -1.8750e+00,  6.7383e-02, -3.0273e-01, -3.1836e-01,
         1.4258e-01,  1.2500e-01,  3.1641e-01, -1.8750e-01,  4.3359e-01,
        -7.1777e-02, -7.4219e-02, -1.7480e-01, -1.1841e-02,  1.8848e-01,
         1.7383e-01,  2.7734e-01,  1.5430e-01,  3.2617e-01,  1.9531e-01,
        -3.9844e-01, -4.5703e-01,  9.6875e-01, -9.2285e-02, -1.5527e-01,
        -2.8125e-01, -4.5703e-01,  7.3047e-01, -9.3750e-02, -1.3733e-02,
        -6.4844e-01, -8.7891e-02,  1.8945e-01,  3.3984e-01,  6.5918e-02,
         2.5781e-01, -1.9238e-01, -1.4771e-02, -2.0117e-01, -7.4609e-01,
         4.0625e-01,  6.7578e-01, -1.2436e-03,  1.2256e-01,  2.4512e-01,
        -3.0029e-02, -1.8438e+00, -3.4912e-02, -1.8848e-01,  2.4414e-04,
        -7.1289e-02,  2.9419e-02, -4.2236e-02,  6.0156e-01,  7.3828e-01,
         8.6426e-02, -5.9326e-02,  1.4609e+00, -2.5024e-02,  1.6113e-01,
         8.7402e-02, -1.7676e-01, -2.7344e-01,  9.6680e-02,  1.5015e-02,
        -2.9297e-01,  5.8594e-02, -3.5645e-02,  9.1797e-02,  2.4219e-01,
        -7.2656e-01,  1.0498e-01, -4.9609e-01,  1.3672e-01, -5.8203e-01,
        -6.5234e-01, -1.0742e-02,  1.3379e-01, -2.0117e-01, -5.5000e+00,
         6.7139e-03,  5.7031e-01,  1.6016e-01,  2.3730e-01, -5.3125e-01,
         2.7539e-01, -1.8945e-01])

llm.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0388, -0.0107,  0.0003,  ...,  0.0730,  0.0497, -0.0045],
        [-0.0099, -0.0181, -0.0204,  ...,  0.0299, -0.0345,  0.0288],
        [-0.0114,  0.0165,  0.0083,  ...,  0.0393,  0.0005, -0.0111],
        ...,
        [-0.0111,  0.0561,  0.0416,  ..., -0.0002, -0.0053,  0.0505],
        [ 0.0267,  0.0157,  0.0192,  ..., -0.0165, -0.0281,  0.0265],
        [-0.0181,  0.0611, -0.0158,  ..., -0.0256,  0.0128, -0.0348]])

llm.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0761,  0.0374, -0.0260,  ..., -0.0459, -0.0091,  0.0273],
        [-0.0150,  0.0521, -0.0275,  ..., -0.0013, -0.0238,  0.0674],
        [ 0.0059,  0.0584,  0.0059,  ...,  0.0353,  0.0460, -0.0429],
        ...,
        [ 0.0318,  0.0402, -0.0009,  ..., -0.0046,  0.0349,  0.0072],
        [ 0.0207,  0.0091, -0.0144,  ...,  0.0246,  0.0226,  0.0180],
        [ 0.0021,  0.0010, -0.0136,  ..., -0.0038, -0.0273,  0.0075]])

llm.base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0193, -0.0093,  0.0066,  ...,  0.0449,  0.0554, -0.0044],
        [ 0.0066, -0.0138, -0.0315,  ...,  0.0004, -0.0099, -0.0366],
        [ 0.0483, -0.0107, -0.0270,  ...,  0.0145,  0.0374,  0.0085],
        ...,
        [-0.0096, -0.0018,  0.0129,  ...,  0.0028,  0.0076,  0.0087],
        [-0.0291,  0.0210, -0.0295,  ..., -0.0198,  0.0070, -0.0045],
        [ 0.0309, -0.0118, -0.0184,  ..., -0.0002, -0.0182, -0.0048]])

llm.base_model.model.model.layers.25.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-6.2500e-01, -4.6094e-01, -3.1250e-01,  1.3855e-02, -3.2031e-01,
        -6.1719e-01, -1.2988e-01,  2.3242e-01,  3.3203e-01,  4.1016e-01,
        -5.1758e-02,  7.7148e-02,  7.0312e-02,  3.9258e-01, -1.7773e-01,
         2.1191e-01, -3.7305e-01,  3.1494e-02,  1.1475e-01, -5.9766e-01,
         2.1680e-01, -9.5703e-02,  3.6328e-01, -1.2402e-01,  1.0693e-01,
        -2.6758e-01, -1.4453e-01, -5.5859e-01,  1.0400e-01, -4.5898e-01,
         3.2031e-01, -3.0859e-01,  5.0781e-01,  2.4219e-01,  2.4121e-01,
         2.2705e-02,  1.7383e-01,  3.3984e-01, -6.1328e-01,  4.4727e-01,
         1.7480e-01, -8.9722e-03, -1.6724e-02, -2.2070e-01,  7.7148e-02,
        -2.1582e-01, -9.5215e-02, -2.2266e-01, -3.8086e-01, -6.6406e-01,
         3.3008e-01,  1.2402e-01, -7.0703e-01, -5.4199e-02,  9.1309e-02,
        -8.9062e-01, -3.1641e-01,  2.4609e-01, -2.4805e-01, -2.4707e-01,
         9.7266e-01, -2.4609e-01, -2.3438e-01, -3.5156e-02,  6.9922e-01,
        -5.6152e-02,  2.0386e-02,  1.5137e-01, -4.8047e-01,  2.8320e-01,
        -3.0273e-01, -2.2070e-01, -1.6602e-01,  3.7109e-01,  3.2617e-01,
         2.1484e-01, -3.5742e-01,  5.8984e-01,  9.5215e-03, -1.4453e-01,
         3.8672e-01,  6.3477e-03, -2.1729e-02, -5.8594e-01,  3.2617e-01,
         1.1133e-01, -1.0156e-01, -1.3086e-01, -1.4453e-01,  1.3867e-01,
        -2.9297e-02,  3.0859e-01,  4.3164e-01,  1.2793e-01,  3.4912e-02,
         1.2207e-01, -8.6426e-02,  3.7500e-01,  1.5527e-01, -3.7500e-01,
         1.3916e-02,  4.7070e-01,  1.3477e-01,  1.3965e-01, -1.3867e-01,
         2.1777e-01, -1.8652e-01,  1.4941e-01,  2.0874e-02,  2.7734e-01,
        -2.0508e-01,  1.2793e-01, -3.4961e-01, -4.6680e-01, -1.2305e-01,
         3.9368e-03, -3.2422e-01, -9.1553e-03, -1.8066e-01, -4.1602e-01,
        -3.3594e-01,  3.6133e-02,  2.3535e-01, -2.5391e-01,  5.7812e-01,
        -4.5703e-01, -4.6875e-01, -1.9531e-01,  3.3936e-02,  5.0293e-02,
         1.4844e-01,  2.2461e-02,  1.0938e-01,  7.2266e-02,  6.3477e-02,
        -7.8125e-02,  5.5664e-02,  1.1084e-01,  5.6152e-02, -1.2256e-01,
        -2.0752e-02,  2.7734e-01, -1.9409e-02,  6.9336e-02, -2.1289e-01,
        -3.4424e-02, -3.5645e-02, -1.8433e-02,  1.4844e-01,  1.3657e-03,
         5.4199e-02,  5.8105e-02, -5.7068e-03, -6.9336e-02,  7.8125e-02,
        -1.5137e-01,  1.7090e-01, -4.3213e-02,  2.1289e-01,  3.0151e-02,
         5.1758e-02, -9.9121e-02, -1.8945e-01,  2.3438e-02, -1.1523e-01,
        -5.3223e-02,  6.2256e-02,  3.7956e-04,  1.2146e-02,  1.4099e-02,
        -1.4648e-01,  5.5908e-02,  4.3945e-02,  3.8574e-02,  7.6172e-02,
        -1.1475e-01,  6.4453e-02,  1.4305e-04, -1.9043e-02,  6.7383e-02,
        -1.3477e-01, -6.8848e-02,  5.6152e-02, -1.0547e-01, -1.3086e-01,
         2.0508e-01,  4.5166e-02,  8.5449e-02, -1.2402e-01,  7.3730e-02,
        -1.5625e-01, -1.5106e-03,  7.0312e-02,  1.1353e-02, -1.3733e-02,
         2.5024e-02, -2.0142e-02,  7.0801e-02,  1.4551e-01,  9.5703e-02,
         2.4658e-02,  1.0529e-03, -1.2512e-02,  8.7402e-02,  1.9336e-01,
         9.7656e-03,  2.1484e-02, -1.4343e-02, -1.4453e-01,  3.0396e-02,
        -6.2012e-02, -1.6724e-02, -7.8125e-02, -8.7891e-02,  1.4551e-01,
         7.5195e-02,  4.7119e-02, -5.5420e-02, -9.0820e-02, -9.1797e-02,
        -5.4932e-02, -2.5635e-02, -4.0039e-02, -1.8921e-02, -1.3477e-01,
         8.3618e-03,  1.3123e-02, -6.1768e-02,  1.2598e-01, -9.8633e-02,
         7.6294e-03,  2.3926e-01,  1.0449e-01, -5.9814e-02, -1.1047e-02,
        -2.2827e-02, -1.2695e-01,  4.5410e-02,  7.4707e-02, -6.9275e-03,
         1.0498e-02,  9.3750e-02,  3.8574e-02, -6.9336e-02, -9.0942e-03,
         9.2773e-02,  6.6406e-02,  6.2256e-03,  3.7598e-02,  6.1340e-03,
        -2.8906e-01, -2.6245e-02, -3.1494e-02,  1.0071e-02,  8.6914e-02,
        -6.4087e-03, -1.2158e-01, -1.1670e-01, -9.1309e-02, -5.0293e-02,
        -1.2939e-02,  9.8145e-02,  1.0596e-01, -8.8379e-02, -6.0547e-02,
        -8.2500e+00, -8.4473e-02,  2.3438e-01, -1.2402e-01, -4.4189e-02,
         1.6602e-02, -6.3477e-02, -1.7212e-02,  4.6143e-02, -3.6133e-02,
         2.0020e-01, -1.0742e-01,  1.4844e-01,  5.4688e-02,  2.1191e-01,
         9.6191e-02,  8.5938e-02, -4.1504e-02,  1.3867e-01, -9.3750e-02,
         1.3867e-01, -1.8262e-01, -1.2695e-01, -3.9062e-02, -3.5645e-02,
        -6.3477e-02,  5.8350e-02,  3.9795e-02, -2.5000e-01,  6.5918e-02,
         5.6152e-03,  1.0498e-01,  3.2715e-02,  2.2095e-02,  9.6680e-02,
        -1.4648e-01,  3.8574e-02, -1.7676e-01, -2.4805e-01, -1.4648e-01,
        -1.4258e-01, -5.4199e-02, -1.4746e-01, -3.9673e-03, -1.5918e-01,
         1.2817e-02, -1.2598e-01,  1.6211e-01, -1.2354e-01,  1.4746e-01,
         4.8340e-02,  1.8457e-01,  9.6191e-02, -1.3574e-01, -1.7871e-01,
         1.1670e-01,  1.6235e-02, -1.3281e-01,  1.0107e-01, -5.0293e-02,
         1.1963e-01, -4.3457e-02, -5.8594e-02,  5.3101e-03,  8.3984e-02,
        -9.4238e-02, -9.6680e-02, -6.8359e-02,  1.1523e-01,  1.8164e-01,
        -1.5723e-01, -1.0840e-01, -2.1851e-02, -2.0874e-02,  6.0303e-02,
        -6.8359e-02, -2.3242e-01, -5.3467e-02, -1.7578e-01,  1.1279e-01,
        -1.7383e-01,  5.2979e-02, -6.3477e-02,  3.3691e-02,  2.3560e-02,
         1.1084e-01,  5.5420e-02,  1.0303e-01, -1.2500e-01, -8.3496e-02,
        -2.2949e-01,  5.8594e-02,  2.2949e-01, -2.0703e-01,  2.0410e-01,
         8.4961e-02, -1.2158e-01,  3.0975e-03, -7.9102e-02,  4.1748e-02,
         1.6479e-02, -3.4375e-01, -2.6855e-02,  1.5015e-02, -7.6172e-02,
         7.2266e-02, -8.6914e-02,  5.1514e-02,  2.2949e-02, -4.2969e-02,
         2.6978e-02,  3.1586e-03,  9.1797e-02,  1.8188e-02, -6.5918e-02,
         1.7090e-01, -1.6211e-01, -1.0925e-02,  1.2988e-01, -1.7285e-01,
        -6.0059e-02,  2.0605e-01,  1.6309e-01, -9.7656e-03,  5.7373e-02,
        -5.4199e-02,  1.5918e-01,  4.7119e-02,  2.1387e-01,  1.8457e-01,
         5.1270e-02, -1.6602e-02, -8.0566e-02,  9.0820e-02,  8.9111e-03,
        -1.0156e-01, -2.4121e-01,  7.8125e-02, -1.8652e-01,  1.1426e-01,
         2.5586e-01,  9.6680e-02,  1.5430e-01,  1.2207e-01, -9.0332e-03,
        -1.8359e-01,  5.2002e-02,  5.8594e-02, -1.2634e-02,  1.3086e-01,
        -7.2266e-02,  2.4170e-02, -2.4658e-02,  2.2949e-01,  8.2520e-02,
        -1.1230e-01, -1.5039e-01,  3.8574e-02, -1.2012e-01,  4.4434e-02,
        -4.4336e-01, -1.8945e-01,  1.0449e-01,  1.7773e-01, -6.8848e-02,
         8.8867e-02,  1.3672e-01,  1.1719e-01, -4.8828e-02, -1.1182e-01,
         2.8442e-02,  7.3242e-02,  1.1963e-01,  8.9844e-02, -7.3242e-02,
        -1.4160e-01, -1.1230e-01,  1.8457e-01,  4.1016e-02, -1.2451e-02,
        -5.7129e-02,  5.9570e-02,  1.1377e-01, -8.3984e-02,  9.9121e-02,
         7.4219e-02,  1.0889e-01, -4.7363e-02, -1.5039e-01,  3.8574e-02,
         9.6436e-03,  9.0332e-02,  1.8359e-01, -2.2070e-01, -1.9531e-01,
         1.1230e-01,  6.9824e-02, -1.3770e-01,  4.3701e-02, -1.7676e-01,
         6.0059e-02, -5.6641e-01,  9.0332e-02,  7.4219e-02, -6.7383e-02,
        -9.6680e-02,  2.7344e-02,  2.5177e-03, -8.2031e-02,  1.3867e-01,
        -1.8359e-01, -1.1963e-01, -6.8359e-02,  1.5137e-01, -7.7637e-02,
        -2.8125e-01,  9.0408e-04,  1.3379e-01, -1.1182e-01, -9.0820e-02,
         4.6875e-02,  3.4424e-02, -2.4609e-01, -7.5195e-02, -2.2656e-01,
         1.0498e-01, -1.6846e-02, -1.7188e-01,  1.0156e-01, -1.1768e-01,
         2.1094e-01,  9.7656e-02, -3.3789e-01, -4.1199e-03,  6.0791e-02,
         1.6504e-01,  2.0386e-02, -5.6152e-02, -7.3730e-02,  7.1777e-02,
         3.3691e-02, -1.4551e-01,  6.0059e-02, -4.4189e-02,  3.6377e-02,
         3.8818e-02,  1.3477e-01])

llm.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0324,  0.0253, -0.0242,  ...,  0.0414, -0.0021,  0.0067],
        [-0.0286, -0.0150, -0.0361,  ..., -0.0387, -0.0147, -0.0069],
        [-0.0408, -0.0186,  0.0413,  ...,  0.0601, -0.0255,  0.0577],
        ...,
        [-0.0583,  0.0235,  0.0102,  ...,  0.0888,  0.0228, -0.0140],
        [ 0.0339, -0.0384, -0.0346,  ..., -0.0186, -0.0084,  0.0021],
        [-0.0059,  0.1186,  0.0032,  ..., -0.0321,  0.0639,  0.0013]])

llm.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0125,  0.0410,  0.0039,  ...,  0.0174,  0.0140, -0.0158],
        [-0.0101,  0.0153, -0.0187,  ...,  0.0082,  0.0318,  0.0062],
        [-0.0342, -0.0077,  0.0576,  ...,  0.0409,  0.0216, -0.0184],
        ...,
        [ 0.0205,  0.0345, -0.0123,  ...,  0.0023, -0.0174,  0.0069],
        [ 0.0274,  0.0291, -0.0150,  ..., -0.0074,  0.1082, -0.0202],
        [-0.0503,  0.0119, -0.0169,  ...,  0.0238, -0.0044, -0.0283]])

llm.base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0205, -0.0136,  0.0152,  ..., -0.0007,  0.0098, -0.0055],
        [-0.0008,  0.0048, -0.0164,  ...,  0.0250,  0.0109,  0.0036],
        [-0.0029,  0.0167,  0.0021,  ...,  0.0071,  0.0181, -0.0004],
        ...,
        [-0.0018, -0.0004, -0.0425,  ..., -0.0184,  0.0153, -0.0256],
        [ 0.0025,  0.0057, -0.0032,  ...,  0.0079, -0.0030,  0.0078],
        [-0.0162, -0.0039,  0.0225,  ...,  0.0208,  0.0046, -0.0203]])

llm.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0360,  0.0130,  0.0009,  ..., -0.0032, -0.0349,  0.0123],
        [ 0.0172, -0.0075, -0.0292,  ...,  0.0489,  0.0252, -0.0006],
        [-0.0316, -0.0085,  0.0157,  ...,  0.0316,  0.0421, -0.0233],
        ...,
        [ 0.0120,  0.0191, -0.0288,  ..., -0.0353,  0.0120, -0.0297],
        [-0.0111, -0.0391,  0.0030,  ...,  0.0025,  0.0068, -0.0030],
        [-0.0220,  0.0193,  0.0234,  ..., -0.0233, -0.0250,  0.0078]])

llm.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0526,  0.0358, -0.0658,  ...,  0.0375,  0.0270,  0.0098],
        [ 0.0405, -0.0492,  0.0337,  ...,  0.0359, -0.0202,  0.0143],
        [-0.0158, -0.0304, -0.0141,  ..., -0.0094,  0.0512, -0.0427],
        ...,
        [-0.0090,  0.0339, -0.0346,  ...,  0.0351, -0.0077,  0.0046],
        [ 0.0191, -0.0673,  0.0076,  ...,  0.0207, -0.0343, -0.0098],
        [-0.0059,  0.0597, -0.0303,  ..., -0.0079, -0.0320, -0.0274]])

llm.base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0150, -0.0011, -0.0089,  ...,  0.0066,  0.0113, -0.0078],
        [-0.0215,  0.0161, -0.0121,  ..., -0.0058, -0.0037, -0.0135],
        [-0.0112,  0.0103,  0.0050,  ..., -0.0388, -0.0056,  0.0152],
        ...,
        [ 0.0254,  0.0037,  0.0087,  ...,  0.0204,  0.0072,  0.0217],
        [-0.0344, -0.0008, -0.0175,  ...,  0.0082, -0.0308,  0.0012],
        [-0.0103, -0.0138,  0.0249,  ..., -0.0023, -0.0094,  0.0120]])

llm.base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0168, -0.0104,  0.0080,  ..., -0.0262, -0.0334,  0.0034],
        [ 0.0012, -0.0002,  0.0177,  ..., -0.0904,  0.0023,  0.0515],
        [-0.0324, -0.0960,  0.0321,  ...,  0.0435, -0.0164,  0.0691],
        ...,
        [ 0.0632, -0.0037,  0.0229,  ...,  0.0087, -0.1142,  0.0331],
        [-0.0711, -0.0674, -0.0314,  ...,  0.0129,  0.0114,  0.0401],
        [ 0.0234,  0.0110,  0.0142,  ..., -0.0459,  0.0490, -0.0521]])

llm.base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 0.0130,  0.0027, -0.0115,  ..., -0.0180,  0.0191,  0.0130],
        [ 0.0146,  0.0111,  0.0657,  ..., -0.0008,  0.0300, -0.0128],
        [ 0.0277, -0.0194,  0.0132,  ..., -0.0237, -0.0132, -0.0066],
        ...,
        [-0.0056,  0.0169,  0.0340,  ...,  0.0298,  0.0102, -0.0518],
        [-0.0028, -0.0206,  0.0040,  ..., -0.0066, -0.0204, -0.0174],
        [-0.0524, -0.0375, -0.0229,  ..., -0.0150, -0.0485,  0.0328]])

llm.base_model.model.model.layers.25.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0208,  0.0272,  0.0033,  ..., -0.0173, -0.0027, -0.0105],
        [ 0.0099, -0.0128, -0.0101,  ..., -0.0010, -0.0075,  0.0193],
        [-0.0013,  0.0219,  0.0043,  ..., -0.0214, -0.0066, -0.0009],
        ...,
        [-0.0057,  0.0036,  0.0304,  ...,  0.0116,  0.0115, -0.0037],
        [ 0.0069, -0.0309,  0.0074,  ..., -0.0026,  0.0097,  0.0032],
        [ 0.0056, -0.0133,  0.0026,  ..., -0.0229, -0.0452, -0.0077]])

llm.base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0254, -0.0175,  0.0641,  ..., -0.0670,  0.0524,  0.0094],
        [ 0.0449, -0.0684, -0.0163,  ..., -0.0618, -0.0600, -0.0641],
        [ 0.0370,  0.0713,  0.0125,  ...,  0.0225,  0.1080,  0.0227],
        ...,
        [ 0.0734, -0.0013,  0.1166,  ...,  0.0176, -0.0786,  0.1395],
        [-0.0518, -0.0445, -0.0297,  ...,  0.0336,  0.0853,  0.0518],
        [ 0.0985,  0.0375, -0.0034,  ...,  0.1105, -0.0394, -0.0492]])

llm.base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-1.3673e-02, -3.0103e-02,  8.9148e-03,  ...,  3.4444e-03,
          1.3985e-03, -3.3078e-03],
        [-1.1251e-02, -2.6348e-05,  2.8853e-02,  ..., -4.1125e-02,
         -1.2746e-02,  1.6847e-02],
        [ 1.6679e-02, -3.3956e-03, -4.4645e-02,  ...,  8.3293e-03,
         -1.3708e-02,  3.4243e-02],
        ...,
        [ 4.5754e-02, -2.0511e-02, -3.4409e-04,  ..., -4.4658e-02,
         -1.1767e-02, -2.5070e-02],
        [-1.1571e-03, -1.5307e-02,  1.7255e-02,  ...,  2.0972e-03,
         -9.2276e-03,  1.8481e-02],
        [ 4.6557e-02, -1.0433e-02,  1.7528e-02,  ...,  1.3877e-02,
         -4.3008e-03,  2.9231e-02]])

llm.base_model.model.model.layers.25.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[-0.0006,  0.0062,  0.0139,  ...,  0.0179,  0.0156,  0.0015],
        [-0.0121, -0.0106,  0.0005,  ...,  0.0078, -0.0140,  0.0128],
        [-0.0002, -0.0122, -0.0121,  ...,  0.0320,  0.0080, -0.0293],
        ...,
        [ 0.0410, -0.0189,  0.0211,  ...,  0.0024, -0.0025, -0.0054],
        [-0.0065,  0.0050,  0.0264,  ..., -0.0152, -0.0359,  0.0249],
        [-0.0076,  0.0116,  0.0090,  ..., -0.0081,  0.0128,  0.0033]])

llm.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0065,  0.0025,  0.0007,  ...,  0.0044,  0.0150,  0.0114],
        [ 0.0146,  0.0036,  0.0296,  ...,  0.0301,  0.0214, -0.0119],
        [-0.0356,  0.0485, -0.0271,  ...,  0.0404,  0.0688, -0.0149],
        ...,
        [-0.0281, -0.0714,  0.0249,  ..., -0.0441, -0.0568, -0.0028],
        [-0.0479,  0.0014, -0.0321,  ...,  0.0180, -0.0205, -0.0152],
        [ 0.0075, -0.0100,  0.0164,  ..., -0.0039, -0.0137,  0.0173]])

llm.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0499, -0.0892,  0.0112,  ...,  0.0078, -0.0093,  0.0355],
        [ 0.0080,  0.0170,  0.0079,  ...,  0.0201, -0.0200, -0.0156],
        [ 0.0385, -0.0208, -0.0192,  ..., -0.0224,  0.0755,  0.0124],
        ...,
        [-0.0250,  0.0200, -0.0106,  ...,  0.0853,  0.0657,  0.0094],
        [-0.0256,  0.0263, -0.0417,  ...,  0.0738, -0.0405,  0.0516],
        [-0.0123, -0.0170, -0.0309,  ...,  0.0132,  0.0122, -0.0148]])

llm.base_model.model.model.layers.25.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.9414, 0.7969, 0.9688,  ..., 0.8672, 0.9141, 0.8789])

llm.base_model.model.model.layers.25.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.8359, 1.7969, 1.8906,  ..., 1.6250, 1.8672, 1.8438])

llm.base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0136,  0.0137,  0.0014,  ...,  0.0123, -0.0145, -0.0145],
        [-0.0043,  0.0280, -0.0064,  ..., -0.0025,  0.0048, -0.0231],
        [-0.0008, -0.0122, -0.0039,  ...,  0.0187,  0.0035,  0.0276],
        ...,
        [-0.0003, -0.0035, -0.0126,  ..., -0.0042,  0.0303, -0.0098],
        [-0.0026,  0.0262, -0.0026,  ...,  0.0004, -0.0156,  0.0048],
        [ 0.0019, -0.0128, -0.0048,  ...,  0.0034,  0.0116,  0.0126]])

llm.base_model.model.model.layers.26.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([-0.1689, -0.1777,  0.1875,  ..., -0.3984,  0.2773,  0.0140])

llm.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0108,  0.0121,  0.0559,  ...,  0.0252, -0.0032, -0.0047],
        [ 0.0012, -0.0143,  0.0159,  ..., -0.0098, -0.0012,  0.0454],
        [-0.0356, -0.0173, -0.0180,  ..., -0.0113,  0.0552, -0.0704],
        ...,
        [-0.0710,  0.0394, -0.0567,  ...,  0.0322,  0.0794, -0.0107],
        [ 0.0071,  0.0345, -0.0110,  ...,  0.0025,  0.0900, -0.0003],
        [ 0.0210, -0.0274, -0.0276,  ...,  0.0390,  0.0157,  0.0457]])

llm.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0019,  0.0155, -0.0097,  ..., -0.0075,  0.0133,  0.0230],
        [-0.0143,  0.0179, -0.0178,  ..., -0.0434,  0.0110, -0.0203],
        [-0.0112,  0.0266, -0.0296,  ...,  0.0006, -0.0193, -0.0083],
        ...,
        [-0.0200, -0.0063, -0.0177,  ...,  0.0019,  0.0026, -0.0103],
        [ 0.0466, -0.0162, -0.0128,  ...,  0.0139,  0.0620,  0.0050],
        [-0.0123, -0.0124, -0.0132,  ..., -0.0133,  0.0013,  0.0181]])

llm.base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0029, -0.0150,  0.0037,  ...,  0.0125,  0.0005, -0.0078],
        [ 0.0053, -0.0010,  0.0101,  ...,  0.0022,  0.0054,  0.0025],
        [-0.0034, -0.0123, -0.0144,  ..., -0.0203,  0.0024,  0.0072],
        ...,
        [-0.0238,  0.0104,  0.0203,  ...,  0.0037,  0.0079,  0.0074],
        [ 0.0361, -0.0068, -0.0161,  ...,  0.0113,  0.0069,  0.0043],
        [ 0.0221, -0.0132, -0.0306,  ..., -0.0220, -0.0103,  0.0002]])

llm.base_model.model.model.layers.26.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-1.8262e-01, -3.7109e-01,  3.2422e-01, -1.9653e-02,  2.9688e-01,
        -3.6914e-01, -4.8096e-02,  4.1797e-01,  2.3071e-02,  3.8867e-01,
         1.2695e-01,  1.4453e-01, -1.5137e-01, -2.1387e-01,  1.4551e-01,
         1.1719e-01, -3.1641e-01, -3.4961e-01, -8.3496e-02, -4.2578e-01,
        -4.9219e-01, -2.3926e-01, -5.6641e-01,  1.4404e-02,  3.3008e-01,
        -3.0859e-01,  1.9336e-01,  1.7969e-01, -1.9629e-01, -6.4844e-01,
         2.3047e-01,  1.5312e+00, -1.8066e-01,  8.0859e-01,  2.7734e-01,
        -1.6602e-01, -1.0742e-01,  8.6719e-01,  1.5723e-01,  1.1719e+00,
        -2.7344e-01,  1.0645e-01,  3.5352e-01,  4.1016e-01, -2.1875e-01,
        -3.9844e-01,  8.6719e-01,  1.8555e-01, -2.6250e+00,  3.0859e-01,
         4.3945e-01, -6.3477e-02,  1.6504e-01, -7.7515e-03, -6.7188e-01,
         7.8906e-01,  4.0771e-02, -2.9492e-01, -1.5859e+00,  8.0859e-01,
        -1.2207e-02,  7.4707e-02,  9.8438e-01, -1.9219e+00, -3.9258e-01,
        -1.8652e-01,  2.5195e-01,  4.0625e-01, -2.9883e-01, -8.2031e-02,
        -1.3672e-01, -4.7607e-02, -1.3184e-01, -5.3223e-02,  2.2949e-02,
         3.3203e-01,  2.0142e-02, -3.3203e-01, -2.2656e-01, -1.7969e-01,
         1.2891e-01,  2.5391e-01, -2.3047e-01,  5.7068e-03,  3.3447e-02,
        -1.2891e-01,  2.8906e-01, -4.3359e-01,  5.1172e-01,  4.4922e-01,
         8.6328e-01,  3.3936e-02,  9.1309e-02,  6.1719e-01, -5.4932e-02,
         2.4707e-01,  1.6357e-02, -3.8672e-01, -1.1963e-01, -3.1445e-01,
         3.1055e-01, -1.3125e+00, -3.5742e-01,  1.3672e-01,  3.7891e-01,
        -2.0117e-01,  2.0000e+00,  4.4141e-01, -2.7734e-01, -1.3867e-01,
        -3.2031e-01, -3.1250e-01, -7.7148e-02,  2.0215e-01, -6.9824e-02,
         2.9492e-01,  7.5195e-02,  5.1562e-01, -6.2988e-02,  2.5195e-01,
         1.3477e-01,  1.9336e-01, -2.3125e+00,  7.6562e-01, -9.0332e-02,
         6.0547e-01,  9.1406e-01, -1.1094e+00, -9.4922e-01, -5.5859e-01,
         5.6250e-01, -7.6172e-01,  2.6953e-01,  6.4062e-01,  1.1426e-01,
         3.9368e-03, -1.5527e-01, -2.7539e-01,  2.2461e-02,  6.0938e-01,
         1.8433e-02, -2.9297e-01,  6.5918e-02, -2.5000e-01, -5.2246e-02,
        -4.5312e-01,  2.5781e-01, -3.5156e-01,  1.5234e-01, -1.7212e-02,
        -7.0312e-01,  1.9287e-02, -5.6152e-02,  2.2266e-01,  4.6094e-01,
         2.5391e-02, -2.2754e-01, -5.7812e-01,  1.9141e-01, -1.3574e-01,
        -3.0518e-02,  1.9287e-02, -7.7148e-02,  2.0215e-01, -1.2598e-01,
         1.3672e-02, -3.2031e-01, -1.8848e-01, -2.5586e-01,  1.8921e-02,
         1.2188e+00,  1.2451e-01, -9.2773e-02, -1.5527e-01,  1.1670e-01,
        -1.3379e-01, -1.7969e-01,  8.7402e-02,  2.2852e-01,  2.0605e-01,
         1.4453e-01, -2.5977e-01,  1.7676e-01,  1.6504e-01, -6.3965e-02,
         3.4766e-01, -4.0625e-01, -2.3633e-01, -5.6641e-01, -4.0625e+00,
        -2.1973e-02,  4.0430e-01, -1.4219e+00, -5.7422e-01, -8.3984e-01,
        -5.1172e-01, -5.4297e-01,  2.2852e-01,  1.9434e-01, -3.8281e-01,
         6.7969e-01,  1.1035e-01, -1.4355e-01,  1.4062e-01, -2.4219e-01,
         2.3242e-01, -3.6621e-02, -2.4121e-01,  1.5918e-01, -4.0283e-03,
         5.9766e-01, -7.6660e-02,  3.8281e-01, -2.6172e-01,  1.2012e-01,
        -4.2383e-01, -2.6562e-01, -1.7578e-01,  1.1377e-01, -4.0234e-01,
        -4.9805e-01,  1.6406e-01,  1.0437e-02, -6.2500e-01, -2.5000e-01,
        -1.0938e+00,  1.4709e-02,  1.0986e-01, -1.3379e-01,  1.4062e-01,
         6.1328e-01, -8.4473e-02,  1.2573e-02, -1.7212e-02,  1.7285e-01,
        -1.1816e-01, -1.7578e-01,  2.3926e-01,  7.5684e-02,  5.0964e-03,
        -2.5312e+00,  7.6172e-02,  1.5137e-01,  4.1016e-02, -1.6992e-01,
        -2.0898e-01, -1.0791e-01, -2.6562e-01,  1.5820e-01,  2.0020e-02,
        -7.4219e-01,  1.1475e-01,  9.1406e-01, -1.2578e+00,  7.5781e-01,
         1.4844e-01, -1.2793e-01, -2.1680e-01,  1.5234e-01, -3.0469e-01,
        -6.2012e-02, -7.0801e-02,  3.7305e-01, -8.7891e-02, -2.7539e-01,
        -1.3672e-01,  2.9492e-01, -2.0410e-01,  3.3789e-01, -4.3750e-01,
         2.0703e-01,  3.6621e-02,  4.5508e-01, -9.4727e-02,  7.6172e-02,
        -3.6914e-01,  1.2146e-02,  8.1055e-02, -8.5547e-01, -3.0518e-02,
         5.6641e-01,  8.7891e-01,  3.4668e-02,  4.0234e-01, -3.4180e-01,
         9.8877e-03, -4.5166e-02,  1.2109e-01,  1.3770e-01,  1.4258e-01,
        -9.9609e-01,  2.7710e-02,  1.1963e-01,  1.1250e+00, -7.1716e-03,
         8.9844e-02,  3.7109e-01,  2.2363e-01, -2.5513e-02, -3.3398e-01,
         8.4766e-01,  4.0820e-01,  8.3496e-02, -2.0312e+00,  3.8672e-01,
         6.3965e-02, -2.5024e-02, -2.0801e-01, -1.8457e-01, -8.0859e-01,
        -7.2266e-02,  8.4375e-01,  1.9727e-01,  3.5547e-01, -2.2656e-01,
        -2.8594e+00, -2.4023e-01, -1.2031e+00, -1.6602e-01, -1.0312e+00,
        -3.1641e-01, -1.1133e-01,  8.0566e-02, -5.0537e-02, -1.0938e-01,
        -1.7578e-01,  1.5527e-01,  1.0205e-01,  1.5137e-01, -2.0605e-01,
         1.4453e-01, -3.1250e-01,  6.2561e-03, -1.0693e-01,  2.9883e-01,
        -1.7285e-01, -4.0039e-01,  1.2451e-01, -5.2344e-01,  1.8555e-01,
         5.1953e-01,  8.1543e-02,  1.4941e-01, -1.6406e-01, -2.9688e-01,
        -1.0986e-01, -5.3906e-01,  1.4258e-01, -7.8516e-01, -1.7969e-01,
         6.3965e-02, -1.8750e+00, -5.2795e-03,  6.6406e-02,  6.0156e-01,
         2.2217e-02,  2.3438e-01,  8.8281e-01,  8.1787e-03, -1.9922e-01,
         2.2969e+00,  6.6406e-02, -2.0605e-01,  5.4688e-01, -4.7119e-02,
        -1.3984e+00,  1.1841e-02, -1.8672e+00, -1.4941e-01,  1.6992e-01,
        -2.0703e-01,  1.2695e-01, -1.5527e-01,  6.0547e-01,  4.3945e-01,
         4.0625e-01,  3.9062e-01,  5.9375e-01,  1.6602e-01,  1.2188e+00,
        -4.0625e-01, -1.0205e-01,  2.9492e-01,  6.5625e-01, -1.4531e+00,
         9.8047e-01,  2.5195e-01,  8.7891e-01, -3.9062e-01, -3.5938e-01,
         1.8457e-01,  5.8594e-01, -2.0312e-01,  6.4453e-01, -1.9653e-02,
         7.6562e-01, -3.4766e-01, -1.1094e+00,  1.6797e-01,  6.9824e-02,
         3.3691e-02,  9.6094e-01,  1.3733e-02, -1.2988e-01, -2.1118e-02,
        -1.1406e+00, -2.0996e-01, -2.1582e-01, -4.5898e-02,  3.0664e-01,
         6.0156e-01,  4.9609e-01,  2.7539e-01, -2.3926e-01, -3.8574e-02,
         1.7188e+00, -1.5430e-01,  5.8105e-02,  2.8320e-01, -3.2812e-01,
         3.2031e-01, -3.4180e-01,  2.1777e-01,  6.2561e-04,  7.4707e-02,
         1.5918e-01,  5.0000e-01,  7.3828e-01,  5.5908e-02,  7.2656e-01,
        -3.1836e-01, -2.1667e-03,  2.8711e-01,  2.2168e-01,  3.8867e-01,
        -1.2085e-02,  1.1816e-01, -5.6250e-01, -4.3359e-01, -2.0605e-01,
        -1.9375e+00, -2.1210e-03,  2.3242e-01,  3.1445e-01,  2.9844e+00,
         6.3672e-01, -3.9648e-01,  7.3047e-01, -1.7188e-01,  6.0547e-02,
         6.6895e-02,  2.1875e-01, -1.5527e-01,  1.0469e+00,  6.5918e-02,
         3.2031e-01,  7.2266e-01,  5.7422e-01, -8.6426e-02,  4.8096e-02,
        -2.7539e-01, -8.9355e-02,  1.3867e-01, -8.3008e-02,  2.2363e-01,
         2.3682e-02, -1.0254e-01, -9.4238e-02,  4.6143e-02,  9.3262e-02,
         1.8945e-01, -3.3789e-01,  1.0000e+00,  1.1875e+00, -1.1230e-01,
         6.3672e-01,  5.6641e-01,  1.0693e-01,  3.7500e-01, -2.6758e-01,
        -2.0410e-01,  2.7710e-02,  4.1406e-01, -1.8652e-01,  4.9072e-02,
        -2.0801e-01,  1.5938e+00,  2.3242e-01, -4.1797e-01,  7.2754e-02,
        -1.2354e-01,  7.0703e-01, -1.7969e+00,  7.0312e-01, -9.2285e-02,
        -8.0078e-01, -3.0664e-01,  2.7930e-01,  2.5586e-01, -2.5781e-01,
         2.4219e-01,  1.6953e+00,  1.6016e-01, -3.6719e-01,  4.3750e-01,
         5.7422e-01, -6.4844e-01, -4.1406e-01,  3.5312e+00, -8.5449e-02,
        -1.3672e-01, -4.3359e-01])

llm.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 2.6986e-02, -2.2649e-05,  4.3671e-02,  ..., -4.6226e-02,
         -1.1821e-02,  1.4552e-02],
        [ 5.0298e-02,  8.2672e-03, -2.8438e-02,  ..., -2.9869e-02,
         -5.6936e-03, -3.8971e-02],
        [-3.2541e-03,  2.9522e-02, -1.1427e-03,  ..., -1.0602e-02,
          2.2447e-02, -1.1287e-03],
        ...,
        [-6.1649e-02,  1.9483e-02, -1.6038e-02,  ..., -2.3436e-02,
          1.9634e-02, -1.8077e-02],
        [-3.9532e-02, -3.1473e-02,  4.7079e-03,  ...,  5.9018e-02,
         -3.5916e-02,  2.4216e-02],
        [-4.0321e-02,  5.2326e-03, -5.7346e-06,  ...,  1.9124e-02,
          2.0506e-02, -1.7712e-02]])

llm.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0163, -0.0045,  0.0132,  ...,  0.0097,  0.0046, -0.0024],
        [-0.0179, -0.0311,  0.0060,  ...,  0.0238,  0.0053, -0.0117],
        [-0.0102, -0.0217,  0.0050,  ...,  0.0011,  0.0238,  0.0064],
        ...,
        [-0.0001,  0.0379, -0.0154,  ...,  0.0136,  0.0228,  0.0036],
        [-0.0242, -0.0324, -0.0030,  ..., -0.0045,  0.0373, -0.0441],
        [ 0.0473, -0.0064, -0.0302,  ..., -0.0009, -0.0324, -0.0771]])

llm.base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[-0.0236,  0.0034, -0.0009,  ..., -0.0154, -0.0102,  0.0072],
        [ 0.0168,  0.0062,  0.0078,  ..., -0.0011, -0.0194, -0.0381],
        [ 0.0018,  0.0413,  0.0004,  ..., -0.0220,  0.0109,  0.0154],
        ...,
        [ 0.0669, -0.0713, -0.0442,  ..., -0.0228, -0.0031,  0.0439],
        [-0.0052,  0.0383, -0.0156,  ...,  0.0469, -0.0142, -0.0209],
        [-0.0144, -0.0270, -0.0330,  ..., -0.0128, -0.0050, -0.0664]])

llm.base_model.model.model.layers.26.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 3.9062e-01,  7.6660e-02, -2.2754e-01,  1.9531e-01,  2.9175e-02,
        -4.6631e-02,  2.1973e-01,  2.3535e-01, -3.1250e-02, -5.7373e-02,
         2.3682e-02, -1.5039e-01, -3.0273e-01,  6.3477e-02,  1.2500e-01,
        -1.7578e-01, -1.5918e-01, -3.9844e-01, -2.1289e-01, -1.5039e-01,
         2.0898e-01, -4.8218e-03,  6.2256e-02, -3.2227e-01,  3.0078e-01,
         7.9956e-03, -2.0801e-01,  1.8652e-01, -2.1875e-01,  1.8066e-01,
         8.1543e-02,  8.9844e-02, -3.1250e-01,  1.3281e-01,  3.6377e-02,
         3.4766e-01, -1.8555e-01, -4.6143e-02,  2.7344e-01, -1.4844e-01,
         7.9727e-04, -3.7305e-01,  2.9602e-03,  2.3438e-02, -2.5391e-01,
         2.7344e-01, -1.0449e-01, -1.3379e-01,  2.0410e-01, -2.5000e-01,
         1.3672e-01,  6.0059e-02,  1.7456e-02, -2.7539e-01,  2.3145e-01,
        -1.2402e-01,  8.8379e-02,  7.2937e-03,  2.0605e-01, -2.3633e-01,
         1.6211e-01, -2.1289e-01,  3.5352e-01,  3.2812e-01, -1.1377e-01,
        -1.2500e-01,  6.0059e-02, -8.8379e-02, -1.8359e-01, -1.7773e-01,
         1.3550e-02, -5.8350e-02, -1.4844e-01,  6.6406e-02,  9.8633e-02,
         1.4258e-01,  1.5723e-01, -2.2168e-01, -5.5908e-02,  8.0078e-02,
         2.4567e-03,  2.2949e-02, -2.6758e-01, -2.4219e-01,  2.2656e-01,
        -1.8555e-01, -6.5430e-02,  3.8281e-01,  3.4668e-02,  2.4805e-01,
         9.4238e-02,  2.7466e-02, -3.0078e-01,  1.7383e-01, -8.3496e-02,
         2.0605e-01,  5.9082e-02,  5.5176e-02, -1.8066e-02, -1.2061e-01,
         3.0859e-01,  2.8711e-01, -2.8125e-01, -6.6406e-02,  3.2715e-02,
         4.8438e-01, -1.6699e-01, -3.1836e-01, -2.2559e-01,  2.2656e-01,
         5.9082e-02, -2.0142e-02, -1.4746e-01, -2.1973e-02, -4.8633e-01,
         3.9795e-02, -2.3828e-01,  1.1414e-02,  1.9922e-01, -2.6172e-01,
         1.0303e-01, -1.6602e-02,  2.3340e-01,  1.7676e-01, -1.6113e-01,
        -2.7148e-01,  1.1279e-01, -6.8848e-02,  3.9844e-01,  1.9336e-01,
        -1.0010e-01, -1.1621e-01,  9.0332e-03, -3.8281e-01,  2.5391e-01,
        -8.3008e-02, -7.0801e-02,  7.0801e-02,  1.7090e-01,  1.0498e-01,
         2.2559e-01, -1.3574e-01,  1.3086e-01, -1.6309e-01,  2.3828e-01,
        -1.1523e-01, -1.3000e-02, -1.8359e-01,  3.3203e-02,  2.7588e-02,
        -1.4746e-01, -4.4141e-01,  8.4229e-03, -3.0078e-01,  9.4727e-02,
        -3.8867e-01, -1.0254e-01,  2.3535e-01,  9.0820e-02, -3.5156e-01,
        -5.5176e-02,  4.8828e-02, -1.7090e-01, -1.5820e-01,  6.9824e-02,
         3.5645e-02, -4.6484e-01, -1.0840e-01, -2.4121e-01, -2.5977e-01,
        -1.7090e-02, -3.8672e-01, -1.7188e-01, -4.2578e-01, -1.2305e-01,
         8.9355e-02,  4.8584e-02, -2.4170e-02, -8.1543e-02, -4.8218e-03,
        -4.1211e-01, -3.2812e-01,  2.6562e-01,  6.8359e-02, -2.5781e-01,
        -9.8145e-02, -1.6309e-01, -1.8262e-01,  2.4023e-01, -4.0039e-02,
         1.9531e-01, -1.8066e-01, -3.3789e-01,  2.4512e-01, -6.8125e+00,
        -5.1953e-01,  4.0430e-01, -2.4707e-01,  2.9492e-01, -4.2383e-01,
         3.5742e-01,  2.1484e-01,  1.9824e-01,  1.2500e-01,  9.9121e-02,
        -8.4961e-02, -5.9509e-03, -3.7109e-01,  1.1475e-01, -2.3682e-02,
         2.5977e-01, -4.1016e-02, -3.2959e-02,  2.3438e-01,  3.8281e-01,
         7.2266e-02,  6.2988e-02,  2.1582e-01,  7.1106e-03, -7.4707e-02,
        -7.4219e-02, -9.9121e-02,  5.1953e-01, -3.8672e-01,  6.1523e-02,
        -1.3867e-01, -1.6699e-01, -4.4531e-01,  3.3789e-01,  7.9590e-02,
         2.1240e-02,  2.0508e-01,  2.4023e-01,  3.9258e-01,  1.9727e-01,
        -5.1758e-02,  1.0547e-01,  2.0117e-01,  8.9844e-02, -3.8086e-02,
        -1.2695e-01, -1.2793e-01, -5.9814e-02,  1.5503e-02, -3.9648e-01,
         2.3730e-01, -2.3535e-01, -1.7090e-01, -4.5410e-02,  1.5527e-01,
         1.3306e-02, -2.9297e-01, -2.9907e-02,  7.3438e-01, -1.5332e-01,
        -1.2012e-01,  4.1504e-02,  1.2891e-01, -1.4746e-01,  7.5000e-01,
         1.0107e-01,  5.0537e-02, -1.6602e-01, -2.2754e-01,  4.7852e-02,
        -1.8066e-02, -5.6885e-02,  2.8076e-02,  3.8574e-02,  2.9297e-01,
        -4.8828e-03, -5.6396e-02, -6.9336e-02, -8.0078e-02, -5.9326e-02,
         1.2695e-02, -9.0332e-02,  5.6396e-02,  4.0283e-02, -1.0059e-01,
        -4.6387e-02, -1.2598e-01,  1.5527e-01,  1.5991e-02, -1.6504e-01,
         1.4941e-01,  4.6387e-02,  4.7461e-01, -1.7578e-01, -7.1777e-02,
        -6.7383e-02,  9.7046e-03, -1.0303e-01, -7.7148e-02, -1.0559e-02,
        -5.1758e-02, -1.1084e-01, -1.5918e-01, -6.3477e-02,  8.7402e-02,
        -1.5411e-03,  3.4180e-02, -2.2461e-01, -6.5918e-02, -8.3618e-03,
         4.3945e-02, -7.8125e-02,  2.3145e-01, -6.4941e-02, -9.8633e-02,
        -1.0986e-01, -9.3750e-02,  2.2168e-01, -1.0803e-02, -4.1199e-03,
         1.0840e-01, -4.5410e-02, -1.2793e-01, -4.7119e-02, -1.0156e-01,
        -8.9844e-02,  1.0156e-01,  1.0693e-01, -9.8633e-02, -1.4062e-01,
        -4.8584e-02,  7.6172e-02, -2.8076e-02,  8.0078e-02,  4.6387e-02,
         2.4658e-02, -1.2207e-01,  1.1768e-01,  3.0859e-01,  6.0059e-02,
         2.2266e-01, -8.3984e-02, -1.9629e-01, -5.1270e-02,  2.4719e-03,
        -1.0449e-01, -1.0547e-01, -7.3242e-03,  1.2891e-01, -6.4941e-02,
         1.8433e-02,  4.6387e-02, -8.3496e-02, -1.0107e-01, -9.3262e-02,
         3.1006e-02, -1.0742e-01, -3.2959e-02, -1.9775e-02,  1.4160e-01,
        -1.2988e-01,  4.1504e-02,  1.5503e-02,  5.4688e-02, -2.3682e-02,
        -4.1748e-02, -8.3008e-02, -1.5527e-01, -1.3477e-01,  1.2634e-02,
        -1.8945e-01, -1.2793e-01,  3.6914e-01, -5.3406e-03, -2.1191e-01,
        -7.2266e-02, -2.1606e-02,  1.7700e-02, -1.9727e-01, -1.2817e-02,
        -1.4453e-01, -2.2339e-02,  5.0049e-02,  6.3324e-04, -1.7188e-01,
         9.0820e-02,  1.2390e-02, -1.1182e-01,  1.0498e-01, -4.3750e-01,
        -3.0469e-01,  3.5938e-01, -1.1768e-01, -4.6484e-01,  2.5391e-01,
        -2.3535e-01, -1.4648e-01, -5.1172e-01, -4.2578e-01,  2.9102e-01,
         8.5449e-02,  2.4805e-01,  1.6699e-01,  2.5977e-01, -6.9922e-01,
        -3.0273e-01, -9.4727e-02, -2.2070e-01, -1.3281e-01, -3.6914e-01,
         4.3555e-01,  2.0312e-01,  4.1797e-01, -5.2002e-02, -8.6426e-02,
         3.2227e-01,  6.7188e-01, -2.2363e-01,  7.1777e-02,  3.2617e-01,
         3.9453e-01,  3.1494e-02, -4.1992e-02, -4.1602e-01, -1.1084e-01,
        -9.1309e-02,  9.6191e-02,  2.6562e-01, -2.4512e-01, -6.6016e-01,
         3.2617e-01,  2.9883e-01, -2.8906e-01,  1.9922e-01,  2.3438e-01,
        -2.0996e-01,  2.0410e-01, -4.4727e-01, -7.7820e-03, -1.5332e-01,
         5.4297e-01,  5.7422e-01, -2.2852e-01,  1.8555e-01, -2.7539e-01,
        -1.9336e-01,  5.0391e-01, -5.1953e-01,  8.2031e-01, -3.6328e-01,
        -6.3281e-01, -4.0234e-01, -2.0801e-01, -1.5625e-01, -2.5391e-01,
         9.8145e-02,  4.0039e-01, -5.7031e-01, -5.2734e-01,  1.6309e-01,
        -3.6328e-01, -1.4648e-01, -3.2227e-01, -5.8984e-01, -6.5234e-01,
        -7.3730e-02, -7.2656e-01,  9.0234e-01,  2.4902e-02,  4.2969e-01,
        -3.4766e-01, -2.5195e-01, -4.3750e-01,  4.5898e-01, -1.3184e-01,
         2.0625e+00,  5.2734e-01,  1.8066e-01,  5.3711e-02, -5.1562e-01,
        -1.1182e-01,  6.2500e-01, -1.5820e-01, -6.3672e-01, -5.0391e-01,
         3.6133e-01,  2.8320e-01, -4.1797e-01,  1.0156e-01, -2.0898e-01,
         5.4688e-01,  2.1875e-01, -2.8320e-01, -2.5195e-01,  8.3496e-02,
         3.3203e-01, -9.7656e-03,  3.3398e-01, -2.0215e-01, -6.4844e-01,
        -5.3125e-01,  6.1328e-01,  6.2109e-01, -6.3281e-01,  2.9492e-01,
        -1.2305e-01,  1.2500e-01, -2.8809e-02, -1.6504e-01, -7.7637e-02,
        -1.6113e-01,  6.7188e-01,  1.1230e-01,  1.0010e-01,  3.2227e-02,
        -5.0391e-01,  1.7383e-01])

llm.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0077, -0.0087,  0.0359,  ..., -0.0118, -0.0145, -0.0295],
        [-0.0143,  0.0190,  0.0161,  ..., -0.0544, -0.0377,  0.0172],
        [ 0.0197,  0.0335, -0.0247,  ..., -0.0217,  0.0378,  0.0039],
        ...,
        [ 0.0350,  0.0109,  0.0102,  ...,  0.0374, -0.0529,  0.0156],
        [-0.0257,  0.0251,  0.0161,  ...,  0.0643,  0.0397,  0.0155],
        [-0.0398, -0.0021,  0.0255,  ...,  0.0040, -0.0173, -0.0104]])

llm.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[-0.0174, -0.0115,  0.0136,  ...,  0.0224, -0.0082, -0.0475],
        [-0.0402,  0.0473,  0.0704,  ...,  0.0381, -0.0344,  0.0393],
        [ 0.0454,  0.0288,  0.0062,  ..., -0.0037,  0.0061,  0.0436],
        ...,
        [ 0.0111, -0.0318,  0.0019,  ..., -0.0390, -0.0420,  0.0041],
        [-0.0501,  0.0158,  0.0427,  ..., -0.0089,  0.0108, -0.0099],
        [ 0.0043,  0.0017, -0.0373,  ..., -0.0184,  0.0416, -0.0293]])

llm.base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[ 0.0046,  0.0292,  0.0020,  ...,  0.0065,  0.0117,  0.0021],
        [-0.0049,  0.0223, -0.0125,  ...,  0.0122, -0.0131, -0.0076],
        [ 0.0374,  0.0266,  0.0223,  ..., -0.0015,  0.0057,  0.0062],
        ...,
        [-0.0131, -0.0112, -0.0118,  ...,  0.0025,  0.0016,  0.0195],
        [-0.0073,  0.0004, -0.0199,  ..., -0.0026,  0.0160, -0.0209],
        [ 0.0088,  0.0043,  0.0092,  ..., -0.0123, -0.0047, -0.0120]])

llm.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0984,  0.0267, -0.0561,  ..., -0.0769, -0.0214,  0.0632],
        [ 0.1635,  0.0690,  0.0859,  ..., -0.0222,  0.0071,  0.0144],
        [ 0.0496,  0.0060,  0.1559,  ..., -0.0139, -0.0258, -0.0544],
        ...,
        [ 0.0175,  0.0019, -0.1230,  ..., -0.0959,  0.0265,  0.0165],
        [ 0.0016,  0.0363, -0.0325,  ...,  0.0081, -0.0117,  0.0194],
        [ 0.0535, -0.0589,  0.0158,  ...,  0.0189,  0.0808,  0.0433]])

llm.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0148, -0.0246, -0.0384,  ...,  0.0155, -0.0225, -0.0271],
        [-0.0422,  0.0251, -0.0019,  ...,  0.0224,  0.0106, -0.0044],
        [ 0.0200, -0.0398,  0.0100,  ..., -0.0032, -0.0262,  0.0432],
        ...,
        [-0.0124, -0.0593, -0.0035,  ..., -0.0632,  0.0004,  0.0057],
        [-0.0267,  0.0100,  0.0070,  ...,  0.0171,  0.0223, -0.0322],
        [ 0.0012, -0.0627,  0.0038,  ..., -0.0070,  0.0107,  0.0196]])

llm.base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-0.0017, -0.0129, -0.0009,  ...,  0.0050, -0.0101, -0.0156],
        [ 0.0080, -0.0236,  0.0086,  ...,  0.0137,  0.0033, -0.0260],
        [-0.0150, -0.0007,  0.0330,  ...,  0.0244, -0.0023, -0.0055],
        ...,
        [ 0.0186,  0.0101,  0.0132,  ..., -0.0172, -0.0001, -0.0045],
        [ 0.0060, -0.0277, -0.0082,  ...,  0.0001,  0.0133, -0.0084],
        [-0.0017,  0.0066,  0.0187,  ...,  0.0111, -0.0077, -0.0173]])

llm.base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0374,  0.1138, -0.0696,  ..., -0.0406, -0.0636,  0.1296],
        [-0.0685, -0.0431,  0.1337,  ..., -0.0920,  0.0768,  0.0108],
        [ 0.0028, -0.0339,  0.1032,  ...,  0.0374,  0.0531,  0.0020],
        ...,
        [-0.0187,  0.0511, -0.0316,  ...,  0.0522,  0.0374, -0.0045],
        [-0.0115, -0.0114, -0.0021,  ..., -0.0094,  0.0062,  0.0329],
        [-0.0382, -0.0129,  0.0650,  ...,  0.0103, -0.0617, -0.0592]])

llm.base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[ 1.9631e-03, -1.6702e-02, -6.0543e-03,  ...,  2.6712e-02,
          3.0018e-02,  2.7690e-02],
        [-1.3543e-02, -4.2723e-03, -3.5512e-02,  ...,  4.0608e-02,
          2.1695e-02, -3.1476e-02],
        [-2.5136e-02, -2.2133e-02, -2.1553e-02,  ..., -4.0331e-02,
         -3.9660e-02, -2.1062e-02],
        ...,
        [ 1.8676e-02, -2.9454e-02, -1.6026e-03,  ..., -4.0402e-02,
         -1.6585e-02,  6.6361e-04],
        [ 2.8807e-02, -1.3088e-02,  5.1903e-03,  ..., -1.9274e-02,
         -1.5118e-02,  2.6806e-02],
        [-1.8000e-02, -1.6027e-02,  2.0153e-05,  ..., -4.0766e-02,
          2.3109e-02,  2.8083e-02]])

llm.base_model.model.model.layers.26.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0123, -0.0037,  0.0054,  ...,  0.0219, -0.0189, -0.0077],
        [ 0.0190,  0.0026, -0.0186,  ..., -0.0024, -0.0115,  0.0222],
        [-0.0064,  0.0091,  0.0103,  ...,  0.0254, -0.0183, -0.0165],
        ...,
        [ 0.0171, -0.0142, -0.0029,  ...,  0.0019,  0.0097,  0.0048],
        [-0.0258, -0.0442, -0.0076,  ...,  0.0273,  0.0197,  0.0068],
        [-0.0016, -0.0112, -0.0121,  ..., -0.0005, -0.0172, -0.0085]])

llm.base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0422, -0.0312, -0.0552,  ...,  0.0614,  0.0673,  0.0771],
        [ 0.0064,  0.0310,  0.0282,  ...,  0.1099, -0.0530,  0.0094],
        [ 0.0238, -0.0051, -0.0891,  ...,  0.0772,  0.0123, -0.0132],
        ...,
        [-0.0850,  0.0544, -0.0131,  ..., -0.0327,  0.0638, -0.0343],
        [ 0.1307, -0.0549, -0.1194,  ..., -0.0565, -0.0826,  0.0097],
        [ 0.0199,  0.0430, -0.1594,  ..., -0.0401,  0.0818,  0.0034]])

llm.base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0278, -0.0670,  0.0449,  ..., -0.0030, -0.0348,  0.0162],
        [ 0.0089, -0.0292,  0.0191,  ...,  0.0150, -0.0284,  0.0255],
        [-0.0828, -0.0194,  0.0803,  ...,  0.0052,  0.0321, -0.0062],
        ...,
        [-0.0294, -0.0168,  0.0135,  ...,  0.0094, -0.0141,  0.0068],
        [-0.0312, -0.0094, -0.0259,  ...,  0.0008,  0.0359,  0.0044],
        [ 0.0517, -0.0078, -0.0060,  ..., -0.0143, -0.0023, -0.0075]])

llm.base_model.model.model.layers.26.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 0.0037, -0.0138, -0.0170,  ..., -0.0005,  0.0125, -0.0135],
        [ 0.0005,  0.0303,  0.0129,  ..., -0.0140,  0.0106,  0.0267],
        [ 0.0066, -0.0176, -0.0150,  ..., -0.0317,  0.0099,  0.0227],
        ...,
        [-0.0198,  0.0007,  0.0266,  ...,  0.0103, -0.0173,  0.0093],
        [ 0.0165,  0.0003, -0.0049,  ..., -0.0067, -0.0042,  0.0010],
        [-0.0038, -0.0037,  0.0045,  ..., -0.0137, -0.0057, -0.0034]])

llm.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[-0.0302, -0.0031, -0.0173,  ...,  0.0045,  0.0124,  0.0052],
        [ 0.0110,  0.0579, -0.0387,  ...,  0.0707, -0.0685,  0.0198],
        [-0.0590,  0.0440, -0.0900,  ..., -0.0664,  0.0617,  0.0300],
        ...,
        [ 0.0383,  0.0268,  0.0094,  ...,  0.0482,  0.0079,  0.0040],
        [-0.0044, -0.0147,  0.0048,  ..., -0.0192,  0.0393, -0.0243],
        [-0.0130,  0.0101, -0.0357,  ...,  0.0600,  0.0289,  0.0116]])

llm.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0313, -0.0058,  0.0221,  ...,  0.0608,  0.0142,  0.0262],
        [-0.0437,  0.0209, -0.0349,  ..., -0.0216, -0.0261,  0.0237],
        [-0.0001, -0.0501,  0.0069,  ...,  0.0688, -0.0067, -0.0656],
        ...,
        [-0.0108, -0.0422, -0.0339,  ..., -0.0007,  0.0349,  0.0173],
        [-0.0157, -0.0285, -0.0116,  ...,  0.0138,  0.0125,  0.0518],
        [ 0.0072,  0.0355, -0.0246,  ..., -0.0089,  0.0134,  0.0213]])

llm.base_model.model.model.layers.26.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.8438, 0.6484, 0.7188,  ..., 0.7344, 0.7344, 0.7188])

llm.base_model.model.model.layers.26.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([1.7656, 1.7422, 1.7969,  ..., 1.6562, 1.7812, 1.7891])

llm.base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0079,  0.0015, -0.0104,  ...,  0.0013, -0.0082,  0.0069],
        [-0.0125,  0.0206,  0.0026,  ...,  0.0044, -0.0084,  0.0050],
        [-0.0100,  0.0038,  0.0075,  ...,  0.0203, -0.0008,  0.0192],
        ...,
        [-0.0012,  0.0154,  0.0124,  ..., -0.0242, -0.0179, -0.0152],
        [-0.0205,  0.0109, -0.0339,  ...,  0.0447,  0.0032,  0.0232],
        [-0.0141, -0.0039, -0.0168,  ...,  0.0164,  0.0076, -0.0084]])

llm.base_model.model.model.layers.27.self_attn.q_proj.base_layer.bias-torch.Size([3584])-torch.float32
tensor([ 0.0359, -0.2773,  0.2158,  ...,  0.1245, -4.0625,  0.4336])

llm.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0008,  0.0136,  0.0464,  ..., -0.0619,  0.0749,  0.0102],
        [ 0.0118,  0.0187, -0.0459,  ..., -0.0257, -0.0429,  0.0567],
        [-0.0201, -0.0261,  0.0147,  ..., -0.0080,  0.0297,  0.0034],
        ...,
        [ 0.0101, -0.0177,  0.0047,  ..., -0.0311, -0.0367,  0.0337],
        [-0.0270,  0.0159,  0.0005,  ..., -0.0153,  0.0243, -0.0226],
        [-0.0255, -0.0060, -0.0321,  ..., -0.0518,  0.0137,  0.0670]])

llm.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[-0.0070,  0.0030, -0.0361,  ...,  0.0063,  0.0171,  0.0338],
        [ 0.0176,  0.0068,  0.0134,  ..., -0.0094, -0.0076, -0.0076],
        [ 0.0002,  0.0343, -0.0565,  ...,  0.0142,  0.0137,  0.0265],
        ...,
        [ 0.0430,  0.0049,  0.0198,  ..., -0.0233,  0.0408,  0.0034],
        [-0.0257,  0.0020,  0.0258,  ..., -0.0021,  0.0081,  0.0129],
        [-0.0014,  0.0078,  0.0312,  ..., -0.0028,  0.0355,  0.0186]])

llm.base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0042, -0.0082, -0.0062,  ...,  0.0035,  0.0093, -0.0041],
        [ 0.0022, -0.0035,  0.0048,  ...,  0.0082, -0.0129,  0.0079],
        [ 0.0004, -0.0129, -0.0058,  ..., -0.0097, -0.0041, -0.0045],
        ...,
        [-0.0057,  0.0047,  0.0045,  ..., -0.0187, -0.0037,  0.0013],
        [-0.0125,  0.0029, -0.0090,  ...,  0.0008, -0.0047, -0.0015],
        [-0.0035, -0.0022, -0.0053,  ..., -0.0225, -0.0128, -0.0023]])

llm.base_model.model.model.layers.27.self_attn.k_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([ 1.9238e-01, -5.6641e-02, -1.0938e-01, -1.7578e-01,  2.1289e-01,
        -1.8457e-01, -3.4570e-01, -1.5039e-01,  2.0020e-01, -1.5723e-01,
         1.3867e-01, -4.1797e-01, -6.8359e-02, -2.3438e-01, -1.9824e-01,
        -1.7676e-01,  1.7969e-01, -1.6504e-01,  5.6641e-01,  4.6289e-01,
         2.1387e-01, -3.1445e-01,  2.4902e-01, -6.4062e-01, -2.4805e-01,
        -1.9922e-01,  9.7656e-01, -1.9727e-01,  5.2734e-01,  3.3008e-01,
        -8.9355e-02, -3.9062e-01, -1.9629e-01,  1.4355e-01,  2.0630e-02,
        -1.0703e+00, -4.1406e-01,  4.4727e-01,  1.2573e-02,  2.5000e+00,
        -5.8350e-02,  3.0469e-01, -1.7480e-01,  1.1250e+00, -2.9688e-01,
         4.5654e-02, -7.4219e-01,  5.5078e-01, -3.5547e-01, -5.8984e-01,
        -4.8828e-01,  4.8633e-01, -7.6562e-01,  4.7656e-01, -2.9492e-01,
         8.7500e-01,  1.1250e+00, -4.3750e-01,  1.1875e+00,  1.8906e+00,
        -1.7969e-01,  3.0273e-01,  5.8203e-01,  3.0469e-01,  8.4473e-02,
        -1.9727e-01,  2.3633e-01,  9.3262e-02, -2.3047e-01, -4.7363e-02,
         1.6797e-01,  9.4727e-02,  2.8516e-01, -2.0020e-01, -6.0059e-02,
         7.1289e-02, -1.7676e-01,  2.6562e-01,  3.2422e-01, -5.5859e-01,
        -7.7637e-02, -2.8125e-01,  2.4219e-01, -4.2578e-01, -6.2891e-01,
         5.4688e-01,  1.4771e-02,  4.4336e-01,  7.3828e-01,  6.0938e-01,
        -2.7148e-01,  6.3281e-01, -1.5527e-01,  6.7188e-01,  3.6328e-01,
         1.8672e+00, -1.0703e+00, -8.4961e-02, -2.7930e-01, -1.2188e+00,
        -1.1963e-01,  3.7500e-01,  1.9824e-01, -4.4141e-01, -2.2559e-01,
        -2.2949e-01, -7.1777e-02,  2.8438e+00,  2.1680e-01,  8.3203e-01,
        -2.8516e-01,  1.3359e+00,  3.3438e+00, -2.4023e-01, -1.3184e-01,
         1.8359e-01,  7.7344e-01,  6.9922e-01, -5.7812e-01,  2.0000e+00,
        -7.5000e-01,  4.7461e-01,  7.9688e-01,  5.3125e+00, -3.6719e-01,
         1.6250e+00, -5.1172e-01, -8.3984e-01,  2.2969e+00,  1.5078e+00,
         1.3984e+00,  5.6250e-01,  5.3516e-01,  1.5156e+00,  1.5312e+00,
         1.0234e+00, -5.5078e-01, -9.4531e-01, -5.7812e-01, -5.1562e-01,
         2.7969e+00, -6.4062e-01,  1.4746e-01,  9.2188e-01, -1.0156e+00,
        -4.8584e-02, -1.2969e+00, -5.7373e-02, -6.8750e-01, -2.8281e+00,
        -5.1953e-01, -9.6094e-01,  9.5312e-01, -3.0625e+00,  4.8340e-02,
        -2.2188e+00,  9.8438e-01, -4.2969e-01,  2.8125e-01, -2.8442e-02,
         2.8125e-01,  8.8379e-02,  1.0547e+00,  1.5156e+00, -1.4375e+00,
         2.9492e-01, -8.9453e-01, -3.7695e-01,  5.9375e-01,  1.1094e+00,
         7.2656e-01,  2.2656e-01,  2.7188e+00, -3.9219e+00, -1.2188e+01,
        -1.0250e+01, -1.0625e+00,  6.9531e-01,  1.2688e+01, -6.3250e+01,
        -2.2188e+00, -4.5750e+01, -3.9375e+00,  2.5125e+01,  8.6000e+01,
        -6.0000e+01,  1.7900e+02,  8.5000e+01,  2.0500e+02, -3.7250e+01,
        -3.9000e+02, -2.9000e+02,  2.2656e+00,  1.7891e+00,  5.8203e-01,
        -2.4375e+00,  1.0078e+00, -6.4844e-01,  7.5391e-01, -4.9805e-02,
         2.9688e-01, -2.6719e+00, -1.5312e+00, -5.4297e-01, -1.1562e+00,
        -4.9805e-01,  5.6250e-01,  1.1641e+00, -1.1250e+00, -1.5391e+00,
         4.8438e-01, -2.1240e-02,  5.0391e-01, -2.0469e+00, -6.0938e-01,
         1.5938e+00, -1.2188e+00,  2.4844e+00,  1.1250e+00,  8.7891e-01,
         6.5234e-01, -4.5703e-01,  9.3750e-01,  1.3750e+00,  1.3750e+00,
        -6.1719e-01, -1.3594e+00,  6.6406e-01, -9.9219e-01, -6.7188e-01,
        -2.5469e+00,  8.2422e-01,  7.4219e-01, -1.2734e+00,  4.8828e-01,
        -2.3438e-01, -2.5938e+00,  3.0156e+00,  1.0562e+01,  1.1000e+01,
        -1.5703e+00,  1.7734e+00, -1.1938e+01, -1.9250e+01,  1.0156e+00,
         1.5375e+01,  6.1250e+00, -1.7344e+00,  2.3500e+01,  8.5000e+01,
        -1.5100e+02, -1.9100e+02,  2.2600e+02, -3.6000e+02, -2.1100e+02,
        -4.0400e+02, -3.3789e-01,  2.0117e-01,  1.7188e-01, -5.5237e-03,
         2.3193e-02,  4.5166e-02, -2.8516e-01, -1.1133e-01, -2.1094e-01,
        -1.8457e-01,  2.1240e-02, -1.8677e-02,  3.7891e-01, -2.0605e-01,
        -1.6699e-01,  1.3184e-02,  2.8125e-01, -3.3398e-01,  5.8594e-01,
        -2.7148e-01, -6.5430e-02,  7.3047e-01,  1.3965e-01,  8.3594e-01,
        -6.0938e-01,  3.1250e-01, -1.3867e-01, -4.5703e-01, -1.3281e-01,
        -1.8555e-01,  7.8125e-01, -1.0107e-01, -9.2773e-02,  8.2520e-02,
         1.2734e+00,  1.8457e-01,  3.1250e-01, -1.4258e-01, -2.2754e-01,
         2.2969e+00, -1.1475e-01,  2.2754e-01,  9.0820e-02,  1.9609e+00,
        -3.6865e-02,  1.4648e-03, -2.3730e-01,  6.8359e-01,  3.1562e+00,
         4.1211e-01, -4.1406e-01,  2.3828e-01,  5.7422e-01, -1.7090e-01,
         1.0234e+00, -5.3125e-01, -5.1875e+00, -3.7109e-01,  1.4141e+00,
        -2.1289e-01,  6.2500e-01,  2.0703e-01, -7.2266e-02,  1.2598e-01,
        -1.4648e-01,  1.7285e-01,  1.3086e-01,  2.9688e-01, -4.0283e-02,
        -6.9336e-02, -2.6245e-02, -1.9629e-01, -7.5195e-02, -1.3379e-01,
         2.6172e-01,  1.6895e-01, -1.1328e-01,  2.2168e-01, -1.6016e-01,
         3.7891e-01, -2.9297e-01,  1.8066e-01, -2.5781e-01,  1.5869e-02,
        -1.9922e-01, -5.8203e-01, -9.9609e-02, -6.0156e-01, -5.9326e-02,
        -4.0039e-01, -2.1582e-01, -1.2266e+00, -3.7109e-01, -1.9141e-01,
         5.3906e-01,  1.7891e+00,  2.4902e-01, -4.1406e-01, -8.9453e-01,
         4.7656e-01,  1.1279e-01, -8.7402e-02,  8.6914e-02,  4.5117e-01,
         4.1016e-01, -5.5664e-02,  1.4062e-01,  2.2969e+00,  3.8281e-01,
         6.9336e-02, -9.8828e-01, -5.1953e-01, -5.2734e-01, -3.5156e-02,
         3.3203e-01,  8.4473e-02, -5.7812e-01,  8.3203e-01,  8.4473e-02,
        -9.0234e-01,  2.1562e+00,  4.5898e-01, -8.2031e-01,  1.8438e+00,
        -6.4844e-01, -9.8438e-01,  4.6484e-01, -2.0020e-01, -5.2734e-02,
         3.1445e-01,  3.2812e-01, -1.7871e-01,  4.7656e-01,  2.0898e-01,
         1.6895e-01, -4.2383e-01, -3.3008e-01, -1.4844e-01,  2.8931e-02,
        -2.2827e-02,  2.1582e-01,  2.7344e-01, -6.0547e-01,  1.2891e-01,
        -1.2695e-01, -1.1914e-01,  2.2656e-01, -4.8633e-01, -3.8281e-01,
        -3.8672e-01, -3.7500e-01, -4.2969e-01,  3.4375e-01, -1.6797e-01,
        -9.0625e-01,  2.1289e-01,  6.0547e-01, -2.9492e-01,  1.2085e-02,
         2.9492e-01,  8.0859e-01, -1.3379e-01,  2.3535e-01,  1.3477e-01,
        -9.0234e-01, -3.1836e-01, -1.7031e+00,  2.7588e-02, -2.7539e-01,
         9.3994e-03,  1.1016e+00, -2.1582e-01,  4.4922e-01, -1.3123e-02,
        -3.4180e-02,  2.3828e-01, -2.2188e+00,  1.8945e-01, -3.7891e-01,
         1.2695e-01, -3.1982e-02,  6.4062e-01, -1.6504e-01,  4.6484e-01,
         1.8281e+00,  1.0547e+00, -2.2754e-01,  7.8906e-01,  8.4229e-03,
        -3.7891e-01,  3.9062e-01, -4.1260e-02, -2.7710e-02, -1.8066e-01,
         4.6730e-04, -7.7148e-02, -2.5635e-02, -4.4922e-02,  2.4023e-01,
        -7.5195e-02,  5.8594e-02, -2.7930e-01, -5.1953e-01, -4.2188e-01,
        -3.5742e-01, -8.7891e-02,  2.1484e-01, -4.2188e-01, -1.4844e-01,
         9.5215e-02,  8.1250e-01,  1.5039e-01,  4.5654e-02,  7.6172e-02,
        -6.4453e-01,  5.0000e-01,  7.6953e-01, -2.4707e-01,  3.5352e-01,
         9.0820e-02,  9.5312e-01, -4.1406e-01, -2.5781e-01,  1.7500e+00,
        -3.6523e-01, -2.3926e-01, -3.6328e-01,  3.4912e-02, -7.4219e-01,
         8.2520e-02, -7.4609e-01,  4.2725e-02, -5.1172e-01, -5.9204e-03,
         2.5469e+00, -4.3359e-01, -1.1035e-01,  4.1016e-02,  2.2070e-01,
         1.5625e+00, -2.8516e-01, -3.1562e+00, -2.3828e-01, -2.3633e-01,
        -4.3945e-02,  7.3828e-01,  4.3164e-01,  4.1602e-01,  4.1250e+00,
        -4.0820e-01, -9.3750e-01,  1.3672e+00, -3.3936e-02, -1.1406e+00,
        -3.5156e-01, -3.2227e-01])

llm.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0300,  0.0227,  0.0173,  ..., -0.0308, -0.0303,  0.0148],
        [-0.0061, -0.0003,  0.0186,  ...,  0.0062,  0.0053,  0.0208],
        [ 0.0334,  0.0484, -0.0501,  ...,  0.0290,  0.0490, -0.0113],
        ...,
        [ 0.0088, -0.0509,  0.0296,  ..., -0.0612, -0.0248, -0.0115],
        [-0.0662,  0.0504,  0.0168,  ...,  0.0455,  0.0527,  0.0126],
        [ 0.0642,  0.0589, -0.0197,  ..., -0.0015, -0.0015,  0.0056]])

llm.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0211,  0.0081, -0.0051,  ..., -0.0549,  0.0259,  0.0074],
        [-0.0008,  0.0168,  0.0188,  ..., -0.0131,  0.0207, -0.0164],
        [-0.0033, -0.0139, -0.0067,  ..., -0.0073, -0.0240,  0.0013],
        ...,
        [ 0.0452,  0.0117,  0.0064,  ..., -0.0172,  0.0110, -0.0232],
        [ 0.0327, -0.0074, -0.0067,  ...,  0.0104,  0.0073,  0.0283],
        [-0.0363,  0.0317, -0.0255,  ...,  0.0225, -0.0334,  0.0079]])

llm.base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight-torch.Size([512, 3584])-torch.float32
tensor([[ 0.0098,  0.0262,  0.0432,  ...,  0.0131,  0.0022,  0.0430],
        [ 0.0157,  0.0077, -0.0077,  ...,  0.0034, -0.0129, -0.0206],
        [-0.0028, -0.0199, -0.0491,  ..., -0.0149,  0.0298, -0.0148],
        ...,
        [-0.0249, -0.0261,  0.0149,  ..., -0.0261, -0.0216,  0.0021],
        [ 0.0045,  0.0069, -0.0016,  ..., -0.0150, -0.0033, -0.0146],
        [-0.0223, -0.0342,  0.0288,  ...,  0.0085,  0.0014, -0.0181]])

llm.base_model.model.model.layers.27.self_attn.v_proj.base_layer.bias-torch.Size([512])-torch.float32
tensor([-2.0898e-01,  2.8906e-01,  4.3555e-01, -4.6631e-02,  2.0508e-01,
         1.5527e-01,  2.4609e-01, -2.2168e-01,  1.7188e-01,  1.5918e-01,
        -1.2512e-02, -2.8516e-01, -1.3672e-01,  3.7695e-01,  3.0664e-01,
        -3.0469e-01,  4.3750e-01, -3.3789e-01, -7.5195e-02, -3.3008e-01,
        -1.4062e-01, -5.2344e-01,  1.6602e-02,  3.3984e-01, -3.6328e-01,
        -1.4648e-01, -3.4570e-01,  1.9336e-01, -7.2754e-02, -2.1973e-01,
        -3.1836e-01,  2.1582e-01, -3.1055e-01,  1.3086e-01, -2.4121e-01,
        -8.6914e-02,  3.1836e-01, -2.2656e-01, -2.4902e-01,  3.1055e-01,
        -2.0508e-01,  2.5195e-01,  2.3145e-01,  3.4375e-01,  2.5781e-01,
         1.6016e-01,  1.3184e-01,  2.6172e-01, -8.5938e-02,  2.7148e-01,
        -2.5000e-01, -2.5781e-01,  1.6309e-01,  5.7861e-02,  6.4453e-02,
        -1.3672e-01, -4.4922e-01,  2.5586e-01, -1.0986e-01,  4.2969e-01,
         2.3633e-01,  1.4746e-01,  2.1484e-01, -2.2559e-01,  1.1328e-01,
        -1.7969e-01, -8.7402e-02, -1.0498e-01, -2.9492e-01, -9.9609e-02,
         1.4844e-01,  1.3770e-01,  2.6367e-01, -2.7148e-01, -4.2188e-01,
         2.9297e-01,  4.3164e-01, -3.5547e-01,  2.3340e-01, -3.3398e-01,
        -7.7637e-02,  4.3555e-01, -5.6250e-01,  1.4551e-01, -9.5215e-02,
        -3.5645e-02,  3.0664e-01, -2.1289e-01, -1.6699e-01,  3.2617e-01,
        -5.0391e-01, -2.6953e-01,  3.3203e-01,  1.2061e-01, -8.2520e-02,
        -2.2559e-01, -2.8906e-01,  1.3672e-01,  3.8281e-01,  1.3965e-01,
         3.8086e-02, -1.7871e-01,  3.8818e-02, -3.8086e-01,  4.1406e-01,
        -1.1523e-01, -2.6172e-01, -3.1445e-01,  5.1172e-01,  1.6406e-01,
         1.8652e-01, -2.5781e-01,  2.1289e-01,  4.5898e-01,  2.0508e-01,
         4.6094e-01, -8.0566e-02, -2.2852e-01, -4.5898e-01, -1.2500e-01,
         4.1016e-02,  1.3965e-01, -1.4941e-01,  1.2109e-01, -1.7676e-01,
         8.6914e-02, -2.2461e-01, -2.9492e-01,  2.2500e+00, -1.7031e+00,
        -2.2969e+00,  3.3438e+00, -2.5156e+00,  3.5547e-01, -7.6660e-02,
        -3.2969e+00, -6.2500e-01,  2.8281e+00,  3.1094e+00, -3.0156e+00,
        -2.9688e+00,  3.8867e-01,  2.5156e+00,  1.4766e+00,  1.5312e+00,
         2.7344e+00,  2.9062e+00,  2.5625e+00, -3.2031e+00,  1.8281e+00,
         1.3750e+00, -2.3281e+00, -2.8281e+00,  1.1016e+00,  3.0000e+00,
         2.1250e+00,  1.4297e+00, -2.5781e+00, -2.8125e+00,  2.4844e+00,
        -6.5234e-01, -1.9922e+00,  2.2812e+00,  5.0625e+00, -2.8750e+00,
         3.8594e+00,  1.2578e+00, -2.1406e+00, -2.2500e+00, -5.0938e+00,
        -3.0938e+00,  2.0469e+00,  6.8359e-02, -2.6406e+00, -2.9062e+00,
         1.4922e+00,  1.6406e+00,  2.7500e+00, -2.3281e+00,  2.6094e+00,
        -2.1562e+00, -1.7734e+00, -2.4219e+00,  2.5156e+00, -2.7031e+00,
         1.8516e+00, -2.7500e+00,  3.4219e+00, -1.4219e+00,  2.4531e+00,
        -2.4062e+00,  2.6719e+00,  2.6406e+00, -2.5000e+00,  2.2031e+00,
         2.5000e+00,  5.4297e-01, -4.6289e-01, -2.5469e+00, -1.8906e+00,
        -2.3750e+00, -2.4844e+00,  3.0156e+00,  2.7812e+00, -2.8281e+00,
        -2.4844e+00, -7.9102e-02,  2.3281e+00,  3.2969e+00,  2.2344e+00,
        -1.9922e+00,  2.5938e+00,  2.2969e+00,  2.3750e+00,  2.5156e+00,
        -3.1445e-01, -3.6406e+00,  3.0000e+00,  3.1094e+00, -1.9453e+00,
        -2.5938e+00,  3.0312e+00,  8.1641e-01, -1.0234e+00,  3.7188e+00,
        -3.4688e+00, -2.4688e+00,  2.9844e+00,  2.2969e+00,  2.5469e+00,
        -2.9844e+00, -2.6562e+00,  1.9141e+00,  1.4844e+00, -2.7500e+00,
         2.6250e+00, -1.0781e+00,  2.5781e+00, -7.5000e-01, -2.2188e+00,
         2.0312e+00, -3.5312e+00,  1.3359e+00, -2.4375e+00, -3.3906e+00,
        -2.5000e+00, -3.0859e-01,  1.3516e+00, -2.1094e+00,  4.8633e-01,
        -2.1719e+00, -3.1562e+00,  1.7422e+00,  3.5625e+00, -2.7969e+00,
        -1.6328e+00, -1.4551e-01,  5.3125e-01, -4.3555e-01, -7.3730e-02,
         2.5781e-01,  7.5684e-02, -7.5378e-03,  1.3184e-01, -1.4551e-01,
        -4.4556e-03,  6.7188e-01, -3.5547e-01,  4.3359e-01,  1.5918e-01,
        -2.6172e-01,  2.1484e-01, -2.4902e-01, -3.2227e-02,  1.2500e-01,
        -5.4688e-02,  5.4932e-03,  1.0986e-01,  3.9062e-01, -1.3965e-01,
         2.6367e-01, -1.0596e-01, -3.3984e-01,  2.6855e-03, -2.0605e-01,
        -7.4219e-02,  2.7148e-01, -4.4189e-02,  2.7930e-01,  4.0039e-01,
        -5.7422e-01,  1.5918e-01,  3.4180e-01, -1.2817e-02,  8.6426e-02,
         2.8711e-01, -3.0273e-01, -2.8906e-01, -4.0283e-02, -2.2070e-01,
        -3.2471e-02, -6.8359e-03, -8.0078e-02, -8.1543e-02,  1.2061e-01,
        -2.3926e-01,  8.2520e-02, -7.5684e-02, -5.3906e-01, -2.0508e-01,
         2.0410e-01, -2.1387e-01, -5.5664e-02,  6.8359e-02,  5.9766e-01,
        -2.8931e-02,  1.6895e-01, -3.4375e-01,  1.7188e-01,  1.1963e-01,
         4.0771e-02, -5.7129e-02,  1.1816e-01,  3.7109e-02,  3.5352e-01,
        -1.3281e-01, -1.1230e-02, -2.1875e-01,  1.9897e-02, -4.2188e-01,
         2.9492e-01, -2.7734e-01, -5.0781e-01, -3.3984e-01,  1.8164e-01,
         8.2031e-02,  4.4531e-01, -1.6504e-01, -3.1445e-01,  2.0264e-02,
         1.9629e-01, -1.9653e-02,  3.8672e-01, -3.1250e-01,  2.6758e-01,
         4.0234e-01,  7.6660e-02, -2.0703e-01, -1.5503e-02, -2.3438e-01,
        -5.1270e-02, -1.0742e-01,  1.1523e-01, -7.0801e-02, -9.5215e-02,
         2.5787e-03, -8.0078e-02,  1.2109e-01,  5.9766e-01, -4.0039e-02,
         3.9258e-01,  2.2852e-01,  2.8711e-01, -2.3193e-02, -1.2988e-01,
         1.7969e-01, -1.5234e-01,  2.4414e-01, -2.5195e-01, -1.5625e-01,
         8.0078e-02,  9.9121e-02, -1.7334e-02,  3.4570e-01,  2.0874e-02,
        -1.3086e-01, -8.4473e-02,  2.1094e-01,  4.4922e-01, -1.6602e-01,
         3.5400e-03, -3.4961e-01, -1.4746e-01, -2.9492e-01,  2.0117e-01,
        -1.5137e-01, -6.9336e-02, -1.7383e-01, -1.5332e-01,  2.1191e-01,
         2.0215e-01, -8.7402e-02, -1.7871e-01,  1.4355e-01,  6.4453e-02,
        -1.1523e-01, -7.4219e-02, -2.2461e-01, -1.5527e-01, -3.3936e-02,
         4.3945e-02,  1.0303e-01,  4.1016e-01, -5.1270e-02, -1.1035e-01,
         8.3008e-02,  3.1494e-02, -1.0498e-01,  2.8516e-01,  1.8359e-01,
        -1.8262e-01, -1.3062e-02,  4.1260e-02, -7.0312e-02,  2.2266e-01,
         1.8164e-01,  2.9102e-01, -4.4678e-02, -1.3867e-01,  1.8164e-01,
         1.7578e-01,  6.9336e-02, -8.1055e-02, -1.8005e-03,  4.2725e-02,
        -3.7842e-02,  1.0938e-01,  1.1963e-01, -2.3242e-01,  3.1445e-01,
        -2.8125e-01,  4.1992e-02,  1.1914e-01, -2.2070e-01, -9.0332e-02,
        -6.0547e-02,  1.2793e-01,  2.6562e-01, -4.8828e-02, -3.9307e-02,
         1.6504e-01, -5.6396e-02, -2.2852e-01,  1.3379e-01,  1.2891e-01,
        -8.8867e-02,  4.7461e-01, -5.8594e-02, -5.2734e-01, -3.4912e-02,
        -9.7046e-03,  8.2520e-02, -2.0117e-01,  1.1230e-02, -4.0430e-01,
        -4.8242e-01, -4.8096e-02,  1.0889e-01,  1.3477e-01,  1.5723e-01,
        -1.9238e-01,  1.1328e-01,  5.9375e+00, -2.2070e-01,  1.0791e-01,
        -1.3086e-01, -2.2363e-01, -1.4551e-01, -4.5166e-02, -3.5742e-01,
         7.3242e-02,  1.9531e-02,  1.5625e-01,  1.8066e-02, -1.3672e-01,
        -1.1572e-01,  1.9824e-01,  4.0430e-01, -2.3145e-01, -1.1426e-01,
         1.7944e-02,  1.5723e-01, -1.9922e-01, -1.7383e-01,  1.0596e-01,
        -2.8125e-01, -2.5195e-01,  1.2256e-01,  1.0059e-01,  5.5312e+00,
        -1.6174e-03,  9.5825e-03,  1.9336e-01, -1.0986e-01, -3.0078e-01,
        -3.6719e-01,  1.3672e-01,  2.9688e-01, -2.9175e-02,  2.7930e-01,
         1.2793e-01, -5.4932e-04,  3.8818e-02,  2.9785e-02,  5.3101e-03,
         5.1270e-02, -7.5684e-02,  3.8867e-01,  3.5400e-02, -3.5938e-01,
         6.7383e-02,  1.0596e-01])

llm.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0142,  0.0159,  0.0388,  ...,  0.0164,  0.0573,  0.0178],
        [-0.0112,  0.0187, -0.0498,  ..., -0.0050,  0.0029, -0.0263],
        [ 0.0376,  0.0255, -0.0091,  ...,  0.0164,  0.0052,  0.0135],
        ...,
        [ 0.0061, -0.0356,  0.0027,  ...,  0.1033, -0.0347, -0.0168],
        [ 0.0868,  0.0212,  0.0439,  ..., -0.0505, -0.0407, -0.0066],
        [-0.0091, -0.0510,  0.0202,  ..., -0.0205, -0.0300, -0.0411]])

llm.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight-torch.Size([512, 64])-torch.float32
tensor([[ 0.0180,  0.0070,  0.0015,  ..., -0.0053,  0.0189,  0.0115],
        [ 0.0062, -0.0118,  0.0265,  ..., -0.0166,  0.0006, -0.0020],
        [ 0.0012,  0.0116,  0.0558,  ...,  0.0079,  0.0011, -0.0105],
        ...,
        [-0.0315, -0.0201, -0.0107,  ...,  0.0975,  0.0463,  0.0703],
        [ 0.0452,  0.0299, -0.0039,  ..., -0.0368, -0.0116, -0.0335],
        [-0.0266,  0.0447,  0.0089,  ..., -0.0021,  0.0111,  0.0207]])

llm.base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-0.0052, -0.0232,  0.0026,  ...,  0.0057, -0.0051, -0.0042],
        [-0.0067,  0.0264,  0.0209,  ...,  0.0153,  0.0106, -0.0165],
        [-0.0002, -0.0003,  0.0215,  ...,  0.0068, -0.0305,  0.0074],
        ...,
        [-0.0128, -0.0101, -0.0039,  ...,  0.0037, -0.0028, -0.0035],
        [-0.0027, -0.0072,  0.0186,  ..., -0.0010,  0.0009,  0.0143],
        [-0.0084, -0.0339,  0.0250,  ..., -0.0303,  0.0256, -0.0063]])

llm.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[-0.0566, -0.0463, -0.0417,  ..., -0.0284,  0.0946, -0.0140],
        [-0.0348,  0.0139,  0.0129,  ...,  0.0624, -0.0238,  0.1506],
        [ 0.0663, -0.0793, -0.0101,  ..., -0.0440, -0.0117,  0.0302],
        ...,
        [ 0.0619, -0.0122, -0.0632,  ...,  0.0728, -0.0548, -0.0158],
        [-0.0200,  0.0503,  0.0466,  ...,  0.0182, -0.0562, -0.0128],
        [ 0.0451, -0.0017, -0.0189,  ...,  0.0248, -0.0708,  0.0522]])

llm.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0120, -0.0158, -0.0159,  ..., -0.0455,  0.0008, -0.0051],
        [-0.0531, -0.0031, -0.0548,  ...,  0.0098, -0.0185, -0.0259],
        [ 0.0216,  0.0218, -0.0175,  ..., -0.0367, -0.0161, -0.0028],
        ...,
        [ 0.0092,  0.0088, -0.0282,  ...,  0.0269,  0.0298, -0.0069],
        [-0.0187, -0.0171,  0.0069,  ..., -0.0434,  0.0240, -0.0125],
        [ 0.0445,  0.0331,  0.0174,  ...,  0.0073, -0.0122,  0.0349]])

llm.base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[-3.6011e-03,  8.6594e-04,  1.8921e-02,  ..., -1.5503e-02,
         -2.1240e-02, -6.3171e-03],
        [-9.5844e-05, -1.3855e-02, -3.3447e-02,  ...,  5.8899e-03,
          6.4087e-03,  2.4658e-02],
        [ 1.2695e-02, -8.4839e-03, -2.8076e-02,  ..., -1.2939e-02,
         -8.6060e-03, -2.7924e-03],
        ...,
        [-4.0283e-03,  3.2349e-03, -4.2236e-02,  ..., -6.7749e-03,
         -1.5198e-02,  1.5869e-02],
        [ 2.9419e-02,  2.7100e-02,  8.0872e-04,  ...,  1.4771e-02,
         -7.1411e-03,  1.9043e-02],
        [ 1.6602e-02,  6.0120e-03,  4.5166e-03,  ...,  1.5564e-02,
         -1.4114e-03, -5.8899e-03]])

llm.base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0616, -0.0561, -0.0155,  ..., -0.0345,  0.0006,  0.0955],
        [ 0.0075,  0.0461,  0.0109,  ..., -0.0291,  0.0828,  0.0448],
        [ 0.0066,  0.0872,  0.0561,  ..., -0.0086,  0.0350, -0.0180],
        ...,
        [-0.0028, -0.0390, -0.0378,  ...,  0.0373, -0.0326,  0.0399],
        [ 0.0240, -0.0536,  0.0466,  ...,  0.0210,  0.0029,  0.0441],
        [ 0.0032,  0.0179, -0.0596,  ...,  0.0360,  0.0238,  0.0445]])

llm.base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0030,  0.0110,  0.0070,  ..., -0.0127,  0.0455,  0.0331],
        [ 0.0130,  0.0180, -0.0221,  ...,  0.0055, -0.0168,  0.0338],
        [ 0.0799, -0.0131,  0.0315,  ..., -0.0358,  0.0450, -0.0471],
        ...,
        [-0.0469,  0.0068,  0.0172,  ..., -0.0064, -0.0095,  0.0006],
        [-0.0042, -0.0540, -0.0334,  ..., -0.0057,  0.0200, -0.0032],
        [-0.0037, -0.0258, -0.0007,  ...,  0.0109,  0.0566, -0.0035]])

llm.base_model.model.model.layers.27.mlp.up_proj.base_layer.weight-torch.Size([18944, 3584])-torch.float32
tensor([[ 0.0219, -0.0145, -0.0128,  ...,  0.0209, -0.0025, -0.0110],
        [ 0.0056,  0.0177, -0.0067,  ..., -0.0182,  0.0074,  0.0227],
        [ 0.0281,  0.0039,  0.0432,  ..., -0.0130,  0.0101, -0.0312],
        ...,
        [ 0.0055, -0.0142,  0.0287,  ...,  0.0352, -0.0154, -0.0131],
        [-0.0201, -0.0110,  0.0021,  ..., -0.0214,  0.0078, -0.0243],
        [-0.0206, -0.0332, -0.0086,  ...,  0.0020,  0.0032,  0.0033]])

llm.base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight-torch.Size([64, 3584])-torch.float32
tensor([[ 0.0368,  0.0048, -0.0376,  ...,  0.0275,  0.0032,  0.0006],
        [-0.0656,  0.0089,  0.0609,  ...,  0.0339,  0.0474,  0.0288],
        [ 0.0306,  0.0285,  0.0002,  ...,  0.0162, -0.0286,  0.0881],
        ...,
        [ 0.0718,  0.0206, -0.0794,  ..., -0.0200, -0.0221,  0.0060],
        [-0.0629, -0.0225,  0.0347,  ..., -0.1487, -0.0130, -0.0393],
        [ 0.0329,  0.0114, -0.0488,  ...,  0.0194,  0.0413, -0.0617]])

llm.base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight-torch.Size([18944, 64])-torch.float32
tensor([[-0.0089, -0.0196, -0.0148,  ..., -0.0171,  0.0072, -0.0524],
        [-0.0074,  0.0049,  0.0118,  ...,  0.0272,  0.0123, -0.0095],
        [ 0.0440,  0.0020, -0.0095,  ...,  0.0137, -0.0237,  0.0358],
        ...,
        [ 0.0075,  0.0157,  0.0530,  ..., -0.0177,  0.0165, -0.0536],
        [ 0.0289, -0.0189,  0.0245,  ..., -0.0091, -0.0424,  0.0020],
        [ 0.0137,  0.0572,  0.0215,  ...,  0.0358,  0.0365, -0.0036]])

llm.base_model.model.model.layers.27.mlp.down_proj.base_layer.weight-torch.Size([3584, 18944])-torch.float32
tensor([[ 3.3112e-03,  1.8311e-04,  1.3184e-02,  ...,  3.2043e-03,
          1.3672e-02,  1.3672e-02],
        [ 3.3112e-03, -5.0964e-03, -1.6357e-02,  ...,  4.5471e-03,
         -1.9897e-02,  5.7068e-03],
        [-1.1108e-02,  1.6968e-02,  8.8501e-03,  ..., -4.7607e-03,
         -8.4229e-03,  7.7515e-03],
        ...,
        [-5.3024e-04,  7.8735e-03, -4.8637e-05,  ..., -2.2827e-02,
          2.9175e-02,  1.4221e-02],
        [-1.5869e-02, -1.3367e-02,  2.1973e-02,  ...,  7.6904e-03,
         -1.3184e-02,  2.5024e-02],
        [ 2.7847e-04, -1.0864e-02, -2.6855e-02,  ...,  1.1658e-02,
          2.1118e-02, -8.6060e-03]])

llm.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight-torch.Size([64, 18944])-torch.float32
tensor([[ 0.0330,  0.0264, -0.0063,  ..., -0.0298, -0.0064,  0.0134],
        [ 0.0723, -0.0102,  0.0525,  ...,  0.0606, -0.0093, -0.0202],
        [-0.0194, -0.0215, -0.0886,  ..., -0.0552, -0.0298,  0.0133],
        ...,
        [-0.0198, -0.0141, -0.0207,  ..., -0.0005, -0.0194, -0.0873],
        [-0.0046, -0.0291, -0.0314,  ..., -0.0173, -0.0733, -0.0134],
        [ 0.0492,  0.0149, -0.0070,  ...,  0.0219,  0.0125,  0.0017]])

llm.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight-torch.Size([3584, 64])-torch.float32
tensor([[ 0.0007, -0.0003,  0.0126,  ...,  0.0270, -0.0182, -0.0073],
        [-0.0112, -0.0023, -0.0121,  ...,  0.0179,  0.0178, -0.0074],
        [ 0.0368,  0.0193,  0.0062,  ..., -0.0237, -0.0132,  0.0229],
        ...,
        [ 0.0077, -0.0038, -0.0164,  ..., -0.0075, -0.0436,  0.0351],
        [-0.0066,  0.0474, -0.0032,  ..., -0.0009, -0.0026,  0.0199],
        [ 0.0062, -0.0154, -0.0188,  ..., -0.0283,  0.0127, -0.0033]])

llm.base_model.model.model.layers.27.input_layernorm.weight-torch.Size([3584])-torch.float32
tensor([0.6641, 0.5625, 0.5859,  ..., 0.6016, 0.6055, 0.5703])

llm.base_model.model.model.layers.27.post_attention_layernorm.weight-torch.Size([3584])-torch.float32
tensor([2.1719, 2.1562, 2.1406,  ..., 2.1719, 2.1250, 2.1562])

llm.base_model.model.model.norm.weight-torch.Size([3584])-torch.float32
tensor([4.2500, 4.3750, 4.1875,  ..., 4.3750, 4.2500, 4.2500])

llm.base_model.model.lm_head.weight-torch.Size([152064, 3584])-torch.float32
tensor([[-0.0017,  0.0040,  0.0086,  ...,  0.0073, -0.0259,  0.0053],
        [ 0.0082,  0.0009,  0.0128,  ...,  0.0088, -0.0142, -0.0156],
        [-0.0019,  0.0069,  0.0045,  ...,  0.0132, -0.0062,  0.0023],
        ...,
        [-0.0039, -0.0008, -0.0047,  ...,  0.0009,  0.0041, -0.0054],
        [-0.0039, -0.0008, -0.0047,  ...,  0.0009,  0.0041, -0.0054],
        [-0.0039, -0.0008, -0.0047,  ...,  0.0009,  0.0041, -0.0054]])

encoder_projector.linear1.weight-torch.Size([3584, 2560])-torch.float32
tensor([[ 0.0210, -0.0168,  0.0068,  ..., -0.0074, -0.0042, -0.0054],
        [ 0.0399,  0.0291, -0.0211,  ...,  0.0194, -0.0006, -0.0315],
        [ 0.0243,  0.0263, -0.0178,  ..., -0.0012, -0.0085,  0.0597],
        ...,
        [-0.0071,  0.0110, -0.0142,  ...,  0.0515, -0.0369, -0.0078],
        [-0.0076, -0.0063, -0.0172,  ..., -0.0044,  0.0039,  0.0373],
        [ 0.0202,  0.0087, -0.0502,  ..., -0.0279,  0.0306,  0.0252]])

encoder_projector.linear1.bias-torch.Size([3584])-torch.float32
tensor([-0.0229, -0.0131,  0.0035,  ..., -0.0208, -0.0361, -0.0174])

encoder_projector.linear2.weight-torch.Size([3584, 3584])-torch.float32
tensor([[-2.1974e-02, -2.7061e-02, -5.1324e-02,  ..., -2.7809e-02,
          3.1815e-02, -3.1440e-02],
        [-2.7168e-02, -1.4593e-02,  1.2588e-02,  ...,  1.6837e-03,
         -2.8486e-05, -1.9284e-02],
        [-1.6621e-02,  1.7509e-02, -9.8796e-03,  ..., -1.0845e-02,
         -3.6803e-02, -1.8330e-02],
        ...,
        [ 3.3374e-02, -8.2270e-03, -1.4185e-03,  ...,  9.2727e-03,
          4.6753e-02, -1.1126e-02],
        [ 1.8605e-02, -6.8721e-03, -3.6814e-02,  ...,  2.7029e-02,
          3.4010e-03, -3.3823e-02],
        [ 8.6218e-03,  1.3890e-02, -1.2088e-02,  ..., -6.1688e-03,
          5.5875e-03,  2.3532e-02]])

encoder_projector.linear2.bias-torch.Size([3584])-torch.float32
tensor([-0.0023,  0.0126, -0.0100,  ..., -0.0084, -0.0140, -0.0049])

